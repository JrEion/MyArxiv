{"2024-11-01T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2403.04182v3","updated":"2024-11-01T17:57:01Z","published":"2024-03-07T03:24:34Z","title":"Regression-aware Inference with LLMs","summary":"  Large language models (LLMs) have shown strong results on a range of\napplications, including regression and scoring tasks. Typically, one obtains\noutputs from an LLM via autoregressive sampling from the model's output\ndistribution. We show that this inference strategy can be sub-optimal for\ncommon regression and scoring evaluation metrics. As a remedy, we build on\nprior work on Minimum Bayes Risk decoding, and propose alternate inference\nstrategies that estimate the Bayes-optimal solution for regression and scoring\nmetrics in closed-form from sampled responses. We show that our proposal\nsignificantly improves over baselines across datasets and models.\n","authors":["Michal Lukasik","Harikrishna Narasimhan","Aditya Krishna Menon","Felix Yu","Sanjiv Kumar"],"pdf_url":"https://arxiv.org/pdf/2403.04182v3.pdf","comment":"EMNLP Findings 2024"},{"id":"http://arxiv.org/abs/2407.12749v2","updated":"2024-11-01T17:31:11Z","published":"2024-07-17T17:11:13Z","title":"HDLCopilot: Natural Language Exploration of Hardware Designs and\n  Libraries","summary":"  Hardware design workflows often involve working with Process Design Kits\n(PDKs) from various fabrication labs, each containing its own set of standard\ncell libraries optimized for metrics such as speed, power, or density. These\nlibraries include multiple views for information on timing and electrical\nproperties of cells, cell layout details, and process design rules. Engineers\ntypically navigate between the design and the target technology to make\ninformed decisions on different design scenarios, such as selecting specific\ngates for area optimization or enhancing critical path speed. Navigating this\ncomplex landscape to retrieve specific information about gates or design rules\nis often time-consuming and error-prone. To address this, we present\nHDLCopilot, a multi-agent collaborative framework powered by large language\nmodels that enables engineers to streamline interactions with hardware design\nand PDKs through natural language queries. HDLCopilot enables engineers to\nquickly access relevant information on gates and design rules, evaluate\ntradeoffs related to area, speed, and power in order to make informed decisions\nefficiently and accurately. The framework achieves an execution accuracy of\n96.33\\% on a diverse set of complex natural language queries. HDLCopilot\npositions itself as a powerful assistant in hardware design workflows,\nenhancing productivity and reducing potential human errors.\n","authors":["Manar Abdelatty","Jacob Rosenstein","Sherief Reda"],"pdf_url":"https://arxiv.org/pdf/2407.12749v2.pdf","comment":"7 pages, 8 figures"},{"id":"http://arxiv.org/abs/2406.01721v3","updated":"2024-11-01T17:12:53Z","published":"2024-06-03T18:27:44Z","title":"DuQuant: Distributing Outliers via Dual Transformation Makes Stronger\n  Quantized LLMs","summary":"  Quantization of large language models (LLMs) faces significant challenges,\nparticularly due to the presence of outlier activations that impede efficient\nlow-bit representation. Traditional approaches predominantly address Normal\nOutliers, which are activations across all tokens with relatively large\nmagnitudes. However, these methods struggle with smoothing Massive Outliers\nthat display significantly larger values, which leads to significant\nperformance degradation in low-bit quantization. In this paper, we introduce\nDuQuant, a novel approach that utilizes rotation and permutation\ntransformations to more effectively mitigate both massive and normal outliers.\nFirst, DuQuant starts by constructing the rotation matrix, using specific\noutlier dimensions as prior knowledge, to redistribute outliers to adjacent\nchannels by block-wise rotation. Second, We further employ a zigzag permutation\nto balance the distribution of outliers across blocks, thereby reducing\nblock-wise variance. A subsequent rotation further smooths the activation\nlandscape, enhancing model performance. DuQuant simplifies the quantization\nprocess and excels in managing outliers, outperforming the state-of-the-art\nbaselines across various sizes and types of LLMs on multiple tasks, even with\n4-bit weight-activation quantization. Our code is available at\nhttps://github.com/Hsu1023/DuQuant.\n","authors":["Haokun Lin","Haobo Xu","Yichen Wu","Jingzhi Cui","Yingtao Zhang","Linzhan Mou","Linqi Song","Zhenan Sun","Ying Wei"],"pdf_url":"https://arxiv.org/pdf/2406.01721v3.pdf","comment":"NeurIPS 2024 Oral, Website at https://duquant.github.io"},{"id":"http://arxiv.org/abs/2410.19499v2","updated":"2024-11-01T16:45:29Z","published":"2024-10-25T11:58:12Z","title":"Introducing MAPO: Momentum-Aided Gradient Descent Prompt Optimization","summary":"  Momentum-Aided Prompt Optimization (MAPO) enhances the efficiency and\nefficacy of prompt optimization for Large Language Models (LLMs). Building on\nProTeGi, MAPO uses positive natural language \"gradients\" and a momentum-based\nextension to refine prompts effectively. By tracking gradient history, MAPO\navoids local minima and oscillations. It also utilizes beam search and an Upper\nConfidence Bound (UCB) algorithm for balanced candidate expansion and\nselection. Benchmark testing shows that MAPO achieves faster convergence time\nwith fewer API calls and higher F1 scores than ProTeGi, proving it as a robust\nand scalable solution for automated prompt engineering in LLMs.\n","authors":["Anthony Cui","Pranav Nandyalam","Ethan Cheung","Kevin Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.19499v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.04559v4","updated":"2024-11-01T16:10:41Z","published":"2024-02-07T03:37:19Z","title":"Can Large Language Model Agents Simulate Human Trust Behavior?","summary":"  Large Language Model (LLM) agents have been increasingly adopted as\nsimulation tools to model humans in social science and role-playing\napplications. However, one fundamental question remains: can LLM agents really\nsimulate human behavior? In this paper, we focus on one critical and elemental\nbehavior in human interactions, trust, and investigate whether LLM agents can\nsimulate human trust behavior. We first find that LLM agents generally exhibit\ntrust behavior, referred to as agent trust, under the framework of Trust Games,\nwhich are widely recognized in behavioral economics. Then, we discover that\nGPT-4 agents manifest high behavioral alignment with humans in terms of trust\nbehavior, indicating the feasibility of simulating human trust behavior with\nLLM agents. In addition, we probe the biases of agent trust and differences in\nagent trust towards other LLM agents and humans. We also explore the intrinsic\nproperties of agent trust under conditions including external manipulations and\nadvanced reasoning strategies. Our study provides new insights into the\nbehaviors of LLM agents and the fundamental analogy between LLMs and humans\nbeyond value alignment. We further illustrate broader implications of our\ndiscoveries for applications where trust is paramount.\n","authors":["Chengxing Xie","Canyu Chen","Feiran Jia","Ziyu Ye","Shiyang Lai","Kai Shu","Jindong Gu","Adel Bibi","Ziniu Hu","David Jurgens","James Evans","Philip Torr","Bernard Ghanem","Guohao Li"],"pdf_url":"https://arxiv.org/pdf/2402.04559v4.pdf","comment":"Accepted to Proceedings of NeurIPS 2024. The first two authors\n  contributed equally. 10 pages for main paper, 56 pages including appendix.\n  Project website: https://agent-trust.camel-ai.org"},{"id":"http://arxiv.org/abs/2410.24198v2","updated":"2024-11-01T16:06:10Z","published":"2024-10-31T17:55:13Z","title":"SelfCodeAlign: Self-Alignment for Code Generation","summary":"  Instruction tuning is a supervised fine-tuning approach that significantly\nimproves the ability of large language models (LLMs) to follow human\ninstructions. We propose SelfCodeAlign, the first fully transparent and\npermissive pipeline for self-aligning code LLMs without extensive human\nannotations or distillation. SelfCodeAlign employs the same base model for\ninference throughout the data generation process. It first extracts diverse\ncoding concepts from high-quality seed snippets to generate new tasks. It then\nsamples multiple responses per task, pairs each with test cases, and validates\nthem in a sandbox environment. Finally, passing examples are selected for\ninstruction tuning. In our primary experiments, we use SelfCodeAlign with\nCodeQwen1.5-7B to generate a dataset of 74k instruction-response pairs.\nFinetuning on this dataset leads to a model that achieves a 67.1 pass@1 on\nHumanEval+, surpassing CodeLlama-70B-Instruct despite being ten times smaller.\nAcross all benchmarks, this finetuned model consistently outperforms the\noriginal version trained with OctoPack, the previous state-of-the-art method\nfor instruction tuning without human annotations or distillation. Additionally,\nwe show that SelfCodeAlign is effective across LLMs of various sizes, from 3B\nto 33B, and that the base models can benefit more from alignment with their own\ndata distribution. We further validate each component's effectiveness in our\npipeline, showing that SelfCodeAlign outperforms both direct distillation from\nGPT-4o and leading GPT-3.5-based distillation methods, such as OSS-Instruct and\nEvol-Instruct. SelfCodeAlign has also led to the creation of\nStarCoder2-Instruct, the first fully transparent, permissively licensed, and\nself-aligned code LLM that achieves state-of-the-art coding performance.\n","authors":["Yuxiang Wei","Federico Cassano","Jiawei Liu","Yifeng Ding","Naman Jain","Zachary Mueller","Harm de Vries","Leandro von Werra","Arjun Guha","Lingming Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.24198v2.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.08964v2","updated":"2024-11-01T15:53:08Z","published":"2024-10-11T16:32:05Z","title":"Language Imbalance Driven Rewarding for Multilingual Self-improving","summary":"  Large Language Models (LLMs) have achieved state-of-the-art performance\nacross numerous tasks. However, these advancements have predominantly benefited\n\"first-class\" languages such as English and Chinese, leaving many other\nlanguages underrepresented. This imbalance, while limiting broader\napplications, generates a natural preference ranking between languages,\noffering an opportunity to bootstrap the multilingual capabilities of LLM in a\nself-improving manner. Thus, we propose $\\textit{Language Imbalance Driven\nRewarding}$, where the inherent imbalance between dominant and non-dominant\nlanguages within LLMs is leveraged as a reward signal. Iterative DPO training\ndemonstrates that this approach not only enhances LLM performance in\nnon-dominant languages but also improves the dominant language's capacity,\nthereby yielding an iterative reward signal. Fine-tuning\nMeta-Llama-3-8B-Instruct over two iterations of this approach results in\ncontinuous improvements in multilingual performance across\ninstruction-following and arithmetic reasoning tasks, evidenced by an average\nimprovement of 7.46% win rate on the X-AlpacaEval leaderboard and 13.9%\naccuracy on the MGSM benchmark. This work serves as an initial exploration,\npaving the way for multilingual self-improvement of LLMs.\n","authors":["Wen Yang","Junhong Wu","Chen Wang","Chengqing Zong","Jiajun Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.08964v2.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2410.18050v2","updated":"2024-11-01T15:36:59Z","published":"2024-10-23T17:24:58Z","title":"LongRAG: A Dual-Perspective Retrieval-Augmented Generation Paradigm for\n  Long-Context Question Answering","summary":"  Long-Context Question Answering (LCQA), a challenging task, aims to reason\nover long-context documents to yield accurate answers to questions. Existing\nlong-context Large Language Models (LLMs) for LCQA often struggle with the\n\"lost in the middle\" issue. Retrieval-Augmented Generation (RAG) mitigates this\nissue by providing external factual evidence. However, its chunking strategy\ndisrupts the global long-context information, and its low-quality retrieval in\nlong contexts hinders LLMs from identifying effective factual details due to\nsubstantial noise. To this end, we propose LongRAG, a general,\ndual-perspective, and robust LLM-based RAG system paradigm for LCQA to enhance\nRAG's understanding of complex long-context knowledge (i.e., global information\nand factual details). We design LongRAG as a plug-and-play paradigm,\nfacilitating adaptation to various domains and LLMs. Extensive experiments on\nthree multi-hop datasets demonstrate that LongRAG significantly outperforms\nlong-context LLMs (up by 6.94%), advanced RAG (up by 6.16%), and Vanilla RAG\n(up by 17.25%). Furthermore, we conduct quantitative ablation studies and\nmulti-dimensional analyses, highlighting the effectiveness of the system's\ncomponents and fine-tuning strategies. Data and code are available at\nhttps://github.com/QingFei1/LongRAG.\n","authors":["Qingfei Zhao","Ruobing Wang","Yukuo Cen","Daren Zha","Shicheng Tan","Yuxiao Dong","Jie Tang"],"pdf_url":"https://arxiv.org/pdf/2410.18050v2.pdf","comment":"EMNLP 2024 Main, Final"},{"id":"http://arxiv.org/abs/2404.15420v3","updated":"2024-11-01T14:56:52Z","published":"2024-04-23T18:10:42Z","title":"XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference","summary":"  In-context learning (ICL) approaches typically leverage prompting to\ncondition decoder-only language model generation on reference information.\nJust-in-time processing of a context is inefficient due to the quadratic cost\nof self-attention operations, and caching is desirable. However, caching\ntransformer states can easily require almost as much space as the model\nparameters. When the right context isn't known in advance, caching ICL can be\nchallenging. This work addresses these limitations by introducing models that,\ninspired by the encoder-decoder architecture, use cross-attention to condition\ngeneration on reference text without the prompt. More precisely, we leverage\npre-trained decoder-only models and only train a small number of added layers.\nWe use Question-Answering (QA) as a testbed to evaluate the ability of our\nmodels to perform conditional generation and observe that they outperform ICL,\nare comparable to fine-tuned prompted LLMs, and drastically reduce the space\nfootprint relative to standard KV caching by two orders of magnitude.\n","authors":["João Monteiro","Étienne Marcotte","Pierre-André Noël","Valentina Zantedeschi","David Vázquez","Nicolas Chapados","Christopher Pal","Perouz Taslakian"],"pdf_url":"https://arxiv.org/pdf/2404.15420v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19381v3","updated":"2024-11-01T14:51:38Z","published":"2024-09-28T15:12:55Z","title":"INC-Math: Integrating Natural Language and Code for Enhanced\n  Mathematical Reasoning in Large Language Models","summary":"  Large Language Models (LLMs) are commonly used to generate solutions for\nmathematical reasoning problems in the following formats: natural language,\ncode, or a combination of both. In this paper, we explore fundamental questions\nrelated to solving mathematical reasoning problems using natural language and\ncode with state-of-the-art LLMs, including GPT-4o-mini and LLama-3.1-8b-Turbo.\nOur findings show that LLMs are better at reasoning in natural language\ncompared to code. Additionally, although natural language and code serve as\ncomplementary forms of reasoning, they can affect each other in a negative way\nin certain scenarios. These insights motivate our development of a new\nprompting method, INC-Math, which leverages an LLM to dynamically select the\nmost appropriate reasoning form, resulting in improved performance over\ncomparable baselines with GPT-4o-mini.\n","authors":["Xuyuan Xiong","Simeng Han","Ziyue Zhou","Arman Cohan"],"pdf_url":"https://arxiv.org/pdf/2409.19381v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14940v3","updated":"2024-11-01T14:49:44Z","published":"2024-10-19T02:07:33Z","title":"Nova: A Practical and Advanced Alignment","summary":"  We introduce Nova, a suite of practical alignment techniques employed in a\nseries of empirically validated high-performing models. This represents the\nfirst comprehensive account of alignment methodologies, offering valuable\ninsights for advancing AI research. We investigate the critical components that\nenhance model performance during the alignment process, including optimization\nmethods, data strategies, capability enhancements, and evaluation processes.\nThe process spans three key stages: Prompt Augmentation System(PAS), Supervised\nFine-Tuning(SFT), and Preference Alignment. The problems encountered, the\nsolutions applied, and the improvements made are thoroughly recorded.\n  Through comparisons across well-established benchmarks, we highlight the\ntechnological advancements enabled by Nova Alignment. Importantly,\nQwen2-Nova-72B and Llama3-PBM-Nova-70B are instruct versions of the Qwen2-72B\nand Llama-3-70B base models, optimized through Nova. The Nova models show\nsignificant core improvements, with user experience gains of 17% to 28%, and\nexcels on specialized benchmarks. In open-source benchmark evaluations, both\nQwen2-Nova-72B and Llama3-PBM-Nova-70B consistently outperform their respective\nofficial instruct versions across nearly all datasets. This report aims to\nclarify the key technologies behind the alignment process, fostering a deeper\nunderstanding within the community. Llama3-PBM-Nova-70B model is available at\nhttps://huggingface.co/PKU-Baichuan-MLSystemLab/Llama3-PBM-Nova-70B.\n","authors":["Mingan Lin","Fan Yang","Yanjun Shen","Haoze Sun","Tianpeng Li","Tao Zhang","Chenzheng Zhu","Tao Zhang","Miao Zheng","Xu Li","Yijie Zhou","Mingyang Chen","Yanzhao Qin","Youquan Li","Hao Liang","Fei Li","Yadong Li","Mang Wang","Guosheng Dong","Kun Fang","Jianhua Xu","Bin Cui","Wentao Zhang","Zenan Zhou","Weipeng Chen"],"pdf_url":"https://arxiv.org/pdf/2410.14940v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.18760v4","updated":"2024-11-01T14:37:37Z","published":"2023-11-30T18:02:44Z","title":"TaskBench: Benchmarking Large Language Models for Task Automation","summary":"  In recent years, the remarkable progress of large language models (LLMs) has\nsparked interest in task automation, which involves decomposing complex tasks\ndescribed by user instructions into sub-tasks and invoking external tools to\nexecute them, playing a central role in autonomous agents. However, there is a\nlack of systematic and standardized benchmarks to promote the development of\nLLMs in task automation. To address this, we introduce TaskBench, a\ncomprehensive framework to evaluate the capability of LLMs in task automation.\nSpecifically, task automation can be divided into three critical stages: task\ndecomposition, tool selection, and parameter prediction. To tackle the\ncomplexities inherent in these stages, we introduce the concept of Tool Graph\nto represent decomposed tasks and adopt a back-instruct method to generate\nhigh-quality user instructions. We propose TaskEval, a multi-faceted evaluation\nmethodology that assesses LLM performance across these three stages. Our\napproach combines automated construction with rigorous human verification,\nensuring high consistency with human evaluation. Experimental results\ndemonstrate that TaskBench effectively reflects the capabilities of various\nLLMs in task automation. It provides insights into model performance across\ndifferent task complexities and domains, pushing the boundaries of what current\nmodels can achieve. TaskBench offers a scalable, adaptable, and reliable\nbenchmark for advancing LLM-based autonomous agents.\n","authors":["Yongliang Shen","Kaitao Song","Xu Tan","Wenqi Zhang","Kan Ren","Siyu Yuan","Weiming Lu","Dongsheng Li","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2311.18760v4.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.23528v2","updated":"2024-11-01T14:27:42Z","published":"2024-10-31T00:29:52Z","title":"Large Language Models for Patient Comments Multi-Label Classification","summary":"  Patient experience and care quality are crucial for a hospital's\nsustainability and reputation. The analysis of patient feedback offers valuable\ninsight into patient satisfaction and outcomes. However, the unstructured\nnature of these comments poses challenges for traditional machine learning\nmethods following a supervised learning paradigm. This is due to the\nunavailability of labeled data and the nuances these texts encompass. This\nresearch explores leveraging Large Language Models (LLMs) in conducting\nMulti-label Text Classification (MLTC) of inpatient comments shared after a\nstay in the hospital. GPT-4 Turbo was leveraged to conduct the classification.\nHowever, given the sensitive nature of patients' comments, a security layer is\nintroduced before feeding the data to the LLM through a Protected Health\nInformation (PHI) detection framework, which ensures patients'\nde-identification. Additionally, using the prompt engineering framework,\nzero-shot learning, in-context learning, and chain-of-thought prompting were\nexperimented with. Results demonstrate that GPT-4 Turbo, whether following a\nzero-shot or few-shot setting, outperforms traditional methods and Pre-trained\nLanguage Models (PLMs) and achieves the highest overall performance with an\nF1-score of 76.12% and a weighted F1-score of 73.61% followed closely by the\nfew-shot learning results. Subsequently, the results' association with other\npatient experience structured variables (e.g., rating) was conducted. The study\nenhances MLTC through the application of LLMs, offering healthcare\npractitioners an efficient method to gain deeper insights into patient feedback\nand deliver prompt, appropriate responses.\n","authors":["Hajar Sakai","Sarah S. Lam","Mohammadsadegh Mikaeili","Joshua Bosire","Franziska Jovin"],"pdf_url":"https://arxiv.org/pdf/2410.23528v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10691v2","updated":"2024-11-01T14:08:31Z","published":"2024-07-15T13:04:09Z","title":"$\\texttt{MixGR}$: Enhancing Retriever Generalization for Scientific\n  Domain through Complementary Granularity","summary":"  Recent studies show the growing significance of document retrieval in the\ngeneration of LLMs, i.e., RAG, within the scientific domain by bridging their\nknowledge gap. However, dense retrievers often struggle with domain-specific\nretrieval and complex query-document relationships, particularly when query\nsegments correspond to various parts of a document. To alleviate such prevalent\nchallenges, this paper introduces $\\texttt{MixGR}$, which improves dense\nretrievers' awareness of query-document matching across various levels of\ngranularity in queries and documents using a zero-shot approach.\n$\\texttt{MixGR}$ fuses various metrics based on these granularities to a united\nscore that reflects a comprehensive query-document similarity. Our experiments\ndemonstrate that $\\texttt{MixGR}$ outperforms previous document retrieval by\n24.7%, 9.8%, and 6.9% on nDCG@5 with unsupervised, supervised, and LLM-based\nretrievers, respectively, averaged on queries containing multiple subqueries\nfrom five scientific retrieval datasets. Moreover, the efficacy of two\ndownstream scientific question-answering tasks highlights the advantage of\n$\\texttt{MixGR}$ to boost the application of LLMs in the scientific domain. The\ncode and experimental datasets are available.\n","authors":["Fengyu Cai","Xinran Zhao","Tong Chen","Sihao Chen","Hongming Zhang","Iryna Gurevych","Heinz Koeppl"],"pdf_url":"https://arxiv.org/pdf/2407.10691v2.pdf","comment":"EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2406.00976v2","updated":"2024-11-01T13:54:48Z","published":"2024-06-03T04:16:30Z","title":"Generative Pre-trained Speech Language Model with Efficient Hierarchical\n  Transformer","summary":"  While recent advancements in speech language models have achieved significant\nprogress, they face remarkable challenges in modeling the long acoustic\nsequences of neural audio codecs. In this paper, we introduce\n\\textbf{G}enerative \\textbf{P}re-trained \\textbf{S}peech \\textbf{T}ransformer\n(GPST), a hierarchical transformer designed for efficient speech language\nmodeling. GPST quantizes audio waveforms into two distinct types of discrete\nspeech representations and integrates them within a hierarchical transformer\narchitecture, allowing for a unified one-stage generation process and enhancing\nHi-Res audio generation capabilities. By training on large corpora of speeches\nin an end-to-end unsupervised manner, GPST can generate syntactically\nconsistent speech with diverse speaker identities. Given a brief 3-second\nprompt, GPST can produce natural and coherent personalized speech,\ndemonstrating in-context learning abilities. Moreover, our approach can be\neasily extended to spoken cross-lingual speech generation by incorporating\nmulti-lingual semantic tokens and universal acoustic tokens. Experimental\nresults indicate that GPST significantly outperforms the existing speech\nlanguage models in terms of word error rate, speech quality, and speaker\nsimilarity. The code is available at \\url{https://github.com/youngsheen/GPST}.\n","authors":["Yongxin Zhu","Dan Su","Liqiang He","Linli Xu","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2406.00976v2.pdf","comment":"Accept in ACL2024-main"},{"id":"http://arxiv.org/abs/2405.13845v3","updated":"2024-11-01T13:25:52Z","published":"2024-05-22T17:13:49Z","title":"Semantic Density: Uncertainty Quantification for Large Language Models\n  through Confidence Measurement in Semantic Space","summary":"  With the widespread application of Large Language Models (LLMs) to various\ndomains, concerns regarding the trustworthiness of LLMs in safety-critical\nscenarios have been raised, due to their unpredictable tendency to hallucinate\nand generate misinformation. Existing LLMs do not have an inherent\nfunctionality to provide the users with an uncertainty/confidence metric for\neach response it generates, making it difficult to evaluate trustworthiness.\nAlthough several studies aim to develop uncertainty quantification methods for\nLLMs, they have fundamental limitations, such as being restricted to\nclassification tasks, requiring additional training and data, considering only\nlexical instead of semantic information, and being prompt-wise but not\nresponse-wise. A new framework is proposed in this paper to address these\nissues. Semantic density extracts uncertainty/confidence information for each\nresponse from a probability distribution perspective in semantic space. It has\nno restriction on task types and is \"off-the-shelf\" for new models and tasks.\nExperiments on seven state-of-the-art LLMs, including the latest Llama 3 and\nMixtral-8x22B models, on four free-form question-answering benchmarks\ndemonstrate the superior performance and robustness of semantic density\ncompared to prior approaches.\n","authors":["Xin Qiu","Risto Miikkulainen"],"pdf_url":"https://arxiv.org/pdf/2405.13845v3.pdf","comment":"Accepted to Neurips 2024"},{"id":"http://arxiv.org/abs/2312.05061v4","updated":"2024-11-01T12:40:45Z","published":"2023-12-08T14:30:08Z","title":"LaCour!: Enabling Research on Argumentation in Hearings of the European\n  Court of Human Rights","summary":"  Why does an argument end up in the final court decision? Was it deliberated\nor questioned during the oral hearings? Was there something in the hearings\nthat triggered a particular judge to write a dissenting opinion? Despite the\navailability of the final judgments of the European Court of Human Rights\n(ECHR), none of these legal research questions can currently be answered as the\nECHR's multilingual oral hearings are not transcribed, structured, or\nspeaker-attributed. We address this fundamental gap by presenting LaCour!, the\nfirst corpus of textual oral arguments of the ECHR, consisting of 154 full\nhearings (2.1 million tokens from over 267 hours of video footage) in English,\nFrench, and other court languages, each linked to the corresponding final\njudgment documents. In addition to the transcribed and partially manually\ncorrected text from the video, we provide sentence-level timestamps and\nmanually annotated role and language labels. We also showcase LaCour! in a set\nof preliminary experiments that explore the interplay between questions and\ndissenting opinions. Apart from the use cases in legal NLP, we hope that law\nstudents or other interested parties will also use LaCour! as a learning\nresource, as it is freely available in various formats at\nhttps://huggingface.co/datasets/TrustHLT/LaCour.\n","authors":["Lena Held","Ivan Habernal"],"pdf_url":"https://arxiv.org/pdf/2312.05061v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22932v2","updated":"2024-11-01T12:37:10Z","published":"2024-10-30T11:38:13Z","title":"Multi-Agent Large Language Models for Conversational Task-Solving","summary":"  In an era where single large language models have dominated the landscape of\nartificial intelligence for years, multi-agent systems arise as new\nprotagonists in conversational task-solving. While previous studies have\nshowcased their potential in reasoning tasks and creative endeavors, an\nanalysis of their limitations concerning the conversational paradigms and the\nimpact of individual agents is missing. It remains unascertained how\nmulti-agent discussions perform across tasks of varying complexity and how the\nstructure of these conversations influences the process. To fill that gap, this\nwork systematically evaluates multi-agent systems across various discussion\nparadigms, assessing their strengths and weaknesses in both generative tasks\nand question-answering tasks. Alongside the experiments, I propose a taxonomy\nof 20 multi-agent research studies from 2022 to 2024, followed by the\nintroduction of a framework for deploying multi-agent LLMs in conversational\ntask-solving. I demonstrate that while multi-agent systems excel in complex\nreasoning tasks, outperforming a single model by leveraging expert personas,\nthey fail on basic tasks. Concretely, I identify three challenges that arise:\n1) While longer discussions enhance reasoning, agents fail to maintain\nconformity to strict task requirements, which leads to problem drift, making\nshorter conversations more effective for basic tasks. 2) Prolonged discussions\nrisk alignment collapse, raising new safety concerns for these systems. 3) I\nshowcase discussion monopolization through long generations, posing the problem\nof fairness in decision-making for tasks like summarization. This work uncovers\nboth the potential and challenges that arise with multi-agent interaction and\nvarying conversational paradigms, providing insights into how future research\ncould improve the efficiency, performance, and safety of multi-agent LLMs.\n","authors":["Jonas Becker"],"pdf_url":"https://arxiv.org/pdf/2410.22932v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.10188v5","updated":"2024-11-01T10:57:37Z","published":"2024-08-19T17:48:08Z","title":"LongVILA: Scaling Long-Context Visual Language Models for Long Videos","summary":"  Long-context capability is critical for multi-modal foundation models,\nespecially for long video understanding. We introduce LongVILA, a full-stack\nsolution for long-context visual-language models by co-designing the algorithm\nand system. For model training, we upgrade existing VLMs to support long video\nunderstanding by incorporating two additional stages, i.e., long context\nextension and long video supervised fine-tuning. However, training on long\nvideo is computationally and memory intensive. We introduce the long-context\nMulti-Modal Sequence Parallelism (MM-SP) system that efficiently parallelizes\nlong video training and inference, enabling 2M context length training on 256\nGPUs without any gradient checkpointing. LongVILA efficiently extends the\nnumber of video frames of VILA from 8 to 2048, improving the long video\ncaptioning score from 2.00 to 3.26 (out of 5), achieving 99.8% accuracy in\n6,000-frame (more than 1 million tokens) video needle-in-a-haystack.\nLongVILA-7B demonstrates strong accuracy on the VideoMME benchmark, i.e., 61.8%\nwith subtitle. Besides, MM-SP is 2.1x - 5.7x faster than ring style sequence\nparallelism and 1.1x - 1.4x faster than Megatron with a hybrid context and\ntensor parallelism. Moreover, it seamlessly integrates with Hugging Face\nTransformers.\n","authors":["Fuzhao Xue","Yukang Chen","Dacheng Li","Qinghao Hu","Ligeng Zhu","Xiuyu Li","Yunhao Fang","Haotian Tang","Shang Yang","Zhijian Liu","Ethan He","Hongxu Yin","Pavlo Molchanov","Jan Kautz","Linxi Fan","Yuke Zhu","Yao Lu","Song Han"],"pdf_url":"https://arxiv.org/pdf/2408.10188v5.pdf","comment":"Code and models are available at\n  https://github.com/NVlabs/VILA/blob/main/LongVILA.md"},{"id":"http://arxiv.org/abs/2311.08385v4","updated":"2024-11-01T10:28:12Z","published":"2023-11-14T18:48:27Z","title":"Aligning Large Language Models with Human Opinions through Persona\n  Selection and Value--Belief--Norm Reasoning","summary":"  Reasoning and predicting human opinions with large language models (LLMs) is\nessential yet challenging. Current methods employ role-playing with personae\nbut face two major issues: LLMs are sensitive to even a single irrelevant\npersona, skewing predictions by up to 30%, and LLMs fail to reason\nstrategically over personae. We propose Chain-of-Opinion (COO), a simple\nfour-step solution modeling which and how to reason with personae, inspired by\nthe Value--Belief--Norm (VBN) theory. COO differentiates between explicit\npersonae (demographics and ideology) and implicit personae (historical\nopinions), involves: (1) filtering irrelevant attributes from explicit\npersonae, (2) ranking implicit personae into a preferential list for selecting\ntop-k, (3) applying novel VBN reasoning to extract user environmental and\npersonal value, belief, and norm variables for accurate and reliable\npredictions, and (4) iterating VBN reasoning with progressively larger lists of\nimplicit personae to handle potential persona insufficiency. COO efficiently\nachieves new state-of-the-art opinion prediction via prompting with only 5\ninference calls, improving prior techniques by up to 4%. Notably, fine-tuning\nLMs with COO data results in significantly better opinion-aligned models, by up\nto 23%.\n","authors":["Do Xuan Long","Kenji Kawaguchi","Min-Yen Kan","Nancy F. Chen"],"pdf_url":"https://arxiv.org/pdf/2311.08385v4.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2405.11459v3","updated":"2024-11-01T09:55:48Z","published":"2024-05-19T06:00:36Z","title":"Du-IN: Discrete units-guided mask modeling for decoding speech from\n  Intracranial Neural signals","summary":"  Invasive brain-computer interfaces with Electrocorticography (ECoG) have\nshown promise for high-performance speech decoding in medical applications, but\nless damaging methods like intracranial stereo-electroencephalography (sEEG)\nremain underexplored. With rapid advances in representation learning,\nleveraging abundant recordings to enhance speech decoding is increasingly\nattractive. However, popular methods often pre-train temporal models based on\nbrain-level tokens, overlooking that brain activities in different regions are\nhighly desynchronized during tasks. Alternatively, they pre-train\nspatial-temporal models based on channel-level tokens but fail to evaluate them\non challenging tasks like speech decoding, which requires intricate processing\nin specific language-related areas. To address this issue, we collected a\nwell-annotated Chinese word-reading sEEG dataset targeting language-related\nbrain networks from 12 subjects. Using this benchmark, we developed the Du-IN\nmodel, which extracts contextual embeddings based on region-level tokens\nthrough discrete codex-guided mask modeling. Our model achieves\nstate-of-the-art performance on the 61-word classification task, surpassing all\nbaselines. Model comparisons and ablation studies reveal that our design\nchoices, including (i) temporal modeling based on region-level tokens by\nutilizing 1D depthwise convolution to fuse channels in the ventral sensorimotor\ncortex (vSMC) and superior temporal gyrus (STG) and (ii) self-supervision\nthrough discrete codex-guided mask modeling, significantly contribute to this\nperformance. Overall, our approach -- inspired by neuroscience findings and\ncapitalizing on region-level representations from specific brain regions -- is\nsuitable for invasive brain modeling and represents a promising neuro-inspired\nAI approach in brain-computer interfaces.\n","authors":["Hui Zheng","Hai-Teng Wang","Wei-Bang Jiang","Zhong-Tao Chen","Li He","Pei-Yang Lin","Peng-Hu Wei","Guo-Guang Zhao","Yun-Zhe Liu"],"pdf_url":"https://arxiv.org/pdf/2405.11459v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14716v3","updated":"2024-11-01T09:38:59Z","published":"2024-10-11T13:17:19Z","title":"A Systematic Survey on Large Language Models for Algorithm Design","summary":"  Algorithm Design (AD) is crucial for effective problem-solving across various\ndomains. The advent of Large Language Models (LLMs) has notably enhanced the\nautomation and innovation within this field, offering new perspectives and\npromising solutions. Over the past three years, the integration of LLMs into AD\n(LLM4AD) has seen substantial progress, with applications spanning\noptimization, machine learning, mathematical reasoning, and scientific\ndiscovery. Given the rapid advancements and expanding scope of this field, a\nsystematic review is both timely and necessary. This paper provides a\nsystematic review of LLM4AD. First, we offer an overview and summary of\nexisting studies. Then, we introduce a taxonomy and review the literature\nacross four dimensions: the roles of LLMs, search methods, prompt methods, and\napplication domains with a discussion of potential and achievements of LLMs in\nAD. Finally, we identify current challenges and highlight several promising\ndirections for future research.\n","authors":["Fei Liu","Yiming Yao","Ping Guo","Zhiyuan Yang","Zhe Zhao","Xi Lin","Xialiang Tong","Mingxuan Yuan","Zhichao Lu","Zhenkun Wang","Qingfu Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.14716v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.05019v2","updated":"2024-11-01T08:55:43Z","published":"2024-04-07T17:17:23Z","title":"Shortcut-connected Expert Parallelism for Accelerating\n  Mixture-of-Experts","summary":"  Expert parallelism has been introduced as a strategy to distribute the\ncomputational workload of sparsely-gated mixture-of-experts (MoE) models across\nmultiple computing devices, facilitating the execution of these increasingly\nlarge-scale models. However, the All-to-All communication intrinsic to expert\nparallelism constitutes a significant overhead, diminishing the MoE models'\nefficiency. Current optimization approaches offer some relief, yet they are\nconstrained by the sequential interdependence of communication and computation\noperations. To address this limitation, we present a novel shortcut-connected\nMoE (ScMoE) architecture with an overlapping parallel strategy, which\neffectively decouples communication from its conventional sequence, allowing\nfor a substantial overlap of 70% to 100% with computation. When compared with\nthe prevalent top-2 MoE architecture, ScMoE demonstrates training speed\nimprovements of 30% and 11%, and inference improvements of 40% and 15%, in our\ndistributed environments with PCIe and NVLink hardware, respectively, where\ncommunication constitutes 60% and 15% of the total MoE time consumption.\nBuilding on the ScMoE architecture, we further implement an expert offloading\nstrategy to facilitate memory-limited inference, optimizing latency through the\noverlap of expert migration. Additionally, extensive experiments and\ntheoretical analyses indicate that ScMoE not only achieves comparable but in\nsome instances surpasses the model quality of existing approaches.\n","authors":["Weilin Cai","Juyong Jiang","Le Qin","Junwei Cui","Sunghun Kim","Jiayi Huang"],"pdf_url":"https://arxiv.org/pdf/2404.05019v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.02657v2","updated":"2024-11-01T08:52:18Z","published":"2024-06-04T17:45:26Z","title":"Block Transformer: Global-to-Local Language Modeling for Fast Inference","summary":"  We introduce the Block Transformer which adopts hierarchical global-to-local\nmodeling to autoregressive transformers to mitigate the inference bottlenecks\nassociated with self-attention. Self-attention requires the key-value (KV)\ncache of all previous sequences to be retrieved from memory at every decoding\nstep to retrieve context information, leading to two primary bottlenecks during\nbatch inference. First, there is a significant delay in obtaining the first\ntoken, as the information of the entire prompt must first be processed to\nprefill the KV cache. Second, computation of subsequent tokens is bottlenecked\nby the high memory I/O demand of fetching the entire KV cache, which grows\nlinearly with sequence length, incurring quadratic memory reads overall. We\ndesign the Block Transformer to strategically mitigate these costs, by\nincorporating coarsity and locality into an integrated global-to-local\narchitecture. At the lower layers, we aggregate tokens into fixed size blocks\nto apply attention across the entire sequence at coarse-grained detail, to\ncapture the global context while minimizing KV cache overhead. At upper layers,\nwe apply attention within each block to decode individual tokens, to model\nfine-grained details with a lightweight local KV cache. We pretrain vanilla and\nBlock Transformers from scratch and demonstrate that Block Transformers reach\n10--20x inference throughput compared to vanilla transformers with equivalent\nperplexity and zero-shot task performance. Code is available at\nhttps://github.com/itsnamgyu/block-transformer.\n","authors":["Namgyu Ho","Sangmin Bae","Taehyeon Kim","Hyunjik Jo","Yireun Kim","Tal Schuster","Adam Fisch","James Thorne","Se-Young Yun"],"pdf_url":"https://arxiv.org/pdf/2406.02657v2.pdf","comment":"37 pages, 24 figures, 7 tables"},{"id":"http://arxiv.org/abs/2409.20012v2","updated":"2024-11-01T08:40:28Z","published":"2024-09-30T07:14:31Z","title":"Towards Robust Multimodal Sentiment Analysis with Incomplete Data","summary":"  The field of Multimodal Sentiment Analysis (MSA) has recently witnessed an\nemerging direction seeking to tackle the issue of data incompleteness.\nRecognizing that the language modality typically contains dense sentiment\ninformation, we consider it as the dominant modality and present an innovative\nLanguage-dominated Noise-resistant Learning Network (LNLN) to achieve robust\nMSA. The proposed LNLN features a dominant modality correction (DMC) module and\ndominant modality based multimodal learning (DMML) module, which enhances the\nmodel's robustness across various noise scenarios by ensuring the quality of\ndominant modality representations. Aside from the methodical design, we perform\ncomprehensive experiments under random data missing scenarios, utilizing\ndiverse and meaningful settings on several popular datasets (\\textit{e.g.,}\nMOSI, MOSEI, and SIMS), providing additional uniformity, transparency, and\nfairness compared to existing evaluations in the literature. Empirically, LNLN\nconsistently outperforms existing baselines, demonstrating superior performance\nacross these challenging and extensive evaluation metrics.\n","authors":["Haoyu Zhang","Wenbin Wang","Tianshu Yu"],"pdf_url":"https://arxiv.org/pdf/2409.20012v2.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2309.08648v4","updated":"2024-11-01T08:06:25Z","published":"2023-09-15T13:15:54Z","title":"MAPLE: Mobile App Prediction Leveraging Large Language Model Embeddings","summary":"  In recent years, predicting mobile app usage has become increasingly\nimportant for areas like app recommendation, user behaviour analysis, and\nmobile resource management. Existing models, however, struggle with the\nheterogeneous nature of contextual data and the user cold start problem. This\nstudy introduces a novel prediction model, Mobile App Prediction Leveraging\nLarge Language Model Embeddings (MAPLE), which employs Large Language Models\n(LLMs) and installed app similarity to overcome these challenges. MAPLE\nutilises the power of LLMs to process contextual data and discern intricate\nrelationships within it effectively. Additionally, we explore the use of\ninstalled app similarity to address the cold start problem, facilitating the\nmodelling of user preferences and habits, even for new users with limited\nhistorical data. In essence, our research presents MAPLE as a novel, potent,\nand practical approach to app usage prediction, making significant strides in\nresolving issues faced by existing models. MAPLE stands out as a comprehensive\nand effective solution, setting a new benchmark for more precise and\npersonalised app usage predictions. In tests on two real-world datasets, MAPLE\nsurpasses contemporary models in both standard and cold start scenarios. These\noutcomes validate MAPLE's capacity for precise app usage predictions and its\nresilience against the cold start problem. This enhanced performance stems from\nthe model's proficiency in capturing complex temporal patterns and leveraging\ncontextual information. As a result, MAPLE can potentially improve personalised\nmobile app usage predictions and user experiences markedly.\n","authors":["Yonchanok Khaokaew","Hao Xue","Flora D. Salim"],"pdf_url":"https://arxiv.org/pdf/2309.08648v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.06190v3","updated":"2024-11-01T07:53:10Z","published":"2023-06-09T18:42:19Z","title":"$FastDoc$: Domain-Specific Fast Continual Pre-training Technique using\n  Document-Level Metadata and Taxonomy","summary":"  In this paper, we propose $FastDoc$ (Fast Continual Pre-training Technique\nusing Document Level Metadata and Taxonomy), a novel, compute-efficient\nframework that utilizes Document metadata and Domain-Specific Taxonomy as\nsupervision signals to continually pre-train transformer encoder on a\ndomain-specific corpus. The main innovation is that during domain-specific\npretraining, an open-domain encoder is continually pre-trained using\nsentence-level embeddings as inputs (to accommodate long documents), however,\nfine-tuning is done with token-level embeddings as inputs to this encoder. We\nperform such domain-specific pre-training on three different domains namely\ncustomer support, scientific, and legal domains, and compare performance on 6\ndifferent downstream tasks and 9 different datasets. The novel use of\ndocument-level supervision along with sentence-level embedding input for\npre-training reduces pre-training compute by around $1,000$, $4,500$, and $500$\ntimes compared to MLM and/or NSP in Customer Support, Scientific, and Legal\nDomains, respectively. The reduced training time does not lead to a\ndeterioration in performance. In fact we show that $FastDoc$ either outperforms\nor performs on par with several competitive transformer-based baselines in\nterms of character-level F1 scores and other automated metrics in the Customer\nSupport, Scientific, and Legal Domains. Moreover, reduced training aids in\nmitigating the risk of catastrophic forgetting. Thus, unlike baselines,\n$FastDoc$ shows a negligible drop in performance on open domain.\n","authors":["Abhilash Nandy","Manav Nitin Kapadnis","Sohan Patnaik","Yash Parag Butala","Pawan Goyal","Niloy Ganguly"],"pdf_url":"https://arxiv.org/pdf/2306.06190v3.pdf","comment":"Accepted to Transactions on Machine Learning Research (TMLR), 36\n  pages, 8 figures"},{"id":"http://arxiv.org/abs/2404.13752v3","updated":"2024-11-01T07:51:36Z","published":"2024-04-21T19:24:15Z","title":"Adversarial Representation Engineering: A General Model Editing\n  Framework for Large Language Models","summary":"  Since the rapid development of Large Language Models (LLMs) has achieved\nremarkable success, understanding and rectifying their internal complex\nmechanisms has become an urgent issue. Recent research has attempted to\ninterpret their behaviors through the lens of inner representation. However,\ndeveloping practical and efficient methods for applying these representations\nfor general and flexible model editing remains challenging. In this work, we\nexplore how to leverage insights from representation engineering to guide the\nediting of LLMs by deploying a representation sensor as an editing oracle. We\nfirst identify the importance of a robust and reliable sensor during editing,\nthen propose an Adversarial Representation Engineering (ARE) framework to\nprovide a unified and interpretable approach for conceptual model editing\nwithout compromising baseline performance. Experiments on multiple tasks\ndemonstrate the effectiveness of ARE in various model editing scenarios. Our\ncode and data are available at\nhttps://github.com/Zhang-Yihao/Adversarial-Representation-Engineering.\n","authors":["Yihao Zhang","Zeming Wei","Jun Sun","Meng Sun"],"pdf_url":"https://arxiv.org/pdf/2404.13752v3.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2408.07832v4","updated":"2024-11-01T07:41:04Z","published":"2024-07-31T14:49:35Z","title":"LADDER: Language Driven Slice Discovery and Error Rectification","summary":"  Error slice discovery associates structured patterns with model errors.\nExisting methods discover error slices by clustering the error-prone samples\nwith similar patterns or assigning discrete attributes to each sample for\npost-hoc analysis. While these methods aim for interpretability and easier\nmitigation through reweighting or rebalancing, they may not capture the full\ncomplexity of error patterns due to incomplete or missing attributes. Contrary\nto the existing approach, this paper utilizes the reasoning capabilities of the\nLarge Language Model (LLM) to analyze complex error patterns and generate\ntestable hypotheses. This paper proposes LADDER: Language Driven slice\nDiscovery and Error Rectification. It first projects the model's representation\ninto a language-aligned feature space (eg CLIP) to preserve semantics in the\noriginal model feature space. This ensures the accurate retrieval of sentences\nthat highlight the model's errors. Next, the LLM utilizes the sentences and\ngenerates hypotheses to discover error slices. Finally, we mitigate the error\nby fine-tuning the classification head by creating a group-balanced dataset\nusing the hypotheses. Our entire method does not require any attribute\nannotation, either explicitly or through external tagging models. We validate\nour method with \\textbf{five} image classification datasets. The code is\navailable (https://github.com/batmanlab/Ladder).\n","authors":["Shantanu Ghosh","Rayan Syed","Chenyu Wang","Clare B. Poynton","Shyam Visweswaran","Kayhan Batmanghelich"],"pdf_url":"https://arxiv.org/pdf/2408.07832v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11406v3","updated":"2024-11-01T06:25:15Z","published":"2024-07-16T05:48:24Z","title":"Revisiting the Impact of Pursuing Modularity for Code Generation","summary":"  Modular programming, which aims to construct the final program by integrating\nsmaller, independent building blocks, has been regarded as a desirable practice\nin software development. However, with the rise of recent code generation\nagents built upon large language models (LLMs), a question emerges: is this\ntraditional practice equally effective for these new tools? In this work, we\nassess the impact of modularity in code generation by introducing a novel\nmetric for its quantitative measurement. Surprisingly, unlike conventional\nwisdom on the topic, we find that modularity is not a core factor for improving\nthe performance of code generation models. We also explore potential\nexplanations for why LLMs do not exhibit a preference for modular code compared\nto non-modular code.\n","authors":["Deokyeong Kang","Ki Jung Seo","Taeuk Kim"],"pdf_url":"https://arxiv.org/pdf/2407.11406v3.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.14155v2","updated":"2024-11-01T06:19:47Z","published":"2024-10-18T03:45:42Z","title":"Towards Faithful Natural Language Explanations: A Study Using Activation\n  Patching in Large Language Models","summary":"  Large Language Models (LLMs) are capable of generating persuasive Natural\nLanguage Explanations (NLEs) to justify their answers. However, the\nfaithfulness of these explanations should not be readily trusted at face value.\nRecent studies have proposed various methods to measure the faithfulness of\nNLEs, typically by inserting perturbations at the explanation or feature level.\nWe argue that these approaches are neither comprehensive nor correctly designed\naccording to the established definition of faithfulness. Moreover, we highlight\nthe risks of grounding faithfulness findings on out-of-distribution samples. In\nthis work, we leverage a causal mediation technique called activation patching,\nto measure the faithfulness of an explanation towards supporting the explained\nanswer. Our proposed metric, Causal Faithfulness quantifies the consistency of\ncausal attributions between explanations and the corresponding model outputs as\nthe indicator of faithfulness. We experimented across models varying from 2B to\n27B parameters and found that models that underwent alignment tuning tend to\nproduce more faithful and plausible explanations. We find that Causal\nFaithfulness is a promising improvement over existing faithfulness tests by\ntaking into account the model's internal computations and avoiding out of\ndistribution concerns that could otherwise undermine the validity of\nfaithfulness assessments. We release the code in\n\\url{https://github.com/wj210/Causal-Faithfulness}\n","authors":["Wei Jie Yeo","Ranjan Satapathy","Erik Cambria"],"pdf_url":"https://arxiv.org/pdf/2410.14155v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2405.16247v3","updated":"2024-11-01T06:13:12Z","published":"2024-05-25T14:11:44Z","title":"AutoManual: Generating Instruction Manuals by LLM Agents via Interactive\n  Environmental Learning","summary":"  Large Language Models (LLM) based agents have shown promise in autonomously\ncompleting tasks across various domains, e.g., robotics, games, and web\nnavigation. However, these agents typically require elaborate design and expert\nprompts to solve tasks in specific domains, which limits their adaptability. We\nintroduce AutoManual, a framework enabling LLM agents to autonomously build\ntheir understanding through interaction and adapt to new environments.\nAutoManual categorizes environmental knowledge into diverse rules and optimizes\nthem in an online fashion by two agents: 1) The Planner codes actionable plans\nbased on current rules for interacting with the environment. 2) The Builder\nupdates the rules through a well-structured rule system that facilitates online\nrule management and essential detail retention. To mitigate hallucinations in\nmanaging rules, we introduce a *case-conditioned prompting* strategy for the\nBuilder. Finally, the Formulator agent compiles these rules into a\ncomprehensive manual. The self-generated manual can not only improve the\nadaptability but also guide the planning of smaller LLMs while being\nhuman-readable. Given only one simple demonstration, AutoManual significantly\nimproves task success rates, achieving 97.4\\% with GPT-4-turbo and 86.2\\% with\nGPT-3.5-turbo on ALFWorld benchmark tasks. The code is available at\nhttps://github.com/minghchen/automanual.\n","authors":["Minghao Chen","Yihang Li","Yanting Yang","Shiyu Yu","Binbin Lin","Xiaofei He"],"pdf_url":"https://arxiv.org/pdf/2405.16247v3.pdf","comment":"Accepted at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.01548v2","updated":"2024-11-01T06:12:33Z","published":"2024-10-02T13:37:54Z","title":"In-Context Transfer Learning: Demonstration Synthesis by Transferring\n  Similar Tasks","summary":"  In-context learning (ICL) is an effective approach to help large language\nmodels (LLMs) adapt to various tasks by providing demonstrations of the target\ntask. Considering the high cost of labeling demonstrations, many methods\npropose synthesizing demonstrations from scratch using LLMs. However, the\nquality of the demonstrations synthesized from scratch is limited by the\ncapabilities and knowledge of LLMs. To address this, inspired by transfer\nlearning, we propose In-Context Transfer Learning (ICTL), which synthesizes\ntarget task demonstrations by transferring labeled demonstrations from similar\nsource tasks. ICTL consists of two steps: source sampling and target transfer.\nFirst, we define an optimization objective, which minimizes transfer error to\nsample source demonstrations similar to the target task. Then, we employ LLMs\nto transfer the sampled source demonstrations to the target task, matching the\ndefinition and format of the target task. Experiments on Super-NI show that\nICTL outperforms synthesis from scratch by 2.0% on average, demonstrating the\neffectiveness of our method.\n","authors":["Dingzirui Wang","Xuanliang Zhang","Qiguang Chen","Longxu Dou","Xiao Xu","Rongyu Cao","Yingwei Ma","Qingfu Zhu","Wanxiang Che","Binhua Li","Fei Huang","Yongbin Li"],"pdf_url":"https://arxiv.org/pdf/2410.01548v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.13698v2","updated":"2024-11-01T06:08:08Z","published":"2024-09-05T02:24:18Z","title":"Lightweight Transducer Based on Frame-Level Criterion","summary":"  The transducer model trained based on sequence-level criterion requires a lot\nof memory due to the generation of the large probability matrix. We proposed a\nlightweight transducer model based on frame-level criterion, which uses the\nresults of the CTC forced alignment algorithm to determine the label for each\nframe. Then the encoder output can be combined with the decoder output at the\ncorresponding time, rather than adding each element output by the encoder to\neach element output by the decoder as in the transducer. This significantly\nreduces memory and computation requirements. To address the problem of\nimbalanced classification caused by excessive blanks in the label, we decouple\nthe blank and non-blank probabilities and truncate the gradient of the blank\nclassifier to the main network. Experiments on the AISHELL-1 demonstrate that\nthis enables the lightweight transducer to achieve similar results to\ntransducer. Additionally, we use richer information to predict the probability\nof blank, achieving superior results to transducer.\n","authors":["Genshun Wan","Mengzhi Wang","Tingzhi Mao","Hang Chen","Zhongfu Ye"],"pdf_url":"https://arxiv.org/pdf/2409.13698v2.pdf","comment":"Accepted by Interspeech 2024, code repository:\n  https://github.com/wangmengzhi/Lightweight-Transducer"},{"id":"http://arxiv.org/abs/2406.04744v2","updated":"2024-11-01T05:30:17Z","published":"2024-06-07T08:43:07Z","title":"CRAG -- Comprehensive RAG Benchmark","summary":"  Retrieval-Augmented Generation (RAG) has recently emerged as a promising\nsolution to alleviate Large Language Model (LLM)'s deficiency in lack of\nknowledge. Existing RAG datasets, however, do not adequately represent the\ndiverse and dynamic nature of real-world Question Answering (QA) tasks. To\nbridge this gap, we introduce the Comprehensive RAG Benchmark (CRAG), a factual\nquestion answering benchmark of 4,409 question-answer pairs and mock APIs to\nsimulate web and Knowledge Graph (KG) search. CRAG is designed to encapsulate a\ndiverse array of questions across five domains and eight question categories,\nreflecting varied entity popularity from popular to long-tail, and temporal\ndynamisms ranging from years to seconds. Our evaluation of this benchmark\nhighlights the gap to fully trustworthy QA. Whereas most advanced LLMs achieve\n<=34% accuracy on CRAG, adding RAG in a straightforward manner improves the\naccuracy only to 44%. State-of-the-art industry RAG solutions only answer 63%\nof questions without any hallucination. CRAG also reveals much lower accuracy\nin answering questions regarding facts with higher dynamism, lower popularity,\nor higher complexity, suggesting future research directions. The CRAG benchmark\nlaid the groundwork for a KDD Cup 2024 challenge and attracted thousands of\nparticipants and submissions. We commit to maintaining CRAG to serve research\ncommunities in advancing RAG solutions and general QA solutions. CRAG is\navailable at https://github.com/facebookresearch/CRAG/.\n","authors":["Xiao Yang","Kai Sun","Hao Xin","Yushi Sun","Nikita Bhalla","Xiangsen Chen","Sajal Choudhary","Rongze Daniel Gui","Ziran Will Jiang","Ziyu Jiang","Lingkun Kong","Brian Moran","Jiaqi Wang","Yifan Ethan Xu","An Yan","Chenyu Yang","Eting Yuan","Hanwen Zha","Nan Tang","Lei Chen","Nicolas Scheffer","Yue Liu","Nirav Shah","Rakesh Wanga","Anuj Kumar","Wen-tau Yih","Xin Luna Dong"],"pdf_url":"https://arxiv.org/pdf/2406.04744v2.pdf","comment":"NeurIPS 2024 Datasets and Benchmarks Track"},{"id":"http://arxiv.org/abs/2409.11724v2","updated":"2024-11-01T04:19:21Z","published":"2024-09-18T06:19:59Z","title":"TART: An Open-Source Tool-Augmented Framework for Explainable\n  Table-based Reasoning","summary":"  Current Large Language Models (LLMs) exhibit limited ability to understand\ntable structures and to apply precise numerical reasoning, which is crucial for\ntasks such as table question answering (TQA) and table-based fact verification\n(TFV). To address these challenges, we introduce our Tool-Augmented Reasoning\nframework for Tables (TART), which integrates LLMs with specialized tools. TART\ncontains three key components: a table formatter to ensure accurate data\nrepresentation, a tool maker to develop specific computational tools, and an\nexplanation generator to maintain explainability. We also present the TOOLTAB\ndataset, a new benchmark designed specifically for training LLMs in table-tool\nintegration. Our experiments indicate that TART achieves substantial\nimprovements over existing methods (e.g., Chain-of-Thought) by improving both\nthe precision of data processing and the clarity of the reasoning process.\nNotably, TART paired with CodeLlama achieves 90.0% of the accuracy of the\nclosed-sourced LLM GPT-3.5-turbo, highlighting its robustness in diverse\nreal-world scenarios. All the code and data are available at\nhttps://github.com/XinyuanLu00/TART.\n","authors":["Xinyuan Lu","Liangming Pan","Yubo Ma","Preslav Nakov","Min-Yen Kan"],"pdf_url":"https://arxiv.org/pdf/2409.11724v2.pdf","comment":"technical report"},{"id":"http://arxiv.org/abs/2402.01763v3","updated":"2024-11-01T03:49:59Z","published":"2024-01-30T23:35:28Z","title":"When Large Language Models Meet Vector Databases: A Survey","summary":"  This survey explores the synergistic potential of Large Language Models\n(LLMs) and Vector Databases (VecDBs), a burgeoning but rapidly evolving\nresearch area. With the proliferation of LLMs comes a host of challenges,\nincluding hallucinations, outdated knowledge, prohibitive commercial\napplication costs, and memory issues. VecDBs emerge as a compelling solution to\nthese issues by offering an efficient means to store, retrieve, and manage the\nhigh-dimensional vector representations intrinsic to LLM operations. Through\nthis nuanced review, we delineate the foundational principles of LLMs and\nVecDBs and critically analyze their integration's impact on enhancing LLM\nfunctionalities. This discourse extends into a discussion on the speculative\nfuture developments in this domain, aiming to catalyze further research into\noptimizing the confluence of LLMs and VecDBs for advanced data handling and\nknowledge extraction capabilities.\n","authors":["Zhi Jing","Yongye Su","Yikun Han"],"pdf_url":"https://arxiv.org/pdf/2402.01763v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.02428v3","updated":"2024-11-01T03:47:51Z","published":"2024-09-04T04:15:14Z","title":"Large Language Models as Efficient Reward Function Searchers for\n  Custom-Environment Multi-Objective Reinforcement Learning","summary":"  Achieving the effective design and improvement of reward functions in\nreinforcement learning (RL) tasks with complex custom environments and multiple\nrequirements presents considerable challenges. In this paper, we propose ERFSL,\nan efficient reward function searcher using LLMs, which enables LLMs to be\neffective white-box searchers and highlights their advanced semantic\nunderstanding capabilities. Specifically, we generate reward components for\neach numerically explicit user requirement and employ a reward critic to\nidentify the correct code form. Then, LLMs assign weights to the reward\ncomponents to balance their values and iteratively adjust the weights without\nambiguity and redundant adjustments by flexibly adopting directional mutation\nand crossover strategies, similar to genetic algorithms, based on the context\nprovided by the training log analyzer. We applied the framework to an\nunderwater data collection RL task without direct human feedback or reward\nexamples (zero-shot learning). The reward critic successfully corrects the\nreward code with only one feedback instance for each requirement, effectively\npreventing unrectifiable errors. The initialization of weights enables the\nacquisition of different reward functions within the Pareto solution set\nwithout the need for weight search. Even in cases where a weight is 500 times\noff, on average, only 5.2 iterations are needed to meet user requirements. The\nERFSL also works well with most prompts utilizing GPT-4o mini, as we decompose\nthe weight searching process to reduce the requirement for numerical and\nlong-context understanding capabilities\n","authors":["Guanwen Xie","Jingzehua Xu","Yiyuan Yang","Yimian Ding","Shuai Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.02428v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04501v3","updated":"2024-11-01T03:42:37Z","published":"2024-10-06T14:45:01Z","title":"Leveraging Large Language Models for Suicide Detection on Social Media\n  with Limited Labels","summary":"  The increasing frequency of suicidal thoughts highlights the importance of\nearly detection and intervention. Social media platforms, where users often\nshare personal experiences and seek help, could be utilized to identify\nindividuals at risk. However, the large volume of daily posts makes manual\nreview impractical. This paper explores the use of Large Language Models (LLMs)\nto automatically detect suicidal content in text-based social media posts. We\npropose a novel method for generating pseudo-labels for unlabeled data by\nprompting LLMs, along with traditional classification fine-tuning techniques to\nenhance label accuracy. To create a strong suicide detection model, we develop\nan ensemble approach involving prompting with Qwen2-72B-Instruct, and using\nfine-tuned models such as Llama3-8B, Llama3.1-8B, and Gemma2-9B. We evaluate\nour approach on the dataset of the Suicide Ideation Detection on Social Media\nChallenge, a track of the IEEE Big Data 2024 Big Data Cup. Additionally, we\nconduct a comprehensive analysis to assess the impact of different models and\nfine-tuning strategies on detection performance. Experimental results show that\nthe ensemble model significantly improves the detection accuracy, by 5% points\ncompared with the individual models. It achieves a weight F1 score of 0.770 on\nthe public test set, and 0.731 on the private test set, providing a promising\nsolution for identifying suicidal content in social media. Our analysis shows\nthat the choice of LLMs affects the prompting performance, with larger models\nproviding better accuracy. Our code and checkpoints are publicly available at\nhttps://github.com/khanhvynguyen/Suicide_Detection_LLMs.\n","authors":["Vy Nguyen","Chau Pham"],"pdf_url":"https://arxiv.org/pdf/2410.04501v3.pdf","comment":"Accepted at IEEE International Conference on Big Data 2024"},{"id":"http://arxiv.org/abs/2410.19878v2","updated":"2024-11-01T03:26:07Z","published":"2024-10-24T13:58:59Z","title":"Parameter-Efficient Fine-Tuning in Large Models: A Survey of\n  Methodologies","summary":"  The large models, as predicted by scaling raw forecasts, have made\ngroundbreaking progress in many fields, particularly in natural language\ngeneration tasks, where they have approached or even surpassed human levels.\nHowever, the unprecedented scale of their parameters brings significant\ncomputational and storage costs. These large models require substantial\ncomputational resources and GPU memory to operate. When adapting large models\nto specific downstream tasks, their massive parameter scale poses a significant\nchallenge in fine-tuning on hardware platforms with limited computational power\nand GPU memory. To address this issue, Parameter-Efficient Fine-Tuning (PEFT)\noffers a practical solution by efficiently adjusting the parameters of large\npre-trained models to suit various downstream tasks. Specifically, PEFT adjusts\nthe parameters of pre-trained large models to adapt to specific tasks or\ndomains, minimizing the introduction of additional parameters and the\ncomputational resources required. This review mainly introduces the preliminary\nknowledge of PEFT, the core ideas and principles of various PEFT algorithms,\nthe applications of PEFT, and potential future research directions. By reading\nthis review, we believe that interested parties can quickly grasp the PEFT\nmethodology, thereby accelerating its development and innovation.\n","authors":["Luping Wang","Sheng Chen","Linnan Jiang","Shu Pan","Runze Cai","Sen Yang","Fei Yang"],"pdf_url":"https://arxiv.org/pdf/2410.19878v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13056v2","updated":"2024-11-01T03:16:30Z","published":"2024-10-16T21:34:41Z","title":"Channel-Wise Mixed-Precision Quantization for Large Language Models","summary":"  Large Language Models (LLMs) have demonstrated remarkable success across a\nwide range of language tasks, but their deployment on edge devices remains\nchallenging due to the substantial memory requirements imposed by their large\nparameter sizes. Weight-only quantization presents a promising solution to\nreduce the memory footprint of LLMs. However, existing approaches primarily\nfocus on integer-bit quantization, limiting their adaptability to\nfractional-bit quantization tasks and preventing the full utilization of\navailable storage space on devices. In this paper, we introduce Channel-Wise\nMixed-Precision Quantization (CMPQ), a novel mixed-precision quantization\nmethod that allocates quantization precision in a channel-wise pattern based on\nactivation distributions. By assigning different precision levels to different\nweight channels, CMPQ can adapt to any bit-width constraint. CMPQ employs a\nnon-uniform quantization strategy and incorporates two outlier extraction\ntechniques that collaboratively preserve the critical information, thereby\nminimizing the quantization loss. Experiments on different sizes of LLMs\ndemonstrate that CMPQ not only enhances performance in integer-bit quantization\ntasks but also achieves significant performance gains with a modest increase in\nmemory usage. CMPQ thus represents an adaptive and effective approach to LLM\nquantization, offering substantial benefits across diverse device capabilities.\n","authors":["Zihan Chen","Bike Xie","Jundong Li","Cong Shen"],"pdf_url":"https://arxiv.org/pdf/2410.13056v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.20646v2","updated":"2024-11-01T03:12:44Z","published":"2024-05-31T07:24:42Z","title":"LLM-ESR: Large Language Models Enhancement for Long-tailed Sequential\n  Recommendation","summary":"  Sequential recommender systems (SRS) aim to predict users' subsequent choices\nbased on their historical interactions and have found applications in diverse\nfields such as e-commerce and social media. However, in real-world systems,\nmost users interact with only a handful of items, while the majority of items\nare seldom consumed. These two issues, known as the long-tail user and\nlong-tail item challenges, often pose difficulties for existing SRS. These\nchallenges can adversely affect user experience and seller benefits, making\nthem crucial to address. Though a few works have addressed the challenges, they\nstill struggle with the seesaw or noisy issues due to the intrinsic scarcity of\ninteractions. The advancements in large language models (LLMs) present a\npromising solution to these problems from a semantic perspective. As one of the\npioneers in this field, we propose the Large Language Models Enhancement\nframework for Sequential Recommendation (LLM-ESR). This framework utilizes\nsemantic embeddings derived from LLMs to enhance SRS without adding extra\ninference load from LLMs. To address the long-tail item challenge, we design a\ndual-view modeling framework that combines semantics from LLMs and\ncollaborative signals from conventional SRS. For the long-tail user challenge,\nwe propose a retrieval augmented self-distillation method to enhance user\npreference representation using more informative interactions from similar\nusers. To verify the effectiveness and versatility of our proposed enhancement\nframework, we conduct extensive experiments on three real-world datasets using\nthree popular SRS models. The results show that our method surpasses existing\nbaselines consistently, and benefits long-tail users and items especially. The\nimplementation code is available at\nhttps://github.com/Applied-Machine-Learning-Lab/LLM-ESR.\n","authors":["Qidong Liu","Xian Wu","Yejing Wang","Zijian Zhang","Feng Tian","Yefeng Zheng","Xiangyu Zhao"],"pdf_url":"https://arxiv.org/pdf/2405.20646v2.pdf","comment":"accepted by NeruIPS'24 (Spotlight)"},{"id":"http://arxiv.org/abs/2302.04391v8","updated":"2024-11-01T02:49:24Z","published":"2023-02-09T01:09:57Z","title":"The Re-Label Method For Data-Centric Machine Learning","summary":"  In industry deep learning application, our manually labeled data has a\ncertain number of noisy data. To solve this problem and achieve more than 90\nscore in dev dataset, we present a simple method to find the noisy data and\nre-label the noisy data by human, given the model predictions as references in\nhuman labeling. In this paper, we illustrate our idea for a broad set of deep\nlearning tasks, includes classification, sequence tagging, object detection,\nsequence generation, click-through rate prediction. The dev dataset evaluation\nresults and human evaluation results verify our idea.\n","authors":["Tong Guo"],"pdf_url":"https://arxiv.org/pdf/2302.04391v8.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09131v4","updated":"2024-11-01T02:43:34Z","published":"2024-03-14T06:49:16Z","title":"ProSwitch: Knowledge-Guided Instruction Tuning to Switch Between\n  Professional and Non-Professional Answers","summary":"  Large Language Models (LLMs) have demonstrated efficacy in various linguistic\napplications, including text summarization and controlled text generation.\nHowever, studies into their capacity of switching between styles via\ninstruction tuning remain underexplored. This study concentrates on the\nstyle-switching abilities of LLMs and introduces a novel approach, named\nProSwitch, which enables a language model to switch between professional and\nnon-professional answers, by tuning and evaluating through the guidance of\ndomain and style knowledge. ProSwitch unfolds across three phases:\nLLM-augmented preparation to collect domain knowledge and QA pairs, instruction\ntuning to optimize LLMs with multiple levels of knowledge, and comprehensive\nevaluation to assess both style discrimination and reference-based quality of\ngenerated text. Comparative analysis of ProSwitch against general and\nspecialized LLMs reveals that our approach outperforms baselines in switching\nbetween professional and non-professional answers.\n","authors":["Chang Zong","Yuyan Chen","Weiming Lu","Jian Shao","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2403.09131v4.pdf","comment":"8 pages main body, 16 pages total"},{"id":"http://arxiv.org/abs/2407.13623v3","updated":"2024-11-01T02:41:36Z","published":"2024-07-18T15:58:54Z","title":"Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies","summary":"  Research on scaling large language models (LLMs) has primarily focused on\nmodel parameters and training data size, overlooking the role of vocabulary\nsize. We investigate how vocabulary size impacts LLM scaling laws by training\nmodels ranging from 33M to 3B parameters on up to 500B characters with various\nvocabulary configurations. We propose three complementary approaches for\npredicting the compute-optimal vocabulary size: IsoFLOPs analysis, derivative\nestimation, and parametric fit of the loss function. Our approaches converge on\nthe conclusion that the optimal vocabulary size depends on the compute budget,\nwith larger models requiring larger vocabularies. Most LLMs, however, use\ninsufficient vocabulary sizes. For example, we predict that the optimal\nvocabulary size of Llama2-70B should have been at least 216K, 7 times larger\nthan its vocabulary of 32K. We validate our predictions empirically by training\nmodels with 3B parameters across different FLOPs budgets. Adopting our\npredicted optimal vocabulary size consistently improves downstream performance\nover commonly used vocabulary sizes. By increasing the vocabulary size from the\nconventional 32K to 43K, we improve performance on ARC-Challenge from 29.1 to\n32.0 with the same 2.3e21 FLOPs. Our work highlights the importance of jointly\nconsidering tokenization and model scaling for efficient pre-training. The code\nand demo are available at https://github.com/sail-sg/scaling-with-vocab and\nhttps://hf.co/spaces/sail/scaling-with-vocab-demo.\n","authors":["Chaofan Tao","Qian Liu","Longxu Dou","Niklas Muennighoff","Zhongwei Wan","Ping Luo","Min Lin","Ngai Wong"],"pdf_url":"https://arxiv.org/pdf/2407.13623v3.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2406.14909v2","updated":"2024-11-01T02:26:18Z","published":"2024-06-21T06:58:37Z","title":"MoA: Mixture of Sparse Attention for Automatic Large Language Model\n  Compression","summary":"  Sparse attention can effectively mitigate the significant memory and\nthroughput demands of Large Language Models (LLMs) in long contexts. Existing\nmethods typically employ a uniform sparse attention mask, applying the same\nsparse pattern across different attention heads and input lengths. However,\nthis uniform approach fails to capture the diverse attention patterns inherent\nin LLMs, ignoring their distinct accuracy-latency trade-offs. To address this\nchallenge, we propose the Mixture of Attention (MoA), which automatically\ntailors distinct sparse attention configurations to different heads and layers.\nMoA constructs and navigates a search space of various attention patterns and\ntheir scaling rules relative to input sequence lengths. It profiles the model,\nevaluates potential configurations, and pinpoints the optimal sparse attention\ncompression plan. MoA adapts to varying input sizes, revealing that some\nattention heads expand their focus to accommodate longer sequences, while other\nheads consistently concentrate on fixed-length local contexts. Experiments show\nthat MoA increases the effective context length by $3.9\\times$ with the same\naverage attention span, boosting retrieval accuracy by $1.5-7.1\\times$ over the\nuniform-attention baseline across Vicuna-{7B,13B}, and Llama3-{8B,70B} models.\nMoreover, MoA narrows the capability gaps between sparse and dense models,\nreducing the maximum relative performance drop from $9\\%-36\\%$ to within $5\\%$\nacross two long-context understanding benchmarks. MoA achieves a\n$1.2-1.4\\times$ GPU memory reduction, boosting decode throughput by\n$6.6-8.2\\times$ and $1.7-1.9\\times$ compared to FlashAttention2 and vLLM, with\nminimal impact on performance. Our code is available at\n\\url{https://github.com/thu-nics/MoA}.\n","authors":["Tianyu Fu","Haofeng Huang","Xuefei Ning","Genghan Zhang","Boju Chen","Tianqi Wu","Hongyi Wang","Zixiao Huang","Shiyao Li","Shengen Yan","Guohao Dai","Huazhong Yang","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2406.14909v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.02834v3","updated":"2024-11-01T02:21:13Z","published":"2024-09-04T16:00:21Z","title":"CMM-Math: A Chinese Multimodal Math Dataset To Evaluate and Enhance the\n  Mathematics Reasoning of Large Multimodal Models","summary":"  Large language models (LLMs) have obtained promising results in mathematical\nreasoning, which is a foundational skill for human intelligence. Most previous\nstudies focus on improving and measuring the performance of LLMs based on\ntextual math reasoning datasets (e.g., MATH, GSM8K). Recently, a few\nresearchers have released English multimodal math datasets (e.g., MATHVISTA and\nMATH-V) to evaluate the effectiveness of large multimodal models (LMMs). In\nthis paper, we release a Chinese multimodal math (CMM-Math) dataset, including\nbenchmark and training parts, to evaluate and enhance the mathematical\nreasoning of LMMs. CMM-Math contains over 28,000 high-quality samples,\nfeaturing a variety of problem types (e.g., multiple-choice, fill-in-the-blank,\nand so on) with detailed solutions across 12 grade levels from elementary to\nhigh school in China. Specifically, the visual context may be present in the\nquestions or opinions, which makes this dataset more challenging. Through\ncomprehensive analysis, we discover that state-of-the-art LMMs on the CMM-Math\ndataset face challenges, emphasizing the necessity for further improvements in\nLMM development. We also propose a Multimodal Mathematical LMM (Math-LMM) to\nhandle the problems with mixed input of multiple images and text segments. We\ntrain our model using three stages, including foundational pre-training,\nfoundational fine-tuning, and mathematical fine-tuning. The extensive\nexperiments indicate that our model effectively improves math reasoning\nperformance by comparing it with the SOTA LMMs over three multimodal\nmathematical datasets.\n","authors":["Wentao Liu","Qianjun Pan","Yi Zhang","Zhuo Liu","Ji Wu","Jie Zhou","Aimin Zhou","Qin Chen","Bo Jiang","Liang He"],"pdf_url":"https://arxiv.org/pdf/2409.02834v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17213v4","updated":"2024-11-01T02:08:03Z","published":"2024-09-25T17:38:39Z","title":"Plurals: A System for Guiding LLMs Via Simulated Social Ensembles","summary":"  Recent debates raised concerns that language models may favor certain\nviewpoints. But what if the solution is not to aim for a 'view from nowhere'\nbut rather to leverage different viewpoints? We introduce Plurals, a system and\nPython library for pluralistic AI deliberation. Plurals consists of Agents\n(LLMs, optionally with personas) which deliberate within customizable\nStructures, with Moderators overseeing deliberation. Plurals is a generator of\nsimulated social ensembles. Plurals integrates with government datasets to\ncreate nationally representative personas, includes deliberation templates\ninspired by democratic deliberation theory, and allows users to customize both\ninformation-sharing structures and deliberation behavior within Structures. Six\ncase studies demonstrate fidelity to theoretical constructs and efficacy. Three\nrandomized experiments show simulated focus groups produced output resonant\nwith an online sample of the relevant audiences (chosen over zero-shot\ngeneration in 75% of trials). Plurals is both a paradigm and a concrete system\nfor pluralistic AI. The Plurals library is available at\nhttps://github.com/josh-ashkinaze/plurals and will be continually updated.\n","authors":["Joshua Ashkinaze","Emily Fry","Narendra Edara","Eric Gilbert","Ceren Budak"],"pdf_url":"https://arxiv.org/pdf/2409.17213v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24126v2","updated":"2024-11-01T01:49:56Z","published":"2024-10-31T16:50:39Z","title":"Multi-environment Topic Models","summary":"  Probabilistic topic models are a powerful tool for extracting latent themes\nfrom large text datasets. In many text datasets, we also observe per-document\ncovariates (e.g., source, style, political affiliation) that act as\nenvironments that modulate a \"global\" (environment-agnostic) topic\nrepresentation. Accurately learning these representations is important for\nprediction on new documents in unseen environments and for estimating the\ncausal effect of topics on real-world outcomes. To this end, we introduce the\nMulti-environment Topic Model (MTM), an unsupervised probabilistic model that\nseparates global and environment-specific terms. Through experimentation on\nvarious political content, from ads to tweets and speeches, we show that the\nMTM produces interpretable global topics with distinct environment-specific\nwords. On multi-environment data, the MTM outperforms strong baselines in and\nout-of-distribution. It also enables the discovery of accurate causal effects.\n","authors":["Dominic Sobhani","Amir Feder","David Blei"],"pdf_url":"https://arxiv.org/pdf/2410.24126v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16342v2","updated":"2024-11-01T01:16:28Z","published":"2024-06-24T06:27:47Z","title":"Is your benchmark truly adversarial? AdvScore: Evaluating Human-Grounded\n  Adversarialness","summary":"  Adversarial datasets should ensure AI robustness that matches human\nperformance. However, as models evolve, datasets can become obsolete. Thus,\nadversarial datasets should be periodically updated based on their degradation\nin adversarialness. Given the lack of a standardized metric for measuring\nadversarialness, we propose AdvScore, a human-grounded evaluation metric.\nAdvScore assesses a dataset's true adversarialness by capturing models' and\nhumans' varying abilities, while also identifying poor examples. AdvScore then\nmotivates a new dataset creation pipeline for realistic and high-quality\nadversarial samples, enabling us to collect an adversarial question answering\n(QA) dataset, AdvQA. We apply AdvScore using 9,347 human responses and ten\nlanguage model predictions to track the models' improvement over five years\n(from 2020 to 2024). AdvScore assesses whether adversarial datasets remain\nsuitable for model evaluation, measures model improvements, and provides\nguidance for better alignment with human capabilities.\n","authors":["Yoo Yeon Sung","Maharshi Gor","Eve Fleisig","Ishani Mondal","Jordan Lee Boyd-Graber"],"pdf_url":"https://arxiv.org/pdf/2406.16342v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2401.11185"},{"id":"http://arxiv.org/abs/2408.02128v2","updated":"2024-11-01T00:34:19Z","published":"2024-08-04T19:54:12Z","title":"Table Transformers for Imputing Textual Attributes","summary":"  Missing data in tabular dataset is a common issue as the performance of\ndownstream tasks usually depends on the completeness of the training dataset.\nPrevious missing data imputation methods focus on numeric and categorical\ncolumns, but we propose a novel end-to-end approach called Table Transformers\nfor Imputing Textual Attributes (TTITA) based on the transformer to impute\nunstructured textual columns using other columns in the table. We conduct\nextensive experiments on three datasets, and our approach shows competitive\nperformance outperforming baseline models such as recurrent neural networks and\nLlama2. The performance improvement is more significant when the target\nsequence has a longer length. Additionally, we incorporate multi-task learning\nto simultaneously impute for heterogeneous columns, boosting the performance\nfor text imputation. We also qualitatively compare with ChatGPT for realistic\napplications.\n","authors":["Ting-Ruen Wei","Yuan Wang","Yoshitaka Inoue","Hsin-Tai Wu","Yi Fang"],"pdf_url":"https://arxiv.org/pdf/2408.02128v2.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2409.16016v2","updated":"2024-11-01T17:44:34Z","published":"2024-09-24T12:19:31Z","title":"VascX Models: Model Ensembles for Retinal Vascular Analysis from Color\n  Fundus Images","summary":"  We introduce VascX models, a comprehensive set of model ensembles for\nanalyzing retinal vasculature from color fundus images (CFIs). Annotated CFIs\nwere aggregated from public datasets . Additional CFIs, mainly from the\npopulation-based Rotterdam Study were annotated by graders for arteries and\nveins at pixel level, resulting in a dataset diverse in patient demographics\nand imaging conditions. VascX models demonstrated superior segmentation\nperformance across datasets, image quality levels, and anatomic regions when\ncompared to existing, publicly available models, likely due to the increased\nsize and variety of our training set. Important improvements were observed in\nartery-vein and disc segmentation performance, particularly in segmentations of\nthese structures on CFIs of intermediate quality, common in large cohorts and\nclinical datasets. Importantly, these improvements translated into\nsignificantly more accurate vascular features when we compared features\nextracted from VascX segmentation masks with features extracted from\nsegmentation masks generated by previous models. With VascX models we provide a\nrobust, ready-to-use set of model ensembles and inference code aimed at\nsimplifying the implementation and enhancing the quality of automated retinal\nvasculature analyses. The precise vessel parameters generated by the model can\nserve as starting points for the identification of disease patterns in and\noutside of the eye.\n","authors":["Jose Vargas Quiros","Bart Liefers","Karin van Garderen","Jeroen Vermeulen","Eyened Reading Center","Sinergia Consortium","Caroline Klaver"],"pdf_url":"https://arxiv.org/pdf/2409.16016v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24211v2","updated":"2024-11-01T17:23:01Z","published":"2024-10-31T17:59:01Z","title":"DELTA: Dense Efficient Long-range 3D Tracking for any video","summary":"  Tracking dense 3D motion from monocular videos remains challenging,\nparticularly when aiming for pixel-level precision over long sequences. We\nintroduce DELTA, a novel method that efficiently tracks every pixel in 3D\nspace, enabling accurate motion estimation across entire videos. Our approach\nleverages a joint global-local attention mechanism for reduced-resolution\ntracking, followed by a transformer-based upsampler to achieve high-resolution\npredictions. Unlike existing methods, which are limited by computational\ninefficiency or sparse tracking, DELTA delivers dense 3D tracking at scale,\nrunning over 8x faster than previous methods while achieving state-of-the-art\naccuracy. Furthermore, we explore the impact of depth representation on\ntracking performance and identify log-depth as the optimal choice. Extensive\nexperiments demonstrate the superiority of DELTA on multiple benchmarks,\nachieving new state-of-the-art results in both 2D and 3D dense tracking tasks.\nOur method provides a robust solution for applications requiring fine-grained,\nlong-term motion tracking in 3D space.\n","authors":["Tuan Duc Ngo","Peiye Zhuang","Chuang Gan","Evangelos Kalogerakis","Sergey Tulyakov","Hsin-Ying Lee","Chaoyang Wang"],"pdf_url":"https://arxiv.org/pdf/2410.24211v2.pdf","comment":"Project Page: https://snap-research.github.io/DELTA/"},{"id":"http://arxiv.org/abs/2402.01335v3","updated":"2024-11-01T16:51:01Z","published":"2024-02-02T11:40:27Z","title":"BehAVE: Behaviour Alignment of Video Game Encodings","summary":"  Domain randomisation enhances the transferability of vision models across\nvisually distinct domains with similar content. However, current methods\nheavily depend on intricate simulation engines, hampering feasibility and\nscalability. This paper introduces BehAVE, a video understanding framework that\nutilises existing commercial video games for domain randomisation without\naccessing their simulation engines. BehAVE taps into the visual diversity of\nvideo games for randomisation and uses textual descriptions of player actions\nto align videos with similar content. We evaluate BehAVE across 25 first-person\nshooter (FPS) games using various video and text foundation models,\ndemonstrating its robustness in domain randomisation. BehAVE effectively aligns\nplayer behavioural patterns and achieves zero-shot transfer to multiple unseen\nFPS games when trained on just one game. In a more challenging scenario, BehAVE\nenhances the zero-shot transferability of foundation models to unseen FPS\ngames, even when trained on a game of a different genre, with improvements of\nup to 22%. BehAVE is available online at https://github.com/nrasajski/BehAVE.\n","authors":["Nemanja Rašajski","Chintan Trivedi","Konstantinos Makantasis","Antonios Liapis","Georgios N. Yannakakis"],"pdf_url":"https://arxiv.org/pdf/2402.01335v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07410v2","updated":"2024-11-01T16:34:04Z","published":"2024-10-09T20:21:43Z","title":"Aligning Motion-Blurred Images Using Contrastive Learning on\n  Overcomplete Pixels","summary":"  We propose a new contrastive objective for learning overcomplete pixel-level\nfeatures that are invariant to motion blur. Other invariances (e.g., pose,\nillumination, or weather) can be learned by applying the corresponding\ntransformations on unlabeled images during self-supervised training. We\nshowcase that a simple U-Net trained with our objective can produce local\nfeatures useful for aligning the frames of an unseen video captured with a\nmoving camera under realistic and challenging conditions. Using a carefully\ndesigned toy example, we also show that the overcomplete pixels can encode the\nidentity of objects in an image and the pixel coordinates relative to these\nobjects.\n","authors":["Leonid Pogorelyuk","Stefan T. Radev"],"pdf_url":"https://arxiv.org/pdf/2410.07410v2.pdf","comment":"8 pages, 3 figures"},{"id":"http://arxiv.org/abs/2410.24204v2","updated":"2024-11-01T16:31:22Z","published":"2024-10-31T17:57:07Z","title":"GeoSplatting: Towards Geometry Guided Gaussian Splatting for\n  Physically-based Inverse Rendering","summary":"  We consider the problem of physically-based inverse rendering using 3D\nGaussian Splatting (3DGS) representations. While recent 3DGS methods have\nachieved remarkable results in novel view synthesis (NVS), accurately capturing\nhigh-fidelity geometry, physically interpretable materials and lighting remains\nchallenging, as it requires precise geometry modeling to provide accurate\nsurface normals, along with physically-based rendering (PBR) techniques to\nensure correct material and lighting disentanglement. Previous 3DGS methods\nresort to approximating surface normals, but often struggle with noisy local\ngeometry, leading to inaccurate normal estimation and suboptimal\nmaterial-lighting decomposition. In this paper, we introduce GeoSplatting, a\nnovel hybrid representation that augments 3DGS with explicit geometric guidance\nand differentiable PBR equations. Specifically, we bridge isosurface and 3DGS\ntogether, where we first extract isosurface mesh from a scalar field, then\nconvert it into 3DGS points and formulate PBR equations for them in a fully\ndifferentiable manner. In GeoSplatting, 3DGS is grounded on the mesh geometry,\nenabling precise surface normal modeling, which facilitates the use of PBR\nframeworks for material decomposition. This approach further maintains the\nefficiency and quality of NVS from 3DGS while ensuring accurate geometry from\nthe isosurface. Comprehensive evaluations across diverse datasets demonstrate\nthe superiority of GeoSplatting, consistently outperforming existing methods\nboth quantitatively and qualitatively.\n","authors":["Kai Ye","Chong Gao","Guanbin Li","Wenzheng Chen","Baoquan Chen"],"pdf_url":"https://arxiv.org/pdf/2410.24204v2.pdf","comment":"Project page: https://pku-vcl-geometry.github.io/GeoSplatting/"},{"id":"http://arxiv.org/abs/2406.00307v4","updated":"2024-11-01T16:26:40Z","published":"2024-06-01T05:41:12Z","title":"HENASY: Learning to Assemble Scene-Entities for Egocentric\n  Video-Language Model","summary":"  Current video-language models (VLMs) rely extensively on instance-level\nalignment between video and language modalities, which presents two major\nlimitations: (1) visual reasoning disobeys the natural perception that humans\ndo in first-person perspective, leading to a lack of reasoning interpretation;\nand (2) learning is limited in capturing inherent fine-grained relationships\nbetween two modalities.\n  In this paper, we take an inspiration from human perception and explore a\ncompositional approach for egocentric video representation. We introduce HENASY\n(Hierarchical ENtities ASsemblY), which includes a spatiotemporal token\ngrouping mechanism to explicitly assemble dynamically evolving scene entities\nthrough time and model their relationship for video representation. By\nleveraging compositional structure understanding, HENASY possesses strong\ninterpretability via visual grounding with free-form text queries. We further\nexplore a suite of multi-grained contrastive losses to facilitate\nentity-centric understandings. This comprises three alignment types:\nvideo-narration, noun-entity, verb-entities alignments.\n  Our method demonstrates strong interpretability in both quantitative and\nqualitative experiments; while maintaining competitive performances on five\ndownstream tasks via zero-shot transfer or as video/text representation,\nincluding video/text retrieval, action recognition, multi-choice query, natural\nlanguage query, and moments query.\n","authors":["Khoa Vo","Thinh Phan","Kashu Yamazaki","Minh Tran","Ngan Le"],"pdf_url":"https://arxiv.org/pdf/2406.00307v4.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2312.14556v3","updated":"2024-11-01T16:12:52Z","published":"2023-12-22T09:29:45Z","title":"CaptainCook4D: A Dataset for Understanding Errors in Procedural\n  Activities","summary":"  Following step-by-step procedures is an essential component of various\nactivities carried out by individuals in their daily lives. These procedures\nserve as a guiding framework that helps to achieve goals efficiently, whether\nit is assembling furniture or preparing a recipe. However, the complexity and\nduration of procedural activities inherently increase the likelihood of making\nerrors. Understanding such procedural activities from a sequence of frames is a\nchallenging task that demands an accurate interpretation of visual information\nand the ability to reason about the structure of the activity. To this end, we\ncollect a new egocentric 4D dataset, CaptainCook4D, comprising 384 recordings\n(94.5 hours) of people performing recipes in real kitchen environments. This\ndataset consists of two distinct types of activity: one in which participants\nadhere to the provided recipe instructions and another in which they deviate\nand induce errors. We provide 5.3K step annotations and 10K fine-grained action\nannotations and benchmark the dataset for the following tasks: supervised error\nrecognition, multistep localization, and procedure learning\n","authors":["Rohith Peddi","Shivvrat Arya","Bharath Challa","Likhitha Pallapothula","Akshay Vyas","Bhavya Gouripeddi","Jikai Wang","Qifan Zhang","Vasundhara Komaragiri","Eric Ragan","Nicholas Ruozzi","Yu Xiang","Vibhav Gogate"],"pdf_url":"https://arxiv.org/pdf/2312.14556v3.pdf","comment":"Accepted to the 2024 Neural Information Processing Systems Datasets\n  and Benchmarks Track, Project Page:\n  https://captaincook4d.github.io/captain-cook/"},{"id":"http://arxiv.org/abs/2410.19869v2","updated":"2024-11-01T16:02:47Z","published":"2024-10-24T00:12:20Z","title":"Comparing YOLO11 and YOLOv8 for instance segmentation of occluded and\n  non-occluded immature green fruits in complex orchard environment","summary":"  This study conducted a comprehensive performance evaluation on YOLO11 and\nYOLOv8, the latest in the \"You Only Look Once\" (YOLO) series, focusing on their\ninstance segmentation capabilities for immature green apples in orchard\nenvironments. YOLO11n-seg achieved the highest mask precision across all\ncategories with a notable score of 0.831, highlighting its effectiveness in\nfruit detection. YOLO11m-seg and YOLO11l-seg excelled in non-occluded and\noccluded fruitlet segmentation with scores of 0.851 and 0.829, respectively.\nAdditionally, YOLO11x-seg led in mask recall for all categories, achieving a\nscore of 0.815, with YOLO11m-seg performing best for non-occluded immature\ngreen fruitlets at 0.858 and YOLOv8x-seg leading the occluded category with\n0.800. In terms of mean average precision at a 50\\% intersection over union\n(mAP@50), YOLO11m-seg consistently outperformed, registering the highest scores\nfor both box and mask segmentation, at 0.876 and 0.860 for the \"All\" class and\n0.908 and 0.909 for non-occluded immature fruitlets, respectively. YOLO11l-seg\nand YOLOv8l-seg shared the top box mAP@50 for occluded immature fruitlets at\n0.847, while YOLO11m-seg achieved the highest mask mAP@50 of 0.810. Despite the\nadvancements in YOLO11, YOLOv8n surpassed its counterparts in image processing\nspeed, with an impressive inference speed of 3.3 milliseconds, compared to the\nfastest YOLO11 series model at 4.8 milliseconds, underscoring its suitability\nfor real-time agricultural applications related to complex green fruit\nenvironments.\n","authors":["Ranjan Sapkota","Manoj Karkee"],"pdf_url":"https://arxiv.org/pdf/2410.19869v2.pdf","comment":"16 Pages, 10 Figures, 3 Tables"},{"id":"http://arxiv.org/abs/2409.00877v2","updated":"2024-11-01T15:41:56Z","published":"2024-09-02T00:11:48Z","title":"Digital Twins in Additive Manufacturing: A Systematic Review","summary":"  Digital Twins (DTs) are becoming popular in Additive Manufacturing (AM) due\nto their ability to create virtual replicas of physical components of AM\nmachines, which helps in real-time production monitoring. Advanced techniques\nsuch as Machine Learning (ML), Augmented Reality (AR), and simulation-based\nmodels play key roles in developing intelligent and adaptable DTs in\nmanufacturing processes. However, questions remain regarding scalability, the\nintegration of high-quality data, and the computational power required for\nreal-time applications in developing DTs. Understanding the current state of\nDTs in AM is essential to address these challenges and fully utilize their\npotential in advancing AM processes. Considering this opportunity, this work\naims to provide a comprehensive overview of DTs in AM by addressing the\nfollowing four research questions: (1) What are the key types of DTs used in AM\nand their specific applications? (2) What are the recent developments and\nimplementations of DTs? (3) How are DTs employed in process improvement and\nhybrid manufacturing? (4) How are DTs integrated with Industry 4.0\ntechnologies? By discussing current applications and techniques, we aim to\noffer a better understanding and potential future research directions for\nresearchers and practitioners in AM and DTs.\n","authors":["Md Manjurul Ahsan","Yingtao Liu","Shivakumar Raman","Zahed Siddique"],"pdf_url":"https://arxiv.org/pdf/2409.00877v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.15615v4","updated":"2024-11-01T15:13:01Z","published":"2023-07-28T15:22:34Z","title":"A survey on deep learning in medical image registration: new\n  technologies, uncertainty, evaluation metrics, and beyond","summary":"  Deep learning technologies have dramatically reshaped the field of medical\nimage registration over the past decade. The initial developments, such as\nregression-based and U-Net-based networks, established the foundation for deep\nlearning in image registration. Subsequent progress has been made in various\naspects of deep learning-based registration, including similarity measures,\ndeformation regularizations, network architectures, and uncertainty estimation.\nThese advancements have not only enriched the field of image registration but\nhave also facilitated its application in a wide range of tasks, including atlas\nconstruction, multi-atlas segmentation, motion estimation, and 2D-3D\nregistration. In this paper, we present a comprehensive overview of the most\nrecent advancements in deep learning-based image registration. We begin with a\nconcise introduction to the core concepts of deep learning-based image\nregistration. Then, we delve into innovative network architectures, loss\nfunctions specific to registration, and methods for estimating registration\nuncertainty. Additionally, this paper explores appropriate evaluation metrics\nfor assessing the performance of deep learning models in registration tasks.\nFinally, we highlight the practical applications of these novel techniques in\nmedical imaging and discuss the future prospects of deep learning-based image\nregistration.\n","authors":["Junyu Chen","Yihao Liu","Shuwen Wei","Zhangxing Bian","Shalini Subramanian","Aaron Carass","Jerry L. Prince","Yong Du"],"pdf_url":"https://arxiv.org/pdf/2307.15615v4.pdf","comment":"Accepted to Medical Image Analysis ((c) MedIA). A list of\n  open-sourced code from the papers reviewed has been organized and is\n  available at https://bit.ly/3QgFJ9z"},{"id":"http://arxiv.org/abs/2406.08773v3","updated":"2024-11-01T14:55:50Z","published":"2024-06-13T03:05:36Z","title":"DenoiseRep: Denoising Model for Representation Learning","summary":"  The denoising model has been proven a powerful generative model but has\nlittle exploration of discriminative tasks. Representation learning is\nimportant in discriminative tasks, which is defined as \"learning\nrepresentations (or features) of the data that make it easier to extract useful\ninformation when building classifiers or other predictors\". In this paper, we\npropose a novel Denoising Model for Representation Learning (DenoiseRep) to\nimprove feature discrimination with joint feature extraction and denoising.\nDenoiseRep views each embedding layer in a backbone as a denoising layer,\nprocessing the cascaded embedding layers as if we are recursively denoise\nfeatures step-by-step. This unifies the frameworks of feature extraction and\ndenoising, where the former progressively embeds features from low-level to\nhigh-level, and the latter recursively denoises features step-by-step. After\nthat, DenoiseRep fuses the parameters of feature extraction and denoising\nlayers, and theoretically demonstrates its equivalence before and after the\nfusion, thus making feature denoising computation-free. DenoiseRep is a\nlabel-free algorithm that incrementally improves features but also\ncomplementary to the label if available. Experimental results on various\ndiscriminative vision tasks, including re-identification (Market-1501,\nDukeMTMC-reID, MSMT17, CUHK-03, vehicleID), image classification (ImageNet,\nUB200, Oxford-Pet, Flowers), object detection (COCO), image segmentation\n(ADE20K) show stability and impressive improvements. We also validate its\neffectiveness on the CNN (ResNet) and Transformer (ViT, Swin, Vmamda)\narchitectures.\n","authors":["Zhengrui Xu","Guan'an Wang","Xiaowen Huang","Jitao Sang"],"pdf_url":"https://arxiv.org/pdf/2406.08773v3.pdf","comment":"Accepted by NeurIPS 2024,oral"},{"id":"http://arxiv.org/abs/2312.03701v4","updated":"2024-11-01T14:48:57Z","published":"2023-12-06T18:59:31Z","title":"Return of Unconditional Generation: A Self-supervised Representation\n  Generation Method","summary":"  Unconditional generation -- the problem of modeling data distribution without\nrelying on human-annotated labels -- is a long-standing and fundamental\nchallenge in generative models, creating a potential of learning from\nlarge-scale unlabeled data. In the literature, the generation quality of an\nunconditional method has been much worse than that of its conditional\ncounterpart. This gap can be attributed to the lack of semantic information\nprovided by labels. In this work, we show that one can close this gap by\ngenerating semantic representations in the representation space produced by a\nself-supervised encoder. These representations can be used to condition the\nimage generator. This framework, called Representation-Conditioned Generation\n(RCG), provides an effective solution to the unconditional generation problem\nwithout using labels. Through comprehensive experiments, we observe that RCG\nsignificantly improves unconditional generation quality: e.g., it achieves a\nnew state-of-the-art FID of 2.15 on ImageNet 256x256, largely reducing the\nprevious best of 5.91 by a relative 64%. Our unconditional results are situated\nin the same tier as the leading class-conditional ones. We hope these\nencouraging observations will attract the community's attention to the\nfundamental problem of unconditional generation. Code is available at\nhttps://github.com/LTH14/rcg.\n","authors":["Tianhong Li","Dina Katabi","Kaiming He"],"pdf_url":"https://arxiv.org/pdf/2312.03701v4.pdf","comment":"Neurips 2024 (Oral)"},{"id":"http://arxiv.org/abs/2312.07955v2","updated":"2024-11-01T14:45:44Z","published":"2023-12-13T08:01:15Z","title":"Erasing Self-Supervised Learning Backdoor by Cluster Activation Masking","summary":"  Self-Supervised Learning (SSL) is an effective paradigm for learning\nrepresentations from unlabeled data, such as text, images, and videos. However,\nresearchers have recently found that SSL is vulnerable to backdoor attacks. The\nattacker can embed hidden SSL backdoors via a few poisoned examples in the\ntraining dataset and maliciously manipulate the behavior of downstream models.\nTo defend against SSL backdoor attacks, a feasible route is to detect and\nremove the poisonous samples in the training set. However, the existing SSL\nbackdoor defense method fails to detect the poisonous samples precisely. In\nthis paper, we propose to erase the SSL backdoor by cluster activation masking\nand propose a novel PoisonCAM method. After obtaining the threat model trained\non the poisoned dataset, our method can precisely detect poisonous samples\nbased on the assumption that masking the backdoor trigger can effectively\nchange the activation of a downstream clustering model. In experiments, our\nPoisonCAM achieves 96\\% accuracy for backdoor trigger detection compared to 3\\%\nof the state-of-the-art method on poisoned ImageNet-100. Moreover, our proposed\nPoisonCAM significantly improves the performance of the trained SSL model under\nbackdoor attacks compared to the state-of-the-art method. Our code, data, and\ntrained models will be open once this paper is accepted.\n","authors":["Shengsheng Qian","Dizhan Xue","Yifei Wang","Shengjie Zhang","Huaiwen Zhang","Changsheng Xu"],"pdf_url":"https://arxiv.org/pdf/2312.07955v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11838v3","updated":"2024-11-01T14:45:36Z","published":"2024-06-17T17:59:58Z","title":"Autoregressive Image Generation without Vector Quantization","summary":"  Conventional wisdom holds that autoregressive models for image generation are\ntypically accompanied by vector-quantized tokens. We observe that while a\ndiscrete-valued space can facilitate representing a categorical distribution,\nit is not a necessity for autoregressive modeling. In this work, we propose to\nmodel the per-token probability distribution using a diffusion procedure, which\nallows us to apply autoregressive models in a continuous-valued space. Rather\nthan using categorical cross-entropy loss, we define a Diffusion Loss function\nto model the per-token probability. This approach eliminates the need for\ndiscrete-valued tokenizers. We evaluate its effectiveness across a wide range\nof cases, including standard autoregressive models and generalized masked\nautoregressive (MAR) variants. By removing vector quantization, our image\ngenerator achieves strong results while enjoying the speed advantage of\nsequence modeling. We hope this work will motivate the use of autoregressive\ngeneration in other continuous-valued domains and applications. Code is\navailable at: https://github.com/LTH14/mar.\n","authors":["Tianhong Li","Yonglong Tian","He Li","Mingyang Deng","Kaiming He"],"pdf_url":"https://arxiv.org/pdf/2406.11838v3.pdf","comment":"Neurips 2024 (Spotlight). Code: https://github.com/LTH14/mar"},{"id":"http://arxiv.org/abs/2407.15794v4","updated":"2024-11-01T14:19:14Z","published":"2024-07-22T16:52:32Z","title":"Disentangling spatio-temporal knowledge for weakly supervised object\n  detection and segmentation in surgical video","summary":"  Weakly supervised video object segmentation (WSVOS) enables the\nidentification of segmentation maps without requiring an extensive training\ndataset of object masks, relying instead on coarse video labels indicating\nobject presence. Current state-of-the-art methods either require multiple\nindependent stages of processing that employ motion cues or, in the case of\nend-to-end trainable networks, lack in segmentation accuracy, in part due to\nthe difficulty of learning segmentation maps from videos with transient object\npresence. This limits the application of WSVOS for semantic annotation of\nsurgical videos where multiple surgical tools frequently move in and out of the\nfield of view, a problem that is more difficult than typically encountered in\nWSVOS. This paper introduces Video Spatio-Temporal Disentanglement Networks\n(VDST-Net), a framework to disentangle spatiotemporal information using\nsemi-decoupled knowledge distillation to predict high-quality class activation\nmaps (CAMs). A teacher network designed to resolve temporal conflicts when\nspecifics about object location and timing in the video are not provided works\nwith a student network that integrates information over time by leveraging\ntemporal dependencies. We demonstrate the efficacy of our framework on a public\nreference dataset and on a more challenging surgical video dataset where\nobjects are, on average, present in less than 60\\% of annotated frames. Our\nmethod outperforms state-of-the-art techniques and generates superior\nsegmentation masks under video-level weak supervision.\n","authors":["Guiqiu Liao","Matjaz Jogan","Sai Koushik","Eric Eaton","Daniel A. Hashimoto"],"pdf_url":"https://arxiv.org/pdf/2407.15794v4.pdf","comment":"Accepted to IEEE/CVF Winter Conference on Applications of Computer\n  Vision (WACV)"},{"id":"http://arxiv.org/abs/2310.16020v3","updated":"2024-11-01T13:59:05Z","published":"2023-10-24T17:30:26Z","title":"ConvBKI: Real-Time Probabilistic Semantic Mapping Network with\n  Quantifiable Uncertainty","summary":"  In this paper, we develop a modular neural network for real-time\n{\\color{black}(> 10 Hz)} semantic mapping in uncertain environments, which\nexplicitly updates per-voxel probabilistic distributions within a neural\nnetwork layer. Our approach combines the reliability of classical probabilistic\nalgorithms with the performance and efficiency of modern neural networks.\nAlthough robotic perception is often divided between modern differentiable\nmethods and classical explicit methods, a union of both is necessary for\nreal-time and trustworthy performance. We introduce a novel Convolutional\nBayesian Kernel Inference (ConvBKI) layer which incorporates semantic\nsegmentation predictions online into a 3D map through a depthwise convolution\nlayer by leveraging conjugate priors. We compare ConvBKI against\nstate-of-the-art deep learning approaches and probabilistic algorithms for\nmapping to evaluate reliability and performance. We also create a Robot\nOperating System (ROS) package of ConvBKI and test it on real-world\nperceptually challenging off-road driving data.\n","authors":["Joey Wilson","Yuewei Fu","Joshua Friesen","Parker Ewen","Andrew Capodieci","Paramsothy Jayakumar","Kira Barton","Maani Ghaffari"],"pdf_url":"https://arxiv.org/pdf/2310.16020v3.pdf","comment":"arXiv admin note: text overlap with arXiv:2209.10663"},{"id":"http://arxiv.org/abs/2311.12056v3","updated":"2024-11-01T12:54:28Z","published":"2023-11-18T13:55:05Z","title":"Kuro Siwo: 33 billion $m^2$ under the water. A global multi-temporal\n  satellite dataset for rapid flood mapping","summary":"  Global floods, exacerbated by climate change, pose severe threats to human\nlife, infrastructure, and the environment. Recent catastrophic events in\nPakistan and New Zealand underscore the urgent need for precise flood mapping\nto guide restoration efforts, understand vulnerabilities, and prepare for\nfuture occurrences. While Synthetic Aperture Radar (SAR) remote sensing offers\nday-and-night, all-weather imaging capabilities, its application in deep\nlearning for flood segmentation is limited by the lack of large annotated\ndatasets. To address this, we introduce Kuro Siwo, a manually annotated\nmulti-temporal dataset, spanning 43 flood events globally. Our dataset maps\nmore than 338 billion $m^2$ of land, with 33 billion designated as either\nflooded areas or permanent water bodies. Kuro Siwo includes a highly processed\nproduct optimized for flood mapping based on SAR Ground Range Detected, and a\nprimal SAR Single Look Complex product with minimal preprocessing, designed to\npromote research on the exploitation of both the phase and amplitude\ninformation and to offer maximum flexibility for downstream task preprocessing.\nTo leverage advances in large scale self-supervised pretraining methods for\nremote sensing data, we augment Kuro Siwo with a large unlabeled set of SAR\nsamples. Finally, we provide an extensive benchmark, namely BlackBench,\noffering strong baselines for a diverse set of flood events from Europe,\nAmerica, Africa, Asia and Australia.\n","authors":["Nikolaos Ioannis Bountos","Maria Sdraka","Angelos Zavras","Ilektra Karasante","Andreas Karavias","Themistocles Herekakis","Angeliki Thanasou","Dimitrios Michail","Ioannis Papoutsis"],"pdf_url":"https://arxiv.org/pdf/2311.12056v3.pdf","comment":"Accepted at the 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024) Track on Datasets and Benchmarks"},{"id":"http://arxiv.org/abs/2409.15246v3","updated":"2024-11-01T12:49:19Z","published":"2024-09-23T17:42:05Z","title":"On-Air Deep Learning Integrated Semantic Inference Models for Enhanced\n  Earth Observation Satellite Networks","summary":"  Earth Observation (EO) systems are crucial for cartography, disaster\nsurveillance, and resource administration. Nonetheless, they encounter\nconsiderable obstacles in the processing and transmission of extensive data,\nespecially in specialized domains such as precision agriculture and real-time\ndisaster response. Earth observation satellites, outfitted with remote sensing\ntechnology, gather data from onboard sensors and IoT-enabled terrestrial\nobjects, delivering important information remotely. Domain-adapted Large\nLanguage Models (LLMs) provide a solution by enabling the integration of raw\nand processed EO data. Through domain adaptation, LLMs improve the assimilation\nand analysis of many data sources, tackling the intricacies of specialized\ndatasets in agriculture and disaster response. This data synthesis, directed by\nLLMs, enhances the precision and pertinence of conveyed information. This study\nprovides a thorough examination of using semantic inference and deep learning\nfor sophisticated EO systems. It presents an innovative architecture for\nsemantic communication in EO satellite networks, designed to improve data\ntransmission efficiency using semantic processing methodologies. Recent\nadvancements in onboard processing technologies enable dependable, adaptable,\nand energy-efficient data management in orbit. These improvements guarantee\nreliable performance in adverse space circumstances using radiation-hardened\nand reconfigurable technology. Collectively, these advancements enable\nnext-generation satellite missions with improved processing capabilities,\ncrucial for operational flexibility and real-time decision-making in 6G\nsatellite communication.\n","authors":["Hong-fu Chou","Vu Nguyen Ha","Prabhu Thiruvasagam","Thanh-Dung Le","Geoffrey Eappen","Ti Ti Nguyen","Luis M. Garces-Socarras","Jorge L. Gonzalez-Rios","Juan Carlos Merlano-Duncan","Symeon Chatzinotas"],"pdf_url":"https://arxiv.org/pdf/2409.15246v3.pdf","comment":"17 pages, 7 figures, Journal"},{"id":"http://arxiv.org/abs/2405.14864v2","updated":"2024-11-01T12:46:26Z","published":"2024-05-23T17:59:40Z","title":"Video Diffusion Models are Training-free Motion Interpreter and\n  Controller","summary":"  Video generation primarily aims to model authentic and customized motion\nacross frames, making understanding and controlling the motion a crucial topic.\nMost diffusion-based studies on video motion focus on motion customization with\ntraining-based paradigms, which, however, demands substantial training\nresources and necessitates retraining for diverse models. Crucially, these\napproaches do not explore how video diffusion models encode cross-frame motion\ninformation in their features, lacking interpretability and transparency in\ntheir effectiveness. To answer this question, this paper introduces a novel\nperspective to understand, localize, and manipulate motion-aware features in\nvideo diffusion models. Through analysis using Principal Component Analysis\n(PCA), our work discloses that robust motion-aware feature already exists in\nvideo diffusion models. We present a new MOtion FeaTure (MOFT) by eliminating\ncontent correlation information and filtering motion channels. MOFT provides a\ndistinct set of benefits, including the ability to encode comprehensive motion\ninformation with clear interpretability, extraction without the need for\ntraining, and generalizability across diverse architectures. Leveraging MOFT,\nwe propose a novel training-free video motion control framework. Our method\ndemonstrates competitive performance in generating natural and faithful motion,\nproviding architecture-agnostic insights and applicability in a variety of\ndownstream tasks.\n","authors":["Zeqi Xiao","Yifan Zhou","Shuai Yang","Xingang Pan"],"pdf_url":"https://arxiv.org/pdf/2405.14864v2.pdf","comment":"Project Page: https://xizaoqu.github.io/moft/"},{"id":"http://arxiv.org/abs/2410.20883v2","updated":"2024-11-01T12:42:49Z","published":"2024-10-28T10:04:40Z","title":"Improving Generalization in Visual Reasoning via Self-Ensemble","summary":"  The cognitive faculty of visual reasoning necessitates the integration of\nmultimodal perceptual processing and commonsense and external knowledge of the\nworld. In recent years, a plethora of large vision-language models (LVLMs) have\nbeen proposed, demonstrating outstanding power and exceptional proficiency in\ncommonsense reasoning across diverse domains and tasks. Nevertheless, training\nsuch LVLMs requires a lot of costly resources. Recent approaches, instead of\ntraining LVLMs from scratch on various large datasets, focus on exploring ways\nto take advantage of the capabilities of many different LVLMs, such as ensemble\nmethods. In this work, we propose self-ensemble, a novel method that improves\nthe generalization and visual reasoning of the model without updating any\nparameters, a training-free method. Our key insight is that we realized that\nLVLM itself can ensemble without the need for any other LVLMs, which helps to\nunlock their internal capabilities. Extensive experiments on various benchmarks\ndemonstrate the effectiveness of our method in achieving state-of-the-art\n(SOTA) performance on SketchyVQA, Outside Knowledge VQA, and\nout-of-distribution VQA tasks.\n","authors":["Tien-Huy Nguyen","Quang-Khai Tran","Anh-Tuan Quang-Hoang"],"pdf_url":"https://arxiv.org/pdf/2410.20883v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23831v2","updated":"2024-11-01T12:11:29Z","published":"2024-10-31T11:21:21Z","title":"FRoundation: Are Foundation Models Ready for Face Recognition?","summary":"  Foundation models are predominantly trained in an unsupervised or\nself-supervised manner on highly diverse and large-scale datasets, making them\nbroadly applicable to various downstream tasks. In this work, we investigate\nfor the first time whether such models are suitable for the specific domain of\nface recognition. We further propose and demonstrate the adaptation of these\nmodels for face recognition across different levels of data availability.\nExtensive experiments are conducted on multiple foundation models and datasets\nof varying scales for training and fine-tuning, with evaluation on a wide range\nof benchmarks. Our results indicate that, despite their versatility,\npre-trained foundation models underperform in face recognition compared to\nsimilar architectures trained specifically for this task. However, fine-tuning\nfoundation models yields promising results, often surpassing models trained\nfrom scratch when training data is limited. Even with access to large-scale\nface recognition training datasets, fine-tuned foundation models perform\ncomparably to models trained from scratch, but with lower training\ncomputational costs and without relying on the assumption of extensive data\navailability. Our analysis also explores bias in face recognition, with\nslightly higher bias observed in some settings when using foundation models.\n","authors":["Tahar Chettaoui","Naser Damer","Fadi Boutros"],"pdf_url":"https://arxiv.org/pdf/2410.23831v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.10188v5","updated":"2024-11-01T10:57:37Z","published":"2024-08-19T17:48:08Z","title":"LongVILA: Scaling Long-Context Visual Language Models for Long Videos","summary":"  Long-context capability is critical for multi-modal foundation models,\nespecially for long video understanding. We introduce LongVILA, a full-stack\nsolution for long-context visual-language models by co-designing the algorithm\nand system. For model training, we upgrade existing VLMs to support long video\nunderstanding by incorporating two additional stages, i.e., long context\nextension and long video supervised fine-tuning. However, training on long\nvideo is computationally and memory intensive. We introduce the long-context\nMulti-Modal Sequence Parallelism (MM-SP) system that efficiently parallelizes\nlong video training and inference, enabling 2M context length training on 256\nGPUs without any gradient checkpointing. LongVILA efficiently extends the\nnumber of video frames of VILA from 8 to 2048, improving the long video\ncaptioning score from 2.00 to 3.26 (out of 5), achieving 99.8% accuracy in\n6,000-frame (more than 1 million tokens) video needle-in-a-haystack.\nLongVILA-7B demonstrates strong accuracy on the VideoMME benchmark, i.e., 61.8%\nwith subtitle. Besides, MM-SP is 2.1x - 5.7x faster than ring style sequence\nparallelism and 1.1x - 1.4x faster than Megatron with a hybrid context and\ntensor parallelism. Moreover, it seamlessly integrates with Hugging Face\nTransformers.\n","authors":["Fuzhao Xue","Yukang Chen","Dacheng Li","Qinghao Hu","Ligeng Zhu","Xiuyu Li","Yunhao Fang","Haotian Tang","Shang Yang","Zhijian Liu","Ethan He","Hongxu Yin","Pavlo Molchanov","Jan Kautz","Linxi Fan","Yuke Zhu","Yao Lu","Song Han"],"pdf_url":"https://arxiv.org/pdf/2408.10188v5.pdf","comment":"Code and models are available at\n  https://github.com/NVlabs/VILA/blob/main/LongVILA.md"},{"id":"http://arxiv.org/abs/2410.20359v2","updated":"2024-11-01T09:33:29Z","published":"2024-10-27T07:25:11Z","title":"Conditional GAN for Enhancing Diffusion Models in Efficient and\n  Authentic Global Gesture Generation from Audios","summary":"  Audio-driven simultaneous gesture generation is vital for human-computer\ncommunication, AI games, and film production. While previous research has shown\npromise, there are still limitations. Methods based on VAEs are accompanied by\nissues of local jitter and global instability, whereas methods based on\ndiffusion models are hampered by low generation efficiency. This is because the\ndenoising process of DDPM in the latter relies on the assumption that the noise\nadded at each step is sampled from a unimodal distribution, and the noise\nvalues are small. DDIM borrows the idea from the Euler method for solving\ndifferential equations, disrupts the Markov chain process, and increases the\nnoise step size to reduce the number of denoising steps, thereby accelerating\ngeneration. However, simply increasing the step size during the step-by-step\ndenoising process causes the results to gradually deviate from the original\ndata distribution, leading to a significant drop in the quality of the\ngenerated actions and the emergence of unnatural artifacts. In this paper, we\nbreak the assumptions of DDPM and achieves breakthrough progress in denoising\nspeed and fidelity. Specifically, we introduce a conditional GAN to capture\naudio control signals and implicitly match the multimodal denoising\ndistribution between the diffusion and denoising steps within the same sampling\nstep, aiming to sample larger noise values and apply fewer denoising steps for\nhigh-speed generation.\n","authors":["Yongkang Cheng","Mingjiang Liang","Shaoli Huang","Gaoge Han","Jifeng Ning","Wei Liu"],"pdf_url":"https://arxiv.org/pdf/2410.20359v2.pdf","comment":"Accepted by WACV 2025 (Round 1)"},{"id":"http://arxiv.org/abs/2410.20358v2","updated":"2024-11-01T09:20:53Z","published":"2024-10-27T07:19:39Z","title":"RopeTP: Global Human Motion Recovery via Integrating Robust Pose\n  Estimation with Diffusion Trajectory Prior","summary":"  We present RopeTP, a novel framework that combines Robust pose estimation\nwith a diffusion Trajectory Prior to reconstruct global human motion from\nvideos. At the heart of RopeTP is a hierarchical attention mechanism that\nsignificantly improves context awareness, which is essential for accurately\ninferring the posture of occluded body parts. This is achieved by exploiting\nthe relationships with visible anatomical structures, enhancing the accuracy of\nlocal pose estimations. The improved robustness of these local estimations\nallows for the reconstruction of precise and stable global trajectories.\nAdditionally, RopeTP incorporates a diffusion trajectory model that predicts\nrealistic human motion from local pose sequences. This model ensures that the\ngenerated trajectories are not only consistent with observed local actions but\nalso unfold naturally over time, thereby improving the realism and stability of\n3D human motion reconstruction. Extensive experimental validation shows that\nRopeTP surpasses current methods on two benchmark datasets, particularly\nexcelling in scenarios with occlusions. It also outperforms methods that rely\non SLAM for initial camera estimates and extensive optimization, delivering\nmore accurate and realistic trajectories.\n","authors":["Mingjiang Liang","Yongkang Cheng","Hualin Liang","Shaoli Huang","Wei Liu"],"pdf_url":"https://arxiv.org/pdf/2410.20358v2.pdf","comment":"Accepted by WACV 2025 (Round 1)"},{"id":"http://arxiv.org/abs/2402.13629v3","updated":"2024-11-01T08:56:48Z","published":"2024-02-21T09:06:04Z","title":"Adversarial Purification and Fine-tuning for Robust UDC Image\n  Restoration","summary":"  This study delves into the enhancement of Under-Display Camera (UDC) image\nrestoration models, focusing on their robustness against adversarial attacks.\nDespite its innovative approach to seamless display integration, UDC technology\nfaces unique image degradation challenges exacerbated by the susceptibility to\nadversarial perturbations. Our research initially conducts an in-depth\nrobustness evaluation of deep-learning-based UDC image restoration models by\nemploying several white-box and black-box attacking methods. This evaluation is\npivotal in understanding the vulnerabilities of current UDC image restoration\ntechniques. Following the assessment, we introduce a defense framework\nintegrating adversarial purification with subsequent fine-tuning processes.\nFirst, our approach employs diffusion-based adversarial purification,\neffectively neutralizing adversarial perturbations. Then, we apply the\nfine-tuning methodologies to refine the image restoration models further,\nensuring that the quality and fidelity of the restored images are maintained.\nThe effectiveness of our proposed approach is validated through extensive\nexperiments, showing marked improvements in resilience against typical\nadversarial attacks.\n","authors":["Zhenbo Song","Zhenyuan Zhang","Kaihao Zhang","Zhaoxin Fan","Jianfeng Lu"],"pdf_url":"https://arxiv.org/pdf/2402.13629v3.pdf","comment":"Failure to meet expectations"},{"id":"http://arxiv.org/abs/2410.23629v2","updated":"2024-11-01T08:38:21Z","published":"2024-10-31T04:42:43Z","title":"Posture-Informed Muscular Force Learning for Robust Hand Pressure\n  Estimation","summary":"  We present PiMForce, a novel framework that enhances hand pressure estimation\nby leveraging 3D hand posture information to augment forearm surface\nelectromyography (sEMG) signals. Our approach utilizes detailed spatial\ninformation from 3D hand poses in conjunction with dynamic muscle activity from\nsEMG to enable accurate and robust whole-hand pressure measurements under\ndiverse hand-object interactions. We also developed a multimodal data\ncollection system that combines a pressure glove, an sEMG armband, and a\nmarkerless finger-tracking module. We created a comprehensive dataset from 21\nparticipants, capturing synchronized data of hand posture, sEMG signals, and\nexerted hand pressure across various hand postures and hand-object interaction\nscenarios using our collection system. Our framework enables precise hand\npressure estimation in complex and natural interaction scenarios. Our approach\nsubstantially mitigates the limitations of traditional sEMG-based or\nvision-based methods by integrating 3D hand posture information with sEMG\nsignals. Video demos, data, and code are available online.\n","authors":["Kyungjin Seo","Junghoon Seo","Hanseok Jeong","Sangpil Kim","Sang Ho Yoon"],"pdf_url":"https://arxiv.org/pdf/2410.23629v2.pdf","comment":"Accepted to NeurIPS 2024. Project Page Link:\n  https://pimforce.hcitech.org/"},{"id":"http://arxiv.org/abs/2408.07832v4","updated":"2024-11-01T07:41:04Z","published":"2024-07-31T14:49:35Z","title":"LADDER: Language Driven Slice Discovery and Error Rectification","summary":"  Error slice discovery associates structured patterns with model errors.\nExisting methods discover error slices by clustering the error-prone samples\nwith similar patterns or assigning discrete attributes to each sample for\npost-hoc analysis. While these methods aim for interpretability and easier\nmitigation through reweighting or rebalancing, they may not capture the full\ncomplexity of error patterns due to incomplete or missing attributes. Contrary\nto the existing approach, this paper utilizes the reasoning capabilities of the\nLarge Language Model (LLM) to analyze complex error patterns and generate\ntestable hypotheses. This paper proposes LADDER: Language Driven slice\nDiscovery and Error Rectification. It first projects the model's representation\ninto a language-aligned feature space (eg CLIP) to preserve semantics in the\noriginal model feature space. This ensures the accurate retrieval of sentences\nthat highlight the model's errors. Next, the LLM utilizes the sentences and\ngenerates hypotheses to discover error slices. Finally, we mitigate the error\nby fine-tuning the classification head by creating a group-balanced dataset\nusing the hypotheses. Our entire method does not require any attribute\nannotation, either explicitly or through external tagging models. We validate\nour method with \\textbf{five} image classification datasets. The code is\navailable (https://github.com/batmanlab/Ladder).\n","authors":["Shantanu Ghosh","Rayan Syed","Chenyu Wang","Clare B. Poynton","Shyam Visweswaran","Kayhan Batmanghelich"],"pdf_url":"https://arxiv.org/pdf/2408.07832v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23806v2","updated":"2024-11-01T07:25:38Z","published":"2024-10-31T10:46:11Z","title":"Human Action Recognition (HAR) Using Skeleton-based Spatial Temporal\n  Relative Transformer Network: ST-RTR","summary":"  Human Action Recognition (HAR) is an interesting research area in\nhuman-computer interaction used to monitor the activities of elderly and\ndisabled individuals affected by physical and mental health. In the recent era,\nskeleton-based HAR has received much attention because skeleton data has shown\nthat it can handle changes in striking, body size, camera views, and complex\nbackgrounds. One key characteristic of ST-GCN is automatically learning spatial\nand temporal patterns from skeleton sequences. It has some limitations, as this\nmethod only works for short-range correlation due to its limited receptive\nfield. Consequently, understanding human action requires long-range\ninterconnection. To address this issue, we developed a spatial-temporal\nrelative transformer ST-RTR model. The ST-RTR includes joint and relay nodes,\nwhich allow efficient communication and data transmission within the network.\nThese nodes help to break the inherent spatial and temporal skeleton\ntopologies, which enables the model to understand long-range human action\nbetter. Furthermore, we combine ST-RTR with a fusion model for further\nperformance improvements. To assess the performance of the ST-RTR method, we\nconducted experiments on three skeleton-based HAR benchmarks: NTU RGB+D 60, NTU\nRGB+D 120, and UAV-Human. It boosted CS and CV by 2.11 % and 1.45% on NTU RGB+D\n60, 1.25% and 1.05% on NTU RGB+D 120. On UAV-Human datasets, accuracy improved\nby 2.54%. The experimental outcomes explain that the proposed ST-RTR model\nsignificantly improves action recognition associated with the standard ST-GCN\nmethod.\n","authors":["Faisal Mehmood","Enqing Chen","Touqeer Abbas","Samah M. Alzanin"],"pdf_url":"https://arxiv.org/pdf/2410.23806v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.03507v6","updated":"2024-11-01T07:04:10Z","published":"2024-04-04T15:10:24Z","title":"DQ-DETR: DETR with Dynamic Query for Tiny Object Detection","summary":"  Despite previous DETR-like methods having performed successfully in generic\nobject detection, tiny object detection is still a challenging task for them\nsince the positional information of object queries is not customized for\ndetecting tiny objects, whose scale is extraordinarily smaller than general\nobjects. Also, DETR-like methods using a fixed number of queries make them\nunsuitable for aerial datasets, which only contain tiny objects, and the\nnumbers of instances are imbalanced between different images. Thus, we present\na simple yet effective model, named DQ-DETR, which consists of three different\ncomponents: categorical counting module, counting-guided feature enhancement,\nand dynamic query selection to solve the above-mentioned problems. DQ-DETR uses\nthe prediction and density maps from the categorical counting module to\ndynamically adjust the number of object queries and improve the positional\ninformation of queries. Our model DQ-DETR outperforms previous CNN-based and\nDETR-like methods, achieving state-of-the-art mAP 30.2% on the AI-TOD-V2\ndataset, which mostly consists of tiny objects. Our code will be available at\nhttps://github.com/hoiliu-0801/DQ-DETR.\n","authors":["Yi-Xin Huang","Hou-I Liu","Hong-Han Shuai","Wen-Huang Cheng"],"pdf_url":"https://arxiv.org/pdf/2404.03507v6.pdf","comment":"Accepted by ECCV 2024. Our code will be available at\n  https://github.com/hoiliu-0801/DQ-DETR"},{"id":"http://arxiv.org/abs/2405.17673v2","updated":"2024-11-01T06:22:30Z","published":"2024-05-27T21:50:16Z","title":"Fast Samplers for Inverse Problems in Iterative Refinement Models","summary":"  Constructing fast samplers for unconditional diffusion and flow-matching\nmodels has received much attention recently; however, existing methods for\nsolving inverse problems, such as super-resolution, inpainting, or deblurring,\nstill require hundreds to thousands of iterative steps to obtain high-quality\nresults. We propose a plug-and-play framework for constructing efficient\nsamplers for inverse problems, requiring only pre-trained diffusion or\nflow-matching models. We present Conditional Conjugate Integrators, which\nleverage the specific form of the inverse problem to project the respective\nconditional diffusion/flow dynamics into a more amenable space for sampling.\nOur method complements popular posterior approximation methods for solving\ninverse problems using diffusion/flow models. We evaluate the proposed method's\nperformance on various linear image restoration tasks across multiple datasets,\nemploying diffusion and flow-matching models. Notably, on challenging inverse\nproblems like 4x super-resolution on the ImageNet dataset, our method can\ngenerate high-quality samples in as few as 5 conditional sampling steps and\noutperforms competing baselines requiring 20-1000 steps. Our code will be\npublicly available at https://github.com/mandt-lab/c-pigdm\n","authors":["Kushagra Pandey","Ruihan Yang","Stephan Mandt"],"pdf_url":"https://arxiv.org/pdf/2405.17673v2.pdf","comment":"43 pages, NeurIPS'24 Camera Ready"},{"id":"http://arxiv.org/abs/2405.15677v3","updated":"2024-11-01T06:19:24Z","published":"2024-05-24T16:17:35Z","title":"SMART: Scalable Multi-agent Real-time Motion Generation via Next-token\n  Prediction","summary":"  Data-driven autonomous driving motion generation tasks are frequently\nimpacted by the limitations of dataset size and the domain gap between\ndatasets, which precludes their extensive application in real-world scenarios.\nTo address this issue, we introduce SMART, a novel autonomous driving motion\ngeneration paradigm that models vectorized map and agent trajectory data into\ndiscrete sequence tokens. These tokens are then processed through a\ndecoder-only transformer architecture to train for the next token prediction\ntask across spatial-temporal series. This GPT-style method allows the model to\nlearn the motion distribution in real driving scenarios. SMART achieves\nstate-of-the-art performance across most of the metrics on the generative Sim\nAgents challenge, ranking 1st on the leaderboards of Waymo Open Motion Dataset\n(WOMD), demonstrating remarkable inference speed. Moreover, SMART represents\nthe generative model in the autonomous driving motion domain, exhibiting\nzero-shot generalization capabilities: Using only the NuPlan dataset for\ntraining and WOMD for validation, SMART achieved a competitive score of 0.72 on\nthe Sim Agents challenge. Lastly, we have collected over 1 billion motion\ntokens from multiple datasets, validating the model's scalability. These\nresults suggest that SMART has initially emulated two important properties:\nscalability and zero-shot generalization, and preliminarily meets the needs of\nlarge-scale real-time simulation applications. We have released all the code to\npromote the exploration of models for motion generation in the autonomous\ndriving field. The source code is available at\nhttps://github.com/rainmaker22/SMART.\n","authors":["Wei Wu","Xiaoxin Feng","Ziyan Gao","Yuheng Kan"],"pdf_url":"https://arxiv.org/pdf/2405.15677v3.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2401.08140v3","updated":"2024-11-01T06:12:07Z","published":"2024-01-16T06:19:18Z","title":"ProvNeRF: Modeling per Point Provenance in NeRFs as a Stochastic Field","summary":"  Neural radiance fields (NeRFs) have gained popularity with multiple works\nshowing promising results across various applications. However, to the best of\nour knowledge, existing works do not explicitly model the distribution of\ntraining camera poses, or consequently the triangulation quality, a key factor\naffecting reconstruction quality dating back to classical vision literature. We\nclose this gap with ProvNeRF, an approach that models the \\textbf{provenance}\nfor each point -- i.e., the locations where it is likely visible -- of NeRFs as\na stochastic field. We achieve this by extending implicit maximum likelihood\nestimation (IMLE) to functional space with an optimizable objective. We show\nthat modeling per-point provenance during the NeRF optimization enriches the\nmodel with information on triangulation leading to improvements in novel view\nsynthesis and uncertainty estimation under the challenging sparse,\nunconstrained view setting against competitive baselines.\n","authors":["Kiyohiro Nakayama","Mikaela Angelina Uy","Yang You","Ke Li","Leonidas J. Guibas"],"pdf_url":"https://arxiv.org/pdf/2401.08140v3.pdf","comment":"38th Conference on Neural Information Processing Systems (NeurIPS\n  2024)"},{"id":"http://arxiv.org/abs/2310.01636v4","updated":"2024-11-01T05:29:34Z","published":"2023-10-02T21:02:23Z","title":"Adaptive Visual Scene Understanding: Incremental Scene Graph Generation","summary":"  Scene graph generation (SGG) analyzes images to extract meaningful\ninformation about objects and their relationships. In the dynamic visual world,\nit is crucial for AI systems to continuously detect new objects and establish\ntheir relationships with existing ones. Recently, numerous studies have focused\non continual learning within the domains of object detection and image\nrecognition. However, a limited amount of research focuses on a more\nchallenging continual learning problem in SGG. This increased difficulty arises\nfrom the intricate interactions and dynamic relationships among objects, and\ntheir associated contexts. Thus, in continual learning, SGG models are often\nrequired to expand, modify, retain, and reason scene graphs within the process\nof adaptive visual scene understanding. To systematically explore Continual\nScene Graph Generation (CSEGG), we present a comprehensive benchmark comprising\nthree learning regimes: relationship incremental, scene incremental, and\nrelationship generalization. Moreover, we introduce a ``Replays via Analysis by\nSynthesis\" method named RAS. This approach leverages the scene graphs,\ndecomposes and re-composes them to represent different scenes, and replays the\nsynthesized scenes based on these compositional scene graphs. The replayed\nsynthesized scenes act as a means to practice and refine proficiency in SGG in\nknown and unknown environments. Our experimental results not only highlight the\nchallenges of directly combining existing continual learning methods with SGG\nbackbones but also demonstrate the effectiveness of our proposed approach,\nenhancing CSEGG efficiency while simultaneously preserving privacy and memory\nusage. All data and source code are publicly available online.\n","authors":["Naitik Khandelwal","Xiao Liu","Mengmi Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.01636v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03918v2","updated":"2024-11-01T05:23:35Z","published":"2024-10-04T20:45:33Z","title":"STONE: A Submodular Optimization Framework for Active 3D Object\n  Detection","summary":"  3D object detection is fundamentally important for various emerging\napplications, including autonomous driving and robotics. A key requirement for\ntraining an accurate 3D object detector is the availability of a large amount\nof LiDAR-based point cloud data. Unfortunately, labeling point cloud data is\nextremely challenging, as accurate 3D bounding boxes and semantic labels are\nrequired for each potential object. This paper proposes a unified active 3D\nobject detection framework, for greatly reducing the labeling cost of training\n3D object detectors. Our framework is based on a novel formulation of\nsubmodular optimization, specifically tailored to the problem of active 3D\nobject detection. In particular, we address two fundamental challenges\nassociated with active 3D object detection: data imbalance and the need to\ncover the distribution of the data, including LiDAR-based point cloud data of\nvarying difficulty levels. Extensive experiments demonstrate that our method\nachieves state-of-the-art performance with high computational efficiency\ncompared to existing active learning methods. The code is available at\nhttps://github.com/RuiyuM/STONE.\n","authors":["Ruiyu Mao","Sarthak Kumar Maharana","Rishabh K Iyer","Yunhui Guo"],"pdf_url":"https://arxiv.org/pdf/2410.03918v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.00986v2","updated":"2024-11-01T05:03:19Z","published":"2024-04-01T08:18:38Z","title":"Make Continual Learning Stronger via C-Flat","summary":"  Model generalization ability upon incrementally acquiring dynamically\nupdating knowledge from sequentially arriving tasks is crucial to tackle the\nsensitivity-stability dilemma in Continual Learning (CL). Weight loss landscape\nsharpness minimization seeking for flat minima lying in neighborhoods with\nuniform low loss or smooth gradient is proven to be a strong training regime\nimproving model generalization compared with loss minimization based optimizer\nlike SGD. Yet only a few works have discussed this training regime for CL,\nproving that dedicated designed zeroth-order sharpness optimizer can improve CL\nperformance. In this work, we propose a Continual Flatness (C-Flat) method\nfeaturing a flatter loss landscape tailored for CL. C-Flat could be easily\ncalled with only one line of code and is plug-and-play to any CL methods. A\ngeneral framework of C-Flat applied to all CL categories and a thorough\ncomparison with loss minima optimizer and flat minima based CL approaches is\npresented in this paper, showing that our method can boost CL performance in\nalmost all cases. Code is available at https://github.com/WanNaa/C-Flat.\n","authors":["Ang Bian","Wei Li","Hangjie Yuan","Chengrong Yu","Mang Wang","Zixiang Zhao","Aojun Lu","Pengliang Ji","Tao Feng"],"pdf_url":"https://arxiv.org/pdf/2404.00986v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.12470v2","updated":"2024-11-01T04:59:31Z","published":"2024-09-19T05:17:44Z","title":"HSIGene: A Foundation Model For Hyperspectral Image Generation","summary":"  Hyperspectral image (HSI) plays a vital role in various fields such as\nagriculture and environmental monitoring. However, due to the expensive\nacquisition cost, the number of hyperspectral images is limited, degenerating\nthe performance of downstream tasks. Although some recent studies have\nattempted to employ diffusion models to synthesize HSIs, they still struggle\nwith the scarcity of HSIs, affecting the reliability and diversity of the\ngenerated images. Some studies propose to incorporate multi-modal data to\nenhance spatial diversity, but the spectral fidelity cannot be ensured. In\naddition, existing HSI synthesis models are typically uncontrollable or only\nsupport single-condition control, limiting their ability to generate accurate\nand reliable HSIs. To alleviate these issues, we propose HSIGene, a novel HSI\ngeneration foundation model which is based on latent diffusion and supports\nmulti-condition control, allowing for more precise and reliable HSI generation.\nTo enhance the spatial diversity of the training data while preserving spectral\nfidelity, we propose a new data augmentation method based on spatial\nsuper-resolution, in which HSIs are upscaled first, and thus abundant training\npatches could be obtained by cropping the high-resolution HSIs. In addition, to\nimprove the perceptual quality of the augmented data, we introduce a novel\ntwo-stage HSI super-resolution framework, which first applies RGB bands\nsuper-resolution and then utilizes our proposed Rectangular Guided Attention\nNetwork (RGAN) for guided HSI super-resolution. Experiments demonstrate that\nthe proposed model is capable of generating a vast quantity of realistic HSIs\nfor downstream tasks such as denoising and super-resolution. The code and\nmodels are available at https://github.com/LiPang/HSIGene.\n","authors":["Li Pang","Xiangyong Cao","Datao Tang","Shuang Xu","Xueru Bai","Feng Zhou","Deyu Meng"],"pdf_url":"https://arxiv.org/pdf/2409.12470v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.20474v2","updated":"2024-11-01T04:33:52Z","published":"2024-10-27T15:30:45Z","title":"GrounDiT: Grounding Diffusion Transformers via Noisy Patch\n  Transplantation","summary":"  We introduce GrounDiT, a novel training-free spatial grounding technique for\ntext-to-image generation using Diffusion Transformers (DiT). Spatial grounding\nwith bounding boxes has gained attention for its simplicity and versatility,\nallowing for enhanced user control in image generation. However, prior\ntraining-free approaches often rely on updating the noisy image during the\nreverse diffusion process via backpropagation from custom loss functions, which\nfrequently struggle to provide precise control over individual bounding boxes.\nIn this work, we leverage the flexibility of the Transformer architecture,\ndemonstrating that DiT can generate noisy patches corresponding to each\nbounding box, fully encoding the target object and allowing for fine-grained\ncontrol over each region. Our approach builds on an intriguing property of DiT,\nwhich we refer to as semantic sharing. Due to semantic sharing, when a smaller\npatch is jointly denoised alongside a generatable-size image, the two become\nsemantic clones. Each patch is denoised in its own branch of the generation\nprocess and then transplanted into the corresponding region of the original\nnoisy image at each timestep, resulting in robust spatial grounding for each\nbounding box. In our experiments on the HRS and DrawBench benchmarks, we\nachieve state-of-the-art performance compared to previous training-free\napproaches.\n","authors":["Phillip Y. Lee","Taehoon Yoon","Minhyuk Sung"],"pdf_url":"https://arxiv.org/pdf/2410.20474v2.pdf","comment":"Accepted to NeurIPS 2024. Project Page:\n  https://groundit-diffusion.github.io/"},{"id":"http://arxiv.org/abs/2410.23775v2","updated":"2024-11-01T03:15:02Z","published":"2024-10-31T09:45:00Z","title":"In-Context LoRA for Diffusion Transformers","summary":"  Recent research arXiv:2410.15027 has explored the use of diffusion\ntransformers (DiTs) for task-agnostic image generation by simply concatenating\nattention tokens across images. However, despite substantial computational\nresources, the fidelity of the generated images remains suboptimal. In this\nstudy, we reevaluate and streamline this framework by hypothesizing that\ntext-to-image DiTs inherently possess in-context generation capabilities,\nrequiring only minimal tuning to activate them. Through diverse task\nexperiments, we qualitatively demonstrate that existing text-to-image DiTs can\neffectively perform in-context generation without any tuning. Building on this\ninsight, we propose a remarkably simple pipeline to leverage the in-context\nabilities of DiTs: (1) concatenate images instead of tokens, (2) perform joint\ncaptioning of multiple images, and (3) apply task-specific LoRA tuning using\nsmall datasets (e.g., $20\\sim 100$ samples) instead of full-parameter tuning\nwith large datasets. We name our models In-Context LoRA (IC-LoRA). This\napproach requires no modifications to the original DiT models, only changes to\nthe training data. Remarkably, our pipeline generates high-fidelity image sets\nthat better adhere to prompts. While task-specific in terms of tuning data, our\nframework remains task-agnostic in architecture and pipeline, offering a\npowerful tool for the community and providing valuable insights for further\nresearch on product-level task-agnostic generation systems. We release our\ncode, data, and models at https://github.com/ali-vilab/In-Context-LoRA\n","authors":["Lianghua Huang","Wei Wang","Zhi-Fan Wu","Yupeng Shi","Huanzhang Dou","Chen Liang","Yutong Feng","Yu Liu","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.23775v2.pdf","comment":"Tech report. Project page:\n  https://ali-vilab.github.io/In-Context-LoRA-Page/"},{"id":"http://arxiv.org/abs/2409.17508v2","updated":"2024-11-01T02:38:53Z","published":"2024-09-26T03:33:26Z","title":"Uni-Med: A Unified Medical Generalist Foundation Model For Multi-Task\n  Learning Via Connector-MoE","summary":"  Multi-modal large language models (MLLMs) have shown impressive capabilities\nas a general-purpose interface for various visual and linguistic tasks.\nHowever, building a unified MLLM for multi-task learning in the medical field\nremains a thorny challenge. To mitigate the tug-of-war problem of multi-modal\nmulti-task optimization in MLLMs, recent advances primarily focus on improving\nthe LLM components, while neglecting the connector that bridges the gap between\nmodalities. In this paper, we introduce Uni-Med, a novel medical generalist\nfoundation model which consists of a universal visual feature extraction\nmodule, a connector mixture-of-experts (CMoE) module, and an LLM. Benefiting\nfrom the proposed CMoE that leverages a well-designed router with a mixture of\nprojection experts at the connector, Uni-Med achieves efficient solution to the\ntug-of-war problem and can perform six different medical tasks including\nquestion answering, visual question answering, report generation, referring\nexpression comprehension, referring expression generation and image\nclassification. To the best of our knowledge, Uni-Med is the first effort to\ntackle multi-task interference at the connector in MLLMs. Extensive ablation\nexperiments validate the effectiveness of introducing CMoE under any\nconfiguration, with up to an average 8% performance gains. We further provide\ninterpretation analysis of the tug-of-war problem from the perspective of\ngradient optimization and parameter statistics. Compared to previous\nstate-of-the-art medical MLLMs, Uni-Med achieves competitive or superior\nevaluation metrics on diverse tasks. Code and resources are available at\nhttps://github.com/tsinghua-msiip/Uni-Med.\n","authors":["Xun Zhu","Ying Hu","Fanbin Mo","Miao Li","Ji Wu"],"pdf_url":"https://arxiv.org/pdf/2409.17508v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09553v4","updated":"2024-11-01T02:25:50Z","published":"2024-06-28T08:21:49Z","title":"DPEC: Dual-Path Error Compensation Method for Enhanced Low-Light Image\n  Clarity","summary":"  For the task of low-light image enhancement, deep learning-based algorithms\nhave demonstrated superiority and effectiveness compared to traditional\nmethods. However, these methods, primarily based on Retinex theory, tend to\noverlook the noise and color distortions in input images, leading to\nsignificant noise amplification and local color distortions in enhanced\nresults. To address these issues, we propose the Dual-Path Error Compensation\n(DPEC) method, designed to improve image quality under low-light conditions by\npreserving local texture details while restoring global image brightness\nwithout amplifying noise. DPEC incorporates precise pixel-level error\nestimation to capture subtle differences and an independent denoising mechanism\nto prevent noise amplification. We introduce the HIS-Retinex loss to guide\nDPEC's training, ensuring the brightness distribution of enhanced images\nclosely aligns with real-world conditions. To balance computational speed and\nresource efficiency while training DPEC for a comprehensive understanding of\nthe global context, we integrated the VMamba architecture into its backbone.\nComprehensive quantitative and qualitative experimental results demonstrate\nthat our algorithm significantly outperforms state-of-the-art methods in\nlow-light image enhancement. The code is publicly available online at\nhttps://github.com/wangshuang233/DPEC.\n","authors":["Shuang Wang","Qianwen Lu","Boxing Peng","Yihe Nie","Qingchuan Tao"],"pdf_url":"https://arxiv.org/pdf/2407.09553v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14135v2","updated":"2024-11-01T02:20:06Z","published":"2024-08-26T09:32:16Z","title":"Foodfusion: A Novel Approach for Food Image Composition via Diffusion\n  Models","summary":"  Food image composition requires the use of existing dish images and\nbackground images to synthesize a natural new image, while diffusion models\nhave made significant advancements in image generation, enabling the\nconstruction of end-to-end architectures that yield promising results. However,\nexisting diffusion models face challenges in processing and fusing information\nfrom multiple images and lack access to high-quality publicly available\ndatasets, which prevents the application of diffusion models in food image\ncomposition. In this paper, we introduce a large-scale, high-quality food image\ncomposite dataset, FC22k, which comprises 22,000 foreground, background, and\nground truth ternary image pairs. Additionally, we propose a novel food image\ncomposition method, Foodfusion, which leverages the capabilities of the\npre-trained diffusion models and incorporates a Fusion Module for processing\nand integrating foreground and background information. This fused information\naligns the foreground features with the background structure by merging the\nglobal structural information at the cross-attention layer of the denoising\nUNet. To further enhance the content and structure of the background, we also\nintegrate a Content-Structure Control Module. Extensive experiments demonstrate\nthe effectiveness and scalability of our proposed method.\n","authors":["Chaohua Shi","Xuan Wang","Si Shi","Xule Wang","Mingrui Zhu","Nannan Wang","Xinbo Gao"],"pdf_url":"https://arxiv.org/pdf/2408.14135v2.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2406.18451v3","updated":"2024-11-01T02:13:59Z","published":"2024-06-26T16:00:35Z","title":"Detecting Brittle Decisions for Free: Leveraging Margin Consistency in\n  Deep Robust Classifiers","summary":"  Despite extensive research on adversarial training strategies to improve\nrobustness, the decisions of even the most robust deep learning models can\nstill be quite sensitive to imperceptible perturbations, creating serious risks\nwhen deploying them for high-stakes real-world applications. While detecting\nsuch cases may be critical, evaluating a model's vulnerability at a\nper-instance level using adversarial attacks is computationally too intensive\nand unsuitable for real-time deployment scenarios. The input space margin is\nthe exact score to detect non-robust samples and is intractable for deep neural\nnetworks. This paper introduces the concept of margin consistency -- a property\nthat links the input space margins and the logit margins in robust models --\nfor efficient detection of vulnerable samples. First, we establish that margin\nconsistency is a necessary and sufficient condition to use a model's logit\nmargin as a score for identifying non-robust samples. Next, through\ncomprehensive empirical analysis of various robustly trained models on CIFAR10\nand CIFAR100 datasets, we show that they indicate high margin consistency with\na strong correlation between their input space margins and the logit margins.\nThen, we show that we can effectively and confidently use the logit margin to\ndetect brittle decisions with such models. Finally, we address cases where the\nmodel is not sufficiently margin-consistent by learning a pseudo-margin from\nthe feature representation. Our findings highlight the potential of leveraging\ndeep representations to assess adversarial vulnerability in deployment\nscenarios efficiently.\n","authors":["Jonas Ngnawé","Sabyasachi Sahoo","Yann Pequignot","Frédéric Precioso","Christian Gagné"],"pdf_url":"https://arxiv.org/pdf/2406.18451v3.pdf","comment":"10 pages, 6 figures, 2 tables. Version Update: Neurips Camera Ready"},{"id":"http://arxiv.org/abs/2410.20595v2","updated":"2024-11-01T01:27:10Z","published":"2024-10-27T21:02:37Z","title":"A Framework for Real-Time Volcano-Seismic Event Recognition Based on\n  Multi-Station Seismograms and Semantic Segmentation Models","summary":"  In volcano monitoring, effective recognition of seismic events is essential\nfor understanding volcanic activity and raising timely warning alerts.\nTraditional methods rely on manual analysis, which can be subjective and\nlabor-intensive. Furthermore, current automatic approaches often tackle\ndetection and classification separately, mostly rely on single station\ninformation and generally require tailored preprocessing and representations to\nperform predictions. These limitations often hinder their application to\nreal-time monitoring and utilization across different volcano conditions. This\nstudy introduces a novel approach that utilizes Semantic Segmentation models to\nautomate seismic event recognition by applying a straight forward\ntransformation of multi-channel 1D signals into 2D representations, enabling\ntheir use as images. Our framework employs a data-driven, end-to-end design\nthat integrates multi-station seismic data with minimal preprocessing,\nperforming both detection and classification simultaneously for five seismic\nevent classes. We evaluated four state-of-the-art segmentation models (UNet,\nUNet++, DeepLabV3+ and SwinUNet) on approximately 25.000 seismic events\nrecorded at four different Chilean volcanoes: Nevados del Chill\\'an Volcanic\nComplex, Laguna del Maule, Villarrica and Puyehue-Cord\\'on Caulle. Among these\nmodels, the UNet architecture was identified as the most effective model,\nachieving mean F1 and Intersection over Union (IoU) scores of up to 0.91 and\n0.88, respectively, and demonstrating superior noise robustness and model\nflexibility to unseen volcano datasets.\n","authors":["Camilo Espinosa-Curilem","Millaray Curilem","Daniel Basualto"],"pdf_url":"https://arxiv.org/pdf/2410.20595v2.pdf","comment":"10 pages, 9 figures. This is a pre-print, it is currently under\n  review for publication"},{"id":"http://arxiv.org/abs/2310.05341v5","updated":"2024-11-01T00:37:44Z","published":"2023-10-09T01:59:49Z","title":"From Question to Exploration: Test-Time Adaptation in Semantic\n  Segmentation?","summary":"  Test-time adaptation (TTA) aims to adapt a model, initially trained on\ntraining data, to test data with potential distribution shifts. Most existing\nTTA methods focus on classification problems. The pronounced success of\nclassification might lead numerous newcomers and engineers to assume that\nclassic TTA techniques can be directly applied to the more challenging task of\nsemantic segmentation. However, this belief is still an open question. In this\npaper, we investigate the applicability of existing classic TTA strategies in\nsemantic segmentation. Our comprehensive results have led to three key\nobservations. First, the classic normalization updating strategy only brings\nslight performance improvement, and in some cases, it might even adversely\naffect the results. Even with the application of advanced distribution\nestimation techniques like batch renormalization, the problem remains\nunresolved. Second, although the teacher-student scheme does enhance the\ntraining stability for segmentation TTA in the presence of noisy pseudo-labels\nand temporal correlation, it cannot directly result in performance improvement\ncompared to the original model without TTA under complex data distribution.\nThird, segmentation TTA suffers a severe long-tailed class-imbalance problem,\nwhich is substantially more complex than that in TTA for classification. This\nlong-tailed challenge negatively affects segmentation TTA performance, even\nwhen the accuracy of pseudo-labels is high. Besides those observations, we find\nthat visual prompt tuning (VisPT) is promising in segmentation TTA and propose\na novel method named TTAP. The outstanding performance of TTAP has also been\nverified. We hope the community can give more attention to this challenging,\nyet important, segmentation TTA task in the future. The source code is\navailable at: \\textit{https://github.com/ycarobot/TTAP\n","authors":["Chang'an Yi","Haotian Chen","Yifan Zhang","Yonghui Xu","Yan Zhou","Lizhen Cui"],"pdf_url":"https://arxiv.org/pdf/2310.05341v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09371v2","updated":"2024-11-01T00:22:26Z","published":"2024-06-13T17:51:00Z","title":"LRM-Zero: Training Large Reconstruction Models with Synthesized Data","summary":"  We present LRM-Zero, a Large Reconstruction Model (LRM) trained entirely on\nsynthesized 3D data, achieving high-quality sparse-view 3D reconstruction. The\ncore of LRM-Zero is our procedural 3D dataset, Zeroverse, which is\nautomatically synthesized from simple primitive shapes with random texturing\nand augmentations (e.g., height fields, boolean differences, and wireframes).\nUnlike previous 3D datasets (e.g., Objaverse) which are often captured or\ncrafted by humans to approximate real 3D data, Zeroverse completely ignores\nrealistic global semantics but is rich in complex geometric and texture details\nthat are locally similar to or even more intricate than real objects. We\ndemonstrate that our LRM-Zero, trained with our fully synthesized Zeroverse,\ncan achieve high visual quality in the reconstruction of real-world objects,\ncompetitive with models trained on Objaverse. We also analyze several critical\ndesign choices of Zeroverse that contribute to LRM-Zero's capability and\ntraining stability. Our work demonstrates that 3D reconstruction, one of the\ncore tasks in 3D vision, can potentially be addressed without the semantics of\nreal-world objects. The Zeroverse's procedural synthesis code and interactive\nvisualization are available at: https://desaixie.github.io/lrm-zero/.\n","authors":["Desai Xie","Sai Bi","Zhixin Shu","Kai Zhang","Zexiang Xu","Yi Zhou","Sören Pirk","Arie Kaufman","Xin Sun","Hao Tan"],"pdf_url":"https://arxiv.org/pdf/2406.09371v2.pdf","comment":"23 pages, 8 figures. Our code and interactive visualization are\n  available at: https://desaixie.github.io/lrm-zero/. v2: NeurIPS 2024 Camera\n  Ready version"},{"id":"http://arxiv.org/abs/2406.17763v2","updated":"2024-11-01T00:08:54Z","published":"2024-06-25T17:48:24Z","title":"DiffusionPDE: Generative PDE-Solving Under Partial Observation","summary":"  We introduce a general framework for solving partial differential equations\n(PDEs) using generative diffusion models. In particular, we focus on the\nscenarios where we do not have the full knowledge of the scene necessary to\napply classical solvers. Most existing forward or inverse PDE approaches\nperform poorly when the observations on the data or the underlying coefficients\nare incomplete, which is a common assumption for real-world measurements. In\nthis work, we propose DiffusionPDE that can simultaneously fill in the missing\ninformation and solve a PDE by modeling the joint distribution of the solution\nand coefficient spaces. We show that the learned generative priors lead to a\nversatile framework for accurately solving a wide range of PDEs under partial\nobservation, significantly outperforming the state-of-the-art methods for both\nforward and inverse directions.\n","authors":["Jiahe Huang","Guandao Yang","Zichen Wang","Jeong Joon Park"],"pdf_url":"https://arxiv.org/pdf/2406.17763v2.pdf","comment":"NeurIPS 2024. Project page:\n  https://jhhuangchloe.github.io/Diffusion-PDE/"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2407.10691v2","updated":"2024-11-01T14:08:31Z","published":"2024-07-15T13:04:09Z","title":"$\\texttt{MixGR}$: Enhancing Retriever Generalization for Scientific\n  Domain through Complementary Granularity","summary":"  Recent studies show the growing significance of document retrieval in the\ngeneration of LLMs, i.e., RAG, within the scientific domain by bridging their\nknowledge gap. However, dense retrievers often struggle with domain-specific\nretrieval and complex query-document relationships, particularly when query\nsegments correspond to various parts of a document. To alleviate such prevalent\nchallenges, this paper introduces $\\texttt{MixGR}$, which improves dense\nretrievers' awareness of query-document matching across various levels of\ngranularity in queries and documents using a zero-shot approach.\n$\\texttt{MixGR}$ fuses various metrics based on these granularities to a united\nscore that reflects a comprehensive query-document similarity. Our experiments\ndemonstrate that $\\texttt{MixGR}$ outperforms previous document retrieval by\n24.7%, 9.8%, and 6.9% on nDCG@5 with unsupervised, supervised, and LLM-based\nretrievers, respectively, averaged on queries containing multiple subqueries\nfrom five scientific retrieval datasets. Moreover, the efficacy of two\ndownstream scientific question-answering tasks highlights the advantage of\n$\\texttt{MixGR}$ to boost the application of LLMs in the scientific domain. The\ncode and experimental datasets are available.\n","authors":["Fengyu Cai","Xinran Zhao","Tong Chen","Sihao Chen","Hongming Zhang","Iryna Gurevych","Heinz Koeppl"],"pdf_url":"https://arxiv.org/pdf/2407.10691v2.pdf","comment":"EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2408.10159v3","updated":"2024-11-01T03:47:59Z","published":"2024-08-19T17:09:32Z","title":"Customizing Language Models with Instance-wise LoRA for Sequential\n  Recommendation","summary":"  Sequential recommendation systems predict the next interaction item based on\nusers' past interactions, aligning recommendations with individual preferences.\nLeveraging the strengths of Large Language Models (LLMs) in knowledge\ncomprehension and reasoning, recent approaches are eager to apply LLMs to\nsequential recommendation. A common paradigm is converting user behavior\nsequences into instruction data, and fine-tuning the LLM with\nparameter-efficient fine-tuning (PEFT) methods like Low-Rank Adaption (LoRA).\nHowever, the uniform application of LoRA across diverse user behaviors is\ninsufficient to capture individual variability, resulting in negative transfer\nbetween disparate sequences. To address these challenges, we propose\nInstance-wise LoRA (iLoRA). We innovatively treat the sequential recommendation\ntask as a form of multi-task learning, integrating LoRA with the Mixture of\nExperts (MoE) framework. This approach encourages different experts to capture\nvarious aspects of user behavior. Additionally, we introduce a sequence\nrepresentation guided gate function that generates customized expert\nparticipation weights for each user sequence, which allows dynamic parameter\nadjustment for instance-wise recommendations. In sequential recommendation,\niLoRA achieves an average relative improvement of 11.4\\% over basic LoRA in the\nhit ratio metric, with less than a 1\\% relative increase in trainable\nparameters. Extensive experiments on three benchmark datasets demonstrate the\neffectiveness of iLoRA, highlighting its superior performance compared to\nexisting methods in mitigating negative transfer and improving recommendation\naccuracy. Our data and code are available at\nhttps://github.com/AkaliKong/iLoRA.\n","authors":["Xiaoyu Kong","Jiancan Wu","An Zhang","Leheng Sheng","Hui Lin","Xiang Wang","Xiangnan He"],"pdf_url":"https://arxiv.org/pdf/2408.10159v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.20646v2","updated":"2024-11-01T03:12:44Z","published":"2024-05-31T07:24:42Z","title":"LLM-ESR: Large Language Models Enhancement for Long-tailed Sequential\n  Recommendation","summary":"  Sequential recommender systems (SRS) aim to predict users' subsequent choices\nbased on their historical interactions and have found applications in diverse\nfields such as e-commerce and social media. However, in real-world systems,\nmost users interact with only a handful of items, while the majority of items\nare seldom consumed. These two issues, known as the long-tail user and\nlong-tail item challenges, often pose difficulties for existing SRS. These\nchallenges can adversely affect user experience and seller benefits, making\nthem crucial to address. Though a few works have addressed the challenges, they\nstill struggle with the seesaw or noisy issues due to the intrinsic scarcity of\ninteractions. The advancements in large language models (LLMs) present a\npromising solution to these problems from a semantic perspective. As one of the\npioneers in this field, we propose the Large Language Models Enhancement\nframework for Sequential Recommendation (LLM-ESR). This framework utilizes\nsemantic embeddings derived from LLMs to enhance SRS without adding extra\ninference load from LLMs. To address the long-tail item challenge, we design a\ndual-view modeling framework that combines semantics from LLMs and\ncollaborative signals from conventional SRS. For the long-tail user challenge,\nwe propose a retrieval augmented self-distillation method to enhance user\npreference representation using more informative interactions from similar\nusers. To verify the effectiveness and versatility of our proposed enhancement\nframework, we conduct extensive experiments on three real-world datasets using\nthree popular SRS models. The results show that our method surpasses existing\nbaselines consistently, and benefits long-tail users and items especially. The\nimplementation code is available at\nhttps://github.com/Applied-Machine-Learning-Lab/LLM-ESR.\n","authors":["Qidong Liu","Xian Wu","Yejing Wang","Zijian Zhang","Feng Tian","Yefeng Zheng","Xiangyu Zhao"],"pdf_url":"https://arxiv.org/pdf/2405.20646v2.pdf","comment":"accepted by NeruIPS'24 (Spotlight)"},{"id":"http://arxiv.org/abs/2402.15235v3","updated":"2024-11-01T02:00:49Z","published":"2024-02-23T09:57:20Z","title":"MACRec: a Multi-Agent Collaboration Framework for Recommendation","summary":"  LLM-based agents have gained considerable attention for their decision-making\nskills and ability to handle complex tasks. Recognizing the current gap in\nleveraging agent capabilities for multi-agent collaboration in recommendation\nsystems, we introduce MACRec, a novel framework designed to enhance\nrecommendation systems through multi-agent collaboration. Unlike existing work\non using agents for user/item simulation, we aim to deploy multi-agents to\ntackle recommendation tasks directly. In our framework, recommendation tasks\nare addressed through the collaborative efforts of various specialized agents,\nincluding Manager, User/Item Analyst, Reflector, Searcher, and Task\nInterpreter, with different working flows. Furthermore, we provide application\nexamples of how developers can easily use MACRec on various recommendation\ntasks, including rating prediction, sequential recommendation, conversational\nrecommendation, and explanation generation of recommendation results. The\nframework and demonstration video are publicly available at\nhttps://github.com/wzf2000/MACRec.\n","authors":["Zhefan Wang","Yuanqing Yu","Wendi Zheng","Weizhi Ma","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.15235v3.pdf","comment":"Accepted by SIGIR2024"},{"id":"http://arxiv.org/abs/2410.23683v2","updated":"2024-11-01T01:21:04Z","published":"2024-10-31T07:19:22Z","title":"Unveiling User Satisfaction and Creator Productivity Trade-Offs in\n  Recommendation Platforms","summary":"  On User-Generated Content (UGC) platforms, recommendation algorithms\nsignificantly impact creators' motivation to produce content as they compete\nfor algorithmically allocated user traffic. This phenomenon subtly shapes the\nvolume and diversity of the content pool, which is crucial for the platform's\nsustainability. In this work, we demonstrate, both theoretically and\nempirically, that a purely relevance-driven policy with low exploration\nstrength boosts short-term user satisfaction but undermines the long-term\nrichness of the content pool. In contrast, a more aggressive exploration policy\nmay slightly compromise user satisfaction but promote higher content creation\nvolume. Our findings reveal a fundamental trade-off between immediate user\nsatisfaction and overall content production on UGC platforms. Building on this\nfinding, we propose an efficient optimization method to identify the optimal\nexploration strength, balancing user and creator engagement. Our model can\nserve as a pre-deployment audit tool for recommendation algorithms on UGC\nplatforms, helping to align their immediate objectives with sustainable,\nlong-term goals.\n","authors":["Fan Yao","Yiming Liao","Jingzhou Liu","Shaoliang Nie","Qifan Wang","Haifeng Xu","Hongning Wang"],"pdf_url":"https://arxiv.org/pdf/2410.23683v2.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2407.18414v2","updated":"2024-11-01T17:47:03Z","published":"2024-07-25T22:12:47Z","title":"Adversarially Robust Decision Transformer","summary":"  Decision Transformer (DT), as one of the representative Reinforcement\nLearning via Supervised Learning (RvS) methods, has achieved strong performance\nin offline learning tasks by leveraging the powerful Transformer architecture\nfor sequential decision-making. However, in adversarial environments, these\nmethods can be non-robust, since the return is dependent on the strategies of\nboth the decision-maker and adversary. Training a probabilistic model\nconditioned on observed return to predict action can fail to generalize, as the\ntrajectories that achieve a return in the dataset might have done so due to a\nsuboptimal behavior adversary. To address this, we propose a worst-case-aware\nRvS algorithm, the Adversarially Robust Decision Transformer (ARDT), which\nlearns and conditions the policy on in-sample minimax returns-to-go. ARDT\naligns the target return with the worst-case return learned through minimax\nexpectile regression, thereby enhancing robustness against powerful test-time\nadversaries. In experiments conducted on sequential games with full data\ncoverage, ARDT can generate a maximin (Nash Equilibrium) strategy, the solution\nwith the largest adversarial robustness. In large-scale sequential games and\ncontinuous adversarial RL environments with partial data coverage, ARDT\ndemonstrates significantly superior robustness to powerful test-time\nadversaries and attains higher worst-case returns compared to contemporary DT\nmethods.\n","authors":["Xiaohang Tang","Afonso Marques","Parameswaran Kamalaruban","Ilija Bogunovic"],"pdf_url":"https://arxiv.org/pdf/2407.18414v2.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2409.01306v2","updated":"2024-11-01T17:40:26Z","published":"2024-09-02T14:56:22Z","title":"Highly Accurate Real-space Electron Densities with Neural Networks","summary":"  Variational ab-initio methods in quantum chemistry stand out among other\nmethods in providing direct access to the wave function. This allows in\nprinciple straightforward extraction of any other observable of interest,\nbesides the energy, but in practice this extraction is often technically\ndifficult and computationally impractical. Here, we consider the electron\ndensity as a central observable in quantum chemistry and introduce a novel\nmethod to obtain accurate densities from real-space many-electron wave\nfunctions by representing the density with a neural network that captures known\nasymptotic properties and is trained from the wave function by score matching\nand noise-contrastive estimation. We use variational quantum Monte Carlo with\ndeep-learning ans\\\"atze (deep QMC) to obtain highly accurate wave functions\nfree of basis set errors, and from them, using our novel method,\ncorrespondingly accurate electron densities, which we demonstrate by\ncalculating dipole moments, nuclear forces, contact densities, and other\ndensity-based properties.\n","authors":["Lixue Cheng","P. Bernát Szabó","Zeno Schätzle","Derk P. Kooi","Jonas Köhler","Klaas J. H. Giesbertz","Frank Noé","Jan Hermann","Paola Gori-Giorgi","Adam Foster"],"pdf_url":"https://arxiv.org/pdf/2409.01306v2.pdf","comment":"12 pages, 9 figures in the main text"},{"id":"http://arxiv.org/abs/2406.12909v4","updated":"2024-11-01T17:09:52Z","published":"2024-06-12T21:21:42Z","title":"Scalable Training of Trustworthy and Energy-Efficient Predictive Graph\n  Foundation Models for Atomistic Materials Modeling: A Case Study with\n  HydraGNN","summary":"  We present our work on developing and training scalable, trustworthy, and\nenergy-efficient predictive graph foundation models (GFMs) using HydraGNN, a\nmulti-headed graph convolutional neural network architecture. HydraGNN expands\nthe boundaries of graph neural network (GNN) computations in both training\nscale and data diversity. It abstracts over message passing algorithms,\nallowing both reproduction of and comparison across algorithmic innovations\nthat define nearest-neighbor convolution in GNNs. This work discusses a series\nof optimizations that have allowed scaling up the GFMs training to tens of\nthousands of GPUs on datasets consisting of hundreds of millions of graphs. Our\nGFMs use multi-task learning (MTL) to simultaneously learn graph-level and\nnode-level properties of atomistic structures, such as energy and atomic\nforces. Using over 154 million atomistic structures for training, we illustrate\nthe performance of our approach along with the lessons learned on two\nstate-of-the-art United States Department of Energy (US-DOE) supercomputers,\nnamely the Perlmutter petascale system at the National Energy Research\nScientific Computing Center and the Frontier exascale system at Oak Ridge\nLeadership Computing Facility. The HydraGNN architecture enables the GFM to\nachieve near-linear strong scaling performance using more than 2,000 GPUs on\nPerlmutter and 16,000 GPUs on Frontier.\n","authors":["Massimiliano Lupo Pasini","Jong Youl Choi","Kshitij Mehta","Pei Zhang","David Rogers","Jonghyun Bae","Khaled Z. Ibrahim","Ashwin M. Aji","Karl W. Schulz","Jorda Polo","Prasanna Balaprakash"],"pdf_url":"https://arxiv.org/pdf/2406.12909v4.pdf","comment":"51 pages, 32 figures"},{"id":"http://arxiv.org/abs/2406.08401v3","updated":"2024-11-01T17:04:09Z","published":"2024-06-12T16:50:12Z","title":"Nyström Kernel Stein Discrepancy","summary":"  Kernel methods underpin many of the most successful approaches in data\nscience and statistics, and they allow representing probability measures as\nelements of a reproducing kernel Hilbert space without loss of information.\nRecently, the kernel Stein discrepancy (KSD), which combines Stein's method\nwith the flexibility of kernel techniques, gained considerable attention.\nThrough the Stein operator, KSD allows the construction of powerful\ngoodness-of-fit tests where it is sufficient to know the target distribution up\nto a multiplicative constant. However, the typical U- and V-statistic-based KSD\nestimators suffer from a quadratic runtime complexity, which hinders their\napplication in large-scale settings. In this work, we propose a Nystr\\\"om-based\nKSD acceleration -- with runtime $\\mathcal O\\left(mn+m^3\\right)$ for $n$\nsamples and $m\\ll n$ Nystr\\\"om points -- , show its $\\sqrt{n}$-consistency with\na classical sub-Gaussian assumption, and demonstrate its applicability for\ngoodness-of-fit testing on a suite of benchmarks.\n","authors":["Florian Kalinke","Zoltan Szabo","Bharath K. Sriperumbudur"],"pdf_url":"https://arxiv.org/pdf/2406.08401v3.pdf","comment":"Broader applicability of main result, consistency of quadratic time\n  estimator"},{"id":"http://arxiv.org/abs/2410.19931v2","updated":"2024-11-01T16:54:46Z","published":"2024-10-25T19:07:29Z","title":"Provable optimal transport with transformers: The essence of depth and\n  prompt engineering","summary":"  Can we establish provable performance guarantees for transformers?\nEstablishing such theoretical guarantees is a milestone in developing\ntrustworthy generative AI. In this paper, we take a step toward addressing this\nquestion by focusing on optimal transport, a fundamental problem at the\nintersection of combinatorial and continuous optimization. Leveraging the\ncomputational power of attention layers, we prove that a transformer with fixed\nparameters can effectively solve the optimal transport problem in Wasserstein-2\nwith entropic regularization for an arbitrary number of points. Consequently,\nthe transformer can sort lists of arbitrary sizes up to an approximation\nfactor. Our results rely on an engineered prompt that enables the transformer\nto implement gradient descent with adaptive stepsizes on the dual optimal\ntransport. Combining the convergence analysis of gradient descent with Sinkhorn\ndynamics, we establish an explicit approximation bound for optimal transport\nwith transformers, which improves as depth increases. Our findings provide\nnovel insights into the essence of prompt engineering and depth for solving\noptimal transport. In particular, prompt engineering boosts the algorithmic\nexpressivity of transformers, allowing them implement an optimization method.\nWith increasing depth, transformers can simulate several iterations of gradient\ndescent.\n","authors":["Hadi Daneshmand"],"pdf_url":"https://arxiv.org/pdf/2410.19931v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22367v2","updated":"2024-11-01T16:53:58Z","published":"2024-10-28T20:45:52Z","title":"MAMMAL -- Molecular Aligned Multi-Modal Architecture and Language","summary":"  Drug discovery typically consists of multiple steps, including identifying a\ntarget protein key to a disease's etiology, validating that interacting with\nthis target could prevent symptoms or cure the disease, discovering a small\nmolecule or biologic therapeutic to interact with it, and optimizing the\ncandidate molecule through a complex landscape of required properties. Drug\ndiscovery related tasks often involve prediction and generation while\nconsidering multiple entities that potentially interact, which poses a\nchallenge for typical AI models. For this purpose we present MAMMAL - Molecular\nAligned Multi-Modal Architecture and Language - a method that we applied to\ncreate a versatile multi-task multi-align foundation model that learns from\nlarge-scale biological datasets (2 billion samples) across diverse modalities,\nincluding proteins, small molecules, and genes. We introduce a prompt syntax\nthat supports a wide range of classification, regression, and generation tasks.\nIt allows combining different modalities and entity types as inputs and/or\noutputs. Our model handles combinations of tokens and scalars and enables the\ngeneration of small molecules and proteins, property prediction, and\ntranscriptomic lab test predictions. We evaluated the model on 11 diverse\ndownstream tasks spanning different steps within a typical drug discovery\npipeline, where it reaches new SOTA in 9 tasks and is comparable to SOTA in 2\ntasks. This performance is achieved while using a unified architecture serving\nall tasks, in contrast to the original SOTA performance achieved using tailored\narchitectures.\n  The model code and pretrained weights are publicly available at\nhttps://github.com/BiomedSciAI/biomed-multi-alignment and\nhttps://huggingface.co/ibm/biomed.omics.bl.sm.ma-ted-458m.\n","authors":["Yoel Shoshan","Moshiko Raboh","Michal Ozery-Flato","Vadim Ratner","Alex Golts","Jeffrey K. Weber","Ella Barkan","Simona Rabinovici-Cohen","Sagi Polaczek","Ido Amos","Ben Shapira","Liam Hazan","Matan Ninio","Sivan Ravid","Michael M. Danziger","Joseph A. Morrone","Parthasarathy Suryanarayanan","Michal Rosen-Zvi","Efrat Hexter"],"pdf_url":"https://arxiv.org/pdf/2410.22367v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19153v2","updated":"2024-11-01T16:47:59Z","published":"2024-05-29T14:59:49Z","title":"A Study of Plasticity Loss in On-Policy Deep Reinforcement Learning","summary":"  Continual learning with deep neural networks presents challenges distinct\nfrom both the fixed-dataset and convex continual learning regimes. One such\nchallenge is plasticity loss, wherein a neural network trained in an online\nfashion displays a degraded ability to fit new tasks. This problem has been\nextensively studied in both supervised learning and off-policy reinforcement\nlearning (RL), where a number of remedies have been proposed. Still, plasticity\nloss has received less attention in the on-policy deep RL setting. Here we\nperform an extensive set of experiments examining plasticity loss and a variety\nof mitigation methods in on-policy deep RL. We demonstrate that plasticity loss\nis pervasive under domain shift in this regime, and that a number of methods\ndeveloped to resolve it in other settings fail, sometimes even performing worse\nthan applying no intervention at all. In contrast, we find that a class of\n``regenerative'' methods are able to consistently mitigate plasticity loss in a\nvariety of contexts, including in gridworld tasks and more challenging\nenvironments like Montezuma's Revenge and ProcGen.\n","authors":["Arthur Juliani","Jordan T. Ash"],"pdf_url":"https://arxiv.org/pdf/2405.19153v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.12032v2","updated":"2024-11-01T16:46:49Z","published":"2023-09-21T12:53:45Z","title":"Human-in-the-Loop Causal Discovery under Latent Confounding using\n  Ancestral GFlowNets","summary":"  Structure learning is the crux of causal inference. Notably, causal discovery\n(CD) algorithms are brittle when data is scarce, possibly inferring imprecise\ncausal relations that contradict expert knowledge -- especially when\nconsidering latent confounders. To aggravate the issue, most CD methods do not\nprovide uncertainty estimates, making it hard for users to interpret results\nand improve the inference process. Surprisingly, while CD is a human-centered\naffair, no works have focused on building methods that both 1) output\nuncertainty estimates that can be verified by experts and 2) interact with\nthose experts to iteratively refine CD. To solve these issues, we start by\nproposing to sample (causal) ancestral graphs proportionally to a belief\ndistribution based on a score function, such as the Bayesian information\ncriterion (BIC), using generative flow networks. Then, we leverage the\ndiversity in candidate graphs and introduce an optimal experimental design to\niteratively probe the expert about the relations among variables, effectively\nreducing the uncertainty of our belief over ancestral graphs. Finally, we\nupdate our samples to incorporate human feedback via importance sampling.\nImportantly, our method does not require causal sufficiency (i.e., unobserved\nconfounders may exist). Experiments with synthetic observational data show that\nour method can accurately sample from distributions over ancestral graphs and\nthat we can greatly improve inference quality with human aid.\n","authors":["Tiago da Silva","Eliezer Silva","António Góis","Dominik Heider","Samuel Kaski","Diego Mesquita","Adèle Ribeiro"],"pdf_url":"https://arxiv.org/pdf/2309.12032v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.15589v3","updated":"2024-11-01T16:39:36Z","published":"2024-05-24T14:20:09Z","title":"Efficient Adversarial Training in LLMs with Continuous Attacks","summary":"  Large language models (LLMs) are vulnerable to adversarial attacks that can\nbypass their safety guardrails. In many domains, adversarial training has\nproven to be one of the most promising methods to reliably improve robustness\nagainst such attacks. Yet, in the context of LLMs, current methods for\nadversarial training are hindered by the high computational costs required to\nperform discrete adversarial attacks at each training iteration. We address\nthis problem by instead calculating adversarial attacks in the continuous\nembedding space of the LLM, which is orders of magnitudes more efficient. We\npropose a fast adversarial training algorithm (C-AdvUL) composed of two losses:\nthe first makes the model robust on continuous embedding attacks computed on an\nadversarial behaviour dataset; the second ensures the usefulness of the final\nmodel by fine-tuning on utility data. Moreover, we introduce C-AdvIPO, an\nadversarial variant of IPO that does not require utility data for adversarially\nrobust alignment. Our empirical evaluation on five models from different\nfamilies (Gemma, Phi3, Mistral, Zephyr, Llama2) and at different scales (2B,\n3.8B, 7B) shows that both algorithms substantially enhance LLM robustness\nagainst discrete attacks (GCG, AutoDAN, PAIR), while maintaining utility. Our\nresults demonstrate that robustness to continuous perturbations can extrapolate\nto discrete threat models. Thereby, we present a path toward scalable\nadversarial training algorithms for robustly aligning LLMs.\n","authors":["Sophie Xhonneux","Alessandro Sordoni","Stephan Günnemann","Gauthier Gidel","Leo Schwinn"],"pdf_url":"https://arxiv.org/pdf/2405.15589v3.pdf","comment":"19 pages, 4 figures"},{"id":"http://arxiv.org/abs/2406.16121v2","updated":"2024-11-01T16:30:00Z","published":"2024-06-23T14:24:14Z","title":"Diffusion Spectral Representation for Reinforcement Learning","summary":"  Diffusion-based models have achieved notable empirical successes in\nreinforcement learning (RL) due to their expressiveness in modeling complex\ndistributions. Despite existing methods being promising, the key challenge of\nextending existing methods for broader real-world applications lies in the\ncomputational cost at inference time, i.e., sampling from a diffusion model is\nconsiderably slow as it often requires tens to hundreds of iterations to\ngenerate even one sample. To circumvent this issue, we propose to leverage the\nflexibility of diffusion models for RL from a representation learning\nperspective. In particular, by exploiting the connection between diffusion\nmodels and energy-based models, we develop Diffusion Spectral Representation\n(Diff-SR), a coherent algorithm framework that enables extracting sufficient\nrepresentations for value functions in Markov decision processes (MDP) and\npartially observable Markov decision processes (POMDP). We further demonstrate\nhow Diff-SR facilitates efficient policy optimization and practical algorithms\nwhile explicitly bypassing the difficulty and inference cost of sampling from\nthe diffusion model. Finally, we provide comprehensive empirical studies to\nverify the benefits of Diff-SR in delivering robust and advantageous\nperformance across various benchmarks with both fully and partially observable\nsettings.\n","authors":["Dmitry Shribak","Chen-Xiao Gao","Yitong Li","Chenjun Xiao","Bo Dai"],"pdf_url":"https://arxiv.org/pdf/2406.16121v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2406.02234v2","updated":"2024-11-01T16:22:33Z","published":"2024-06-04T11:56:19Z","title":"On the Limitations of Fractal Dimension as a Measure of Generalization","summary":"  Bounding and predicting the generalization gap of overparameterized neural\nnetworks remains a central open problem in theoretical machine learning. There\nis a recent and growing body of literature that proposes the framework of\nfractals to model optimization trajectories of neural networks, motivating\ngeneralization bounds and measures based on the fractal dimension of the\ntrajectory. Notably, the persistent homology dimension has been proposed to\ncorrelate with the generalization gap. This paper performs an empirical\nevaluation of these persistent homology-based generalization measures, with an\nin-depth statistical analysis. Our study reveals confounding effects in the\nobserved correlation between generalization and topological measures due to the\nvariation of hyperparameters. We also observe that fractal dimension fails to\npredict generalization of models trained from poor initializations. We lastly\nreveal the intriguing manifestation of model-wise double descent in these\ntopological generalization measures. Our work forms a basis for a deeper\ninvestigation of the causal relationships between fractal geometry, topological\ndata analysis, and neural network optimization.\n","authors":["Charlie B. Tan","Inés García-Redondo","Qiquan Wang","Michael M. Bronstein","Anthea Monod"],"pdf_url":"https://arxiv.org/pdf/2406.02234v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24198v2","updated":"2024-11-01T16:06:10Z","published":"2024-10-31T17:55:13Z","title":"SelfCodeAlign: Self-Alignment for Code Generation","summary":"  Instruction tuning is a supervised fine-tuning approach that significantly\nimproves the ability of large language models (LLMs) to follow human\ninstructions. We propose SelfCodeAlign, the first fully transparent and\npermissive pipeline for self-aligning code LLMs without extensive human\nannotations or distillation. SelfCodeAlign employs the same base model for\ninference throughout the data generation process. It first extracts diverse\ncoding concepts from high-quality seed snippets to generate new tasks. It then\nsamples multiple responses per task, pairs each with test cases, and validates\nthem in a sandbox environment. Finally, passing examples are selected for\ninstruction tuning. In our primary experiments, we use SelfCodeAlign with\nCodeQwen1.5-7B to generate a dataset of 74k instruction-response pairs.\nFinetuning on this dataset leads to a model that achieves a 67.1 pass@1 on\nHumanEval+, surpassing CodeLlama-70B-Instruct despite being ten times smaller.\nAcross all benchmarks, this finetuned model consistently outperforms the\noriginal version trained with OctoPack, the previous state-of-the-art method\nfor instruction tuning without human annotations or distillation. Additionally,\nwe show that SelfCodeAlign is effective across LLMs of various sizes, from 3B\nto 33B, and that the base models can benefit more from alignment with their own\ndata distribution. We further validate each component's effectiveness in our\npipeline, showing that SelfCodeAlign outperforms both direct distillation from\nGPT-4o and leading GPT-3.5-based distillation methods, such as OSS-Instruct and\nEvol-Instruct. SelfCodeAlign has also led to the creation of\nStarCoder2-Instruct, the first fully transparent, permissively licensed, and\nself-aligned code LLM that achieves state-of-the-art coding performance.\n","authors":["Yuxiang Wei","Federico Cassano","Jiawei Liu","Yifeng Ding","Naman Jain","Zachary Mueller","Harm de Vries","Leandro von Werra","Arjun Guha","Lingming Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.24198v2.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2404.05678v3","updated":"2024-11-01T16:03:23Z","published":"2024-04-08T16:57:44Z","title":"Flexible Fairness-Aware Learning via Inverse Conditional Permutation","summary":"  Equalized odds, as a popular notion of algorithmic fairness, aims to ensure\nthat sensitive variables, such as race and gender, do not unfairly influence\nthe algorithm's prediction when conditioning on the true outcome. Despite rapid\nadvancements, current research primarily focuses on equalized odds violations\ncaused by a single sensitive attribute, leaving the challenge of simultaneously\naccounting for multiple attributes largely unaddressed. We bridge this gap by\nintroducing an in-processing fairness-aware learning approach, FairICP, which\nintegrates adversarial learning with a novel inverse conditional permutation\nscheme. FairICP offers a theoretically justified, flexible, and efficient\nscheme to promote equalized odds under fairness conditions described by complex\nand multidimensional sensitive attributes. The efficacy and adaptability of our\nmethod are demonstrated through both simulation studies and empirical analyses\nof real-world datasets.\n","authors":["Yuheng Lai","Leying Guan"],"pdf_url":"https://arxiv.org/pdf/2404.05678v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.17835v2","updated":"2024-11-01T15:55:34Z","published":"2024-01-31T13:52:11Z","title":"Simplifying Latent Dynamics with Softly State-Invariant World Models","summary":"  To solve control problems via model-based reasoning or planning, an agent\nneeds to know how its actions affect the state of the world. The actions an\nagent has at its disposal often change the state of the environment in\nsystematic ways. However, existing techniques for world modelling do not\nguarantee that the effect of actions are represented in such systematic ways.\nWe introduce the Parsimonious Latent Space Model (PLSM), a world model that\nregularizes the latent dynamics to make the effect of the agent's actions more\npredictable. Our approach minimizes the mutual information between latent\nstates and the change that an action produces in the agent's latent state, in\nturn minimizing the dependence the state has on the dynamics. This makes the\nworld model softly state-invariant. We combine PLSM with different model\nclasses used for i) future latent state prediction, ii) planning, and iii)\nmodel-free reinforcement learning. We find that our regularization improves\naccuracy, generalization, and performance in downstream tasks, highlighting the\nimportance of systematic treatment of actions in world models.\n","authors":["Tankred Saanum","Peter Dayan","Eric Schulz"],"pdf_url":"https://arxiv.org/pdf/2401.17835v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.01684v2","updated":"2024-11-01T15:46:35Z","published":"2023-10-02T22:42:52Z","title":"Designing User-Centric Behavioral Interventions to Prevent Dysglycemia\n  with Novel Counterfactual Explanations","summary":"  Monitoring unexpected health events and taking actionable measures to avert\nthem beforehand is central to maintaining health and preventing disease.\nTherefore, a tool capable of predicting adverse health events and offering\nusers actionable feedback about how to make changes in their diet, exercise,\nand medication to prevent abnormal health events could have significant\nsocietal impacts. Counterfactual explanations can provide insights into why a\nmodel made a particular prediction by generating hypothetical instances that\nare similar to the original input but lead to a different prediction outcome.\nTherefore, counterfactuals can be viewed as a means to design AI-driven health\ninterventions to not only predict but also prevent adverse health outcomes such\nas blood glucose spikes, diabetes, and heart disease. In this paper, we design\n\\textit{\\textbf{ExAct}}, a novel model-agnostic framework for generating\ncounterfactual explanations for chronic disease prevention and management.\nLeveraging insights from adversarial learning, ExAct characterizes the decision\nboundary for high-dimensional data and performs a grid search to generate\nactionable interventions. ExAct is unique in integrating prior knowledge about\nuser preferences of feasible explanations into the process of counterfactual\ngeneration. ExAct is evaluated extensively using four real-world datasets and\nexternal simulators. With $82.8\\%$ average validity in the simulation-aided\nvalidation, ExAct surpasses the state-of-the-art techniques for generating\ncounterfactual explanations by at least $10\\%$. Besides, counterfactuals from\nExAct exhibit at least $6.6\\%$ improved proximity compared to previous\nresearch.\n","authors":["Asiful Arefeen","Hassan Ghasemzadeh"],"pdf_url":"https://arxiv.org/pdf/2310.01684v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03901v2","updated":"2024-11-01T15:19:18Z","published":"2024-10-04T20:08:24Z","title":"Improving Node Representation by Boosting Target-Aware Contrastive Loss","summary":"  Graphs model complex relationships between entities, with nodes and edges\ncapturing intricate connections. Node representation learning involves\ntransforming nodes into low-dimensional embeddings. These embeddings are\ntypically used as features for downstream tasks. Therefore, their quality has a\nsignificant impact on task performance. Existing approaches for node\nrepresentation learning span (semi-)supervised, unsupervised, and\nself-supervised paradigms. In graph domains, (semi-)supervised learning often\nonly optimizes models based on class labels, neglecting other abundant graph\nsignals, which limits generalization. While self-supervised or unsupervised\nlearning produces representations that better capture underlying graph signals,\nthe usefulness of these captured signals for downstream target tasks can vary.\nTo bridge this gap, we introduce Target-Aware Contrastive Learning\n(Target-aware CL) which aims to enhance target task performance by maximizing\nthe mutual information between the target task and node representations with a\nself-supervised learning process. This is achieved through a sampling function,\nXGBoost Sampler (XGSampler), to sample proper positive examples for the\nproposed Target-Aware Contrastive Loss (XTCL). By minimizing XTCL, Target-aware\nCL increases the mutual information between the target task and node\nrepresentations, such that model generalization is improved. Additionally,\nXGSampler enhances the interpretability of each signal by showing the weights\nfor sampling the proper positive examples. We show experimentally that XTCL\nsignificantly improves the performance on two target tasks: node classification\nand link prediction tasks, compared to state-of-the-art models.\n","authors":["Ying-Chun Lin","Jennifer Neville"],"pdf_url":"https://arxiv.org/pdf/2410.03901v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.15699v2","updated":"2024-11-01T15:13:19Z","published":"2024-05-24T16:43:26Z","title":"Dimension-free deterministic equivalents for random feature regression","summary":"  In this work we investigate the generalization performance of random feature\nridge regression (RFRR). Our main contribution is a general deterministic\nequivalent for the test error of RFRR. Specifically, under a certain\nconcentration property, we show that the test error is well approximated by a\nclosed-form expression that only depends on the feature map eigenvalues.\nNotably, our approximation guarantee is non-asymptotic, multiplicative, and\nindependent of the feature map dimension -- allowing for infinite-dimensional\nfeatures. We expect this deterministic equivalent to hold broadly beyond our\ntheoretical analysis, and we empirically validate its predictions on various\nreal and synthetic datasets. As an application, we derive sharp excess error\nrates under standard power-law assumptions of the spectrum and target decay. In\nparticular, we provide a tight result for the smallest number of features\nachieving optimal minimax error rate.\n","authors":["Leonardo Defilippis","Bruno Loureiro","Theodor Misiakiewicz"],"pdf_url":"https://arxiv.org/pdf/2405.15699v2.pdf","comment":"NeurIPS 2024 camera-ready version"},{"id":"http://arxiv.org/abs/2410.22283v2","updated":"2024-11-01T15:00:44Z","published":"2024-10-29T17:36:10Z","title":"Leveraging Recurrent Neural Networks for Predicting Motor Movements from\n  Primate Motor Cortex Neural Recordings","summary":"  This paper presents an efficient deep learning solution for decoding motor\nmovements from neural recordings in non-human primates. An Autoencoder Gated\nRecurrent Unit (AEGRU) model was adopted as the model architecture for this\ntask. The autoencoder is only used during the training stage to achieve better\ngeneralization. Together with the preprocessing techniques, our model achieved\n0.71 $R^2$ score, surpassing the baseline models in Neurobench and is ranked\nfirst for $R^2$ in the IEEE BioCAS 2024 Grand Challenge on Neural Decoding.\nModel pruning is also applied leading to a reduction of 41.4% of the\nmultiply-accumulate (MAC) operations with little change in the $R^2$ score\ncompared to the unpruned model.\n","authors":["Yuanxi Wang","Zuowen Wang","Shih-Chii Liu"],"pdf_url":"https://arxiv.org/pdf/2410.22283v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14940v3","updated":"2024-11-01T14:49:44Z","published":"2024-10-19T02:07:33Z","title":"Nova: A Practical and Advanced Alignment","summary":"  We introduce Nova, a suite of practical alignment techniques employed in a\nseries of empirically validated high-performing models. This represents the\nfirst comprehensive account of alignment methodologies, offering valuable\ninsights for advancing AI research. We investigate the critical components that\nenhance model performance during the alignment process, including optimization\nmethods, data strategies, capability enhancements, and evaluation processes.\nThe process spans three key stages: Prompt Augmentation System(PAS), Supervised\nFine-Tuning(SFT), and Preference Alignment. The problems encountered, the\nsolutions applied, and the improvements made are thoroughly recorded.\n  Through comparisons across well-established benchmarks, we highlight the\ntechnological advancements enabled by Nova Alignment. Importantly,\nQwen2-Nova-72B and Llama3-PBM-Nova-70B are instruct versions of the Qwen2-72B\nand Llama-3-70B base models, optimized through Nova. The Nova models show\nsignificant core improvements, with user experience gains of 17% to 28%, and\nexcels on specialized benchmarks. In open-source benchmark evaluations, both\nQwen2-Nova-72B and Llama3-PBM-Nova-70B consistently outperform their respective\nofficial instruct versions across nearly all datasets. This report aims to\nclarify the key technologies behind the alignment process, fostering a deeper\nunderstanding within the community. Llama3-PBM-Nova-70B model is available at\nhttps://huggingface.co/PKU-Baichuan-MLSystemLab/Llama3-PBM-Nova-70B.\n","authors":["Mingan Lin","Fan Yang","Yanjun Shen","Haoze Sun","Tianpeng Li","Tao Zhang","Chenzheng Zhu","Tao Zhang","Miao Zheng","Xu Li","Yijie Zhou","Mingyang Chen","Yanzhao Qin","Youquan Li","Hao Liang","Fei Li","Yadong Li","Mang Wang","Guosheng Dong","Kun Fang","Jianhua Xu","Bin Cui","Wentao Zhang","Zenan Zhou","Weipeng Chen"],"pdf_url":"https://arxiv.org/pdf/2410.14940v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07955v2","updated":"2024-11-01T14:45:44Z","published":"2023-12-13T08:01:15Z","title":"Erasing Self-Supervised Learning Backdoor by Cluster Activation Masking","summary":"  Self-Supervised Learning (SSL) is an effective paradigm for learning\nrepresentations from unlabeled data, such as text, images, and videos. However,\nresearchers have recently found that SSL is vulnerable to backdoor attacks. The\nattacker can embed hidden SSL backdoors via a few poisoned examples in the\ntraining dataset and maliciously manipulate the behavior of downstream models.\nTo defend against SSL backdoor attacks, a feasible route is to detect and\nremove the poisonous samples in the training set. However, the existing SSL\nbackdoor defense method fails to detect the poisonous samples precisely. In\nthis paper, we propose to erase the SSL backdoor by cluster activation masking\nand propose a novel PoisonCAM method. After obtaining the threat model trained\non the poisoned dataset, our method can precisely detect poisonous samples\nbased on the assumption that masking the backdoor trigger can effectively\nchange the activation of a downstream clustering model. In experiments, our\nPoisonCAM achieves 96\\% accuracy for backdoor trigger detection compared to 3\\%\nof the state-of-the-art method on poisoned ImageNet-100. Moreover, our proposed\nPoisonCAM significantly improves the performance of the trained SSL model under\nbackdoor attacks compared to the state-of-the-art method. Our code, data, and\ntrained models will be open once this paper is accepted.\n","authors":["Shengsheng Qian","Dizhan Xue","Yifei Wang","Shengjie Zhang","Huaiwen Zhang","Changsheng Xu"],"pdf_url":"https://arxiv.org/pdf/2312.07955v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.02552v2","updated":"2024-11-01T14:44:44Z","published":"2024-02-04T15:54:37Z","title":"Neur2BiLO: Neural Bilevel Optimization","summary":"  Bilevel optimization deals with nested problems in which a leader takes the\nfirst decision to minimize their objective function while accounting for a\nfollower's best-response reaction. Constrained bilevel problems with integer\nvariables are particularly notorious for their hardness. While exact solvers\nhave been proposed for mixed-integer linear bilevel optimization, they tend to\nscale poorly with problem size and are hard to generalize to the non-linear\ncase. On the other hand, problem-specific algorithms (exact and heuristic) are\nlimited in scope. Under a data-driven setting in which similar instances of a\nbilevel problem are solved routinely, our proposed framework, Neur2BiLO, embeds\na neural network approximation of the leader's or follower's value function,\ntrained via supervised regression, into an easy-to-solve mixed-integer program.\nNeur2BiLO serves as a heuristic that produces high-quality solutions extremely\nfast for four applications with linear and non-linear objectives and pure and\nmixed-integer variables.\n","authors":["Justin Dumouchelle","Esther Julien","Jannis Kurtz","Elias B. Khalil"],"pdf_url":"https://arxiv.org/pdf/2402.02552v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.00132v2","updated":"2024-11-01T14:36:49Z","published":"2024-05-31T18:47:30Z","title":"QuanTA: Efficient High-Rank Fine-Tuning of LLMs with Quantum-Informed\n  Tensor Adaptation","summary":"  We propose Quantum-informed Tensor Adaptation (QuanTA), a novel,\neasy-to-implement, fine-tuning method with no inference overhead for\nlarge-scale pre-trained language models. By leveraging quantum-inspired methods\nderived from quantum circuit structures, QuanTA enables efficient high-rank\nfine-tuning, surpassing the limitations of Low-Rank Adaptation (LoRA)--low-rank\napproximation may fail for complicated downstream tasks. Our approach is\ntheoretically supported by the universality theorem and the rank representation\ntheorem to achieve efficient high-rank adaptations. Experiments demonstrate\nthat QuanTA significantly enhances commonsense reasoning, arithmetic reasoning,\nand scalability compared to traditional methods. Furthermore, QuanTA shows\nsuperior performance with fewer trainable parameters compared to other\napproaches and can be designed to integrate with existing fine-tuning\nalgorithms for further improvement, providing a scalable and efficient solution\nfor fine-tuning large language models and advancing state-of-the-art in natural\nlanguage processing.\n","authors":["Zhuo Chen","Rumen Dangovski","Charlotte Loh","Owen Dugan","Di Luo","Marin Soljačić"],"pdf_url":"https://arxiv.org/pdf/2406.00132v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17423v3","updated":"2024-11-01T14:32:12Z","published":"2024-02-27T11:32:14Z","title":"Reinforced In-Context Black-Box Optimization","summary":"  Black-Box Optimization (BBO) has found successful applications in many fields\nof science and engineering. Recently, there has been a growing interest in\nmeta-learning particular components of BBO algorithms to speed up optimization\nand get rid of tedious hand-crafted heuristics. As an extension, learning the\nentire algorithm from data requires the least labor from experts and can\nprovide the most flexibility. In this paper, we propose RIBBO, a method to\nreinforce-learn a BBO algorithm from offline data in an end-to-end fashion.\nRIBBO employs expressive sequence models to learn the optimization histories\nproduced by multiple behavior algorithms and tasks, leveraging the in-context\nlearning ability of large models to extract task information and make decisions\naccordingly. Central to our method is to augment the optimization histories\nwith \\textit{regret-to-go} tokens, which are designed to represent the\nperformance of an algorithm based on cumulative regret over the future part of\nthe histories. The integration of regret-to-go tokens enables RIBBO to\nautomatically generate sequences of query points that satisfy the user-desired\nregret, which is verified by its universally good empirical performance on\ndiverse problems, including BBO benchmark functions, hyper-parameter\noptimization and robot control problems.\n","authors":["Lei Song","Chenxiao Gao","Ke Xue","Chenyang Wu","Dong Li","Jianye Hao","Zongzhang Zhang","Chao Qian"],"pdf_url":"https://arxiv.org/pdf/2402.17423v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.18947v3","updated":"2024-11-01T13:53:44Z","published":"2024-04-27T07:22:28Z","title":"Multimodal Fusion on Low-quality Data: A Comprehensive Survey","summary":"  Multimodal fusion focuses on integrating information from multiple modalities\nwith the goal of more accurate prediction, which has achieved remarkable\nprogress in a wide range of scenarios, including autonomous driving and medical\ndiagnosis. However, the reliability of multimodal fusion remains largely\nunexplored especially under low-quality data settings. This paper surveys the\ncommon challenges and recent advances of multimodal fusion in the wild and\npresents them in a comprehensive taxonomy. From a data-centric view, we\nidentify four main challenges that are faced by multimodal fusion on\nlow-quality data, namely (1) noisy multimodal data that are contaminated with\nheterogeneous noises, (2) incomplete multimodal data that some modalities are\nmissing, (3) imbalanced multimodal data that the qualities or properties of\ndifferent modalities are significantly different and (4) quality-varying\nmultimodal data that the quality of each modality dynamically changes with\nrespect to different samples. This new taxonomy will enable researchers to\nunderstand the state of the field and identify several potential directions. We\nalso provide discussion for the open problems in this field together with\ninteresting future research directions.\n","authors":["Qingyang Zhang","Yake Wei","Zongbo Han","Huazhu Fu","Xi Peng","Cheng Deng","Qinghua Hu","Cai Xu","Jie Wen","Di Hu","Changqing Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.18947v3.pdf","comment":"Feel free to comment on our manuscript: qingyangzhang@tju$.$edu$.$cn"},{"id":"http://arxiv.org/abs/2402.13622v2","updated":"2024-11-01T13:33:58Z","published":"2024-02-21T08:50:33Z","title":"Analysis of Bootstrap and Subsampling in High-dimensional Regularized\n  Regression","summary":"  We investigate popular resampling methods for estimating the uncertainty of\nstatistical models, such as subsampling, bootstrap and the jackknife, and their\nperformance in high-dimensional supervised regression tasks. We provide a tight\nasymptotic description of the biases and variances estimated by these methods\nin the context of generalized linear models, such as ridge and logistic\nregression, taking the limit where the number of samples $n$ and dimension $d$\nof the covariates grow at a comparable fixed rate $\\alpha\\!=\\! n/d$. Our\nfindings are three-fold: i) resampling methods are fraught with problems in\nhigh dimensions and exhibit the double-descent-like behavior typical of these\nsituations; ii) only when $\\alpha$ is large enough do they provide consistent\nand reliable error estimations (we give convergence rates); iii) in the\nover-parametrized regime $\\alpha\\!<\\!1$ relevant to modern machine learning\npractice, their predictions are not consistent, even with optimal\nregularization.\n","authors":["Lucas Clarté","Adrien Vandenbroucque","Guillaume Dalle","Bruno Loureiro","Florent Krzakala","Lenka Zdeborová"],"pdf_url":"https://arxiv.org/pdf/2402.13622v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.19047v3","updated":"2024-11-01T13:28:59Z","published":"2024-02-29T11:20:16Z","title":"Theoretical Foundations of Deep Selective State-Space Models","summary":"  Structured state-space models (SSMs) such as S4, stemming from the seminal\nwork of Gu et al., are gaining popularity as effective approaches for modeling\nsequential data. Deep SSMs demonstrate outstanding performance across a diverse\nset of domains, at a reduced training and inference cost compared to\nattention-based transformers. Recent developments show that if the linear\nrecurrence powering SSMs allows for multiplicative interactions between inputs\nand hidden states (e.g. GateLoop, Mamba, GLA), then the resulting architecture\ncan surpass in both in accuracy and efficiency attention-powered foundation\nmodels trained on text, at scales of billion parameters. In this paper, we give\ntheoretical grounding to this recent finding using tools from Rough Path\nTheory: we show that when random linear recurrences are equipped with simple\ninput-controlled transitions (selectivity mechanism), then the hidden state is\nprovably a low-dimensional projection of a powerful mathematical object called\nthe signature of the input -- capturing non-linear interactions between tokens\nat distinct timescales. Our theory not only motivates the success of modern\nselective state-space models such as Mamba but also provides a solid framework\nto understand the expressive power of future SSM variants.\n","authors":["Nicola Muca Cirone","Antonio Orvieto","Benjamin Walker","Cristopher Salvi","Terry Lyons"],"pdf_url":"https://arxiv.org/pdf/2402.19047v3.pdf","comment":"NeurIPS Version w/ minor edits"},{"id":"http://arxiv.org/abs/2210.08650v3","updated":"2024-11-01T13:02:25Z","published":"2022-10-16T22:28:36Z","title":"Accelerating Transfer Learning with Near-Data Computation on Cloud\n  Object Stores","summary":"  Storage disaggregation underlies today's cloud and is naturally complemented\nby pushing down some computation to storage, thus mitigating the potential\nnetwork bottleneck between the storage and compute tiers. We show how ML\ntraining benefits from storage pushdowns by focusing on transfer learning (TL),\nthe widespread technique that democratizes ML by reusing existing knowledge on\nrelated tasks. We propose HAPI, a new TL processing system centered around two\ncomplementary techniques that address challenges introduced by disaggregation.\nFirst, applications must carefully balance execution across tiers for\nperformance. HAPI judiciously splits the TL computation during the feature\nextraction phase yielding pushdowns that not only improve network time but also\nimprove total TL training time by overlapping the execution of consecutive\ntraining iterations across tiers. Second, operators want resource efficiency\nfrom the storage-side computational resources. HAPI employs storage-side batch\nsize adaptation allowing increased storage-side pushdown concurrency without\naffecting training accuracy. HAPI yields up to 2.5x training speed-up while\nchoosing in 86.8% of cases the best performing split point or one that is at\nmost 5% off from the best.\n","authors":["Diana Petrescu","Arsany Guirguis","Do Le Quoc","Javier Picorel","Rachid Guerraoui","Florin Dinu"],"pdf_url":"https://arxiv.org/pdf/2210.08650v3.pdf","comment":"To appear in the proceedings of SoCC '24"},{"id":"http://arxiv.org/abs/2311.12056v3","updated":"2024-11-01T12:54:28Z","published":"2023-11-18T13:55:05Z","title":"Kuro Siwo: 33 billion $m^2$ under the water. A global multi-temporal\n  satellite dataset for rapid flood mapping","summary":"  Global floods, exacerbated by climate change, pose severe threats to human\nlife, infrastructure, and the environment. Recent catastrophic events in\nPakistan and New Zealand underscore the urgent need for precise flood mapping\nto guide restoration efforts, understand vulnerabilities, and prepare for\nfuture occurrences. While Synthetic Aperture Radar (SAR) remote sensing offers\nday-and-night, all-weather imaging capabilities, its application in deep\nlearning for flood segmentation is limited by the lack of large annotated\ndatasets. To address this, we introduce Kuro Siwo, a manually annotated\nmulti-temporal dataset, spanning 43 flood events globally. Our dataset maps\nmore than 338 billion $m^2$ of land, with 33 billion designated as either\nflooded areas or permanent water bodies. Kuro Siwo includes a highly processed\nproduct optimized for flood mapping based on SAR Ground Range Detected, and a\nprimal SAR Single Look Complex product with minimal preprocessing, designed to\npromote research on the exploitation of both the phase and amplitude\ninformation and to offer maximum flexibility for downstream task preprocessing.\nTo leverage advances in large scale self-supervised pretraining methods for\nremote sensing data, we augment Kuro Siwo with a large unlabeled set of SAR\nsamples. Finally, we provide an extensive benchmark, namely BlackBench,\noffering strong baselines for a diverse set of flood events from Europe,\nAmerica, Africa, Asia and Australia.\n","authors":["Nikolaos Ioannis Bountos","Maria Sdraka","Angelos Zavras","Ilektra Karasante","Andreas Karavias","Themistocles Herekakis","Angeliki Thanasou","Dimitrios Michail","Ioannis Papoutsis"],"pdf_url":"https://arxiv.org/pdf/2311.12056v3.pdf","comment":"Accepted at the 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024) Track on Datasets and Benchmarks"},{"id":"http://arxiv.org/abs/2409.15246v3","updated":"2024-11-01T12:49:19Z","published":"2024-09-23T17:42:05Z","title":"On-Air Deep Learning Integrated Semantic Inference Models for Enhanced\n  Earth Observation Satellite Networks","summary":"  Earth Observation (EO) systems are crucial for cartography, disaster\nsurveillance, and resource administration. Nonetheless, they encounter\nconsiderable obstacles in the processing and transmission of extensive data,\nespecially in specialized domains such as precision agriculture and real-time\ndisaster response. Earth observation satellites, outfitted with remote sensing\ntechnology, gather data from onboard sensors and IoT-enabled terrestrial\nobjects, delivering important information remotely. Domain-adapted Large\nLanguage Models (LLMs) provide a solution by enabling the integration of raw\nand processed EO data. Through domain adaptation, LLMs improve the assimilation\nand analysis of many data sources, tackling the intricacies of specialized\ndatasets in agriculture and disaster response. This data synthesis, directed by\nLLMs, enhances the precision and pertinence of conveyed information. This study\nprovides a thorough examination of using semantic inference and deep learning\nfor sophisticated EO systems. It presents an innovative architecture for\nsemantic communication in EO satellite networks, designed to improve data\ntransmission efficiency using semantic processing methodologies. Recent\nadvancements in onboard processing technologies enable dependable, adaptable,\nand energy-efficient data management in orbit. These improvements guarantee\nreliable performance in adverse space circumstances using radiation-hardened\nand reconfigurable technology. Collectively, these advancements enable\nnext-generation satellite missions with improved processing capabilities,\ncrucial for operational flexibility and real-time decision-making in 6G\nsatellite communication.\n","authors":["Hong-fu Chou","Vu Nguyen Ha","Prabhu Thiruvasagam","Thanh-Dung Le","Geoffrey Eappen","Ti Ti Nguyen","Luis M. Garces-Socarras","Jorge L. Gonzalez-Rios","Juan Carlos Merlano-Duncan","Symeon Chatzinotas"],"pdf_url":"https://arxiv.org/pdf/2409.15246v3.pdf","comment":"17 pages, 7 figures, Journal"},{"id":"http://arxiv.org/abs/2405.16405v2","updated":"2024-11-01T12:15:36Z","published":"2024-05-26T02:12:02Z","title":"Intruding with Words: Towards Understanding Graph Injection Attacks at\n  the Text Level","summary":"  Graph Neural Networks (GNNs) excel across various applications but remain\nvulnerable to adversarial attacks, particularly Graph Injection Attacks (GIAs),\nwhich inject malicious nodes into the original graph and pose realistic\nthreats. Text-attributed graphs (TAGs), where nodes are associated with textual\nfeatures, are crucial due to their prevalence in real-world applications and\nare commonly used to evaluate these vulnerabilities. However, existing research\nonly focuses on embedding-level GIAs, which inject node embeddings rather than\nactual textual content, limiting their applicability and simplifying detection.\nIn this paper, we pioneer the exploration of GIAs at the text level, presenting\nthree novel attack designs that inject textual content into the graph. Through\ntheoretical and empirical analysis, we demonstrate that text interpretability,\na factor previously overlooked at the embedding level, plays a crucial role in\nattack strength. Among the designs we investigate, the Word-frequency-based\nText-level GIA (WTGIA) is particularly notable for its balance between\nperformance and interpretability. Despite the success of WTGIA, we discover\nthat defenders can easily enhance their defenses with customized text embedding\nmethods or large language model (LLM)--based predictors. These insights\nunderscore the necessity for further research into the potential and practical\nsignificance of text-level GIAs.\n","authors":["Runlin Lei","Yuwei Hu","Yuchen Ren","Zhewei Wei"],"pdf_url":"https://arxiv.org/pdf/2405.16405v2.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2402.06963v3","updated":"2024-11-01T11:46:22Z","published":"2024-02-10T14:36:31Z","title":"Tree Ensembles for Contextual Bandits","summary":"  We propose a new framework for contextual multi-armed bandits based on tree\nensembles. Our framework adapts two widely used bandit methods, Upper\nConfidence Bound and Thompson Sampling, for both standard and combinatorial\nsettings. As part of this framework, we propose a novel method of estimating\nthe uncertainty in tree ensemble predictions. We further demonstrate the\neffectiveness of our framework via several experimental studies, employing\nXGBoost and random forests, two popular tree ensemble methods. Compared to\nstate-of-the-art methods based on decision trees and neural networks, our\nmethods exhibit superior performance in terms of both regret minimization and\ncomputational runtime, when applied to benchmark datasets and the real-world\napplication of navigation over road networks.\n","authors":["Hannes Nilsson","Rikard Johansson","Niklas Åkerblom","Morteza Haghir Chehreghani"],"pdf_url":"https://arxiv.org/pdf/2402.06963v3.pdf","comment":"The first two authors contributed equally to this work"},{"id":"http://arxiv.org/abs/2410.06726v2","updated":"2024-11-01T10:28:17Z","published":"2024-10-09T09:50:06Z","title":"Bounds and Sensitivity Analysis of the Causal Effect Under\n  Outcome-Independent MNAR Confounding","summary":"  We report assumption-free bounds for any contrast between the probabilities\nof the potential outcome under exposure and non-exposure when the confounders\nare missing not at random. We assume that the missingness mechanism is\noutcome-independent. We also report a sensitivity analysis method to complement\nour bounds.\n","authors":["Jose M. Peña"],"pdf_url":"https://arxiv.org/pdf/2410.06726v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05300v4","updated":"2024-11-01T10:19:01Z","published":"2024-03-08T13:29:46Z","title":"Unity by Diversity: Improved Representation Learning in Multimodal VAEs","summary":"  Variational Autoencoders for multimodal data hold promise for many tasks in\ndata analysis, such as representation learning, conditional generation, and\nimputation. Current architectures either share the encoder output, decoder\ninput, or both across modalities to learn a shared representation. Such\narchitectures impose hard constraints on the model. In this work, we show that\na better latent representation can be obtained by replacing these hard\nconstraints with a soft constraint. We propose a new mixture-of-experts prior,\nsoftly guiding each modality's latent representation towards a shared aggregate\nposterior. This approach results in a superior latent representation and allows\neach encoding to preserve information better from its uncompressed original\nfeatures. In extensive experiments on multiple benchmark datasets and two\nchallenging real-world datasets, we show improved learned latent\nrepresentations and imputation of missing data modalities compared to existing\nmethods.\n","authors":["Thomas M. Sutter","Yang Meng","Andrea Agostini","Daphné Chopard","Norbert Fortin","Julia E. Vogt","Bahbak Shahbaba","Stephan Mandt"],"pdf_url":"https://arxiv.org/pdf/2403.05300v4.pdf","comment":"Accepted at Neurips 2024"},{"id":"http://arxiv.org/abs/2405.14669v2","updated":"2024-11-01T09:56:53Z","published":"2024-05-23T15:06:02Z","title":"Efficiency for Free: Ideal Data Are Transportable Representations","summary":"  Data, the seminal opportunity and challenge in modern machine learning,\ncurrently constrains the scalability of representation learning and impedes the\npace of model evolution. In this work, we investigate the efficiency properties\nof data from both optimization and generalization perspectives. Our theoretical\nand empirical analysis reveals an unexpected finding: for a given task,\nutilizing a publicly available, task- and architecture-agnostic model (referred\nto as the `prior model' in this paper) can effectively produce efficient data.\nBuilding on this insight, we propose the Representation Learning Accelerator\n(\\algopt), which promotes the formation and utilization of efficient data,\nthereby accelerating representation learning. Utilizing a ResNet-18 pre-trained\non CIFAR-10 as a prior model to inform ResNet-50 training on ImageNet-1K\nreduces computational costs by 50% while maintaining the same accuracy as the\nmodel trained with the original BYOL, which requires 100% cost. Our code is\navailable at: \\url{https://github.com/LINs-lab/ReLA}.\n","authors":["Peng Sun","Yi Jiang","Tao Lin"],"pdf_url":"https://arxiv.org/pdf/2405.14669v2.pdf","comment":"Code: https://github.com/LINs-lab/ReLA"},{"id":"http://arxiv.org/abs/2405.20778v2","updated":"2024-11-01T09:53:53Z","published":"2024-05-28T06:10:12Z","title":"Improved Generation of Adversarial Examples Against Safety-aligned LLMs","summary":"  Adversarial prompts generated using gradient-based methods exhibit\noutstanding performance in performing automatic jailbreak attacks against\nsafety-aligned LLMs. Nevertheless, due to the discrete nature of texts, the\ninput gradient of LLMs struggles to precisely reflect the magnitude of loss\nchange that results from token replacements in the prompt, leading to limited\nattack success rates against safety-aligned LLMs, even in the white-box\nsetting. In this paper, we explore a new perspective on this problem,\nsuggesting that it can be alleviated by leveraging innovations inspired in\ntransfer-based attacks that were originally proposed for attacking black-box\nimage classification models. For the first time, we appropriate the ideologies\nof effective methods among these transfer-based attacks, i.e., Skip Gradient\nMethod and Intermediate Level Attack, into gradient-based adversarial prompt\ngeneration and achieve significant performance gains without introducing\nobvious computational cost. Meanwhile, by discussing mechanisms behind the\ngains, new insights are drawn, and proper combinations of these methods are\nalso developed. Our empirical results show that 87% of the query-specific\nadversarial suffixes generated by the developed combination can induce\nLlama-2-7B-Chat to produce the output that exactly matches the target string on\nAdvBench. This match rate is 33% higher than that of a very strong baseline\nknown as GCG, demonstrating advanced discrete optimization for adversarial\nprompt generation against LLMs. In addition, without introducing obvious cost,\nthe combination achieves >30% absolute increase in attack success rates\ncompared with GCG when generating both query-specific (38% -> 68%) and\nuniversal adversarial prompts (26.68% -> 60.32%) for attacking the\nLlama-2-7B-Chat model on AdvBench. Code at:\nhttps://github.com/qizhangli/Gradient-based-Jailbreak-Attacks.\n","authors":["Qizhang Li","Yiwen Guo","Wangmeng Zuo","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2405.20778v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23495v2","updated":"2024-11-01T09:49:24Z","published":"2024-10-30T22:57:54Z","title":"DASH: Warm-Starting Neural Network Training in Stationary Settings\n  without Loss of Plasticity","summary":"  Warm-starting neural network training by initializing networks with\npreviously learned weights is appealing, as practical neural networks are often\ndeployed under a continuous influx of new data. However, it often leads to loss\nof plasticity, where the network loses its ability to learn new information,\nresulting in worse generalization than training from scratch. This occurs even\nunder stationary data distributions, and its underlying mechanism is poorly\nunderstood. We develop a framework emulating real-world neural network training\nand identify noise memorization as the primary cause of plasticity loss when\nwarm-starting on stationary data. Motivated by this, we propose Direction-Aware\nSHrinking (DASH), a method aiming to mitigate plasticity loss by selectively\nforgetting memorized noise while preserving learned features. We validate our\napproach on vision tasks, demonstrating improvements in test accuracy and\ntraining efficiency.\n","authors":["Baekrok Shin","Junsoo Oh","Hanseul Cho","Chulhee Yun"],"pdf_url":"https://arxiv.org/pdf/2410.23495v2.pdf","comment":"Published at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.19192v2","updated":"2024-11-01T09:45:29Z","published":"2024-10-24T22:50:21Z","title":"TEAM: Topological Evolution-aware Framework for Traffic\n  Forecasting--Extended Version","summary":"  Due to the global trend towards urbanization, people increasingly move to and\nlive in cities that then continue to grow. Traffic forecasting plays an\nimportant role in the intelligent transportation systems of cities as well as\nin spatio-temporal data mining. State-of-the-art forecasting is achieved by\ndeep-learning approaches due to their ability to contend with complex\nspatio-temporal dynamics. However, existing methods assume the input is\nfixed-topology road networks and static traffic time series. These assumptions\nfail to align with urbanization, where time series are collected continuously\nand road networks evolve over time. In such settings, deep-learning models\nrequire frequent re-initialization and re-training, imposing high computational\ncosts. To enable much more efficient training without jeopardizing model\naccuracy, we propose the Topological Evolution-aware Framework (TEAM) for\ntraffic forecasting that incorporates convolution and attention. This\ncombination of mechanisms enables better adaptation to newly collected time\nseries, while being able to maintain learned knowledge from old time series.\nTEAM features a continual learning module based on the Wasserstein metric that\nacts as a buffer that can identify the most stable and the most changing\nnetwork nodes. Then, only data related to stable nodes is employed for\nre-training when consolidating a model. Further, only data of new nodes and\ntheir adjacent nodes as well as data pertaining to changing nodes are used to\nre-train the model. Empirical studies with two real-world traffic datasets\noffer evidence that TEAM is capable of much lower re-training costs than\nexisting methods are, without jeopardizing forecasting accuracy.\n","authors":["Duc Kieu","Tung Kieu","Peng Han","Bin Yang","Christian S. Jensen","Bac Le"],"pdf_url":"https://arxiv.org/pdf/2410.19192v2.pdf","comment":"16 pages. An extended version of \"TEAM: Topological Evolution-aware\n  Framework for Traffic Forecasting\" accepted at PVLDB 2025"},{"id":"http://arxiv.org/abs/2410.24070v2","updated":"2024-11-01T09:41:09Z","published":"2024-10-31T16:07:21Z","title":"Dynamical similarity analysis uniquely captures how computations develop\n  in RNNs","summary":"  Methods for analyzing representations in neural systems are increasingly\npopular tools in neuroscience and mechanistic interpretability. Measures\ncomparing neural activations across conditions, architectures, and species give\nscalable ways to understand information transformation within different neural\nnetworks. However, recent findings show that some metrics respond to spurious\nsignals, leading to misleading results. Establishing benchmark test cases is\nthus essential for identifying the most reliable metric and potential\nimprovements. We propose that compositional learning in recurrent neural\nnetworks (RNNs) can provide a test case for dynamical representation alignment\nmetrics. Implementing this case allows us to evaluate if metrics can identify\nrepresentations that develop throughout learning and determine if\nrepresentations identified by metrics reflect the network's actual\ncomputations. Building both attractor and RNN based test cases, we show that\nthe recently proposed Dynamical Similarity Analysis (DSA) is more noise robust\nand reliably identifies behaviorally relevant representations compared to prior\nmetrics (Procrustes, CKA). We also demonstrate how such test cases can extend\nbeyond metric evaluation to study new architectures. Specifically, testing DSA\nin modern (Mamba) state space models suggests that these models, unlike RNNs,\nmay not require changes in recurrent dynamics due to their expressive hidden\nstates. Overall, we develop test cases that showcase how DSA's enhanced ability\nto detect dynamical motifs makes it highly effective for identifying ongoing\ncomputations in RNNs and revealing how networks learn tasks.\n","authors":["Quentin Guilhot","Michał Wójcik","Jascha Achterberg","Rui Ponte Costa"],"pdf_url":"https://arxiv.org/pdf/2410.24070v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14716v3","updated":"2024-11-01T09:38:59Z","published":"2024-10-11T13:17:19Z","title":"A Systematic Survey on Large Language Models for Algorithm Design","summary":"  Algorithm Design (AD) is crucial for effective problem-solving across various\ndomains. The advent of Large Language Models (LLMs) has notably enhanced the\nautomation and innovation within this field, offering new perspectives and\npromising solutions. Over the past three years, the integration of LLMs into AD\n(LLM4AD) has seen substantial progress, with applications spanning\noptimization, machine learning, mathematical reasoning, and scientific\ndiscovery. Given the rapid advancements and expanding scope of this field, a\nsystematic review is both timely and necessary. This paper provides a\nsystematic review of LLM4AD. First, we offer an overview and summary of\nexisting studies. Then, we introduce a taxonomy and review the literature\nacross four dimensions: the roles of LLMs, search methods, prompt methods, and\napplication domains with a discussion of potential and achievements of LLMs in\nAD. Finally, we identify current challenges and highlight several promising\ndirections for future research.\n","authors":["Fei Liu","Yiming Yao","Ping Guo","Zhiyuan Yang","Zhe Zhao","Xi Lin","Xialiang Tong","Mingxuan Yuan","Zhichao Lu","Zhenkun Wang","Qingfu Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.14716v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.16208v4","updated":"2024-11-01T09:07:41Z","published":"2023-06-28T13:43:46Z","title":"Continuous-time q-learning for mean-field control problems","summary":"  This paper studies the q-learning, recently coined as the continuous time\ncounterpart of Q-learning by Jia and Zhou (2023), for continuous time\nMckean-Vlasov control problems in the setting of entropy-regularized\nreinforcement learning. In contrast to the single agent's control problem in\nJia and Zhou (2023), the mean-field interaction of agents renders the\ndefinition of the q-function more subtle, for which we reveal that two distinct\nq-functions naturally arise: (i) the integrated q-function (denoted by $q$) as\nthe first-order approximation of the integrated Q-function introduced in Gu,\nGuo, Wei and Xu (2023), which can be learnt by a weak martingale condition\ninvolving test policies; and (ii) the essential q-function (denoted by $q_e$)\nthat is employed in the policy improvement iterations. We show that two\nq-functions are related via an integral representation under all test policies.\nBased on the weak martingale condition and our proposed searching method of\ntest policies, some model-free learning algorithms are devised. In two\nexamples, one in LQ control framework and one beyond LQ control framework, we\ncan obtain the exact parameterization of the optimal value function and\nq-functions and illustrate our algorithms with simulation experiments.\n","authors":["Xiaoli Wei","Xiang Yu"],"pdf_url":"https://arxiv.org/pdf/2306.16208v4.pdf","comment":"Keywords: Continuous-time reinforcement learning, integrated\n  q-function, mean-field control, weak martingale characterization, test\n  policies"},{"id":"http://arxiv.org/abs/2404.05019v2","updated":"2024-11-01T08:55:43Z","published":"2024-04-07T17:17:23Z","title":"Shortcut-connected Expert Parallelism for Accelerating\n  Mixture-of-Experts","summary":"  Expert parallelism has been introduced as a strategy to distribute the\ncomputational workload of sparsely-gated mixture-of-experts (MoE) models across\nmultiple computing devices, facilitating the execution of these increasingly\nlarge-scale models. However, the All-to-All communication intrinsic to expert\nparallelism constitutes a significant overhead, diminishing the MoE models'\nefficiency. Current optimization approaches offer some relief, yet they are\nconstrained by the sequential interdependence of communication and computation\noperations. To address this limitation, we present a novel shortcut-connected\nMoE (ScMoE) architecture with an overlapping parallel strategy, which\neffectively decouples communication from its conventional sequence, allowing\nfor a substantial overlap of 70% to 100% with computation. When compared with\nthe prevalent top-2 MoE architecture, ScMoE demonstrates training speed\nimprovements of 30% and 11%, and inference improvements of 40% and 15%, in our\ndistributed environments with PCIe and NVLink hardware, respectively, where\ncommunication constitutes 60% and 15% of the total MoE time consumption.\nBuilding on the ScMoE architecture, we further implement an expert offloading\nstrategy to facilitate memory-limited inference, optimizing latency through the\noverlap of expert migration. Additionally, extensive experiments and\ntheoretical analyses indicate that ScMoE not only achieves comparable but in\nsome instances surpasses the model quality of existing approaches.\n","authors":["Weilin Cai","Juyong Jiang","Le Qin","Junwei Cui","Sunghun Kim","Jiayi Huang"],"pdf_url":"https://arxiv.org/pdf/2404.05019v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.02657v2","updated":"2024-11-01T08:52:18Z","published":"2024-06-04T17:45:26Z","title":"Block Transformer: Global-to-Local Language Modeling for Fast Inference","summary":"  We introduce the Block Transformer which adopts hierarchical global-to-local\nmodeling to autoregressive transformers to mitigate the inference bottlenecks\nassociated with self-attention. Self-attention requires the key-value (KV)\ncache of all previous sequences to be retrieved from memory at every decoding\nstep to retrieve context information, leading to two primary bottlenecks during\nbatch inference. First, there is a significant delay in obtaining the first\ntoken, as the information of the entire prompt must first be processed to\nprefill the KV cache. Second, computation of subsequent tokens is bottlenecked\nby the high memory I/O demand of fetching the entire KV cache, which grows\nlinearly with sequence length, incurring quadratic memory reads overall. We\ndesign the Block Transformer to strategically mitigate these costs, by\nincorporating coarsity and locality into an integrated global-to-local\narchitecture. At the lower layers, we aggregate tokens into fixed size blocks\nto apply attention across the entire sequence at coarse-grained detail, to\ncapture the global context while minimizing KV cache overhead. At upper layers,\nwe apply attention within each block to decode individual tokens, to model\nfine-grained details with a lightweight local KV cache. We pretrain vanilla and\nBlock Transformers from scratch and demonstrate that Block Transformers reach\n10--20x inference throughput compared to vanilla transformers with equivalent\nperplexity and zero-shot task performance. Code is available at\nhttps://github.com/itsnamgyu/block-transformer.\n","authors":["Namgyu Ho","Sangmin Bae","Taehyeon Kim","Hyunjik Jo","Yireun Kim","Tal Schuster","Adam Fisch","James Thorne","Se-Young Yun"],"pdf_url":"https://arxiv.org/pdf/2406.02657v2.pdf","comment":"37 pages, 24 figures, 7 tables"},{"id":"http://arxiv.org/abs/2407.16131v2","updated":"2024-11-01T08:25:56Z","published":"2024-07-23T02:31:06Z","title":"CrysToGraph: A Comprehensive Predictive Model for Crystal Materials\n  Properties and the Benchmark","summary":"  The ionic bonding across the lattice and ordered microscopic structures endow\ncrystals with unique symmetry and determine their macroscopic properties.\nUnconventional crystals, in particular, exhibit non-traditional lattice\nstructures or possess exotic physical properties, making them intriguing\nsubjects for investigation. Therefore, to accurately predict the physical and\nchemical properties of crystals, it is crucial to consider long-range orders.\nWhile GNN excels at capturing the local environment of atoms in crystals, they\noften face challenges in effectively capturing longer-ranged interactions due\nto their limited depth. In this paper, we propose CrysToGraph\n($\\textbf{Crys}$tals with $\\textbf{T}$ransformers $\\textbf{o}$n\n$\\textbf{Graph}$s), a novel transformer-based geometric graph network designed\nspecifically for unconventional crystalline systems, and UnconvBench, a\ncomprehensive benchmark to evaluate models' predictive performance on\nunconventional crystal materials such as defected crystals, low-dimension\ncrystals and MOF. CrysToGraph effectively captures short-range interactions\nwith transformer-based graph convolution blocks as well as long-range\ninteractions with graph-wise transformer blocks. CrysToGraph proofs its\neffectiveness in modelling unconventional crystal materials in multiple tasks,\nand moreover, it outperforms most existing methods, achieving new\nstate-of-the-art results on the benchmarks of both unconventional crystals and\ntraditional crystals.\n","authors":["Hongyi Wang","Ji Sun","Jinzhe Liang","Li Zhai","Zitian Tang","Zijian Li","Wei Zhai","Xusheng Wang","Weihao Gao","Sheng Gong"],"pdf_url":"https://arxiv.org/pdf/2407.16131v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.04234v5","updated":"2024-11-01T08:16:52Z","published":"2023-12-07T11:40:32Z","title":"Graph Convolutions Enrich the Self-Attention in Transformers!","summary":"  Transformers, renowned for their self-attention mechanism, have achieved\nstate-of-the-art performance across various tasks in natural language\nprocessing, computer vision, time-series modeling, etc. However, one of the\nchallenges with deep Transformer models is the oversmoothing problem, where\nrepresentations across layers converge to indistinguishable values, leading to\nsignificant performance degradation. We interpret the original self-attention\nas a simple graph filter and redesign it from a graph signal processing (GSP)\nperspective. We propose a graph-filter-based self-attention (GFSA) to learn a\ngeneral yet effective one, whose complexity, however, is slightly larger than\nthat of the original self-attention mechanism. We demonstrate that GFSA\nimproves the performance of Transformers in various fields, including computer\nvision, natural language processing, graph-level tasks, speech recognition, and\ncode classification.\n","authors":["Jeongwhan Choi","Hyowon Wi","Jayoung Kim","Yehjin Shin","Kookjin Lee","Nathaniel Trask","Noseong Park"],"pdf_url":"https://arxiv.org/pdf/2312.04234v5.pdf","comment":"Accepted to NeurIPS 2024. Jeongwhan Choi and Hyowon Wi are co-first\n  authors with equal contributions"},{"id":"http://arxiv.org/abs/2410.23994v2","updated":"2024-11-01T07:55:34Z","published":"2024-10-31T14:52:01Z","title":"Breaking Determinism: Fuzzy Modeling of Sequential Recommendation Using\n  Discrete State Space Diffusion Model","summary":"  Sequential recommendation (SR) aims to predict items that users may be\ninterested in based on their historical behavior sequences. We revisit SR from\na novel information-theoretic perspective and find that conventional sequential\nmodeling methods fail to adequately capture the randomness and unpredictability\nof user behavior. Inspired by fuzzy information processing theory, this paper\nintroduces the DDSR model, which uses fuzzy sets of interaction sequences to\novercome the limitations and better capture the evolution of users' real\ninterests. Formally based on diffusion transition processes in discrete state\nspaces, which is unlike common diffusion models such as DDPM that operate in\ncontinuous domains. It is better suited for discrete data, using structured\ntransitions instead of arbitrary noise introduction to avoid information loss.\nAdditionally, to address the inefficiency of matrix transformations due to the\nvast discrete space, we use semantic labels derived from quantization or RQ-VAE\nto replace item IDs, enhancing efficiency and improving cold start issues.\nTesting on three public benchmark datasets shows that DDSR outperforms existing\nstate-of-the-art methods in various settings, demonstrating its potential and\neffectiveness in handling SR tasks.\n","authors":["Wenjia Xie","Hao Wang","Luankang Zhang","Rui Zhou","Defu Lian","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2410.23994v2.pdf","comment":"NeurIPS'2024, 10 pages"},{"id":"http://arxiv.org/abs/2306.06190v3","updated":"2024-11-01T07:53:10Z","published":"2023-06-09T18:42:19Z","title":"$FastDoc$: Domain-Specific Fast Continual Pre-training Technique using\n  Document-Level Metadata and Taxonomy","summary":"  In this paper, we propose $FastDoc$ (Fast Continual Pre-training Technique\nusing Document Level Metadata and Taxonomy), a novel, compute-efficient\nframework that utilizes Document metadata and Domain-Specific Taxonomy as\nsupervision signals to continually pre-train transformer encoder on a\ndomain-specific corpus. The main innovation is that during domain-specific\npretraining, an open-domain encoder is continually pre-trained using\nsentence-level embeddings as inputs (to accommodate long documents), however,\nfine-tuning is done with token-level embeddings as inputs to this encoder. We\nperform such domain-specific pre-training on three different domains namely\ncustomer support, scientific, and legal domains, and compare performance on 6\ndifferent downstream tasks and 9 different datasets. The novel use of\ndocument-level supervision along with sentence-level embedding input for\npre-training reduces pre-training compute by around $1,000$, $4,500$, and $500$\ntimes compared to MLM and/or NSP in Customer Support, Scientific, and Legal\nDomains, respectively. The reduced training time does not lead to a\ndeterioration in performance. In fact we show that $FastDoc$ either outperforms\nor performs on par with several competitive transformer-based baselines in\nterms of character-level F1 scores and other automated metrics in the Customer\nSupport, Scientific, and Legal Domains. Moreover, reduced training aids in\nmitigating the risk of catastrophic forgetting. Thus, unlike baselines,\n$FastDoc$ shows a negligible drop in performance on open domain.\n","authors":["Abhilash Nandy","Manav Nitin Kapadnis","Sohan Patnaik","Yash Parag Butala","Pawan Goyal","Niloy Ganguly"],"pdf_url":"https://arxiv.org/pdf/2306.06190v3.pdf","comment":"Accepted to Transactions on Machine Learning Research (TMLR), 36\n  pages, 8 figures"},{"id":"http://arxiv.org/abs/2404.13752v3","updated":"2024-11-01T07:51:36Z","published":"2024-04-21T19:24:15Z","title":"Adversarial Representation Engineering: A General Model Editing\n  Framework for Large Language Models","summary":"  Since the rapid development of Large Language Models (LLMs) has achieved\nremarkable success, understanding and rectifying their internal complex\nmechanisms has become an urgent issue. Recent research has attempted to\ninterpret their behaviors through the lens of inner representation. However,\ndeveloping practical and efficient methods for applying these representations\nfor general and flexible model editing remains challenging. In this work, we\nexplore how to leverage insights from representation engineering to guide the\nediting of LLMs by deploying a representation sensor as an editing oracle. We\nfirst identify the importance of a robust and reliable sensor during editing,\nthen propose an Adversarial Representation Engineering (ARE) framework to\nprovide a unified and interpretable approach for conceptual model editing\nwithout compromising baseline performance. Experiments on multiple tasks\ndemonstrate the effectiveness of ARE in various model editing scenarios. Our\ncode and data are available at\nhttps://github.com/Zhang-Yihao/Adversarial-Representation-Engineering.\n","authors":["Yihao Zhang","Zeming Wei","Jun Sun","Meng Sun"],"pdf_url":"https://arxiv.org/pdf/2404.13752v3.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.19400v4","updated":"2024-11-01T07:20:10Z","published":"2024-10-25T09:01:37Z","title":"Offline Reinforcement Learning with OOD State Correction and OOD Action\n  Suppression","summary":"  In offline reinforcement learning (RL), addressing the out-of-distribution\n(OOD) action issue has been a focus, but we argue that there exists an OOD\nstate issue that also impairs performance yet has been underexplored. Such an\nissue describes the scenario when the agent encounters states out of the\noffline dataset during the test phase, leading to uncontrolled behavior and\nperformance degradation. To this end, we propose SCAS, a simple yet effective\napproach that unifies OOD state correction and OOD action suppression in\noffline RL. Technically, SCAS achieves value-aware OOD state correction,\ncapable of correcting the agent from OOD states to high-value in-distribution\nstates. Theoretical and empirical results show that SCAS also exhibits the\neffect of suppressing OOD actions. On standard offline RL benchmarks, SCAS\nachieves excellent performance without additional hyperparameter tuning.\nMoreover, benefiting from its OOD state correction feature, SCAS demonstrates\nenhanced robustness against environmental perturbations.\n","authors":["Yixiu Mao","Qi Wang","Chen Chen","Yun Qu","Xiangyang Ji"],"pdf_url":"https://arxiv.org/pdf/2410.19400v4.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2404.01475v2","updated":"2024-11-01T07:05:33Z","published":"2024-04-01T20:56:25Z","title":"Are large language models superhuman chemists?","summary":"  Large language models (LLMs) have gained widespread interest due to their\nability to process human language and perform tasks on which they have not been\nexplicitly trained.\n  However, we possess only a limited systematic understanding of the chemical\ncapabilities of LLMs, which would be required to improve models and mitigate\npotential harm. Here, we introduce \"ChemBench,\" an automated framework for\nevaluating the chemical knowledge and reasoning abilities of state-of-the-art\nLLMs against the expertise of chemists.\n  We curated more than 2,700 question-answer pairs, evaluated leading open- and\nclosed-source LLMs, and found that the best models outperformed the best human\nchemists in our study on average. However, the models struggle with some basic\ntasks and provide overconfident predictions.\n  These findings reveal LLMs' impressive chemical capabilities while\nemphasizing the need for further research to improve their safety and\nusefulness. They also suggest adapting chemistry education and show the value\nof benchmarking frameworks for evaluating LLMs in specific domains.\n","authors":["Adrian Mirza","Nawaf Alampara","Sreekanth Kunchapu","Martiño Ríos-García","Benedict Emoekabu","Aswanth Krishnan","Tanya Gupta","Mara Schilling-Wilhelmi","Macjonathan Okereke","Anagha Aneesh","Amir Mohammad Elahi","Mehrdad Asgari","Juliane Eberhardt","Hani M. Elbeheiry","María Victoria Gil","Maximilian Greiner","Caroline T. Holick","Christina Glaubitz","Tim Hoffmann","Abdelrahman Ibrahim","Lea C. Klepsch","Yannik Köster","Fabian Alexander Kreth","Jakob Meyer","Santiago Miret","Jan Matthias Peschel","Michael Ringleb","Nicole Roesner","Johanna Schreiber","Ulrich S. Schubert","Leanne M. Stafast","Dinga Wonanke","Michael Pieler","Philippe Schwaller","Kevin Maik Jablonka"],"pdf_url":"https://arxiv.org/pdf/2404.01475v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15665v2","updated":"2024-11-01T06:57:43Z","published":"2024-10-21T06:09:30Z","title":"Long Term Memory: The Foundation of AI Self-Evolution","summary":"  Large language models (LLMs) like GPTs, trained on vast datasets, have\ndemonstrated impressive capabilities in language understanding, reasoning, and\nplanning, achieving human-level performance in various tasks. Most studies\nfocus on enhancing these models by training on ever-larger datasets to build\nmore powerful foundation models. While training stronger models is important,\nenabling models to evolve during inference is equally crucial, a process we\nrefer to as AI self-evolution. Unlike large-scale training, self-evolution may\nrely on limited data or interactions. Inspired by the columnar organization of\nthe human cerebral cortex, we hypothesize that AI models could develop\ncognitive abilities and build internal representations through iterative\ninteractions with their environment. To achieve this, models need long-term\nmemory (LTM) to store and manage processed interaction data. LTM supports\nself-evolution by representing diverse experiences across environments and\nagents. In this report, we explore AI self-evolution and its potential to\nenhance models during inference. We examine LTM's role in lifelong learning,\nallowing models to evolve based on accumulated interactions. We outline the\nstructure of LTM and the systems needed for effective data retention and\nrepresentation. We also classify approaches for building personalized models\nwith LTM data and show how these models achieve self-evolution through\ninteraction. Using LTM, our multi-agent framework OMNE achieved first place on\nthe GAIA benchmark, demonstrating LTM's potential for AI self-evolution.\nFinally, we present a roadmap for future research, emphasizing the importance\nof LTM for advancing AI technology and its practical applications.\n","authors":["Xun Jiang","Feng Li","Han Zhao","Jiaying Wang","Jun Shao","Shihao Xu","Shu Zhang","Weiling Chen","Xavier Tang","Yize Chen","Mengyue Wu","Weizhi Ma","Mengdi Wang","Tianqiao Chen"],"pdf_url":"https://arxiv.org/pdf/2410.15665v2.pdf","comment":"56 pages, 13 figures"},{"id":"http://arxiv.org/abs/2403.12553v3","updated":"2024-11-01T06:45:41Z","published":"2024-03-19T08:56:20Z","title":"Pretraining Codomain Attention Neural Operators for Solving Multiphysics\n  PDEs","summary":"  Existing neural operator architectures face challenges when solving\nmultiphysics problems with coupled partial differential equations (PDEs) due to\ncomplex geometries, interactions between physical variables, and the limited\namounts of high-resolution training data. To address these issues, we propose\nCodomain Attention Neural Operator (CoDA-NO), which tokenizes functions along\nthe codomain or channel space, enabling self-supervised learning or pretraining\nof multiple PDE systems. Specifically, we extend positional encoding,\nself-attention, and normalization layers to function spaces. CoDA-NO can learn\nrepresentations of different PDE systems with a single model. We evaluate\nCoDA-NO's potential as a backbone for learning multiphysics PDEs over multiple\nsystems by considering few-shot learning settings. On complex downstream tasks\nwith limited data, such as fluid flow simulations, fluid-structure\ninteractions, and Rayleigh-B\\'enard convection, we found CoDA-NO to outperform\nexisting methods by over 36%.\n","authors":["Md Ashiqur Rahman","Robert Joseph George","Mogab Elleithy","Daniel Leibovici","Zongyi Li","Boris Bonev","Colin White","Julius Berner","Raymond A. Yeh","Jean Kossaifi","Kamyar Azizzadenesheli","Anima Anandkumar"],"pdf_url":"https://arxiv.org/pdf/2403.12553v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09345v5","updated":"2024-11-01T06:30:11Z","published":"2024-02-14T17:49:07Z","title":"InfoRM: Mitigating Reward Hacking in RLHF via Information-Theoretic\n  Reward Modeling","summary":"  Despite the success of reinforcement learning from human feedback (RLHF) in\naligning language models with human values, reward hacking, also termed reward\noveroptimization, remains a critical challenge. This issue primarily arises\nfrom reward misgeneralization, where reward models (RMs) compute reward using\nspurious features that are irrelevant to human preferences. In this work, we\ntackle this problem from an information-theoretic perspective and propose a\nframework for reward modeling, namely InfoRM, by introducing a variational\ninformation bottleneck objective to filter out irrelevant information. Notably,\nwe further identify a correlation between overoptimization and outliers in the\nIB latent space of InfoRM, establishing it as a promising tool for detecting\nreward overoptimization. Inspired by this finding, we propose the Cluster\nSeparation Index (CSI), which quantifies deviations in the IB latent space, as\nan indicator of reward overoptimization to facilitate the development of online\nmitigation strategies. Extensive experiments on a wide range of settings and RM\nscales (70M, 440M, 1.4B, and 7B) demonstrate the effectiveness of InfoRM.\nFurther analyses reveal that InfoRM's overoptimization detection mechanism is\nnot only effective but also robust across a broad range of datasets, signifying\na notable advancement in the field of RLHF. The code will be released upon\nacceptance.\n","authors":["Yuchun Miao","Sen Zhang","Liang Ding","Rong Bao","Lefei Zhang","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2402.09345v5.pdf","comment":"The paper has been accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2405.17673v2","updated":"2024-11-01T06:22:30Z","published":"2024-05-27T21:50:16Z","title":"Fast Samplers for Inverse Problems in Iterative Refinement Models","summary":"  Constructing fast samplers for unconditional diffusion and flow-matching\nmodels has received much attention recently; however, existing methods for\nsolving inverse problems, such as super-resolution, inpainting, or deblurring,\nstill require hundreds to thousands of iterative steps to obtain high-quality\nresults. We propose a plug-and-play framework for constructing efficient\nsamplers for inverse problems, requiring only pre-trained diffusion or\nflow-matching models. We present Conditional Conjugate Integrators, which\nleverage the specific form of the inverse problem to project the respective\nconditional diffusion/flow dynamics into a more amenable space for sampling.\nOur method complements popular posterior approximation methods for solving\ninverse problems using diffusion/flow models. We evaluate the proposed method's\nperformance on various linear image restoration tasks across multiple datasets,\nemploying diffusion and flow-matching models. Notably, on challenging inverse\nproblems like 4x super-resolution on the ImageNet dataset, our method can\ngenerate high-quality samples in as few as 5 conditional sampling steps and\noutperforms competing baselines requiring 20-1000 steps. Our code will be\npublicly available at https://github.com/mandt-lab/c-pigdm\n","authors":["Kushagra Pandey","Ruihan Yang","Stephan Mandt"],"pdf_url":"https://arxiv.org/pdf/2405.17673v2.pdf","comment":"43 pages, NeurIPS'24 Camera Ready"},{"id":"http://arxiv.org/abs/2410.01548v2","updated":"2024-11-01T06:12:33Z","published":"2024-10-02T13:37:54Z","title":"In-Context Transfer Learning: Demonstration Synthesis by Transferring\n  Similar Tasks","summary":"  In-context learning (ICL) is an effective approach to help large language\nmodels (LLMs) adapt to various tasks by providing demonstrations of the target\ntask. Considering the high cost of labeling demonstrations, many methods\npropose synthesizing demonstrations from scratch using LLMs. However, the\nquality of the demonstrations synthesized from scratch is limited by the\ncapabilities and knowledge of LLMs. To address this, inspired by transfer\nlearning, we propose In-Context Transfer Learning (ICTL), which synthesizes\ntarget task demonstrations by transferring labeled demonstrations from similar\nsource tasks. ICTL consists of two steps: source sampling and target transfer.\nFirst, we define an optimization objective, which minimizes transfer error to\nsample source demonstrations similar to the target task. Then, we employ LLMs\nto transfer the sampled source demonstrations to the target task, matching the\ndefinition and format of the target task. Experiments on Super-NI show that\nICTL outperforms synthesis from scratch by 2.0% on average, demonstrating the\neffectiveness of our method.\n","authors":["Dingzirui Wang","Xuanliang Zhang","Qiguang Chen","Longxu Dou","Xiao Xu","Rongyu Cao","Yingwei Ma","Qingfu Zhu","Wanxiang Che","Binhua Li","Fei Huang","Yongbin Li"],"pdf_url":"https://arxiv.org/pdf/2410.01548v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11802v3","updated":"2024-11-01T05:14:40Z","published":"2024-10-15T17:23:49Z","title":"FoundTS: Comprehensive and Unified Benchmarking of Foundation Models for\n  Time Series Forecasting","summary":"  Time Series Forecasting (TSF) is key functionality in numerous fields,\nincluding in finance, weather services, and energy management. While TSF\nmethods are emerging these days, many of them require domain-specific data\ncollection and model training and struggle with poor generalization performance\non new domains. Foundation models aim to overcome this limitation. Pre-trained\non large-scale language or time series data, they exhibit promising inferencing\ncapabilities in new or unseen data. This has spurred a surge in new TSF\nfoundation models. We propose a new benchmark, FoundTS, to enable thorough and\nfair evaluation and comparison of such models. FoundTS covers a variety of TSF\nfoundation models, including those based on large language models and those\npretrained on time series. Next, FoundTS supports different forecasting\nstrategies, including zero-shot, few-shot, and full-shot, thereby facilitating\nmore thorough evaluations. Finally, FoundTS offers a pipeline that standardizes\nevaluation processes such as dataset splitting, loading, normalization, and\nfew-shot sampling, thereby facilitating fair evaluations. Building on this, we\nreport on an extensive evaluation of TSF foundation models on a broad range of\ndatasets from diverse domains and with different statistical characteristics.\nSpecifically, we identify pros and cons and inherent limitations of existing\nfoundation models, and we identify directions for future model design. We make\nour code and datasets available at\nhttps://anonymous.4open.science/r/FoundTS-C2B0.\n","authors":["Zhe Li","Xiangfei Qiu","Peng Chen","Yihang Wang","Hanyin Cheng","Yang Shu","Jilin Hu","Chenjuan Guo","Aoying Zhou","Qingsong Wen","Christian S. Jensen","Bin Yang"],"pdf_url":"https://arxiv.org/pdf/2410.11802v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.00986v2","updated":"2024-11-01T05:03:19Z","published":"2024-04-01T08:18:38Z","title":"Make Continual Learning Stronger via C-Flat","summary":"  Model generalization ability upon incrementally acquiring dynamically\nupdating knowledge from sequentially arriving tasks is crucial to tackle the\nsensitivity-stability dilemma in Continual Learning (CL). Weight loss landscape\nsharpness minimization seeking for flat minima lying in neighborhoods with\nuniform low loss or smooth gradient is proven to be a strong training regime\nimproving model generalization compared with loss minimization based optimizer\nlike SGD. Yet only a few works have discussed this training regime for CL,\nproving that dedicated designed zeroth-order sharpness optimizer can improve CL\nperformance. In this work, we propose a Continual Flatness (C-Flat) method\nfeaturing a flatter loss landscape tailored for CL. C-Flat could be easily\ncalled with only one line of code and is plug-and-play to any CL methods. A\ngeneral framework of C-Flat applied to all CL categories and a thorough\ncomparison with loss minima optimizer and flat minima based CL approaches is\npresented in this paper, showing that our method can boost CL performance in\nalmost all cases. Code is available at https://github.com/WanNaa/C-Flat.\n","authors":["Ang Bian","Wei Li","Hangjie Yuan","Chengrong Yu","Mang Wang","Zixiang Zhao","Aojun Lu","Pengliang Ji","Tao Feng"],"pdf_url":"https://arxiv.org/pdf/2404.00986v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.05234v3","updated":"2024-11-01T04:34:07Z","published":"2024-02-07T20:14:22Z","title":"QGFN: Controllable Greediness with Action Values","summary":"  Generative Flow Networks (GFlowNets; GFNs) are a family of energy-based\ngenerative methods for combinatorial objects, capable of generating diverse and\nhigh-utility samples. However, consistently biasing GFNs towards producing\nhigh-utility samples is non-trivial. In this work, we leverage connections\nbetween GFNs and reinforcement learning (RL) and propose to combine the GFN\npolicy with an action-value estimate, $Q$, to create greedier sampling policies\nwhich can be controlled by a mixing parameter. We show that several variants of\nthe proposed method, QGFN, are able to improve on the number of high-reward\nsamples generated in a variety of tasks without sacrificing diversity.\n","authors":["Elaine Lau","Stephen Zhewen Lu","Ling Pan","Doina Precup","Emmanuel Bengio"],"pdf_url":"https://arxiv.org/pdf/2402.05234v3.pdf","comment":"Accepted by 38th Conference on Neural Information Processing Systems\n  (NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2410.23938v2","updated":"2024-11-01T04:28:59Z","published":"2024-10-31T13:52:59Z","title":"Learning Macroscopic Dynamics from Partial Microscopic Observations","summary":"  Macroscopic observables of a system are of keen interest in real applications\nsuch as the design of novel materials. Current methods rely on microscopic\ntrajectory simulations, where the forces on all microscopic coordinates need to\nbe computed or measured. However, this can be computationally prohibitive for\nrealistic systems. In this paper, we propose a method to learn macroscopic\ndynamics requiring only force computations on a subset of the microscopic\ncoordinates. Our method relies on a sparsity assumption: the force on each\nmicroscopic coordinate relies only on a small number of other coordinates. The\nmain idea of our approach is to map the training procedure on the macroscopic\ncoordinates back to the microscopic coordinates, on which partial force\ncomputations can be used as stochastic estimation to update model parameters.\nWe provide a theoretical justification of this under suitable conditions. We\ndemonstrate the accuracy, force computation efficiency, and robustness of our\nmethod on learning macroscopic closure models from a variety of microscopic\nsystems, including those modeled by partial differential equations or molecular\ndynamics simulations.\n","authors":["Mengyi Chen","Qianxiao Li"],"pdf_url":"https://arxiv.org/pdf/2410.23938v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.19207v2","updated":"2024-11-01T04:14:52Z","published":"2024-10-24T23:36:39Z","title":"Equitable Federated Learning with Activation Clustering","summary":"  Federated learning is a prominent distributed learning paradigm that\nincorporates collaboration among diverse clients, promotes data locality, and\nthus ensures privacy. These clients have their own technological, cultural, and\nother biases in the process of data generation. However, the present standard\noften ignores this bias/heterogeneity, perpetuating bias against certain groups\nrather than mitigating it. In response to this concern, we propose an equitable\nclustering-based framework where the clients are categorized/clustered based on\nhow similar they are to each other. We propose a unique way to construct the\nsimilarity matrix that uses activation vectors. Furthermore, we propose a\nclient weighing mechanism to ensure that each cluster receives equal importance\nand establish $O(1/\\sqrt{K})$ rate of convergence to reach an\n$\\epsilon-$stationary solution. We assess the effectiveness of our proposed\nstrategy against common baselines, demonstrating its efficacy in terms of\nreducing the bias existing amongst various client clusters and consequently\nameliorating algorithmic bias against specific groups.\n","authors":["Antesh Upadhyay","Abolfazl Hashemi"],"pdf_url":"https://arxiv.org/pdf/2410.19207v2.pdf","comment":"28 pages"},{"id":"http://arxiv.org/abs/2310.13220v2","updated":"2024-11-01T04:04:44Z","published":"2023-10-20T01:55:34Z","title":"Towards Understanding How Transformers Learn In-context Through a\n  Representation Learning Lens","summary":"  Pre-trained large language models based on Transformers have demonstrated\nremarkable in-context learning (ICL) abilities. With just a few demonstration\nexamples, the models can implement new tasks without any parameter updates.\nHowever, it is still an open question to understand the mechanism of ICL. In\nthis paper, we attempt to explore the ICL process in Transformers through a\nlens of representation learning. Initially, leveraging kernel methods, we\nfigure out a dual model for one softmax attention layer. The ICL inference\nprocess of the attention layer aligns with the training procedure of its dual\nmodel, generating token representation predictions that are equivalent to the\ndual model's test outputs. We delve into the training process of this dual\nmodel from a representation learning standpoint and further derive a\ngeneralization error bound related to the quantity of demonstration tokens.\nSubsequently, we extend our theoretical conclusions to more complicated\nscenarios, including one Transformer layer and multiple attention layers.\nFurthermore, drawing inspiration from existing representation learning methods\nespecially contrastive learning, we propose potential modifications for the\nattention layer. Finally, experiments are designed to support our findings.\n","authors":["Ruifeng Ren","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2310.13220v2.pdf","comment":"35 pages"},{"id":"http://arxiv.org/abs/2405.00957v2","updated":"2024-11-01T03:51:18Z","published":"2024-05-02T02:38:32Z","title":"IntraMix: Intra-Class Mixup Generation for Accurate Labels and Neighbors","summary":"  Graph Neural Networks (GNNs) have shown great performance in various tasks,\nwith the core idea of learning from data labels and aggregating messages within\nthe neighborhood of nodes. However, the common challenges in graphs are\ntwofold: insufficient accurate (high-quality) labels and limited neighbors for\nnodes, resulting in weak GNNs. Existing graph augmentation methods typically\naddress only one of these challenges, often adding training costs or relying on\noversimplified or knowledge-intensive strategies, limiting their\ngeneralization. To simultaneously address both challenges faced by graphs in a\ngeneralized way, we propose an elegant method called IntraMix. Considering the\nincompatibility of vanilla Mixup with the complex topology of graphs, IntraMix\ninnovatively employs Mixup among inaccurate labeled data of the same class,\ngenerating high-quality labeled data at minimal cost. Additionally, it finds\ndata with high confidence of being clustered into the same group as the\ngenerated data to serve as their neighbors, thereby enriching the neighborhoods\nof graphs. IntraMix efficiently tackles both issues faced by graphs and\nchallenges the prior notion of the limited effectiveness of Mixup in node\nclassification. IntraMix is a theoretically grounded plug-in-play method that\ncan be readily applied to all GNNs. Extensive experiments demonstrate the\neffectiveness of IntraMix across various GNNs and datasets. Our code is\navailable at: https://github.com/Zhengsh123/IntraMix.\n","authors":["Shenghe Zheng","Hongzhi Wang","Xianglong Liu"],"pdf_url":"https://arxiv.org/pdf/2405.00957v2.pdf","comment":"Accepted by NeurIPS2024"},{"id":"http://arxiv.org/abs/2402.01763v3","updated":"2024-11-01T03:49:59Z","published":"2024-01-30T23:35:28Z","title":"When Large Language Models Meet Vector Databases: A Survey","summary":"  This survey explores the synergistic potential of Large Language Models\n(LLMs) and Vector Databases (VecDBs), a burgeoning but rapidly evolving\nresearch area. With the proliferation of LLMs comes a host of challenges,\nincluding hallucinations, outdated knowledge, prohibitive commercial\napplication costs, and memory issues. VecDBs emerge as a compelling solution to\nthese issues by offering an efficient means to store, retrieve, and manage the\nhigh-dimensional vector representations intrinsic to LLM operations. Through\nthis nuanced review, we delineate the foundational principles of LLMs and\nVecDBs and critically analyze their integration's impact on enhancing LLM\nfunctionalities. This discourse extends into a discussion on the speculative\nfuture developments in this domain, aiming to catalyze further research into\noptimizing the confluence of LLMs and VecDBs for advanced data handling and\nknowledge extraction capabilities.\n","authors":["Zhi Jing","Yongye Su","Yikun Han"],"pdf_url":"https://arxiv.org/pdf/2402.01763v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23858v2","updated":"2024-11-01T03:49:57Z","published":"2024-10-31T12:07:52Z","title":"Neural Network Matrix Product Operator: A Multi-Dimensionally Integrable\n  Machine Learning Potential","summary":"  A neural network-based machine learning potential energy surface (PES)\nexpressed in a matrix product operator (NN-MPO) is proposed. The MPO form\nenables efficient evaluation of high-dimensional integrals that arise in\nsolving the time-dependent and time-independent Schr\\\"odinger equation and\neffectively overcomes the so-called curse of dimensionality. This starkly\ncontrasts with other neural network-based machine learning PES methods, such as\nmulti-layer perceptrons (MLPs), where evaluating high-dimensional integrals is\nnot straightforward due to the fully connected topology in their backbone\narchitecture. Nevertheless, the NN-MPO retains the high representational\ncapacity of neural networks. NN-MPO can achieve spectroscopic accuracy with a\ntest mean absolute error (MAE) of 3.03 cm$^{-1}$ for a fully coupled\nsix-dimensional ab initio PES, using only 625 training points distributed\nacross a 0 to 17,000 cm$^{-1}$ energy range. Our Python implementation is\navailable at https://github.com/KenHino/Pompon.\n","authors":["Kentaro Hino","Yuki Kurashige"],"pdf_url":"https://arxiv.org/pdf/2410.23858v2.pdf","comment":"11 pages, 10 figures"},{"id":"http://arxiv.org/abs/2409.02428v3","updated":"2024-11-01T03:47:51Z","published":"2024-09-04T04:15:14Z","title":"Large Language Models as Efficient Reward Function Searchers for\n  Custom-Environment Multi-Objective Reinforcement Learning","summary":"  Achieving the effective design and improvement of reward functions in\nreinforcement learning (RL) tasks with complex custom environments and multiple\nrequirements presents considerable challenges. In this paper, we propose ERFSL,\nan efficient reward function searcher using LLMs, which enables LLMs to be\neffective white-box searchers and highlights their advanced semantic\nunderstanding capabilities. Specifically, we generate reward components for\neach numerically explicit user requirement and employ a reward critic to\nidentify the correct code form. Then, LLMs assign weights to the reward\ncomponents to balance their values and iteratively adjust the weights without\nambiguity and redundant adjustments by flexibly adopting directional mutation\nand crossover strategies, similar to genetic algorithms, based on the context\nprovided by the training log analyzer. We applied the framework to an\nunderwater data collection RL task without direct human feedback or reward\nexamples (zero-shot learning). The reward critic successfully corrects the\nreward code with only one feedback instance for each requirement, effectively\npreventing unrectifiable errors. The initialization of weights enables the\nacquisition of different reward functions within the Pareto solution set\nwithout the need for weight search. Even in cases where a weight is 500 times\noff, on average, only 5.2 iterations are needed to meet user requirements. The\nERFSL also works well with most prompts utilizing GPT-4o mini, as we decompose\nthe weight searching process to reduce the requirement for numerical and\nlong-context understanding capabilities\n","authors":["Guanwen Xie","Jingzehua Xu","Yiyuan Yang","Yimian Ding","Shuai Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.02428v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08159v3","updated":"2024-11-01T03:44:33Z","published":"2024-07-11T03:25:40Z","title":"Model-agnostic clean-label backdoor mitigation in cybersecurity\n  environments","summary":"  The training phase of machine learning models is a delicate step, especially\nin cybersecurity contexts. Recent research has surfaced a series of insidious\ntraining-time attacks that inject backdoors in models designed for security\nclassification tasks without altering the training labels. With this work, we\npropose new techniques that leverage insights in cybersecurity threat models to\neffectively mitigate these clean-label poisoning attacks, while preserving the\nmodel utility. By performing density-based clustering on a carefully chosen\nfeature subspace, and progressively isolating the suspicious clusters through a\nnovel iterative scoring procedure, our defensive mechanism can mitigate the\nattacks without requiring many of the common assumptions in the existing\nbackdoor defense literature. To show the generality of our proposed mitigation,\nwe evaluate it on two clean-label model-agnostic attacks on two different\nclassic cybersecurity data modalities: network flows classification and malware\nclassification, using gradient boosting and neural network models.\n","authors":["Giorgio Severi","Simona Boboila","John Holodnak","Kendra Kratkiewicz","Rauf Izmailov","Michael J. De Lucia","Alina Oprea"],"pdf_url":"https://arxiv.org/pdf/2407.08159v3.pdf","comment":"14 pages, 8 figures"},{"id":"http://arxiv.org/abs/2410.04501v3","updated":"2024-11-01T03:42:37Z","published":"2024-10-06T14:45:01Z","title":"Leveraging Large Language Models for Suicide Detection on Social Media\n  with Limited Labels","summary":"  The increasing frequency of suicidal thoughts highlights the importance of\nearly detection and intervention. Social media platforms, where users often\nshare personal experiences and seek help, could be utilized to identify\nindividuals at risk. However, the large volume of daily posts makes manual\nreview impractical. This paper explores the use of Large Language Models (LLMs)\nto automatically detect suicidal content in text-based social media posts. We\npropose a novel method for generating pseudo-labels for unlabeled data by\nprompting LLMs, along with traditional classification fine-tuning techniques to\nenhance label accuracy. To create a strong suicide detection model, we develop\nan ensemble approach involving prompting with Qwen2-72B-Instruct, and using\nfine-tuned models such as Llama3-8B, Llama3.1-8B, and Gemma2-9B. We evaluate\nour approach on the dataset of the Suicide Ideation Detection on Social Media\nChallenge, a track of the IEEE Big Data 2024 Big Data Cup. Additionally, we\nconduct a comprehensive analysis to assess the impact of different models and\nfine-tuning strategies on detection performance. Experimental results show that\nthe ensemble model significantly improves the detection accuracy, by 5% points\ncompared with the individual models. It achieves a weight F1 score of 0.770 on\nthe public test set, and 0.731 on the private test set, providing a promising\nsolution for identifying suicidal content in social media. Our analysis shows\nthat the choice of LLMs affects the prompting performance, with larger models\nproviding better accuracy. Our code and checkpoints are publicly available at\nhttps://github.com/khanhvynguyen/Suicide_Detection_LLMs.\n","authors":["Vy Nguyen","Chau Pham"],"pdf_url":"https://arxiv.org/pdf/2410.04501v3.pdf","comment":"Accepted at IEEE International Conference on Big Data 2024"},{"id":"http://arxiv.org/abs/2405.16978v3","updated":"2024-11-01T03:40:24Z","published":"2024-05-27T09:21:40Z","title":"OSLO: One-Shot Label-Only Membership Inference Attacks","summary":"  We introduce One-Shot Label-Only (OSLO) membership inference attacks (MIAs),\nwhich accurately infer a given sample's membership in a target model's training\nset with high precision using just \\emph{a single query}, where the target\nmodel only returns the predicted hard label. This is in contrast to\nstate-of-the-art label-only attacks which require $\\sim6000$ queries, yet get\nattack precisions lower than OSLO's. OSLO leverages transfer-based black-box\nadversarial attacks. The core idea is that a member sample exhibits more\nresistance to adversarial perturbations than a non-member. We compare OSLO\nagainst state-of-the-art label-only attacks and demonstrate that, despite\nrequiring only one query, our method significantly outperforms previous attacks\nin terms of precision and true positive rate (TPR) under the same false\npositive rates (FPR). For example, compared to previous label-only MIAs, OSLO\nachieves a TPR that is at least 7$\\times$ higher under a 1\\% FPR and at least\n22$\\times$ higher under a 0.1\\% FPR on CIFAR100 for a ResNet18 model. We\nevaluated multiple defense mechanisms against OSLO.\n","authors":["Yuefeng Peng","Jaechul Roh","Subhransu Maji","Amir Houmansadr"],"pdf_url":"https://arxiv.org/pdf/2405.16978v3.pdf","comment":"To appear at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.24184v2","updated":"2024-11-01T03:29:29Z","published":"2024-10-31T17:47:01Z","title":"Group Crosscoders for Mechanistic Analysis of Symmetry","summary":"  We introduce group crosscoders, an extension of crosscoders that\nsystematically discover and analyse symmetrical features in neural networks.\nWhile neural networks often develop equivariant representations without\nexplicit architectural constraints, understanding these emergent symmetries has\ntraditionally relied on manual analysis. Group crosscoders automate this\nprocess by performing dictionary learning across transformed versions of inputs\nunder a symmetry group. Applied to InceptionV1's mixed3b layer using the\ndihedral group $\\mathrm{D}_{32}$, our method reveals several key insights:\nFirst, it naturally clusters features into interpretable families that\ncorrespond to previously hypothesised feature types, providing more precise\nseparation than standard sparse autoencoders. Second, our transform block\nanalysis enables the automatic characterisation of feature symmetries,\nrevealing how different geometric features (such as curves versus lines)\nexhibit distinct patterns of invariance and equivariance. These results\ndemonstrate that group crosscoders can provide systematic insights into how\nneural networks represent symmetry, offering a promising new tool for\nmechanistic interpretability.\n","authors":["Liv Gorton"],"pdf_url":"https://arxiv.org/pdf/2410.24184v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.19878v2","updated":"2024-11-01T03:26:07Z","published":"2024-10-24T13:58:59Z","title":"Parameter-Efficient Fine-Tuning in Large Models: A Survey of\n  Methodologies","summary":"  The large models, as predicted by scaling raw forecasts, have made\ngroundbreaking progress in many fields, particularly in natural language\ngeneration tasks, where they have approached or even surpassed human levels.\nHowever, the unprecedented scale of their parameters brings significant\ncomputational and storage costs. These large models require substantial\ncomputational resources and GPU memory to operate. When adapting large models\nto specific downstream tasks, their massive parameter scale poses a significant\nchallenge in fine-tuning on hardware platforms with limited computational power\nand GPU memory. To address this issue, Parameter-Efficient Fine-Tuning (PEFT)\noffers a practical solution by efficiently adjusting the parameters of large\npre-trained models to suit various downstream tasks. Specifically, PEFT adjusts\nthe parameters of pre-trained large models to adapt to specific tasks or\ndomains, minimizing the introduction of additional parameters and the\ncomputational resources required. This review mainly introduces the preliminary\nknowledge of PEFT, the core ideas and principles of various PEFT algorithms,\nthe applications of PEFT, and potential future research directions. By reading\nthis review, we believe that interested parties can quickly grasp the PEFT\nmethodology, thereby accelerating its development and innovation.\n","authors":["Luping Wang","Sheng Chen","Linnan Jiang","Shu Pan","Runze Cai","Sen Yang","Fei Yang"],"pdf_url":"https://arxiv.org/pdf/2410.19878v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15711v2","updated":"2024-11-01T03:17:03Z","published":"2024-09-24T03:59:32Z","title":"Adversarial Federated Consensus Learning for Surface Defect\n  Classification Under Data Heterogeneity in IIoT","summary":"  The challenge of data scarcity hinders the application of deep learning in\nindustrial surface defect classification (SDC), as it's difficult to collect\nand centralize sufficient training data from various entities in Industrial\nInternet of Things (IIoT) due to privacy concerns. Federated learning (FL)\nprovides a solution by enabling collaborative global model training across\nclients while maintaining privacy. However, performance may suffer due to data\nheterogeneity-discrepancies in data distributions among clients. In this paper,\nwe propose a novel personalized FL (PFL) approach, named Adversarial Federated\nConsensus Learning (AFedCL), for the challenge of data heterogeneity across\ndifferent clients in SDC. First, we develop a dynamic consensus construction\nstrategy to mitigate the performance degradation caused by data heterogeneity.\nThrough adversarial training, local models from different clients utilize the\nglobal model as a bridge to achieve distribution alignment, alleviating the\nproblem of global knowledge forgetting. Complementing this strategy, we propose\na consensus-aware aggregation mechanism. It assigns aggregation weights to\ndifferent clients based on their efficacy in global knowledge learning, thereby\nenhancing the global model's generalization capabilities. Finally, we design an\nadaptive feature fusion module to further enhance global knowledge utilization\nefficiency. Personalized fusion weights are gradually adjusted for each client\nto optimally balance global and local features. Compared with state-of-the-art\nFL methods like FedALA, the proposed AFedCL method achieves an accuracy\nincrease of up to 5.67% on three SDC datasets.\n","authors":["Jixuan Cui","Jun Li","Zhen Mei","Yiyang Ni","Wen Chen","Zengxiang Li"],"pdf_url":"https://arxiv.org/pdf/2409.15711v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.04391v8","updated":"2024-11-01T02:49:24Z","published":"2023-02-09T01:09:57Z","title":"The Re-Label Method For Data-Centric Machine Learning","summary":"  In industry deep learning application, our manually labeled data has a\ncertain number of noisy data. To solve this problem and achieve more than 90\nscore in dev dataset, we present a simple method to find the noisy data and\nre-label the noisy data by human, given the model predictions as references in\nhuman labeling. In this paper, we illustrate our idea for a broad set of deep\nlearning tasks, includes classification, sequence tagging, object detection,\nsequence generation, click-through rate prediction. The dev dataset evaluation\nresults and human evaluation results verify our idea.\n","authors":["Tong Guo"],"pdf_url":"https://arxiv.org/pdf/2302.04391v8.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23749v2","updated":"2024-11-01T02:47:29Z","published":"2024-10-31T09:09:39Z","title":"LSEAttention is All You Need for Time Series Forecasting","summary":"  Transformer-based architectures have achieved remarkable success in natural\nlanguage processing and computer vision. However, their performance in\nmultivariate long-term forecasting often lags behind simpler linear baselines.\nPrevious studies have identified the traditional attention mechanism as a\nsignificant factor contributing to this limitation. To unlock the full\npotential of transformers for multivariate time series forecasting, I introduce\n\\textbf{LSEAttention}, an approach designed to address entropy collapse and\ntraining instability commonly observed in transformer models. I validate the\neffectiveness of LSEAttention across various real-world multivariate time\nseries datasets, demonstrating that it not only outperforms existing time\nseries transformer models but also exceeds the performance of some\nstate-of-the-art models on specific datasets.\n","authors":["Dizhen Liang"],"pdf_url":"https://arxiv.org/pdf/2410.23749v2.pdf","comment":"7 pages with referencing, 1 figure, 3 tables"},{"id":"http://arxiv.org/abs/2409.17508v2","updated":"2024-11-01T02:38:53Z","published":"2024-09-26T03:33:26Z","title":"Uni-Med: A Unified Medical Generalist Foundation Model For Multi-Task\n  Learning Via Connector-MoE","summary":"  Multi-modal large language models (MLLMs) have shown impressive capabilities\nas a general-purpose interface for various visual and linguistic tasks.\nHowever, building a unified MLLM for multi-task learning in the medical field\nremains a thorny challenge. To mitigate the tug-of-war problem of multi-modal\nmulti-task optimization in MLLMs, recent advances primarily focus on improving\nthe LLM components, while neglecting the connector that bridges the gap between\nmodalities. In this paper, we introduce Uni-Med, a novel medical generalist\nfoundation model which consists of a universal visual feature extraction\nmodule, a connector mixture-of-experts (CMoE) module, and an LLM. Benefiting\nfrom the proposed CMoE that leverages a well-designed router with a mixture of\nprojection experts at the connector, Uni-Med achieves efficient solution to the\ntug-of-war problem and can perform six different medical tasks including\nquestion answering, visual question answering, report generation, referring\nexpression comprehension, referring expression generation and image\nclassification. To the best of our knowledge, Uni-Med is the first effort to\ntackle multi-task interference at the connector in MLLMs. Extensive ablation\nexperiments validate the effectiveness of introducing CMoE under any\nconfiguration, with up to an average 8% performance gains. We further provide\ninterpretation analysis of the tug-of-war problem from the perspective of\ngradient optimization and parameter statistics. Compared to previous\nstate-of-the-art medical MLLMs, Uni-Med achieves competitive or superior\nevaluation metrics on diverse tasks. Code and resources are available at\nhttps://github.com/tsinghua-msiip/Uni-Med.\n","authors":["Xun Zhu","Ying Hu","Fanbin Mo","Miao Li","Ji Wu"],"pdf_url":"https://arxiv.org/pdf/2409.17508v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14909v2","updated":"2024-11-01T02:26:18Z","published":"2024-06-21T06:58:37Z","title":"MoA: Mixture of Sparse Attention for Automatic Large Language Model\n  Compression","summary":"  Sparse attention can effectively mitigate the significant memory and\nthroughput demands of Large Language Models (LLMs) in long contexts. Existing\nmethods typically employ a uniform sparse attention mask, applying the same\nsparse pattern across different attention heads and input lengths. However,\nthis uniform approach fails to capture the diverse attention patterns inherent\nin LLMs, ignoring their distinct accuracy-latency trade-offs. To address this\nchallenge, we propose the Mixture of Attention (MoA), which automatically\ntailors distinct sparse attention configurations to different heads and layers.\nMoA constructs and navigates a search space of various attention patterns and\ntheir scaling rules relative to input sequence lengths. It profiles the model,\nevaluates potential configurations, and pinpoints the optimal sparse attention\ncompression plan. MoA adapts to varying input sizes, revealing that some\nattention heads expand their focus to accommodate longer sequences, while other\nheads consistently concentrate on fixed-length local contexts. Experiments show\nthat MoA increases the effective context length by $3.9\\times$ with the same\naverage attention span, boosting retrieval accuracy by $1.5-7.1\\times$ over the\nuniform-attention baseline across Vicuna-{7B,13B}, and Llama3-{8B,70B} models.\nMoreover, MoA narrows the capability gaps between sparse and dense models,\nreducing the maximum relative performance drop from $9\\%-36\\%$ to within $5\\%$\nacross two long-context understanding benchmarks. MoA achieves a\n$1.2-1.4\\times$ GPU memory reduction, boosting decode throughput by\n$6.6-8.2\\times$ and $1.7-1.9\\times$ compared to FlashAttention2 and vLLM, with\nminimal impact on performance. Our code is available at\n\\url{https://github.com/thu-nics/MoA}.\n","authors":["Tianyu Fu","Haofeng Huang","Xuefei Ning","Genghan Zhang","Boju Chen","Tianqi Wu","Hongyi Wang","Zixiao Huang","Shiyao Li","Shengen Yan","Guohao Dai","Huazhong Yang","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2406.14909v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18451v3","updated":"2024-11-01T02:13:59Z","published":"2024-06-26T16:00:35Z","title":"Detecting Brittle Decisions for Free: Leveraging Margin Consistency in\n  Deep Robust Classifiers","summary":"  Despite extensive research on adversarial training strategies to improve\nrobustness, the decisions of even the most robust deep learning models can\nstill be quite sensitive to imperceptible perturbations, creating serious risks\nwhen deploying them for high-stakes real-world applications. While detecting\nsuch cases may be critical, evaluating a model's vulnerability at a\nper-instance level using adversarial attacks is computationally too intensive\nand unsuitable for real-time deployment scenarios. The input space margin is\nthe exact score to detect non-robust samples and is intractable for deep neural\nnetworks. This paper introduces the concept of margin consistency -- a property\nthat links the input space margins and the logit margins in robust models --\nfor efficient detection of vulnerable samples. First, we establish that margin\nconsistency is a necessary and sufficient condition to use a model's logit\nmargin as a score for identifying non-robust samples. Next, through\ncomprehensive empirical analysis of various robustly trained models on CIFAR10\nand CIFAR100 datasets, we show that they indicate high margin consistency with\na strong correlation between their input space margins and the logit margins.\nThen, we show that we can effectively and confidently use the logit margin to\ndetect brittle decisions with such models. Finally, we address cases where the\nmodel is not sufficiently margin-consistent by learning a pseudo-margin from\nthe feature representation. Our findings highlight the potential of leveraging\ndeep representations to assess adversarial vulnerability in deployment\nscenarios efficiently.\n","authors":["Jonas Ngnawé","Sabyasachi Sahoo","Yann Pequignot","Frédéric Precioso","Christian Gagné"],"pdf_url":"https://arxiv.org/pdf/2406.18451v3.pdf","comment":"10 pages, 6 figures, 2 tables. Version Update: Neurips Camera Ready"},{"id":"http://arxiv.org/abs/2006.11444v2","updated":"2024-11-01T02:12:36Z","published":"2020-06-20T00:17:44Z","title":"Optimizing Monotone Chance-Constrained Submodular Functions Using\n  Evolutionary Multi-Objective Algorithms","summary":"  Many real-world optimization problems can be stated in terms of submodular\nfunctions. Furthermore, these real-world problems often involve uncertainties\nwhich may lead to the violation of given constraints. A lot of evolutionary\nmulti-objective algorithms following the Pareto optimization approach have\nrecently been analyzed and applied to submodular problems with different types\nof constraints. We present a first runtime analysis of evolutionary\nmulti-objective algorithms based on Pareto optimization for chance-constrained\nsubmodular functions. Here the constraint involves stochastic components and\nthe constraint can only be violated with a small probability of alpha. We\ninvestigate the classical GSEMO algorithm for two different bi-objective\nformulations using tail bounds to determine the feasibility of solutions. We\nshow that the algorithm GSEMO obtains the same worst case performance\nguarantees for monotone submodular functions as recently analyzed greedy\nalgorithms for the case of uniform IID weights and uniformly distributed\nweights with the same dispersion when using the appropriate bi-objective\nformulation. As part of our investigations, we also point out situations where\nthe use of tail bounds in the first bi-objective formulation can prevent GSEMO\nfrom obtaining good solutions in the case of uniformly distributed weights with\nthe same dispersion if the objective function is submodular but non-monotone\ndue to a single element impacting monotonicity. Furthermore, we investigate the\nbehavior of the evolutionary multi-objective algorithms GSEMO, NSGA-II and\nSPEA2 on different submodular chance-constrained network problems. Our\nexperimental results show that the use of evolutionary multi-objective\nalgorithms leads to significant performance improvements compared to\nstate-of-the-art greedy algorithms for submodular optimization.\n","authors":["Aneta Neumann","Frank Neumann"],"pdf_url":"https://arxiv.org/pdf/2006.11444v2.pdf","comment":"To appear in the Evolutionary Computation Journal 2024"},{"id":"http://arxiv.org/abs/2205.10287v3","updated":"2024-11-01T02:01:18Z","published":"2022-05-20T16:39:03Z","title":"On the SDEs and Scaling Rules for Adaptive Gradient Algorithms","summary":"  Approximating Stochastic Gradient Descent (SGD) as a Stochastic Differential\nEquation (SDE) has allowed researchers to enjoy the benefits of studying a\ncontinuous optimization trajectory while carefully preserving the stochasticity\nof SGD. Analogous study of adaptive gradient methods, such as RMSprop and Adam,\nhas been challenging because there were no rigorously proven SDE approximations\nfor these methods. This paper derives the SDE approximations for RMSprop and\nAdam, giving theoretical guarantees of their correctness as well as\nexperimental validation of their applicability to common large-scaling vision\nand language settings. A key practical result is the derivation of a\n$\\textit{square root scaling rule}$ to adjust the optimization hyperparameters\nof RMSprop and Adam when changing batch size, and its empirical validation in\ndeep learning settings.\n","authors":["Sadhika Malladi","Kaifeng Lyu","Abhishek Panigrahi","Sanjeev Arora"],"pdf_url":"https://arxiv.org/pdf/2205.10287v3.pdf","comment":"revised for correcting errors in some figures"},{"id":"http://arxiv.org/abs/2405.00636v2","updated":"2024-11-01T01:55:41Z","published":"2024-05-01T17:04:20Z","title":"Robustness of graph embedding methods for community detection","summary":"  This study investigates the robustness of graph embedding methods for\ncommunity detection in the face of network perturbations, specifically edge\ndeletions. Graph embedding techniques, which represent nodes as low-dimensional\nvectors, are widely used for various graph machine learning tasks due to their\nability to capture structural properties of networks effectively. However, the\nimpact of perturbations on the performance of these methods remains relatively\nunderstudied. The research considers state-of-the-art graph embedding methods\nfrom two families: matrix factorization (e.g., LE, LLE, HOPE, M-NMF) and random\nwalk-based (e.g., DeepWalk, LINE, node2vec). Through experiments conducted on\nboth synthetic and real-world networks, the study reveals varying degrees of\nrobustness within each family of graph embedding methods. The robustness is\nfound to be influenced by factors such as network size, initial community\npartition strength, and the type of perturbation. Notably, node2vec and LLE\nconsistently demonstrate higher robustness for community detection across\ndifferent scenarios, including networks with degree and community size\nheterogeneity. These findings highlight the importance of selecting an\nappropriate graph embedding method based on the specific characteristics of the\nnetwork and the task at hand, particularly in scenarios where robustness to\nperturbations is crucial.\n","authors":["Zhi-Feng Wei","Pablo Moriano","Ramakrishnan Kannan"],"pdf_url":"https://arxiv.org/pdf/2405.00636v2.pdf","comment":"17 pages, 26 figures, 3 tables. Comments are welcome"},{"id":"http://arxiv.org/abs/2406.06976v2","updated":"2024-11-01T01:51:35Z","published":"2024-06-11T06:16:33Z","title":"Discrete Dictionary-based Decomposition Layer for Structured\n  Representation Learning","summary":"  Neuro-symbolic neural networks have been extensively studied to integrate\nsymbolic operations with neural networks, thereby improving systematic\ngeneralization. Specifically, Tensor Product Representation (TPR) framework\nenables neural networks to perform differentiable symbolic operations by\nencoding the symbolic structure of data within vector spaces. However,\nTPR-based neural networks often struggle to decompose unseen data into\nstructured TPR representations, undermining their symbolic operations. To\naddress this decomposition problem, we propose a Discrete Dictionary-based\nDecomposition (D3) layer designed to enhance the decomposition capabilities of\nTPR-based models. D3 employs discrete, learnable key-value dictionaries trained\nto capture symbolic features essential for decomposition operations. It\nleverages the prior knowledge acquired during training to generate structured\nTPR representations by mapping input data to pre-learned symbolic features\nwithin these dictionaries. D3 is a straightforward drop-in layer that can be\nseamlessly integrated into any TPR-based model without modifications. Our\nexperimental results demonstrate that D3 significantly improves the systematic\ngeneralization of various TPR-based models while requiring fewer additional\nparameters. Notably, D3 outperforms baseline models on the synthetic task that\ndemands the systematic decomposition of unseen combinatorial data.\n","authors":["Taewon Park","Hyun-Chul Kim","Minho Lee"],"pdf_url":"https://arxiv.org/pdf/2406.06976v2.pdf","comment":"Published in NeurIPS 2024"},{"id":"http://arxiv.org/abs/2405.18781v2","updated":"2024-11-01T01:45:27Z","published":"2024-05-29T05:41:28Z","title":"On the Role of Attention Masks and LayerNorm in Transformers","summary":"  Self-attention is the key mechanism of transformers, which are the essential\nbuilding blocks of modern foundation models. Recent studies have shown that\npure self-attention suffers from an increasing degree of rank collapse as depth\nincreases, limiting model expressivity and further utilization of model depth.\nThe existing literature on rank collapse, however, has mostly overlooked other\ncritical components in transformers that may alleviate the rank collapse issue.\nIn this paper, we provide a general analysis of rank collapse under\nself-attention, taking into account the effects of attention masks and layer\nnormalization (LayerNorm). In particular, we find that although pure masked\nattention still suffers from exponential collapse to a rank one subspace,\nsparse or local masked attention can provably slow down the collapse rate. In\nthe case of self-attention with LayerNorm, we first show that for certain\nclasses of value matrices, collapse to a rank one subspace still happens\nexponentially. However, through construction of nontrivial counterexamples, we\nthen establish that with proper choice of value matrices, a general class of\nsequences may not converge to a rank one subspace, and the self-attention\ndynamics with LayerNorm can simultaneously possess a rich set of equilibria\nwith any possible rank between one and full. Our result refutes the previous\nhypothesis that LayerNorm plays no role in the rank collapse of self-attention\nand suggests that self-attention with LayerNorm constitutes a much more\nexpressive, versatile nonlinear dynamical system than what was originally\nthought.\n","authors":["Xinyi Wu","Amir Ajorlou","Yifei Wang","Stefanie Jegelka","Ali Jadbabaie"],"pdf_url":"https://arxiv.org/pdf/2405.18781v2.pdf","comment":"NeurIPS 2024. Fixed errors in v1 and added new remarks"},{"id":"http://arxiv.org/abs/2410.20595v2","updated":"2024-11-01T01:27:10Z","published":"2024-10-27T21:02:37Z","title":"A Framework for Real-Time Volcano-Seismic Event Recognition Based on\n  Multi-Station Seismograms and Semantic Segmentation Models","summary":"  In volcano monitoring, effective recognition of seismic events is essential\nfor understanding volcanic activity and raising timely warning alerts.\nTraditional methods rely on manual analysis, which can be subjective and\nlabor-intensive. Furthermore, current automatic approaches often tackle\ndetection and classification separately, mostly rely on single station\ninformation and generally require tailored preprocessing and representations to\nperform predictions. These limitations often hinder their application to\nreal-time monitoring and utilization across different volcano conditions. This\nstudy introduces a novel approach that utilizes Semantic Segmentation models to\nautomate seismic event recognition by applying a straight forward\ntransformation of multi-channel 1D signals into 2D representations, enabling\ntheir use as images. Our framework employs a data-driven, end-to-end design\nthat integrates multi-station seismic data with minimal preprocessing,\nperforming both detection and classification simultaneously for five seismic\nevent classes. We evaluated four state-of-the-art segmentation models (UNet,\nUNet++, DeepLabV3+ and SwinUNet) on approximately 25.000 seismic events\nrecorded at four different Chilean volcanoes: Nevados del Chill\\'an Volcanic\nComplex, Laguna del Maule, Villarrica and Puyehue-Cord\\'on Caulle. Among these\nmodels, the UNet architecture was identified as the most effective model,\nachieving mean F1 and Intersection over Union (IoU) scores of up to 0.91 and\n0.88, respectively, and demonstrating superior noise robustness and model\nflexibility to unseen volcano datasets.\n","authors":["Camilo Espinosa-Curilem","Millaray Curilem","Daniel Basualto"],"pdf_url":"https://arxiv.org/pdf/2410.20595v2.pdf","comment":"10 pages, 9 figures. This is a pre-print, it is currently under\n  review for publication"},{"id":"http://arxiv.org/abs/2406.05972v2","updated":"2024-11-01T00:50:56Z","published":"2024-06-10T02:14:19Z","title":"Decision-Making Behavior Evaluation Framework for LLMs under Uncertain\n  Context","summary":"  When making decisions under uncertainty, individuals often deviate from\nrational behavior, which can be evaluated across three dimensions: risk\npreference, probability weighting, and loss aversion. Given the widespread use\nof large language models (LLMs) in decision-making processes, it is crucial to\nassess whether their behavior aligns with human norms and ethical expectations\nor exhibits potential biases. Several empirical studies have investigated the\nrationality and social behavior performance of LLMs, yet their internal\ndecision-making tendencies and capabilities remain inadequately understood.\nThis paper proposes a framework, grounded in behavioral economics, to evaluate\nthe decision-making behaviors of LLMs. Through a multiple-choice-list\nexperiment, we estimate the degree of risk preference, probability weighting,\nand loss aversion in a context-free setting for three commercial LLMs:\nChatGPT-4.0-Turbo, Claude-3-Opus, and Gemini-1.0-pro. Our results reveal that\nLLMs generally exhibit patterns similar to humans, such as risk aversion and\nloss aversion, with a tendency to overweight small probabilities. However,\nthere are significant variations in the degree to which these behaviors are\nexpressed across different LLMs. We also explore their behavior when embedded\nwith socio-demographic features, uncovering significant disparities. For\ninstance, when modeled with attributes of sexual minority groups or physical\ndisabilities, Claude-3-Opus displays increased risk aversion, leading to more\nconservative choices. These findings underscore the need for careful\nconsideration of the ethical implications and potential biases in deploying\nLLMs in decision-making scenarios. Therefore, this study advocates for\ndeveloping standards and guidelines to ensure that LLMs operate within ethical\nboundaries while enhancing their utility in complex decision-making\nenvironments.\n","authors":["Jingru Jia","Zehua Yuan","Junhao Pan","Paul E. McNamara","Deming Chen"],"pdf_url":"https://arxiv.org/pdf/2406.05972v2.pdf","comment":"Jingru Jia and Zehua Yuan have equal contribution"},{"id":"http://arxiv.org/abs/2406.09371v2","updated":"2024-11-01T00:22:26Z","published":"2024-06-13T17:51:00Z","title":"LRM-Zero: Training Large Reconstruction Models with Synthesized Data","summary":"  We present LRM-Zero, a Large Reconstruction Model (LRM) trained entirely on\nsynthesized 3D data, achieving high-quality sparse-view 3D reconstruction. The\ncore of LRM-Zero is our procedural 3D dataset, Zeroverse, which is\nautomatically synthesized from simple primitive shapes with random texturing\nand augmentations (e.g., height fields, boolean differences, and wireframes).\nUnlike previous 3D datasets (e.g., Objaverse) which are often captured or\ncrafted by humans to approximate real 3D data, Zeroverse completely ignores\nrealistic global semantics but is rich in complex geometric and texture details\nthat are locally similar to or even more intricate than real objects. We\ndemonstrate that our LRM-Zero, trained with our fully synthesized Zeroverse,\ncan achieve high visual quality in the reconstruction of real-world objects,\ncompetitive with models trained on Objaverse. We also analyze several critical\ndesign choices of Zeroverse that contribute to LRM-Zero's capability and\ntraining stability. Our work demonstrates that 3D reconstruction, one of the\ncore tasks in 3D vision, can potentially be addressed without the semantics of\nreal-world objects. The Zeroverse's procedural synthesis code and interactive\nvisualization are available at: https://desaixie.github.io/lrm-zero/.\n","authors":["Desai Xie","Sai Bi","Zhixin Shu","Kai Zhang","Zexiang Xu","Yi Zhou","Sören Pirk","Arie Kaufman","Xin Sun","Hao Tan"],"pdf_url":"https://arxiv.org/pdf/2406.09371v2.pdf","comment":"23 pages, 8 figures. Our code and interactive visualization are\n  available at: https://desaixie.github.io/lrm-zero/. v2: NeurIPS 2024 Camera\n  Ready version"},{"id":"http://arxiv.org/abs/2406.17763v2","updated":"2024-11-01T00:08:54Z","published":"2024-06-25T17:48:24Z","title":"DiffusionPDE: Generative PDE-Solving Under Partial Observation","summary":"  We introduce a general framework for solving partial differential equations\n(PDEs) using generative diffusion models. In particular, we focus on the\nscenarios where we do not have the full knowledge of the scene necessary to\napply classical solvers. Most existing forward or inverse PDE approaches\nperform poorly when the observations on the data or the underlying coefficients\nare incomplete, which is a common assumption for real-world measurements. In\nthis work, we propose DiffusionPDE that can simultaneously fill in the missing\ninformation and solve a PDE by modeling the joint distribution of the solution\nand coefficient spaces. We show that the learned generative priors lead to a\nversatile framework for accurately solving a wide range of PDEs under partial\nobservation, significantly outperforming the state-of-the-art methods for both\nforward and inverse directions.\n","authors":["Jiahe Huang","Guandao Yang","Zichen Wang","Jeong Joon Park"],"pdf_url":"https://arxiv.org/pdf/2406.17763v2.pdf","comment":"NeurIPS 2024. Project page:\n  https://jhhuangchloe.github.io/Diffusion-PDE/"},{"id":"http://arxiv.org/abs/2406.16218v2","updated":"2024-11-01T00:01:01Z","published":"2024-06-23T21:05:31Z","title":"Trace is the Next AutoDiff: Generative Optimization with Rich Feedback,\n  Execution Traces, and LLMs","summary":"  We study a class of optimization problems motivated by automating the design\nand update of AI systems like coding assistants, robots, and copilots. AutoDiff\nframeworks, like PyTorch, enable efficient end-to-end optimization of\ndifferentiable systems. However, general computational workflows can be\nnon-differentiable and involve rich feedback (e.g. console output or user's\nresponses), heterogeneous parameters (e.g. prompts, codes), and intricate\nobjectives (beyond maximizing a score). We investigate end-to-end generative\noptimization -- using generative models such as LLMs within the optimizer for\nautomatic updating of general computational workflows. We discover that\nworkflow execution traces are akin to back-propagated gradients in AutoDiff and\ncan provide key information to interpret feedback for efficient optimization.\nFormally, we frame a new mathematical setup, Optimization with Trace Oracle\n(OPTO). In OPTO, an optimizer receives an execution trace along with feedback\non the computed output and updates parameters iteratively. We provide a Python\nlibrary, Trace, that efficiently converts a workflow optimization problem into\nan OPTO instance using PyTorch-like syntax. Using Trace, we develop a general\nLLM-based generative optimizer called OptoPrime. In empirical studies, we find\nthat OptoPrime is capable of first-order numerical optimization, prompt\noptimization, hyper-parameter tuning, robot controller design, code debugging,\netc., and is often competitive with specialized optimizers for each domain. We\nenvision Trace as an open research platform for devising novel generative\noptimizers and developing the next generation of interactive learning agents.\nWebsite: https://microsoft.github.io/Trace/.\n","authors":["Ching-An Cheng","Allen Nie","Adith Swaminathan"],"pdf_url":"https://arxiv.org/pdf/2406.16218v2.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2409.20012v2","updated":"2024-11-01T08:40:28Z","published":"2024-09-30T07:14:31Z","title":"Towards Robust Multimodal Sentiment Analysis with Incomplete Data","summary":"  The field of Multimodal Sentiment Analysis (MSA) has recently witnessed an\nemerging direction seeking to tackle the issue of data incompleteness.\nRecognizing that the language modality typically contains dense sentiment\ninformation, we consider it as the dominant modality and present an innovative\nLanguage-dominated Noise-resistant Learning Network (LNLN) to achieve robust\nMSA. The proposed LNLN features a dominant modality correction (DMC) module and\ndominant modality based multimodal learning (DMML) module, which enhances the\nmodel's robustness across various noise scenarios by ensuring the quality of\ndominant modality representations. Aside from the methodical design, we perform\ncomprehensive experiments under random data missing scenarios, utilizing\ndiverse and meaningful settings on several popular datasets (\\textit{e.g.,}\nMOSI, MOSEI, and SIMS), providing additional uniformity, transparency, and\nfairness compared to existing evaluations in the literature. Empirically, LNLN\nconsistently outperforms existing baselines, demonstrating superior performance\nacross these challenging and extensive evaluation metrics.\n","authors":["Haoyu Zhang","Wenbin Wang","Tianshu Yu"],"pdf_url":"https://arxiv.org/pdf/2409.20012v2.pdf","comment":"Accepted to NeurIPS 2024"}]},"2024-11-04T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2411.02398v1","updated":"2024-11-04T18:59:51Z","published":"2024-11-04T18:59:51Z","title":"Prompting with Phonemes: Enhancing LLM Multilinguality for non-Latin\n  Script Languages","summary":"  Multilingual LLMs have achieved remarkable benchmark performance, but we find\nthey continue to underperform on non-Latin script languages across contemporary\nLLM families. This discrepancy arises from the fact that LLMs are pretrained\nwith orthographic scripts, which are dominated by Latin characters that obscure\ntheir shared phonology with non-Latin scripts. We propose leveraging phonemic\ntranscriptions as complementary signals to induce script-invariant\nrepresentations. Our study demonstrates that integrating phonemic signals\nimproves performance across both non-Latin and Latin languages, with a\nparticularly significant impact on closing the performance gap between the two.\nThrough detailed experiments, we show that phonemic and orthographic scripts\nretrieve distinct examples for in-context learning (ICL). This motivates our\nproposed Mixed-ICL retrieval strategy, where further aggregation leads to our\nsignificant performance improvements for both Latin script languages (up to\n12.6%) and non-Latin script languages (up to 15.1%) compared to randomized ICL\nretrieval.\n","authors":["Hoang Nguyen","Khyati Mahajan","Vikas Yadav","Philip S. Yu","Masoud Hashemi","Rishabh Maheshwary"],"pdf_url":"https://arxiv.org/pdf/2411.02398v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02391v1","updated":"2024-11-04T18:56:42Z","published":"2024-11-04T18:56:42Z","title":"Attacking Vision-Language Computer Agents via Pop-ups","summary":"  Autonomous agents powered by large vision and language models (VLM) have\ndemonstrated significant potential in completing daily computer tasks, such as\nbrowsing the web to book travel and operating desktop software, which requires\nagents to understand these interfaces. Despite such visual inputs becoming more\nintegrated into agentic applications, what types of risks and attacks exist\naround them still remain unclear. In this work, we demonstrate that VLM agents\ncan be easily attacked by a set of carefully designed adversarial pop-ups,\nwhich human users would typically recognize and ignore. This distraction leads\nagents to click these pop-ups instead of performing the tasks as usual.\nIntegrating these pop-ups into existing agent testing environments like OSWorld\nand VisualWebArena leads to an attack success rate (the frequency of the agent\nclicking the pop-ups) of 86% on average and decreases the task success rate by\n47%. Basic defense techniques such as asking the agent to ignore pop-ups or\nincluding an advertisement notice, are ineffective against the attack.\n","authors":["Yanzhe Zhang","Tao Yu","Diyi Yang"],"pdf_url":"https://arxiv.org/pdf/2411.02391v1.pdf","comment":"10 pages, preprint"},{"id":"http://arxiv.org/abs/2411.02382v1","updated":"2024-11-04T18:50:00Z","published":"2024-11-04T18:50:00Z","title":"Improving Scientific Hypothesis Generation with Knowledge Grounded Large\n  Language Models","summary":"  Large language models (LLMs) have demonstrated remarkable capabilities in\nvarious scientific domains, from natural language processing to complex\nproblem-solving tasks. Their ability to understand and generate human-like text\nhas opened up new possibilities for advancing scientific research, enabling\ntasks such as data analysis, literature review, and even experimental design.\nOne of the most promising applications of LLMs in this context is hypothesis\ngeneration, where they can identify novel research directions by analyzing\nexisting knowledge. However, despite their potential, LLMs are prone to\ngenerating ``hallucinations'', outputs that are plausible-sounding but\nfactually incorrect. Such a problem presents significant challenges in\nscientific fields that demand rigorous accuracy and verifiability, potentially\nleading to erroneous or misleading conclusions. To overcome these challenges,\nwe propose KG-CoI (Knowledge Grounded Chain of Ideas), a novel system that\nenhances LLM hypothesis generation by integrating external, structured\nknowledge from knowledge graphs (KGs). KG-CoI guides LLMs through a structured\nreasoning process, organizing their output as a chain of ideas (CoI), and\nincludes a KG-supported module for the detection of hallucinations. With\nexperiments on our newly constructed hypothesis generation dataset, we\ndemonstrate that KG-CoI not only improves the accuracy of LLM-generated\nhypotheses but also reduces the hallucination in their reasoning chains,\nhighlighting its effectiveness in advancing real-world scientific research.\n","authors":["Guangzhi Xiong","Eric Xie","Amir Hassan Shariatmadari","Sikun Guo","Stefan Bekiranov","Aidong Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.02382v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17717v4","updated":"2024-11-04T18:48:34Z","published":"2024-02-27T17:52:33Z","title":"AmbigNLG: Addressing Task Ambiguity in Instruction for NLG","summary":"  We introduce AmbigNLG, a novel task designed to tackle the challenge of task\nambiguity in instructions for Natural Language Generation (NLG). Ambiguous\ninstructions often impede the performance of Large Language Models (LLMs),\nespecially in complex NLG tasks. To tackle this issue, we propose an ambiguity\ntaxonomy that categorizes different types of instruction ambiguities and\nrefines initial instructions with clearer specifications. Accompanying this\ntask, we present AmbigSNI-NLG, a dataset comprising 2,500 instances annotated\nto facilitate research in AmbigNLG. Through comprehensive experiments with\nstate-of-the-art LLMs, we demonstrate that our method significantly enhances\nthe alignment of generated text with user expectations, achieving up to a\n15.02-point increase in ROUGE scores. Our findings highlight the critical\nimportance of addressing task ambiguity to fully harness the capabilities of\nLLMs in NLG tasks. Furthermore, we confirm the effectiveness of our method in\npractical settings involving interactive ambiguity mitigation with users,\nunderscoring the benefits of leveraging LLMs for interactive clarification.\n","authors":["Ayana Niwa","Hayate Iso"],"pdf_url":"https://arxiv.org/pdf/2402.17717v4.pdf","comment":"EMNLP 2024 (main)"},{"id":"http://arxiv.org/abs/2410.23262v2","updated":"2024-11-04T18:44:20Z","published":"2024-10-30T17:46:31Z","title":"EMMA: End-to-End Multimodal Model for Autonomous Driving","summary":"  We introduce EMMA, an End-to-end Multimodal Model for Autonomous driving.\nBuilt on a multi-modal large language model foundation, EMMA directly maps raw\ncamera sensor data into various driving-specific outputs, including planner\ntrajectories, perception objects, and road graph elements. EMMA maximizes the\nutility of world knowledge from the pre-trained large language models, by\nrepresenting all non-sensor inputs (e.g. navigation instructions and ego\nvehicle status) and outputs (e.g. trajectories and 3D locations) as natural\nlanguage text. This approach allows EMMA to jointly process various driving\ntasks in a unified language space, and generate the outputs for each task using\ntask-specific prompts. Empirically, we demonstrate EMMA's effectiveness by\nachieving state-of-the-art performance in motion planning on nuScenes as well\nas competitive results on the Waymo Open Motion Dataset (WOMD). EMMA also\nyields competitive results for camera-primary 3D object detection on the Waymo\nOpen Dataset (WOD). We show that co-training EMMA with planner trajectories,\nobject detection, and road graph tasks yields improvements across all three\ndomains, highlighting EMMA's potential as a generalist model for autonomous\ndriving applications. However, EMMA also exhibits certain limitations: it can\nprocess only a small amount of image frames, does not incorporate accurate 3D\nsensing modalities like LiDAR or radar and is computationally expensive. We\nhope that our results will inspire further research to mitigate these issues\nand to further evolve the state of the art in autonomous driving model\narchitectures.\n","authors":["Jyh-Jing Hwang","Runsheng Xu","Hubert Lin","Wei-Chih Hung","Jingwei Ji","Kristy Choi","Di Huang","Tong He","Paul Covington","Benjamin Sapp","Yin Zhou","James Guo","Dragomir Anguelov","Mingxing Tan"],"pdf_url":"https://arxiv.org/pdf/2410.23262v2.pdf","comment":"Blog post: https://waymo.com/blog/2024/10/introducing-emma/"},{"id":"http://arxiv.org/abs/2411.02348v1","updated":"2024-11-04T18:18:38Z","published":"2024-11-04T18:18:38Z","title":"Can Large Language Models generalize analogy solving like people can?","summary":"  When we solve an analogy we transfer information from a known context to a\nnew one through abstract rules and relational similarity. In people, the\nability to solve analogies such as \"body : feet :: table : ?\" emerges in\nchildhood, and appears to transfer easily to other domains, such as the visual\ndomain \"( : ) :: < : ?\". Recent research shows that large language models\n(LLMs) can solve various forms of analogies. However, can LLMs generalize\nanalogy solving to new domains like people can? To investigate this, we had\nchildren, adults, and LLMs solve a series of letter-string analogies (e.g., a b\n: a c :: j k : ?) in the Latin alphabet, in a near transfer domain (Greek\nalphabet), and a far transfer domain (list of symbols). As expected, children\nand adults easily generalized their knowledge to unfamiliar domains, whereas\nLLMs did not. This key difference between human and AI performance is evidence\nthat these LLMs still struggle with robust human-like analogical transfer.\n","authors":["Claire E. Stevenson","Alexandra Pafford","Han L. J. van der Maas","Melanie Mitchell"],"pdf_url":"https://arxiv.org/pdf/2411.02348v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02344v1","updated":"2024-11-04T18:14:07Z","published":"2024-11-04T18:14:07Z","title":"Seq-VCR: Preventing Collapse in Intermediate Transformer Representations\n  for Enhanced Reasoning","summary":"  Decoder-only Transformers often struggle with complex reasoning tasks,\nparticularly arithmetic reasoning requiring multiple sequential operations. In\nthis work, we identify representation collapse in the model's intermediate\nlayers as a key factor limiting their reasoning capabilities. To address this,\nwe propose Sequential Variance-Covariance Regularization (Seq-VCR), which\nenhances the entropy of intermediate representations and prevents collapse.\nCombined with dummy pause tokens as substitutes for chain-of-thought (CoT)\ntokens, our method significantly improves performance in arithmetic reasoning\nproblems. In the challenging $5 \\times 5$ integer multiplication task, our\napproach achieves $99.5\\%$ exact match accuracy, outperforming models of the\nsame size (which yield $0\\%$ accuracy) and GPT-4 with five-shot CoT prompting\n($44\\%$). We also demonstrate superior results on arithmetic expression and\nlongest increasing subsequence (LIS) datasets. Our findings highlight the\nimportance of preventing intermediate layer representation collapse to enhance\nthe reasoning capabilities of Transformers and show that Seq-VCR offers an\neffective solution without requiring explicit CoT supervision.\n","authors":["Md Rifat Arefin","Gopeshh Subbaraj","Nicolas Gontier","Yann LeCun","Irina Rish","Ravid Shwartz-Ziv","Christopher Pal"],"pdf_url":"https://arxiv.org/pdf/2411.02344v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02337v1","updated":"2024-11-04T17:59:58Z","published":"2024-11-04T17:59:58Z","title":"WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum\n  Reinforcement Learning","summary":"  Large language models (LLMs) have shown remarkable potential as autonomous\nagents, particularly in web-based tasks. However, existing LLM web agents\nheavily rely on expensive proprietary LLM APIs, while open LLMs lack the\nnecessary decision-making capabilities. This paper introduces WebRL, a\nself-evolving online curriculum reinforcement learning framework designed to\ntrain high-performance web agents using open LLMs. WebRL addresses three key\nchallenges in building LLM web agents, including the scarcity of training\ntasks, sparse feedback signals, and policy distribution drift in online\nlearning. Specifically, WebRL incorporates 1) a self-evolving curriculum that\ngenerates new tasks from unsuccessful attempts, 2) a robust outcome-supervised\nreward model (ORM), and 3) adaptive reinforcement learning strategies to ensure\nconsistent improvements. We apply WebRL to transform open Llama-3.1 and GLM-4\nmodels into proficient web agents. On WebArena-Lite, WebRL improves the success\nrate of Llama-3.1-8B from 4.8% to 42.4%, and from 6.1% to 43% for GLM-4-9B.\nThese open models significantly surpass the performance of GPT-4-Turbo (17.6%)\nand GPT-4o (13.9%) and outperform previous state-of-the-art web agents trained\non open LLMs (AutoWebGLM, 18.2%). Our findings demonstrate WebRL's\neffectiveness in bridging the gap between open and proprietary LLM-based web\nagents, paving the way for more accessible and powerful autonomous web\ninteraction systems.\n","authors":["Zehan Qi","Xiao Liu","Iat Long Iong","Hanyu Lai","Xueqiao Sun","Xinyue Yang","Jiadai Sun","Yu Yang","Shuntian Yao","Tianjie Zhang","Wei Xu","Jie Tang","Yuxiao Dong"],"pdf_url":"https://arxiv.org/pdf/2411.02337v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02335v1","updated":"2024-11-04T17:59:04Z","published":"2024-11-04T17:59:04Z","title":"Sparsing Law: Towards Large Language Models with Greater Activation\n  Sparsity","summary":"  Activation sparsity denotes the existence of substantial weakly-contributed\nelements within activation outputs that can be eliminated, benefiting many\nimportant applications concerned with large language models (LLMs). Although\npromoting greater activation sparsity within LLMs deserves deep studies,\nexisting works lack comprehensive and quantitative research on the correlation\nbetween activation sparsity and potentially influential factors. In this paper,\nwe present a comprehensive study on the quantitative scaling properties and\ninfluential factors of the activation sparsity within decoder-only\nTransformer-based LLMs. Specifically, we propose PPL-$p\\%$ sparsity, a precise\nand performance-aware activation sparsity metric that is applicable to any\nactivation function. Through extensive experiments, we find several important\nphenomena. Firstly, different activation functions exhibit comparable\nperformance but opposite training-time sparsity trends. The activation ratio\n(i.e., $1-\\mathrm{sparsity\\ ratio}$) evolves as a convergent increasing\npower-law and decreasing logspace power-law with the amount of training data\nfor SiLU-activated and ReLU-activated LLMs, respectively. These demonstrate\nthat ReLU is more efficient as the activation function than SiLU and can\nleverage more training data to improve activation sparsity. Secondly, the\nactivation ratio linearly increases with the width-depth ratio below a certain\nbottleneck point, indicating the potential advantage of a deeper architecture\nat a fixed parameter scale. Finally, at similar width-depth ratios, we\nsurprisingly find that the limit value of activation sparsity varies weakly\nwith the parameter scale, i.e., the activation patterns within LLMs are\ninsensitive to the parameter scale. These empirical laws towards LLMs with\ngreater activation sparsity have important implications for making LLMs more\nefficient and interpretable.\n","authors":["Yuqi Luo","Chenyang Song","Xu Han","Yingfa Chen","Chaojun Xiao","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2411.02335v1.pdf","comment":"23 pages, 13 figures, 6 tables"},{"id":"http://arxiv.org/abs/2411.00294v2","updated":"2024-11-04T17:57:43Z","published":"2024-11-01T01:11:58Z","title":"LLM-Ref: Enhancing Reference Handling in Technical Writing with Large\n  Language Models","summary":"  Large Language Models (LLMs) excel in data synthesis but can be inaccurate in\ndomain-specific tasks, which retrieval-augmented generation (RAG) systems\naddress by leveraging user-provided data. However, RAGs require optimization in\nboth retrieval and generation stages, which can affect output quality. In this\npaper, we present LLM-Ref, a writing assistant tool that aids researchers in\nwriting articles from multiple source documents with enhanced reference\nsynthesis and handling capabilities. Unlike traditional RAG systems that use\nchunking and indexing, our tool retrieves and generates content directly from\ntext paragraphs. This method facilitates direct reference extraction from the\ngenerated outputs, a feature unique to our tool. Additionally, our tool employs\niterative response generation, effectively managing lengthy contexts within the\nlanguage model's constraints. Compared to baseline RAG-based systems, our\napproach achieves a $3.25\\times$ to $6.26\\times$ increase in Ragas score, a\ncomprehensive metric that provides a holistic view of a RAG system's ability to\nproduce accurate, relevant, and contextually appropriate responses. This\nimprovement shows our method enhances the accuracy and contextual relevance of\nwriting assistance tools.\n","authors":["Kazi Ahmed Asif Fuad","Lizhong Chen"],"pdf_url":"https://arxiv.org/pdf/2411.00294v2.pdf","comment":"20 pages, 7 figures"},{"id":"http://arxiv.org/abs/2411.02316v1","updated":"2024-11-04T17:40:39Z","published":"2024-11-04T17:40:39Z","title":"Evaluating Creative Short Story Generation in Humans and Large Language\n  Models","summary":"  Storytelling is a fundamental aspect of human communication, relying heavily\non creativity to produce narratives that are novel, appropriate, and\nsurprising. While large language models (LLMs) have recently demonstrated the\nability to generate high-quality stories, their creative capabilities remain\nunderexplored. Previous research has either focused on creativity tests\nrequiring short responses or primarily compared model performance in story\ngeneration to that of professional writers. However, the question of whether\nLLMs exhibit creativity in writing short stories on par with the average human\nremains unanswered. In this work, we conduct a systematic analysis of\ncreativity in short story generation across LLMs and everyday people. Using a\nfive-sentence creative story task, commonly employed in psychology to assess\nhuman creativity, we automatically evaluate model- and human-generated stories\nacross several dimensions of creativity, including novelty, surprise, and\ndiversity. Our findings reveal that while LLMs can generate stylistically\ncomplex stories, they tend to fall short in terms of creativity when compared\nto average human writers.\n","authors":["Mete Ismayilzada","Claire Stevenson","Lonneke van der Plas"],"pdf_url":"https://arxiv.org/pdf/2411.02316v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2411.02310v1","updated":"2024-11-04T17:36:40Z","published":"2024-11-04T17:36:40Z","title":"MdEval: Massively Multilingual Code Debugging","summary":"  Code large language models (LLMs) have made significant progress in code\ndebugging by directly generating the correct code based on the buggy code\nsnippet. Programming benchmarks, typically consisting of buggy code snippet and\ntheir associated test cases, are used to assess the debugging capabilities of\nLLMs. However, many existing benchmarks primarily focus on Python and are often\nlimited in terms of language diversity (e.g., DebugBench and DebugEval). To\nadvance the field of multilingual debugging with LLMs, we propose the first\nmassively multilingual debugging benchmark, which includes 3.6K test samples of\n18 programming languages and covers the automated program repair (APR) task,\nthe code review (CR) task, and the bug identification (BI) task. Further, we\nintroduce the debugging instruction corpora MDEVAL-INSTRUCT by injecting bugs\ninto the correct multilingual queries and solutions (xDebugGen). Further, a\nmultilingual debugger xDebugCoder trained on MDEVAL-INSTRUCT as a strong\nbaseline specifically to handle the bugs of a wide range of programming\nlanguages (e.g. \"Missing Mut\" in language Rust and \"Misused Macro Definition\"\nin language C). Our extensive experiments on MDEVAL reveal a notable\nperformance gap between open-source models and closed-source LLMs (e.g., GPT\nand Claude series), highlighting huge room for improvement in multilingual code\ndebugging scenarios.\n","authors":["Shukai Liu","Linzheng Chai","Jian Yang","Jiajun Shi","He Zhu","Liran Wang","Ke Jin","Wei Zhang","Hualei Zhu","Shuyue Guo","Tao Sun","Jiaheng Liu","Yunlong Duan","Yu Hao","Liqun Yang","Guanglin Niu","Ge Zhang","Zhoujun Li"],"pdf_url":"https://arxiv.org/pdf/2411.02310v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2407.14679v2","updated":"2024-11-04T17:36:38Z","published":"2024-07-19T21:47:57Z","title":"Compact Language Models via Pruning and Knowledge Distillation","summary":"  Large language models (LLMs) targeting different deployment scales and sizes\nare currently produced by training each variant from scratch; this is extremely\ncompute-intensive. In this paper, we investigate if pruning an existing LLM and\nthen re-training it with a fraction (<3%) of the original training data can be\na suitable alternative to repeated, full retraining. To this end, we develop a\nset of practical and effective compression best practices for LLMs that combine\ndepth, width, attention and MLP pruning with knowledge distillation-based\nretraining; we arrive at these best practices through a detailed empirical\nexploration of pruning strategies for each axis, methods to combine axes,\ndistillation strategies, and search techniques for arriving at optimal\ncompressed architectures. We use this guide to compress the Nemotron-4 family\nof LLMs by a factor of 2-4x, and compare their performance to similarly-sized\nmodels on a variety of language modeling tasks. Deriving 8B and 4B models from\nan already pretrained 15B model using our approach requires up to 40x fewer\ntraining tokens per model compared to training from scratch; this results in\ncompute cost savings of 1.8x for training the full model family (15B, 8B, and\n4B). Minitron models exhibit up to a 16% improvement in MMLU scores compared to\ntraining from scratch, perform comparably to other community models such as\nMistral 7B, Gemma 7B and Llama-3 8B, and outperform state-of-the-art\ncompression techniques from the literature. We have open-sourced Minitron model\nweights on Huggingface, with corresponding supplementary material including\nexample code available on GitHub.\n","authors":["Saurav Muralidharan","Sharath Turuvekere Sreenivas","Raviraj Joshi","Marcin Chochowski","Mostofa Patwary","Mohammad Shoeybi","Bryan Catanzaro","Jan Kautz","Pavlo Molchanov"],"pdf_url":"https://arxiv.org/pdf/2407.14679v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02305v1","updated":"2024-11-04T17:30:51Z","published":"2024-11-04T17:30:51Z","title":"CRMArena: Understanding the Capacity of LLM Agents to Perform\n  Professional CRM Tasks in Realistic Environments","summary":"  Customer Relationship Management (CRM) systems are vital for modern\nenterprises, providing a foundation for managing customer interactions and\ndata. Integrating AI agents into CRM systems can automate routine processes and\nenhance personalized service. However, deploying and evaluating these agents is\nchallenging due to the lack of realistic benchmarks that reflect the complexity\nof real-world CRM tasks. To address this issue, we introduce CRMArena, a novel\nbenchmark designed to evaluate AI agents on realistic tasks grounded in\nprofessional work environments. Following guidance from CRM experts and\nindustry best practices, we designed CRMArena with nine customer service tasks\ndistributed across three personas: service agent, analyst, and manager. The\nbenchmark includes 16 commonly used industrial objects (e.g., account, order,\nknowledge article, case) with high interconnectivity, along with latent\nvariables (e.g., complaint habits, policy violations) to simulate realistic\ndata distributions. Experimental results reveal that state-of-the-art LLM\nagents succeed in less than 40% of the tasks with ReAct prompting, and less\nthan 55% even with function-calling abilities. Our findings highlight the need\nfor enhanced agent capabilities in function-calling and rule-following to be\ndeployed in real-world work environments. CRMArena is an open challenge to the\ncommunity: systems that can reliably complete tasks showcase direct business\nvalue in a popular work environment.\n","authors":["Kung-Hsiang Huang","Akshara Prabhakar","Sidharth Dhawan","Yixin Mao","Huan Wang","Silvio Savarese","Caiming Xiong","Philippe Laban","Chien-Sheng Wu"],"pdf_url":"https://arxiv.org/pdf/2411.02305v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02280v1","updated":"2024-11-04T17:09:10Z","published":"2024-11-04T17:09:10Z","title":"The LLM Language Network: A Neuroscientific Approach for Identifying\n  Causally Task-Relevant Units","summary":"  Large language models (LLMs) exhibit remarkable capabilities on not just\nlanguage tasks, but also various tasks that are not linguistic in nature, such\nas logical reasoning and social inference. In the human brain, neuroscience has\nidentified a core language system that selectively and causally supports\nlanguage processing. We here ask whether similar specialization for language\nemerges in LLMs. We identify language-selective units within 18 popular LLMs,\nusing the same localization approach that is used in neuroscience. We then\nestablish the causal role of these units by demonstrating that ablating LLM\nlanguage-selective units -- but not random units -- leads to drastic deficits\nin language tasks. Correspondingly, language-selective LLM units are more\naligned to brain recordings from the human language system than random units.\nFinally, we investigate whether our localization method extends to other\ncognitive domains: while we find specialized networks in some LLMs for\nreasoning and social capabilities, there are substantial differences among\nmodels. These findings provide functional and causal evidence for\nspecialization in large language models, and highlight parallels with the\nfunctional organization in the brain.\n","authors":["Badr AlKhamissi","Greta Tuckute","Antoine Bosselut","Martin Schrimpf"],"pdf_url":"https://arxiv.org/pdf/2411.02280v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2411.00664v2","updated":"2024-11-04T17:05:58Z","published":"2024-11-01T15:28:03Z","title":"Optimizing Contextual Speech Recognition Using Vector Quantization for\n  Efficient Retrieval","summary":"  Neural contextual biasing allows speech recognition models to leverage\ncontextually relevant information, leading to improved transcription accuracy.\nHowever, the biasing mechanism is typically based on a cross-attention module\nbetween the audio and a catalogue of biasing entries, which means computational\ncomplexity can pose severe practical limitations on the size of the biasing\ncatalogue and consequently on accuracy improvements. This work proposes an\napproximation to cross-attention scoring based on vector quantization and\nenables compute- and memory-efficient use of large biasing catalogues. We\npropose to use this technique jointly with a retrieval based contextual biasing\napproach. First, we use an efficient quantized retrieval module to shortlist\nbiasing entries by grounding them on audio. Then we use retrieved entries for\nbiasing. Since the proposed approach is agnostic to the biasing method, we\ninvestigate using full cross-attention, LLM prompting, and a combination of the\ntwo. We show that retrieval based shortlisting allows the system to efficiently\nleverage biasing catalogues of several thousands of entries, resulting in up to\n71% relative error rate reduction in personal entity recognition. At the same\ntime, the proposed approximation algorithm reduces compute time by 20% and\nmemory usage by 85-95%, for lists of up to one million entries, when compared\nto standard dot-product cross-attention.\n","authors":["Nikolaos Flemotomos","Roger Hsiao","Pawel Swietojanski","Takaaki Hori","Dogan Can","Xiaodan Zhuang"],"pdf_url":"https://arxiv.org/pdf/2411.00664v2.pdf","comment":"13 pages, 7 figures, submitted to IEEE/ACM Transactions on Audio,\n  Speech, and Language Processing"},{"id":"http://arxiv.org/abs/2411.02272v1","updated":"2024-11-04T17:03:55Z","published":"2024-11-04T17:03:55Z","title":"Combining Induction and Transduction for Abstract Reasoning","summary":"  When learning an input-output mapping from very few examples, is it better to\nfirst infer a latent function that explains the examples, or is it better to\ndirectly predict new test outputs, e.g. using a neural network? We study this\nquestion on ARC, a highly diverse dataset of abstract reasoning tasks. We train\nneural models for induction (inferring latent functions) and transduction\n(directly predicting the test output for a given test input). Our models are\ntrained on synthetic data generated by prompting LLMs to produce Python code\nspecifying a function to be inferred, plus a stochastic subroutine for\ngenerating inputs to that function. We find inductive and transductive models\nsolve very different problems, despite training on the same problems, and\ndespite sharing the same neural architecture.\n","authors":["Wen-Ding Li","Keya Hu","Carter Larsen","Yuqing Wu","Simon Alford","Caleb Woo","Spencer M. Dunn","Hao Tang","Michelangelo Naim","Dat Nguyen","Wei-Long Zheng","Zenna Tavares","Yewen Pu","Kevin Ellis"],"pdf_url":"https://arxiv.org/pdf/2411.02272v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02265v1","updated":"2024-11-04T16:56:26Z","published":"2024-11-04T16:56:26Z","title":"Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated\n  Parameters by Tencent","summary":"  In this paper, we introduce Hunyuan-Large, which is currently the largest\nopen-source Transformer-based mixture of experts model, with a total of 389\nbillion parameters and 52 billion activation parameters, capable of handling up\nto 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior\nperformance across various benchmarks including language understanding and\ngeneration, logical reasoning, mathematical problem-solving, coding,\nlong-context, and aggregated tasks, where it outperforms LLama3.1-70B and\nexhibits comparable performance when compared to the significantly larger\nLLama3.1-405B model. Key practice of Hunyuan-Large include large-scale\nsynthetic data that is orders larger than in previous literature, a mixed\nexpert routing strategy, a key-value cache compression technique, and an\nexpert-specific learning rate strategy. Additionally, we also investigate the\nscaling laws and learning rate schedule of mixture of experts models, providing\nvaluable insights and guidances for future model development and optimization.\nThe code and checkpoints of Hunyuan-Large are released to facilitate future\ninnovations and applications.\n  Codes: https://github.com/Tencent/Hunyuan-Large\n  Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large\n","authors":["Xingwu Sun","Yanfeng Chen","Yiqing Huang","Ruobing Xie","Jiaqi Zhu","Kai Zhang","Shuaipeng Li","Zhen Yang","Jonny Han","Xiaobo Shu","Jiahao Bu","Zhongzhi Chen","Xuemeng Huang","Fengzong Lian","Saiyong Yang","Jianfeng Yan","Yuyuan Zeng","Xiaoqin Ren","Chao Yu","Lulu Wu","Yue Mao","Tao Yang","Suncong Zheng","Kan Wu","Dian Jiao","Jinbao Xue","Xipeng Zhang","Decheng Wu","Kai Liu","Dengpeng Wu","Guanghui Xu","Shaohua Chen","Shuang Chen","Xiao Feng","Yigeng Hong","Junqiang Zheng","Chengcheng Xu","Zongwei Li","Xiong Kuang","Jianglu Hu","Yiqi Chen","Yuchi Deng","Guiyang Li","Ao Liu","Chenchen Zhang","Shihui Hu","Zilong Zhao","Zifan Wu","Yao Ding","Weichao Wang","Han Liu","Roberts Wang","Hao Fei","Peijie She","Ze Zhao","Xun Cao","Hai Wang","Fusheng Xiang","Mengyuan Huang","Zhiyuan Xiong","Bin Hu","Xuebin Hou","Lei Jiang","Jiajia Wu","Yaping Deng","Yi Shen","Qian Wang","Weijie Liu","Jie Liu","Meng Chen","Liang Dong","Weiwen Jia","Hu Chen","Feifei Liu","Rui Yuan","Huilin Xu","Zhenxiang Yan","Tengfei Cao","Zhichao Hu","Xinhua Feng","Dong Du","Tinghao She","Yangyu Tao","Feng Zhang","Jianchen Zhu","Chengzhong Xu","Xirui Li","Chong Zha","Wen Ouyang","Yinben Xia","Xiang Li","Zekun He","Rongpeng Chen","Jiawei Song","Ruibin Chen","Fan Jiang","Chongqing Zhao","Bo Wang","Hao Gong","Rong Gan","Winston Hu","Zhanhui Kang","Yong Yang","Yuhong Liu","Di Wang","Jie Jiang"],"pdf_url":"https://arxiv.org/pdf/2411.02265v1.pdf","comment":"17 pages, 4 Figures"},{"id":"http://arxiv.org/abs/2407.15055v2","updated":"2024-11-04T16:56:13Z","published":"2024-07-21T04:52:38Z","title":"Training Zero-Shot Generalizable End-to-End Task-Oriented Dialog System\n  Without Turn-level Dialog Annotations","summary":"  Task-oriented dialogue (TOD) systems enable users to achieve their goals\nthrough natural language interactions. Traditionally, these systems have relied\non turn-level manually annotated metadata, such as dialogue states and policy\nannotations, which are expensive, time-consuming, and often inconsistent or\nerror-prone. This dependence limits the potential to leverage vast amounts of\nreadily available conversational data for training TOD systems. Additionally, a\ncritical challenge in TOD system design is determining when and how to access\nand integrate information from external sources. Current approaches typically\nexpect this information to be provided alongside the dialogue context, rather\nthan learning to identify and retrieve it autonomously. While pre-trained large\nlanguage models (LLMs) have been used to develop TOD systems, their potential\nto train such systems without laborious annotations remains largely unexplored.\nThis work employs multi-task instruction fine-tuning to create more efficient\nand scalable TOD systems that can effectively leverage natural language\nconversational data without manual annotations, while autonomously managing\nexternal information retrieval. Our extensive experimental evaluations, using\nthree diverse TOD datasets and three LLMs of varying sizes, demonstrate that\nour approach can generalize to new, unseen domains. Notably, our approach\noutperforms both state-of-the-art models trained on annotated data and\nbillion-scale parameter off-the-shelf ChatGPT models.\n","authors":["Adib Mosharrof","A. B. Siddique"],"pdf_url":"https://arxiv.org/pdf/2407.15055v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.19128v2","updated":"2024-11-04T16:44:42Z","published":"2024-10-24T19:56:28Z","title":"Retrieving Implicit and Explicit Emotional Events Using Large Language\n  Models","summary":"  Large language models (LLMs) have garnered significant attention in recent\nyears due to their impressive performance. While considerable research has\nevaluated these models from various perspectives, the extent to which LLMs can\nperform implicit and explicit emotion retrieval remains largely unexplored. To\naddress this gap, this study investigates LLMs' emotion retrieval capabilities\nin commonsense. Through extensive experiments involving multiple models, we\nsystematically evaluate the ability of LLMs on emotion retrieval. Specifically,\nwe propose a supervised contrastive probing method to verify LLMs' performance\nfor implicit and explicit emotion retrieval, as well as the diversity of the\nemotional events they retrieve. The results offer valuable insights into the\nstrengths and limitations of LLMs in handling emotion retrieval.\n","authors":["Guimin Hu","Hasti Seifi"],"pdf_url":"https://arxiv.org/pdf/2410.19128v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02223v1","updated":"2024-11-04T16:15:28Z","published":"2024-11-04T16:15:28Z","title":"Positive Experience Reflection for Agents in Interactive Text\n  Environments","summary":"  Intelligent agents designed for interactive environments face significant\nchallenges in text-based games, a domain that demands complex reasoning and\nadaptability. While agents based on large language models (LLMs) using\nself-reflection have shown promise, they struggle when initially successful and\nexhibit reduced effectiveness when using smaller LLMs. We introduce Sweet&Sour,\na novel approach that addresses these limitations in existing reflection\nmethods by incorporating positive experiences and managed memory to enrich the\ncontext available to the agent at decision time. Our comprehensive analysis\nspans both closed- and open-source LLMs and demonstrates the effectiveness of\nSweet&Sour in improving agent performance, particularly in scenarios where\nprevious approaches fall short.\n","authors":["Philip Lippmann","Matthijs T. J. Spaan","Jie Yang"],"pdf_url":"https://arxiv.org/pdf/2411.02223v1.pdf","comment":"To appear at NeurIPS 2024 Language Gamification workshop"},{"id":"http://arxiv.org/abs/2411.02209v1","updated":"2024-11-04T16:01:43Z","published":"2024-11-04T16:01:43Z","title":"The Role of DevOps in Enhancing Enterprise Software Delivery Success\n  through R&D Efficiency and Source Code Management","summary":"  This study examines the impact of DevOps practices on enterprise software\ndelivery success, focusing on enhancing R&D efficiency and source code\nmanagement (SCM). Using a qualitative methodology, data were collected from\ncase studies of large-scale enterprises implementing DevOps to explore how\nthese practices streamline software development processes. Findings reveal that\nDevOps significantly improves R&D productivity by fostering cross-functional\ncollaboration, reducing development cycle times, and enhancing software quality\nthrough effective SCM practices, such as version control and continuous\nintegration. Additionally, SCM tools within DevOps enable precise change\ntracking and reliable code maintenance, further supporting faster, more robust\nsoftware delivery. However, the study identifies challenges, including cultural\nresistance and tool integration issues, that can hinder DevOps implementation.\nAdditionally, This research contributes to the growing body of DevOps\nliterature by highlighting the role of R&D efficiency and SCM as crucial\nfactors for software delivery success. Future studies should investigate these\nfactors across diverse industries to validate findings.\n","authors":["Jun Cui"],"pdf_url":"https://arxiv.org/pdf/2411.02209v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17860v3","updated":"2024-11-04T15:59:19Z","published":"2024-03-26T16:49:25Z","title":"Illuminating Blind Spots of Language Models with Targeted\n  Agent-in-the-Loop Synthetic Data","summary":"  Language models (LMs) have achieved impressive accuracy across a variety of\ntasks but remain vulnerable to high-confidence misclassifications, also\nreferred to as unknown unknowns (UUs). These UUs cluster into blind spots in\nthe feature space, leading to significant risks in high-stakes applications.\nThis is particularly relevant for smaller, lightweight LMs that are more\nsusceptible to such errors. While the identification of UUs has been\nextensively studied, their mitigation remains an open challenge, including how\nto use identified UUs to eliminate unseen blind spots. In this work, we propose\na novel approach to address blind spot mitigation through the use of\nintelligent agents -- either humans or large LMs -- as teachers to characterize\nUU-type errors. By leveraging the generalization capabilities of intelligent\nagents, we identify patterns in high-confidence misclassifications and use them\nto generate targeted synthetic samples to improve model robustness and reduce\nblind spots. We conduct an extensive evaluation of our method on three\nclassification tasks and demonstrate its effectiveness in reducing the number\nof UUs, all while maintaining a similar level of accuracy. We find that the\neffectiveness of human computation has a high ceiling but is highly dependent\non familiarity with the underlying task. Moreover, the cost gap between humans\nand LMs surpasses an order of magnitude, as LMs attain human-like\ngeneralization and generation performance while being more scalable.\n","authors":["Philip Lippmann","Matthijs T. J. Spaan","Jie Yang"],"pdf_url":"https://arxiv.org/pdf/2403.17860v3.pdf","comment":"Preprint. Under review"},{"id":"http://arxiv.org/abs/2404.15127v2","updated":"2024-11-04T15:54:21Z","published":"2024-04-23T15:27:19Z","title":"GSCo: Towards Generalizable AI in Medicine via Generalist-Specialist\n  Collaboration","summary":"  Generalist foundation models (GFMs) are renowned for their exceptional\ncapability and flexibility in effectively generalizing across diverse tasks and\nmodalities. In the field of medicine, while GFMs exhibit superior\ngeneralizability based on their extensive intrinsic knowledge as well as\nproficiency in instruction following and in-context learning, specialist models\nexcel in precision due to their domain knowledge. In this work, for the first\ntime, we explore the synergy between the GFM and specialist models, to enable\nprecise medical image analysis on a broader scope. Specifically, we propose a\ncooperative framework, Generalist-Specialist Collaboration (GSCo), which\nconsists of two stages, namely the construction of GFM and specialists, and\ncollaborative inference on downstream tasks. In the construction stage, we\ndevelop MedDr, the largest open-source GFM tailored for medicine, showcasing\nexceptional instruction-following and in-context learning capabilities.\nMeanwhile, a series of lightweight specialists are crafted for downstream tasks\nwith low computational cost. In the collaborative inference stage, we introduce\ntwo cooperative mechanisms, Mixture-of-Expert Diagnosis and Retrieval-Augmented\nDiagnosis, to harvest the generalist's in-context learning abilities alongside\nthe specialists' domain expertise. For a comprehensive evaluation, we curate a\nlarge-scale benchmark featuring 28 datasets and about 250,000 images. Extensive\nresults demonstrate that MedDr consistently outperforms state-of-the-art GFMs\non downstream datasets. Furthermore, GSCo exceeds both GFMs and specialists\nacross all out-of-domain disease diagnosis datasets. These findings indicate a\nsignificant paradigm shift in the application of GFMs, transitioning from\nseparate models for specific tasks to a collaborative approach between GFMs and\nspecialists, thereby advancing the frontiers of generalizable AI in medicine.\n","authors":["Sunan He","Yuxiang Nie","Hongmei Wang","Shu Yang","Yihui Wang","Zhiyuan Cai","Zhixuan Chen","Yingxue Xu","Luyang Luo","Huiling Xiang","Xi Lin","Mingxiang Wu","Yifan Peng","George Shih","Ziyang Xu","Xian Wu","Qiong Wang","Ronald Cheong Kin Chan","Varut Vardhanabhuti","Winnie Chiu Wing Chu","Yefeng Zheng","Pranav Rajpurkar","Kang Zhang","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2404.15127v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19266v3","updated":"2024-11-04T15:49:41Z","published":"2024-05-29T16:59:38Z","title":"PediatricsGPT: Large Language Models as Chinese Medical Assistants for\n  Pediatric Applications","summary":"  Developing intelligent pediatric consultation systems offers promising\nprospects for improving diagnostic efficiency, especially in China, where\nhealthcare resources are scarce. Despite recent advances in Large Language\nModels (LLMs) for Chinese medicine, their performance is sub-optimal in\npediatric applications due to inadequate instruction data and vulnerable\ntraining procedures. To address the above issues, this paper builds PedCorpus,\na high-quality dataset of over 300,000 multi-task instructions from pediatric\ntextbooks, guidelines, and knowledge graph resources to fulfil diverse\ndiagnostic demands. Upon well-designed PedCorpus, we propose PediatricsGPT, the\nfirst Chinese pediatric LLM assistant built on a systematic and robust training\npipeline. In the continuous pre-training phase, we introduce a hybrid\ninstruction pre-training mechanism to mitigate the internal-injected knowledge\ninconsistency of LLMs for medical domain adaptation. Immediately, the\nfull-parameter Supervised Fine-Tuning (SFT) is utilized to incorporate the\ngeneral medical knowledge schema into the models. After that, we devise a\ndirect following preference optimization to enhance the generation of\npediatrician-like humanistic responses. In the parameter-efficient secondary\nSFT phase, a mixture of universal-specific experts strategy is presented to\nresolve the competency conflict between medical generalist and pediatric\nexpertise mastery. Extensive results based on the metrics, GPT-4, and doctor\nevaluations on distinct doctor downstream tasks show that PediatricsGPT\nconsistently outperforms previous Chinese medical LLMs. Our model and dataset\nwill be open-source for community development.\n","authors":["Dingkang Yang","Jinjie Wei","Dongling Xiao","Shunli Wang","Tong Wu","Gang Li","Mingcheng Li","Shuaibing Wang","Jiawei Chen","Yue Jiang","Qingyao Xu","Ke Li","Peng Zhai","Lihua Zhang"],"pdf_url":"https://arxiv.org/pdf/2405.19266v3.pdf","comment":"Accepted by NeurIPS 2024. A Technical Report on a Chinese Medical\n  Large Language Model"},{"id":"http://arxiv.org/abs/2411.02193v1","updated":"2024-11-04T15:46:20Z","published":"2024-11-04T15:46:20Z","title":"Improving Steering Vectors by Targeting Sparse Autoencoder Features","summary":"  To control the behavior of language models, steering methods attempt to\nensure that outputs of the model satisfy specific pre-defined properties.\nAdding steering vectors to the model is a promising method of model control\nthat is easier than finetuning, and may be more robust than prompting. However,\nit can be difficult to anticipate the effects of steering vectors produced by\nalmost all existing methods, such as CAA (Panickssery et al., 2024) or the\ndirect use of SAE latents (Templeton et al., 2024). In our work, we address\nthis issue by using SAEs to measure the effects of steering vectors, giving us\na method that can be used to understand the causal effect of any steering\nvector intervention. We use this method for measuring causal effects to develop\nan improved steering method, SAE-Targeted Steering (SAE-TS), which finds\nsteering vectors to target specific SAE features while minimizing unintended\nside effects. We show that overall, SAE-TS balances steering effects with\ncoherence better than CAA and SAE feature steering, when evaluated on a range\nof tasks.\n","authors":["Sviatoslav Chalnev","Matthew Siu","Arthur Conmy"],"pdf_url":"https://arxiv.org/pdf/2411.02193v1.pdf","comment":"8 maintext pages and 9 appendix pages"},{"id":"http://arxiv.org/abs/2411.00053v2","updated":"2024-11-04T15:20:16Z","published":"2024-10-30T19:09:02Z","title":"ACC-Debate: An Actor-Critic Approach to Multi-Agent Debate","summary":"  Large language models (LLMs) have demonstrated a remarkable ability to serve\nas general-purpose tools for various language-based tasks. Recent works have\ndemonstrated that the efficacy of such models can be improved through iterative\ndialog between multiple models, frequently referred to as multi-agent debate\n(MAD). While debate shows promise as a means of improving model efficacy, most\nworks in this area treat debate as an emergent behavior, rather than a learned\nbehavior. In doing so, current debate frameworks rely on collaborative\nbehaviors to have been sufficiently trained into off-the-shelf models. To\naddress this limitation, we propose ACC-Debate, an Actor-Critic based learning\nframework to produce a two-agent team specialized in debate. We demonstrate\nthat ACC-Debate outperforms SotA debate techniques on a wide array of\nbenchmarks.\n","authors":["Andrew Estornell","Jean-Francois Ton","Yuanshun Yao","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2411.00053v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17935v3","updated":"2024-11-04T15:07:18Z","published":"2024-05-28T08:01:26Z","title":"Tool Learning with Large Language Models: A Survey","summary":"  Recently, tool learning with large language models (LLMs) has emerged as a\npromising paradigm for augmenting the capabilities of LLMs to tackle highly\ncomplex problems. Despite growing attention and rapid advancements in this\nfield, the existing literature remains fragmented and lacks systematic\norganization, posing barriers to entry for newcomers. This gap motivates us to\nconduct a comprehensive survey of existing works on tool learning with LLMs. In\nthis survey, we focus on reviewing existing literature from the two primary\naspects (1) why tool learning is beneficial and (2) how tool learning is\nimplemented, enabling a comprehensive understanding of tool learning with LLMs.\nWe first explore the \"why\" by reviewing both the benefits of tool integration\nand the inherent benefits of the tool learning paradigm from six specific\naspects. In terms of \"how\", we systematically review the literature according\nto a taxonomy of four key stages in the tool learning workflow: task planning,\ntool selection, tool calling, and response generation. Additionally, we provide\na detailed summary of existing benchmarks and evaluation methods, categorizing\nthem according to their relevance to different stages. Finally, we discuss\ncurrent challenges and outline potential future directions, aiming to inspire\nboth researchers and industrial developers to further explore this emerging and\npromising area. We also maintain a GitHub repository to continually keep track\nof the relevant papers and resources in this rising area at\nhttps://github.com/quchangle1/LLM-Tool-Survey.\n","authors":["Changle Qu","Sunhao Dai","Xiaochi Wei","Hengyi Cai","Shuaiqiang Wang","Dawei Yin","Jun Xu","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2405.17935v3.pdf","comment":"The article has been accepted by Frontiers of Computer Science (FCS),\n  with the DOI: {10.1007/s11704-024-40678-2}"},{"id":"http://arxiv.org/abs/2411.02118v1","updated":"2024-11-04T14:30:57Z","published":"2024-11-04T14:30:57Z","title":"Grounding Emotional Descriptions to Electrovibration Haptic Signals","summary":"  Designing and displaying haptic signals with sensory and emotional attributes\ncan improve the user experience in various applications. Free-form user\nlanguage provides rich sensory and emotional information for haptic design\n(e.g., ``This signal feels smooth and exciting''), but little work exists on\nlinking user descriptions to haptic signals (i.e., language grounding). To\naddress this gap, we conducted a study where 12 users described the feel of 32\nsignals perceived on a surface haptics (i.e., electrovibration) display. We\ndeveloped a computational pipeline using natural language processing (NLP)\ntechniques, such as GPT-3.5 Turbo and word embedding methods, to extract\nsensory and emotional keywords and group them into semantic clusters (i.e.,\nconcepts). We linked the keyword clusters to haptic signal features (e.g.,\npulse count) using correlation analysis. The proposed pipeline demonstrates the\nviability of a computational approach to analyzing haptic experiences. We\ndiscuss our future plans for creating a predictive model of haptic experience.\n","authors":["Guimin Hu","Zirui Zhao","Lukas Heilmann","Yasemin Vardar","Hasti Seifi"],"pdf_url":"https://arxiv.org/pdf/2411.02118v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02117v1","updated":"2024-11-04T14:29:49Z","published":"2024-11-04T14:29:49Z","title":"AVSS: Layer Importance Evaluation in Large Language Models via\n  Activation Variance-Sparsity Analysis","summary":"  The evaluation of layer importance in deep learning has been an active area\nof research, with significant implications for model optimization and\ninterpretability. Recently, large language models (LLMs) have gained prominence\nacross various domains, yet limited studies have explored the functional\nimportance and performance contributions of individual layers within LLMs,\nespecially from the perspective of activation distribution. In this work, we\npropose the Activation Variance-Sparsity Score (AVSS), a novel metric combining\nnormalized activation variance and sparsity to assess each layer's contribution\nto model performance. By identifying and removing approximately the lowest 25%\nof layers based on AVSS, we achieve over 90% of original model performance\nacross tasks such as question answering, language modeling, and sentiment\nclassification, indicating that these layers may be non-essential. Our approach\nprovides a systematic method for identifying less critical layers, contributing\nto efficient large language model architectures.\n","authors":["Zichen Song","Yuxin Wu","Sitan Huang","Zhongfeng Kang"],"pdf_url":"https://arxiv.org/pdf/2411.02117v1.pdf","comment":"4 pages, 1 figure"},{"id":"http://arxiv.org/abs/2411.02116v1","updated":"2024-11-04T14:29:28Z","published":"2024-11-04T14:29:28Z","title":"Advancements and limitations of LLMs in replicating human color-word\n  associations","summary":"  Color-word associations play a fundamental role in human cognition and design\napplications. Large Language Models (LLMs) have become widely available and\ndemonstrated intelligent behaviors in various benchmarks with natural\nconversation skills. However, their ability to replicate human color-word\nassociations remains understudied. We compared multiple generations of LLMs\n(from GPT-3 to GPT- 4o) against human color-word associations using data\ncollected from over 10,000 Japanese participants, involving 17 colors and words\nfrom eight categories in Japanese. Our findings reveal a clear progression in\nLLM performance across generations, with GPT-4o achieving the highest accuracy\nin predicting the best voted word for each color and category, particularly\nwhen using visual inputs rather than text-based color codes. However, the\nhighest median performance was approximately 50% even for GPT4-o with visual\ninputs (chance level is 10%), and the performance levels varied significantly\nacross word categories and colors, indicating a failure to fully replicate\nhuman color-word associations. On the other hand, color discrimination ability\nestimated from our color-word association data showed that LLMs demonstrated\nhigh correlation with human color discrimination patterns, similarly to\nprevious studies. Our study highlights both the advancements in LLM\ncapabilities and their persistent limitations, suggesting differences in\nsemantic memory structures between humans and LLMs in representing color-word\nassociations.\n","authors":["Makoto Fukushima","Shusuke Eshita","Hiroshige Fukuhara"],"pdf_url":"https://arxiv.org/pdf/2411.02116v1.pdf","comment":"20 pages, 7 figures, 3 tables"},{"id":"http://arxiv.org/abs/2405.17992v2","updated":"2024-11-04T14:01:50Z","published":"2024-05-28T09:24:52Z","title":"fMRI predictors based on language models of increasing complexity\n  recover brain left lateralization","summary":"  Over the past decade, studies of naturalistic language processing where\nparticipants are scanned while listening to continuous text have flourished.\nUsing word embeddings at first, then large language models, researchers have\ncreated encoding models to analyze the brain signals. Presenting these models\nwith the same text as the participants allows to identify brain areas where\nthere is a significant correlation between the functional magnetic resonance\nimaging (fMRI) time series and the ones predicted by the models' artificial\nneurons. One intriguing finding from these studies is that they have revealed\nhighly symmetric bilateral activation patterns, somewhat at odds with the\nwell-known left lateralization of language processing. Here, we report analyses\nof an fMRI dataset where we manipulate the complexity of large language models,\ntesting 28 pretrained models from 8 different families, ranging from 124M to\n14.2B parameters. First, we observe that the performance of models in\npredicting brain responses follows a scaling law, where the fit with brain\nactivity increases linearly with the logarithm of the number of parameters of\nthe model (and its performance on natural language processing tasks). Second,\nalthough this effect is present in both hemispheres, it is stronger in the left\nthan in the right hemisphere. Specifically, the left-right difference in brain\ncorrelation follows a scaling law with the number of parameters. This finding\nreconciles computational analyses of brain activity using large language models\nwith the classic observation from aphasic patients showing left hemisphere\ndominance for language.\n","authors":["Laurent Bonnasse-Gahot","Christophe Pallier"],"pdf_url":"https://arxiv.org/pdf/2405.17992v2.pdf","comment":"38th Conference on Neural Information Processing Systems (NeurIPS\n  2024)"},{"id":"http://arxiv.org/abs/2410.06511v2","updated":"2024-11-04T13:52:23Z","published":"2024-10-09T03:26:11Z","title":"TorchTitan: One-stop PyTorch native solution for production ready LLM\n  pre-training","summary":"  The development of large language models (LLMs) has been instrumental in\nadvancing state-of-the-art natural language processing applications. Training\nLLMs with billions of parameters and trillions of tokens require sophisticated\ndistributed systems that enable composing and comparing several\nstate-of-the-art techniques in order to efficiently scale across thousands of\naccelerators. However, existing solutions are complex, scattered across\nmultiple libraries/repositories, lack interoperability, and are cumbersome to\nmaintain. Thus, curating and empirically comparing training recipes require\nnon-trivial engineering effort.\n  This paper introduces TorchTitan, an open-source, PyTorch-native distributed\ntraining system that unifies state-of-the-art techniques, streamlining\nintegration and reducing overhead. TorchTitan enables 3D parallelism in a\nmodular manner with elastic scaling, providing comprehensive logging,\ncheckpointing, and debugging tools for production-ready training. It also\nincorporates hardware-software co-designed solutions, leveraging features like\nFloat8 training and SymmetricMemory. As a flexible test bed, TorchTitan\nfacilitates custom recipe curation and comparison, allowing us to develop\noptimized training recipes for Llama 3.1 and provide guidance on selecting\ntechniques for maximum efficiency based on our experiences.\n  We thoroughly assess TorchTitan on the Llama 3.1 family of LLMs, spanning 8\nbillion to 405 billion parameters, and showcase its exceptional performance,\nmodular composability, and elastic scalability. By stacking training\noptimizations, we demonstrate accelerations of 65.08% with 1D parallelism at\nthe 128-GPU scale (Llama 3.1 8B), an additional 12.59% with 2D parallelism at\nthe 256-GPU scale (Llama 3.1 70B), and an additional 30% with 3D parallelism at\nthe 512-GPU scale (Llama 3.1 405B) on NVIDIA H100 GPUs over optimized\nbaselines.\n","authors":["Wanchao Liang","Tianyu Liu","Less Wright","Will Constable","Andrew Gu","Chien-Chin Huang","Iris Zhang","Wei Feng","Howard Huang","Junjie Wang","Sanket Purandare","Gokul Nadathur","Stratos Idreos"],"pdf_url":"https://arxiv.org/pdf/2410.06511v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02083v1","updated":"2024-11-04T13:43:24Z","published":"2024-11-04T13:43:24Z","title":"Regress, Don't Guess -- A Regression-like Loss on Number Tokens for\n  Language Models","summary":"  While language models have exceptional capabilities at text generation, they\nlack a natural inductive bias for emitting numbers and thus struggle in tasks\ninvolving reasoning over quantities, especially arithmetics. This has\nparticular relevance in scientific datasets where combinations of text and\nnumerical data are abundant. One fundamental limitation is the nature of the CE\nloss, which assumes a nominal (categorical) scale and thus cannot convey\nproximity between generated number tokens. As a remedy, we here present two\nversions of a number token loss. The first is based on an $L_p$ loss between\nthe ground truth token value and the weighted sum of the predicted class\nprobabilities. The second loss minimizes the Wasserstein-1 distance between the\ndistribution of the predicted output probabilities and the ground truth\ndistribution. These regression-like losses can easily be added to any language\nmodel and extend the CE objective during training. We compare the proposed\nschemes on a mathematics dataset against existing tokenization, encoding, and\ndecoding schemes for improving number representation in language models. Our\nresults reveal a significant improvement in numerical accuracy when equipping a\nstandard T5 model with the proposed loss schemes.\n","authors":["Jonas Zausinger","Lars Pennig","Kacper Chlodny","Vincent Limbach","Anna Ketteler","Thorben Prein","Vishwa Mohan Singh","Michael Morris Danziger","Jannis Born"],"pdf_url":"https://arxiv.org/pdf/2411.02083v1.pdf","comment":"5-page version for NeurIPS 2024 (MathAI workshop)"},{"id":"http://arxiv.org/abs/2408.01963v4","updated":"2024-11-04T13:32:40Z","published":"2024-08-04T08:43:09Z","title":"A Novel Metric for Measuring the Robustness of Large Language Models in\n  Non-adversarial Scenarios","summary":"  We evaluate the robustness of several large language models on multiple\ndatasets. Robustness here refers to the relative insensitivity of the model's\nanswers to meaning-preserving variants of their input. Benchmark datasets are\nconstructed by introducing naturally-occurring, non-malicious perturbations, or\nby generating semantically equivalent paraphrases of input questions or\nstatements. We further propose a novel metric for assessing a model robustness,\nand demonstrate its benefits in the non-adversarial scenario by empirical\nevaluation of several models on the created datasets.\n","authors":["Samuel Ackerman","Ella Rabinovich","Eitan Farchi","Ateret Anaby-Tavor"],"pdf_url":"https://arxiv.org/pdf/2408.01963v4.pdf","comment":"Published in the 2024 Conference on Empirical Methods in Natural\n  Language Processing (EMNLP) findings"},{"id":"http://arxiv.org/abs/2401.11944v4","updated":"2024-11-04T13:28:48Z","published":"2024-01-22T13:34:34Z","title":"CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding\n  Benchmark","summary":"  As the capabilities of large multimodal models (LMMs) continue to advance,\nevaluating the performance of LMMs emerges as an increasing need. Additionally,\nthere is an even larger gap in evaluating the advanced knowledge and reasoning\nabilities of LMMs in non-English contexts such as Chinese. We introduce CMMMU,\na new Chinese Massive Multi-discipline Multimodal Understanding benchmark\ndesigned to evaluate LMMs on tasks demanding college-level subject knowledge\nand deliberate reasoning in a Chinese context. CMMMU is inspired by and\nstrictly follows the annotation and analysis pattern of MMMU. CMMMU includes\n12k manually collected multimodal questions from college exams, quizzes, and\ntextbooks, covering six core disciplines: Art & Design, Business, Science,\nHealth & Medicine, Humanities & Social Science, and Tech & Engineering, like\nits companion, MMMU. These questions span 30 subjects and comprise 39 highly\nheterogeneous image types, such as charts, diagrams, maps, tables, music\nsheets, and chemical structures. CMMMU focuses on complex perception and\nreasoning with domain-specific knowledge in the Chinese context. We evaluate 11\nopen-source LLMs and one proprietary GPT-4V(ision). Even GPT-4V only achieves\naccuracies of 42%, indicating a large space for improvement. CMMMU will boost\nthe community to build the next-generation LMMs towards expert artificial\nintelligence and promote the democratization of LMMs by providing diverse\nlanguage contexts.\n","authors":["Ge Zhang","Xinrun Du","Bei Chen","Yiming Liang","Tongxu Luo","Tianyu Zheng","Kang Zhu","Yuyang Cheng","Chunpu Xu","Shuyue Guo","Haoran Zhang","Xingwei Qu","Junjie Wang","Ruibin Yuan","Yizhi Li","Zekun Wang","Yudong Liu","Yu-Hsuan Tsai","Fengji Zhang","Chenghua Lin","Wenhao Huang","Jie Fu"],"pdf_url":"https://arxiv.org/pdf/2401.11944v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09952v2","updated":"2024-11-04T13:26:07Z","published":"2024-06-14T11:58:49Z","title":"BiVLC: Extending Vision-Language Compositionality Evaluation with\n  Text-to-Image Retrieval","summary":"  Existing Vision-Language Compositionality (VLC) benchmarks like SugarCrepe\nare formulated as image-to-text retrieval problems, where, given an image, the\nmodels need to select between the correct textual description and a synthetic\nhard negative text. In this work, we present the Bidirectional Vision-Language\nCompositionality (BiVLC) dataset. The novelty of BiVLC is to add a synthetic\nhard negative image generated from the synthetic text, resulting in two\nimage-to-text retrieval examples (one for each image) and, more importantly,\ntwo text-to-image retrieval examples (one for each text). Human annotators\nfilter out ill-formed examples ensuring the validity of the benchmark. The\nexperiments on BiVLC uncover a weakness of current multimodal models, as they\nperform poorly in the text-to-image direction. In fact, when considering both\nretrieval directions, the conclusions obtained in previous works change\nsignificantly. In addition to the benchmark, we show that a contrastive model\ntrained using synthetic images and texts significantly improves over the base\nmodel in SugarCrepe and in BiVLC for both retrieval directions. The gap to\nhuman performance in BiVLC confirms that Vision-Language Compositionality is\nstill a challenging problem. BiVLC and code are available at\nhttps://imirandam.github.io/BiVLC_project_page.\n","authors":["Imanol Miranda","Ander Salaberria","Eneko Agirre","Gorka Azkune"],"pdf_url":"https://arxiv.org/pdf/2406.09952v2.pdf","comment":"Accepted to NeurIPS 24 Datasets and Benchmarks Track; Project page\n  at: https://imirandam.github.io/BiVLC_project_page/"},{"id":"http://arxiv.org/abs/2407.15612v3","updated":"2024-11-04T13:25:31Z","published":"2024-07-22T13:14:27Z","title":"Can GPT-4 learn to analyse moves in research article abstracts?","summary":"  One of the most powerful and enduring ideas in written discourse analysis is\nthat genres can be described in terms of the moves which structure a writer's\npurpose. Considerable research has sought to identify these distinct\ncommunicative acts, but analyses have been beset by problems of subjectivity,\nreliability and the time-consuming need for multiple coders to confirm\nanalyses. In this paper we employ the affordances of GPT-4 to automate the\nannotation process by using natural language prompts. Focusing on abstracts\nfrom articles in four applied linguistics journals, we devise prompts which\nenable the model to identify moves effectively. The annotated outputs of these\nprompts were evaluated by two assessors with a third addressing disagreements.\nThe results show that an 8-shot prompt was more effective than one using two,\nconfirming that the inclusion of examples illustrating areas of variability can\nenhance GPT-4's ability to recognize multiple moves in a single sentence and\nreduce bias related to textual position. We suggest that GPT-4 offers\nconsiderable potential in automating this annotation process, when human actors\nwith domain specific linguistic expertise inform the prompting process.\n","authors":["Danni Yu","Marina Bondi","Ken Hyland"],"pdf_url":"https://arxiv.org/pdf/2407.15612v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.00533v2","updated":"2024-11-04T13:15:56Z","published":"2024-11-01T12:08:08Z","title":"ReverseNER: A Self-Generated Example-Driven Framework for Zero-Shot\n  Named Entity Recognition with Large Language Models","summary":"  This paper presents ReverseNER, a framework aimed at overcoming the\nlimitations of large language models (LLMs) in zero-shot Named Entity\nRecognition (NER) tasks, particularly in cases where certain entity types have\nambiguous boundaries. ReverseNER tackles this challenge by constructing a\nreliable example library with the reversed process of NER. Rather than\nbeginning with sentences, this method uses an LLM to generate entities based on\ntheir definitions and then expands them into full sentences. During sentence\ngeneration, the LLM is guided to replicate the structure of a specific 'feature\nsentence', extracted from the task sentences by clustering. This results in\nwell-annotated sentences with clearly labeled entities, while preserving\nsemantic and structural similarity to the task sentences. Once the example\nlibrary is constructed, the method selects the most semantically similar\nexample labels for each task sentence to support the LLM's inference. We also\npropose an entity-level self-consistency scoring mechanism to improve NER\nperformance with LLMs. Experiments show that ReverseNER significantly\noutperforms traditional zero-shot NER with LLMs and surpasses several few-shot\nmethods, marking a notable improvement in NER for domains with limited labeled\ndata.\n","authors":["Anbang Wang"],"pdf_url":"https://arxiv.org/pdf/2411.00533v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02063v1","updated":"2024-11-04T13:06:17Z","published":"2024-11-04T13:06:17Z","title":"Scalable Efficient Training of Large Language Models with\n  Low-dimensional Projected Attention","summary":"  Improving the effectiveness and efficiency of large language models (LLMs)\nsimultaneously is a critical yet challenging research goal. In this paper, we\nfind that low-rank pre-training, normally considered as efficient methods that\nwill compromise performance, can be scalably effective when reduced parameters\nare precisely targeted. Specifically, applying the low-dimensional module only\nto the attention layer -- resolves this issue and enhances both effectiveness\nand efficiency. We refer to this structure as Low-dimensional Projected\nAttention (LPA) and provide an explanatory analysis. Through extensive\nexperimentation at parameter scales of 130M, 370M, and scaling up to 3B, we\nhave validated the effectiveness and scalability of LPA. Our results show that\nLPA model can save up to 12.4% in time while achieving an approximate 5%\nimprovement in test perplexity (ppl) and on downstream tasks compared with the\nvanilla Transformer.\n","authors":["Xingtai Lv","Ning Ding","Kaiyan Zhang","Ermo Hua","Ganqu Cui","Bowen Zhou"],"pdf_url":"https://arxiv.org/pdf/2411.02063v1.pdf","comment":"Accepted to EMNLP 2024 (Main Conference)"},{"id":"http://arxiv.org/abs/2403.04317v2","updated":"2024-11-04T13:02:33Z","published":"2024-03-07T08:34:57Z","title":"Online Adaptation of Language Models with a Memory of Amortized Contexts","summary":"  Due to the rapid generation and dissemination of information, large language\nmodels (LLMs) quickly run out of date despite enormous development costs. To\naddress the crucial need to keep models updated, online learning has emerged as\na critical tool when utilizing LLMs for real-world applications. However, given\nthe ever-expanding corpus of unseen documents and the large parameter space of\nmodern LLMs, efficient adaptation is essential. To address these challenges, we\npropose Memory of Amortized Contexts (MAC), an efficient and effective online\nadaptation framework for LLMs with strong knowledge retention. We propose a\nfeature extraction and memory-augmentation approach to compress and extract\ninformation from new documents into compact modulations stored in a memory\nbank. When answering questions, our model attends to and extracts relevant\nknowledge from this memory bank. To learn informative modulations in an\nefficient manner, we utilize amortization-based meta-learning, which\nsubstitutes an otherwise required optimization process with a single forward\npass of the encoder. Subsequently, we learn to choose from and aggregate\nselected documents into a single modulation by conditioning on the question,\nallowing us to adapt a frozen language model during test time without requiring\nfurther gradient updates. Our experiment demonstrates the superiority of MAC in\nmultiple aspects, including online adaptation performance, time, and memory\nefficiency. In addition, we show how MAC can be combined with and improve the\nperformance of popular alternatives such as retrieval augmented generations\n(RAGs). Code is available at: https://github.com/jihoontack/MAC.\n","authors":["Jihoon Tack","Jaehyung Kim","Eric Mitchell","Jinwoo Shin","Yee Whye Teh","Jonathan Richard Schwarz"],"pdf_url":"https://arxiv.org/pdf/2403.04317v2.pdf","comment":"Published as a conference proceeding for NeurIPS 2024"},{"id":"http://arxiv.org/abs/2402.11295v5","updated":"2024-11-04T12:55:44Z","published":"2024-02-17T14:26:57Z","title":"OneBit: Towards Extremely Low-bit Large Language Models","summary":"  Model quantification uses low bit-width values to represent the weight\nmatrices of existing models to be quantized, which is a promising approach to\nreduce both storage and computational overheads of deploying highly anticipated\nLLMs. However, current quantization methods suffer severe performance\ndegradation when the bit-width is extremely reduced, and thus focus on\nutilizing 4-bit or 8-bit values to quantize models. This paper boldly quantizes\nthe weight matrices of LLMs to 1-bit, paving the way for the extremely low\nbit-width deployment of LLMs. For this target, we introduce a 1-bit model\ncompressing framework named OneBit, including a novel 1-bit parameter\nrepresentation method to better quantize LLMs as well as an effective parameter\ninitialization method based on matrix decomposition to improve the convergence\nspeed of the quantization framework. Sufficient experimental results indicate\nthat OneBit achieves good performance (at least 81% of the non-quantized\nperformance on LLaMA models) with robust training processes when only using\n1-bit weight matrices.\n","authors":["Yuzhuang Xu","Xu Han","Zonghan Yang","Shuo Wang","Qingfu Zhu","Zhiyuan Liu","Weidong Liu","Wanxiang Che"],"pdf_url":"https://arxiv.org/pdf/2402.11295v5.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2408.10724v3","updated":"2024-11-04T12:42:18Z","published":"2024-08-20T10:45:36Z","title":"Crafting Tomorrow's Headlines: Neural News Generation and Detection in\n  English, Turkish, Hungarian, and Persian","summary":"  In the era dominated by information overload and its facilitation with Large\nLanguage Models (LLMs), the prevalence of misinformation poses a significant\nthreat to public discourse and societal well-being. A critical concern at\npresent involves the identification of machine-generated news. In this work, we\ntake a significant step by introducing a benchmark dataset designed for neural\nnews detection in four languages: English, Turkish, Hungarian, and Persian. The\ndataset incorporates outputs from multiple multilingual generators (in both,\nzero-shot and fine-tuned setups) such as BloomZ, LLaMa-2, Mistral, Mixtral, and\nGPT-4. Next, we experiment with a variety of classifiers, ranging from those\nbased on linguistic features to advanced Transformer-based models and LLMs\nprompting. We present the detection results aiming to delve into the\ninterpretablity and robustness of machine-generated texts detectors across all\ntarget languages.\n","authors":["Cem Üyük","Danica Rovó","Shaghayegh Kolli","Rabia Varol","Georg Groh","Daryna Dementieva"],"pdf_url":"https://arxiv.org/pdf/2408.10724v3.pdf","comment":"EMNLP 2024 NLP4PI Workshop"},{"id":"http://arxiv.org/abs/2410.13409v2","updated":"2024-11-04T12:40:19Z","published":"2024-10-17T10:16:56Z","title":"Attr-Int: A Simple and Effective Entity Alignment Framework for\n  Heterogeneous Knowledge Graphs","summary":"  Entity alignment (EA) refers to the task of linking entities in different\nknowledge graphs (KGs). Existing EA methods rely heavily on structural\nisomorphism. However, in real-world KGs, aligned entities usually have\nnon-isomorphic neighborhood structures, which paralyses the application of\nthese structure-dependent methods. In this paper, we investigate and tackle the\nproblem of entity alignment between heterogeneous KGs. First, we propose two\nnew benchmarks to closely simulate real-world EA scenarios of heterogeneity.\nThen we conduct extensive experiments to evaluate the performance of\nrepresentative EA methods on the new benchmarks. Finally, we propose a simple\nand effective entity alignment framework called Attr-Int, in which innovative\nattribute information interaction methods can be seamlessly integrated with any\nembedding encoder for entity alignment, improving the performance of existing\nentity alignment techniques. Experiments demonstrate that our framework\noutperforms the state-of-the-art approaches on two new benchmarks.\n","authors":["Linyan Yang","Jingwei Cheng","Chuanhao Xu","Xihao Wang","Jiayi Li","Fu Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.13409v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02036v1","updated":"2024-11-04T12:38:08Z","published":"2024-11-04T12:38:08Z","title":"Explainable cognitive decline detection in free dialogues with a Machine\n  Learning approach based on pre-trained Large Language Models","summary":"  Cognitive and neurological impairments are very common, but only a small\nproportion of affected individuals are diagnosed and treated, partly because of\nthe high costs associated with frequent screening. Detecting pre-illness stages\nand analyzing the progression of neurological disorders through effective and\nefficient intelligent systems can be beneficial for timely diagnosis and early\nintervention. We propose using Large Language Models to extract features from\nfree dialogues to detect cognitive decline. These features comprise high-level\nreasoning content-independent features (such as comprehension, decreased\nawareness, increased distraction, and memory problems). Our solution comprises\n(i) preprocessing, (ii) feature engineering via Natural Language Processing\ntechniques and prompt engineering, (iii) feature analysis and selection to\noptimize performance, and (iv) classification, supported by automatic\nexplainability. We also explore how to improve Chatgpt's direct cognitive\nimpairment prediction capabilities using the best features in our models.\nEvaluation metrics obtained endorse the effectiveness of a mixed approach\ncombining feature extraction with Chatgpt and a specialized Machine Learning\nmodel to detect cognitive decline within free-form conversational dialogues\nwith older adults. Ultimately, our work may facilitate the development of an\ninexpensive, non-invasive, and rapid means of detecting and explaining\ncognitive decline.\n","authors":["Francisco de Arriba-Pérez","Silvia García-Méndez","Javier Otero-Mosquera","Francisco J. González-Castaño"],"pdf_url":"https://arxiv.org/pdf/2411.02036v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.11430v2","updated":"2024-11-04T12:21:52Z","published":"2024-05-19T03:08:02Z","title":"MHPP: Exploring the Capabilities and Limitations of Language Models\n  Beyond Basic Code Generation","summary":"  Recent advancements in large language models (LLMs) have greatly improved\ncode generation, specifically at the function level. For instance, GPT-4o has\nachieved a 91.0\\% pass rate on HumanEval. However, this draws into question the\nadequacy of existing benchmarks in thoroughly assessing function-level code\ngeneration capabilities. Our study analyzed two common benchmarks, HumanEval\nand MBPP, and found that these might not thoroughly evaluate LLMs' code\ngeneration capacities due to limitations in quality, difficulty, and\ngranularity. To resolve this, we introduce the Mostly Hard Python Problems\n(MHPP) dataset, consisting of 210 unique human-curated problems. By focusing on\nthe combination of natural language and code reasoning, MHPP gauges LLMs'\nabilities to comprehend specifications and restrictions, engage in multi-step\nreasoning, and apply coding knowledge effectively. Initial evaluations of 26\nLLMs using MHPP showed many high-performing models on HumanEval failed to\nachieve similar success on MHPP. Moreover, MHPP highlighted various previously\nundiscovered limitations within various LLMs, leading us to believe that it\ncould pave the way for a better understanding of LLMs' capabilities and\nlimitations. MHPP, evaluation pipeline, and leaderboard can be found in\nhttps://github.com/SparksofAGI/MHPP.\n","authors":["Jianbo Dai","Jianqiao Lu","Yunlong Feng","Dong Huang","Guangtao Zeng","Rongju Ruan","Ming Cheng","Haochen Tan","Zhijiang Guo"],"pdf_url":"https://arxiv.org/pdf/2405.11430v2.pdf","comment":"43 pages, dataset and code are available at\n  https://github.com/SparksofAGI/MHPP, leaderboard can be found at\n  https://sparksofagi.github.io/MHPP/"},{"id":"http://arxiv.org/abs/2408.11327v2","updated":"2024-11-04T12:17:42Z","published":"2024-08-21T04:20:55Z","title":"Plug, Play, and Fuse: Zero-Shot Joint Decoding via Word-Level Re-ranking\n  Across Diverse Vocabularies","summary":"  Recent advancements in NLP have resulted in models with specialized\nstrengths, such as processing multimodal inputs or excelling in specific\ndomains. However, real-world tasks, like multimodal translation, often require\na combination of these strengths, such as handling both translation and image\nprocessing. While individual translation and vision models are powerful, they\ntypically lack the ability to perform both tasks in a single system. Combining\nthese models poses challenges, particularly due to differences in their\nvocabularies, which limit the effectiveness of traditional ensemble methods to\npost-generation techniques like N-best list re-ranking. In this work, we\npropose a novel zero-shot ensembling strategy that allows for the integration\nof different models during the decoding phase without the need for additional\ntraining. Our approach re-ranks beams during decoding by combining scores at\nthe word level, using heuristics to predict when a word is completed. We\ndemonstrate the effectiveness of this method in machine translation scenarios,\nshowing that it enables the generation of translations that are both speech-\nand image-aware while also improving overall translation quality (We will\nrelease the code upon paper acceptance.).\n","authors":["Sai Koneru","Matthias Huck","Miriam Exel","Jan Niehues"],"pdf_url":"https://arxiv.org/pdf/2408.11327v2.pdf","comment":"WMT 2024"},{"id":"http://arxiv.org/abs/2407.03227v2","updated":"2024-11-04T12:14:13Z","published":"2024-07-03T15:55:14Z","title":"Improving Retrieval-augmented Text-to-SQL with AST-based Ranking and\n  Schema Pruning","summary":"  We focus on Text-to-SQL semantic parsing from the perspective of\nretrieval-augmented generation. Motivated by challenges related to the size of\ncommercial database schemata and the deployability of business intelligence\nsolutions, we propose $\\text{ASTReS}$ that dynamically retrieves input database\ninformation and uses abstract syntax trees to select few-shot examples for\nin-context learning.\n  Furthermore, we investigate the extent to which an in-parallel semantic\nparser can be leveraged for generating approximated versions of the expected\nSQL queries, to support our retrieval. We take this approach to the extreme--we\nadapt a model consisting of less than $500$M parameters, to act as an extremely\nefficient approximator, enhancing it with the ability to process schemata in a\nparallelised manner. We apply $\\text{ASTReS}$ to monolingual and cross-lingual\nbenchmarks for semantic parsing, showing improvements over state-of-the-art\nbaselines. Comprehensive experiments highlight the contribution of modules\ninvolved in this retrieval-augmented generation setting, revealing interesting\ndirections for future work.\n","authors":["Zhili Shen","Pavlos Vougiouklis","Chenxin Diao","Kaustubh Vyas","Yuanyi Ji","Jeff Z. Pan"],"pdf_url":"https://arxiv.org/pdf/2407.03227v2.pdf","comment":"EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2411.02018v1","updated":"2024-11-04T12:13:04Z","published":"2024-11-04T12:13:04Z","title":"Shortcut Learning in In-Context Learning: A Survey","summary":"  Shortcut learning refers to the phenomenon where models employ simple,\nnon-robust decision rules in practical tasks, which hinders their\ngeneralization and robustness. With the rapid development of large language\nmodels (LLMs) in recent years, an increasing number of studies have shown the\nimpact of shortcut learning on LLMs. This paper provides a novel perspective to\nreview relevant research on shortcut learning in In-Context Learning (ICL). It\nconducts a detailed exploration of the types of shortcuts in ICL tasks, their\ncauses, available benchmarks, and strategies for mitigating shortcuts. Based on\ncorresponding observations, it summarizes the unresolved issues in existing\nresearch and attempts to outline the future research landscape of shortcut\nlearning.\n","authors":["Rui Song","Yingji Li","Fausto Giunchiglia","Hao Xu"],"pdf_url":"https://arxiv.org/pdf/2411.02018v1.pdf","comment":"15 pages, 2 figures"},{"id":"http://arxiv.org/abs/2411.01996v1","updated":"2024-11-04T11:31:18Z","published":"2024-11-04T11:31:18Z","title":"Culinary Class Wars: Evaluating LLMs using ASH in Cuisine Transfer Task","summary":"  The advent of Large Language Models (LLMs) have shown promise in various\ncreative domains, including culinary arts. However, many LLMs still struggle to\ndeliver the desired level of culinary creativity, especially when tasked with\nadapting recipes to meet specific cultural requirements. This study focuses on\ncuisine transfer-applying elements of one cuisine to another-to assess LLMs'\nculinary creativity. We employ a diverse set of LLMs to generate and evaluate\nculturally adapted recipes, comparing their evaluations against LLM and human\njudgments. We introduce the ASH (authenticity, sensitivity, harmony) benchmark\nto evaluate LLMs' recipe generation abilities in the cuisine transfer task,\nassessing their cultural accuracy and creativity in the culinary domain. Our\nfindings reveal crucial insights into both generative and evaluative\ncapabilities of LLMs in the culinary domain, highlighting strengths and\nlimitations in understanding and applying cultural nuances in recipe creation.\nThe code and dataset used in this project will be openly available in\n\\url{http://github.com/dmis-lab/CulinaryASH}.\n","authors":["Hoonick Lee","Mogan Gim","Donghyeon Park","Donghee Choi","Jaewoo Kang"],"pdf_url":"https://arxiv.org/pdf/2411.01996v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19803v2","updated":"2024-11-04T11:28:18Z","published":"2024-06-28T10:24:31Z","title":"Scalable and Domain-General Abstractive Proposition Segmentation","summary":"  Segmenting text into fine-grained units of meaning is important to a wide\nrange of NLP applications. The default approach of segmenting text into\nsentences is often insufficient, especially since sentences are usually complex\nenough to include multiple units of meaning that merit separate treatment in\nthe downstream task. We focus on the task of abstractive proposition\nsegmentation (APS): transforming text into simple, self-contained, well-formed\nsentences. Several recent works have demonstrated the utility of proposition\nsegmentation with few-shot prompted LLMs for downstream tasks such as\nretrieval-augmented grounding and fact verification. However, this approach\ndoes not scale to large amounts of text and may not always extract all the\nfacts from the input text. In this paper, we first introduce evaluation metrics\nfor the task to measure several dimensions of quality. We then propose a\nscalable, yet accurate, proposition segmentation model. We model proposition\nsegmentation as a supervised task by training LLMs on existing annotated\ndatasets and show that training yields significantly improved results. We\nfurther show that by using the fine-tuned LLMs (Gemini Pro and Gemini Ultra) as\nteachers for annotating large amounts of multi-domain synthetic distillation\ndata, we can train smaller student models (Gemma 1 2B and 7B) with results\nsimilar to the teacher LLMs. We then demonstrate that our technique leads to\neffective domain generalization, by annotating data in two domains outside the\noriginal training data and evaluating on them. Finally, as a key contribution\nof the paper, we share an easy-to-use API for NLP practitioners to use.\n","authors":["Mohammad Javad Hosseini","Yang Gao","Tim Baumgärtner","Alex Fabrikant","Reinald Kim Amplayo"],"pdf_url":"https://arxiv.org/pdf/2406.19803v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07099v2","updated":"2024-11-04T11:15:28Z","published":"2024-06-18T07:46:13Z","title":"Nash CoT: Multi-Path Inference with Preference Equilibrium","summary":"  Chain of thought (CoT) is a reasoning framework that can enhance the\nperformance of Large Language Models (LLMs) on complex inference tasks. In\nparticular, among various studies related to CoT, multi-path inference stands\nout as a simple yet effective improvement. However, there is no optimal setting\nfor the number of inference paths. Therefore, we have to increase the number of\ninference paths to obtain better results, which in turn increases the inference\ncost. To address this limitation, we can utilize question-related role\ntemplates to guide LLMs into relevant roles, thereby increasing the possibility\nof correct inferences for each path and further reducing dependence on the\nnumber of inference paths while improving reasoning accuracy. However, placing\nLLMs into specific roles may reduce their reasoning diversity and performance\non a few tasks where role dependence is low. To alleviate the excessive\nimmersion of the LLM into a specific role, we propose Nash CoT by constructing\na competitive system on each path that balances the generation from\nrole-specific LLMs' and the general LLMs' generation, thereby ensuring both\neffective role adoption and diversity in LLM generation further maintaining the\nperformance of multi-path inference while reducing the requirement of the\nnumber of inference paths. We evaluate Nash CoT across various inference tasks,\nincluding Arabic Reasoning, Commonsense Question Answering, and Symbolic\nInference, achieving results that are comparable to or better than those of\nmulti-path CoT with the equal number of inference paths.\n","authors":["Ziqi Zhang","Cunxiang Wang","Xiong Xiao","Yue Zhang","Donglin Wang"],"pdf_url":"https://arxiv.org/pdf/2407.07099v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07217v2","updated":"2024-11-04T11:06:40Z","published":"2024-06-11T12:50:53Z","title":"A Synthetic Dataset for Personal Attribute Inference","summary":"  Recently, powerful Large Language Models (LLMs) have become easily accessible\nto hundreds of millions of users world-wide. However, their strong capabilities\nand vast world knowledge do not come without associated privacy risks. In this\nwork, we focus on the emerging privacy threat LLMs pose -- the ability to\naccurately infer personal information from online texts. Despite the growing\nimportance of LLM-based author profiling, research in this area has been\nhampered by a lack of suitable public datasets, largely due to ethical and\nprivacy concerns associated with real personal data. We take two steps to\naddress this problem: (i) we construct a simulation framework for the popular\nsocial media platform Reddit using LLM agents seeded with synthetic personal\nprofiles; (ii) using this framework, we generate SynthPAI, a diverse synthetic\ndataset of over 7800 comments manually labeled for personal attributes. We\nvalidate our dataset with a human study showing that humans barely outperform\nrandom guessing on the task of distinguishing our synthetic comments from real\nones. Further, we verify that our dataset enables meaningful personal attribute\ninference research by showing across 18 state-of-the-art LLMs that our\nsynthetic comments allow us to draw the same conclusions as real-world data.\nCombined, our experimental results, dataset and pipeline form a strong basis\nfor future privacy-preserving research geared towards understanding and\nmitigating inference-based privacy threats that LLMs pose.\n","authors":["Hanna Yukhymenko","Robin Staab","Mark Vero","Martin Vechev"],"pdf_url":"https://arxiv.org/pdf/2406.07217v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.05188v2","updated":"2024-11-04T10:42:01Z","published":"2024-04-08T04:30:33Z","title":"Have You Merged My Model? On The Robustness of Large Language Model IP\n  Protection Methods Against Model Merging","summary":"  Model merging is a promising lightweight model empowerment technique that\ndoes not rely on expensive computing devices (e.g., GPUs) or require the\ncollection of specific training data. Instead, it involves editing different\nupstream model parameters to absorb their downstream task capabilities.\nHowever, uncertified model merging can infringe upon the Intellectual Property\n(IP) rights of the original upstream models. In this paper, we conduct the\nfirst study on the robustness of IP protection methods under model merging\nscenarios. Specifically, we investigate two state-of-the-art IP protection\ntechniques: Quantization Watermarking and Instructional Fingerprint, along with\nvarious advanced model merging technologies, such as Task Arithmetic,\nTIES-MERGING, and so on. Experimental results indicate that current Large\nLanguage Model (LLM) watermarking techniques cannot survive in the merged\nmodels, whereas model fingerprinting techniques can. Our research aims to\nhighlight that model merging should be an indispensable consideration in the\nrobustness assessment of model IP protection techniques, thereby promoting the\nhealthy development of the open-source LLM community. Our code is available at\nhttps://github.com/ThuCCSLab/MergeGuard.\n","authors":["Tianshuo Cong","Delong Ran","Zesen Liu","Xinlei He","Jinyuan Liu","Yichen Gong","Qi Li","Anyu Wang","Xiaoyun Wang"],"pdf_url":"https://arxiv.org/pdf/2404.05188v2.pdf","comment":"Accepted by ACM CCS-LAMPS 2024 (Best Paper Award)"},{"id":"http://arxiv.org/abs/2407.11930v3","updated":"2024-11-04T10:30:16Z","published":"2024-07-16T17:23:16Z","title":"Localizing and Mitigating Errors in Long-form Question Answering","summary":"  Long-form question answering (LFQA) aims to provide thorough and in-depth\nanswers to complex questions, enhancing comprehension. However, such detailed\nresponses are prone to hallucinations and factual inconsistencies, challenging\ntheir faithful evaluation. This work introduces HaluQuestQA, the first\nhallucination dataset with localized error annotations for human-written and\nmodel-generated LFQA answers. HaluQuestQA comprises 698 QA pairs with 1.8k\nspan-level error annotations for five different error types by expert\nannotators, along with preference judgments. Using our collected data, we\nthoroughly analyze the shortcomings of long-form answers and find that they\nlack comprehensiveness and provide unhelpful references. We train an automatic\nfeedback model on this dataset that predicts error spans with incomplete\ninformation and provides associated explanations. Finally, we propose a\nprompt-based approach, Error-informed refinement, that uses signals from the\nlearned feedback model to refine generated answers, which we show reduces\nerrors and improves answer quality across multiple models. Furthermore, humans\nfind answers generated by our approach comprehensive and highly prefer them\n(84%) over the baseline answers.\n","authors":["Rachneet Sachdeva","Yixiao Song","Mohit Iyyer","Iryna Gurevych"],"pdf_url":"https://arxiv.org/pdf/2407.11930v3.pdf","comment":"Code and data are available:\n  https://github.com/UKPLab/arxiv2024-lfqa-hallucination"},{"id":"http://arxiv.org/abs/2406.15227v3","updated":"2024-11-04T09:56:55Z","published":"2024-06-21T15:11:33Z","title":"A LLM-Based Ranking Method for the Evaluation of Automatic\n  Counter-Narrative Generation","summary":"  This paper proposes a novel approach to evaluate Counter Narrative (CN)\ngeneration using a Large Language Model (LLM) as an evaluator. We show that\ntraditional automatic metrics correlate poorly with human judgements and fail\nto capture the nuanced relationship between generated CNs and human perception.\nTo alleviate this, we introduce a model ranking pipeline based on pairwise\ncomparisons of generated CNs from different models, organized in a\ntournament-style format. The proposed evaluation method achieves a high\ncorrelation with human preference, with a $\\rho$ score of 0.88. As an\nadditional contribution, we leverage LLMs as zero-shot CN generators and\nprovide a comparative analysis of chat, instruct, and base models, exploring\ntheir respective strengths and limitations. Through meticulous evaluation,\nincluding fine-tuning experiments, we elucidate the differences in performance\nand responsiveness to domain-specific data. We conclude that chat-aligned\nmodels in zero-shot are the best option for carrying out the task, provided\nthey do not refuse to generate an answer due to security concerns.\n","authors":["Irune Zubiaga","Aitor Soroa","Rodrigo Agerri"],"pdf_url":"https://arxiv.org/pdf/2406.15227v3.pdf","comment":"Accepted for Findings of the Association for Computational\n  Linguistics: EMNLP 2024"},{"id":"http://arxiv.org/abs/2407.12342v2","updated":"2024-11-04T09:52:25Z","published":"2024-07-17T06:36:09Z","title":"Word Embedding Dimension Reduction via Weakly-Supervised Feature\n  Selection","summary":"  As a fundamental task in natural language processing, word embedding converts\neach word into a representation in a vector space. A challenge with word\nembedding is that as the vocabulary grows, the vector space's dimension\nincreases, which can lead to a vast model size. Storing and processing word\nvectors are resource-demanding, especially for mobile edge-devices\napplications. This paper explores word embedding dimension reduction. To\nbalance computational costs and performance, we propose an efficient and\neffective weakly-supervised feature selection method named WordFS. It has two\nvariants, each utilizing novel criteria for feature selection. Experiments on\nvarious tasks (e.g., word and sentence similarity and binary and multi-class\nclassification) indicate that the proposed WordFS model outperforms other\ndimension reduction methods at lower computational costs. We have released the\ncode for reproducibility along with the paper.\n","authors":["Jintang Xue","Yun-Cheng Wang","Chengwei Wei","C. -C. Jay Kuo"],"pdf_url":"https://arxiv.org/pdf/2407.12342v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.16528v4","updated":"2024-11-04T09:50:00Z","published":"2024-05-26T11:29:57Z","title":"LoQT: Low-Rank Adapters for Quantized Pretraining","summary":"  Despite advances using low-rank adapters and quantization, pretraining of\nlarge models on consumer hardware has not been possible without model sharding,\noffloading during training, or per-layer gradient updates. To address these\nlimitations, we propose Low-Rank Adapters for Quantized Training (LoQT), a\nmethod for efficiently training quantized models. LoQT uses gradient-based\ntensor factorization to initialize low-rank trainable weight matrices that are\nperiodically merged into quantized full-rank weight matrices. Our approach is\nsuitable for both pretraining and fine-tuning models. We demonstrate this for\nlanguage modeling and downstream task adaptation, finding that LoQT enables\nefficient training of models up to 7B parameters on a 24GB GPU. We also\ndemonstrate the feasibility of training a 13B model using per-layer gradient\nupdates on the same hardware.\n","authors":["Sebastian Loeschcke","Mads Toftrup","Michael J. Kastoryano","Serge Belongie","Vésteinn Snæbjarnarson"],"pdf_url":"https://arxiv.org/pdf/2405.16528v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.02115v2","updated":"2024-11-04T09:32:30Z","published":"2024-04-02T17:18:48Z","title":"GINopic: Topic Modeling with Graph Isomorphism Network","summary":"  Topic modeling is a widely used approach for analyzing and exploring large\ndocument collections. Recent research efforts have incorporated pre-trained\ncontextualized language models, such as BERT embeddings, into topic modeling.\nHowever, they often neglect the intrinsic informational value conveyed by\nmutual dependencies between words. In this study, we introduce GINopic, a topic\nmodeling framework based on graph isomorphism networks to capture the\ncorrelation between words. By conducting intrinsic (quantitative as well as\nqualitative) and extrinsic evaluations on diverse benchmark datasets, we\ndemonstrate the effectiveness of GINopic compared to existing topic models and\nhighlight its potential for advancing topic modeling.\n","authors":["Suman Adhya","Debarshi Kumar Sanyal"],"pdf_url":"https://arxiv.org/pdf/2404.02115v2.pdf","comment":"Accepted as a long paper for NAACL 2024 main conference"},{"id":"http://arxiv.org/abs/2403.08312v3","updated":"2024-11-04T09:17:45Z","published":"2024-03-13T07:44:14Z","title":"StreamingDialogue: Prolonged Dialogue Learning via Long Context\n  Compression with Minimal Losses","summary":"  Standard Large Language Models (LLMs) struggle with handling dialogues with\nlong contexts due to efficiency and consistency issues. According to our\nobservation, dialogue contexts are highly structured, and the special token of\n\\textit{End-of-Utterance} (EoU) in dialogues has the potential to aggregate\ninformation. We refer to the EoU tokens as ``conversational attention sinks''\n(conv-attn sinks). Accordingly, we introduce StreamingDialogue, which\ncompresses long dialogue history into conv-attn sinks with minimal losses, and\nthus reduces computational complexity quadratically with the number of sinks\n(i.e., the number of utterances). Current LLMs already demonstrate the ability\nto handle long context window, e.g., a window size of 200K or more. To this\nend, by compressing utterances into EoUs, our method has the potential to\nhandle more than 200K of utterances, resulting in a prolonged dialogue\nlearning. In order to minimize information losses from reconstruction after\ncompression, we design two learning strategies of short-memory reconstruction\n(SMR) and long-memory reactivation (LMR). Our method outperforms strong\nbaselines in dialogue tasks and achieves a 4 $\\times$ speedup while reducing\nmemory usage by 18 $\\times$ compared to dense attention recomputation.\n","authors":["Jia-Nan Li","Quan Tu","Cunli Mao","Zhengtao Yu","Ji-Rong Wen","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2403.08312v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.00119v2","updated":"2024-11-04T09:07:25Z","published":"2024-08-28T08:45:29Z","title":"3-in-1: 2D Rotary Adaptation for Efficient Finetuning, Efficient\n  Batching and Composability","summary":"  Parameter-efficient finetuning (PEFT) methods effectively adapt large\nlanguage models (LLMs) to diverse downstream tasks, reducing storage and GPU\nmemory demands. Despite these advantages, several applications pose new\nchallenges to PEFT beyond mere parameter efficiency. One notable challenge\ninvolves the efficient deployment of LLMs equipped with multiple task- or\nuser-specific adapters, particularly when different adapters are needed for\ndistinct requests within the same batch. Another challenge is the\ninterpretability of LLMs, which is crucial for understanding how LLMs function.\nPrevious studies introduced various approaches to address different challenges.\nIn this paper, we introduce a novel method, RoAd, which employs a\nstraightforward 2D rotation to adapt LLMs and addresses all the above\nchallenges: (1) RoAd is remarkably parameter-efficient, delivering optimal\nperformance on GLUE, eight commonsense reasoning tasks and four arithmetic\nreasoning tasks with $<0.1\\%$ trainable parameters; (2) RoAd facilitates the\nefficient serving of requests requiring different adapters within a batch, with\nan overhead comparable to element-wise multiplication instead of batch matrix\nmultiplication; (3) RoAd enhances LLM's interpretability through integration\nwithin a framework of distributed interchange intervention, demonstrated via\ncomposition experiments.\n","authors":["Baohao Liao","Christof Monz"],"pdf_url":"https://arxiv.org/pdf/2409.00119v2.pdf","comment":"Accepted to NeurIPS 2024. Code: https://github.com/BaohaoLiao/road"},{"id":"http://arxiv.org/abs/2409.17353v3","updated":"2024-11-04T08:01:22Z","published":"2024-09-25T20:59:12Z","title":"Internalizing ASR with Implicit Chain of Thought for Efficient\n  Speech-to-Speech Conversational LLM","summary":"  Current speech-based LLMs are predominantly trained on extensive ASR and TTS\ndatasets, excelling in tasks related to these domains. However, their ability\nto handle direct speech-to-speech conversations remains notably constrained.\nThese models often rely on an ASR-to-TTS chain-of-thought pipeline, converting\nspeech into text for processing before generating audio responses, which\nintroduces latency and loses audio features. We propose a method that\nimplicitly internalizes ASR chain of thought into a speech LLM, enhancing its\nnative speech understanding capabilities. Our approach reduces latency and\nimproves the model's native understanding of speech, paving the way for more\nefficient and natural real-time audio interactions. We also release a\nlarge-scale synthetic conversational dataset to facilitate further research.\n","authors":["Robin Shing-Hei Yuen","Timothy Tin-Long Tse","Jian Zhu"],"pdf_url":"https://arxiv.org/pdf/2409.17353v3.pdf","comment":"Updated for reviewer comments"},{"id":"http://arxiv.org/abs/2406.05967v2","updated":"2024-11-04T07:55:31Z","published":"2024-06-10T01:59:00Z","title":"CVQA: Culturally-diverse Multilingual Visual Question Answering\n  Benchmark","summary":"  Visual Question Answering (VQA) is an important task in multimodal AI, and it\nis often used to test the ability of vision-language models to understand and\nreason on knowledge present in both visual and textual data. However, most of\nthe current VQA models use datasets that are primarily focused on English and a\nfew major world languages, with images that are typically Western-centric.\nWhile recent efforts have tried to increase the number of languages covered on\nVQA datasets, they still lack diversity in low-resource languages. More\nimportantly, although these datasets often extend their linguistic range via\ntranslation or some other approaches, they usually keep images the same,\nresulting in narrow cultural representation. To address these limitations, we\nconstruct CVQA, a new Culturally-diverse multilingual Visual Question Answering\nbenchmark, designed to cover a rich set of languages and cultures, where we\nengage native speakers and cultural experts in the data collection process. As\na result, CVQA includes culturally-driven images and questions from across 30\ncountries on four continents, covering 31 languages with 13 scripts, providing\na total of 10k questions. We then benchmark several Multimodal Large Language\nModels (MLLMs) on CVQA, and show that the dataset is challenging for the\ncurrent state-of-the-art models. This benchmark can serve as a probing\nevaluation suite for assessing the cultural capability and bias of multimodal\nmodels and hopefully encourage more research efforts toward increasing cultural\nawareness and linguistic diversity in this field.\n","authors":["David Romero","Chenyang Lyu","Haryo Akbarianto Wibowo","Teresa Lynn","Injy Hamed","Aditya Nanda Kishore","Aishik Mandal","Alina Dragonetti","Artem Abzaliev","Atnafu Lambebo Tonja","Bontu Fufa Balcha","Chenxi Whitehouse","Christian Salamea","Dan John Velasco","David Ifeoluwa Adelani","David Le Meur","Emilio Villa-Cueva","Fajri Koto","Fauzan Farooqui","Frederico Belcavello","Ganzorig Batnasan","Gisela Vallejo","Grainne Caulfield","Guido Ivetta","Haiyue Song","Henok Biadglign Ademtew","Hernán Maina","Holy Lovenia","Israel Abebe Azime","Jan Christian Blaise Cruz","Jay Gala","Jiahui Geng","Jesus-German Ortiz-Barajas","Jinheon Baek","Jocelyn Dunstan","Laura Alonso Alemany","Kumaranage Ravindu Yasas Nagasinghe","Luciana Benotti","Luis Fernando D'Haro","Marcelo Viridiano","Marcos Estecha-Garitagoitia","Maria Camila Buitrago Cabrera","Mario Rodríguez-Cantelar","Mélanie Jouitteau","Mihail Mihaylov","Mohamed Fazli Mohamed Imam","Muhammad Farid Adilazuarda","Munkhjargal Gochoo","Munkh-Erdene Otgonbold","Naome Etori","Olivier Niyomugisha","Paula Mónica Silva","Pranjal Chitale","Raj Dabre","Rendi Chevi","Ruochen Zhang","Ryandito Diandaru","Samuel Cahyawijaya","Santiago Góngora","Soyeong Jeong","Sukannya Purkayastha","Tatsuki Kuribayashi","Teresa Clifford","Thanmay Jayakumar","Tiago Timponi Torrent","Toqeer Ehsan","Vladimir Araujo","Yova Kementchedjhieva","Zara Burzo","Zheng Wei Lim","Zheng Xin Yong","Oana Ignat","Joan Nwatu","Rada Mihalcea","Thamar Solorio","Alham Fikri Aji"],"pdf_url":"https://arxiv.org/pdf/2406.05967v2.pdf","comment":"38th Conference on Neural Information Processing Systems (NeurIPS\n  2024) Track on Datasets and Benchmarks"},{"id":"http://arxiv.org/abs/2410.06479v2","updated":"2024-11-04T07:28:02Z","published":"2024-10-09T02:14:39Z","title":"Large Language Model Compression with Neural Architecture Search","summary":"  Large language models (LLMs) exhibit remarkable reasoning abilities, allowing\nthem to generalize across a wide range of downstream tasks, such as commonsense\nreasoning or instruction following. However, as LLMs scale, inference costs\nbecome increasingly prohibitive, accumulating significantly over their life\ncycle. This poses the question: Can we compress pre-trained LLMs to meet\ndiverse size and latency requirements? We leverage Neural Architecture Search\n(NAS) to compress LLMs by pruning structural components, such as attention\nheads, neurons, and layers, aiming to achieve a Pareto-optimal balance between\nperformance and efficiency. While NAS already achieved promising results on\nsmall language models in previous work, in this paper we propose various\nextensions that allow us to scale to LLMs. Compared to structural pruning\nbaselines, we show that NAS improves performance up to 3.4% on MMLU with an\non-device latency speedup.\n","authors":["Rhea Sanjay Sukthanker","Benedikt Staffler","Frank Hutter","Aaron Klein"],"pdf_url":"https://arxiv.org/pdf/2410.06479v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.11252v2","updated":"2024-11-04T07:27:37Z","published":"2024-09-17T15:00:31Z","title":"WER We Stand: Benchmarking Urdu ASR Models","summary":"  This paper presents a comprehensive evaluation of Urdu Automatic Speech\nRecognition (ASR) models. We analyze the performance of three ASR model\nfamilies: Whisper, MMS, and Seamless-M4T using Word Error Rate (WER), along\nwith a detailed examination of the most frequent wrong words and error types\nincluding insertions, deletions, and substitutions. Our analysis is conducted\nusing two types of datasets, read speech and conversational speech. Notably, we\npresent the first conversational speech dataset designed for benchmarking Urdu\nASR models. We find that seamless-large outperforms other ASR models on the\nread speech dataset, while whisper-large performs best on the conversational\nspeech dataset. Furthermore, this evaluation highlights the complexities of\nassessing ASR models for low-resource languages like Urdu using quantitative\nmetrics alone and emphasizes the need for a robust Urdu text normalization\nsystem. Our findings contribute valuable insights for developing robust ASR\nsystems for low-resource languages like Urdu.\n","authors":["Samee Arif","Sualeha Farid","Aamina Jamal Khan","Mustafa Abbas","Agha Ali Raza","Awais Athar"],"pdf_url":"https://arxiv.org/pdf/2409.11252v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01855v1","updated":"2024-11-04T07:10:24Z","published":"2024-11-04T07:10:24Z","title":"Can Language Models Learn to Skip Steps?","summary":"  Trained on vast corpora of human language, language models demonstrate\nemergent human-like reasoning abilities. Yet they are still far from true\nintelligence, which opens up intriguing opportunities to explore the parallels\nof humans and model behaviors. In this work, we study the ability to skip steps\nin reasoning - a hallmark of human expertise developed through practice. Unlike\nhumans, who may skip steps to enhance efficiency or to reduce cognitive load,\nmodels do not inherently possess such motivations to minimize reasoning steps.\nTo address this, we introduce a controlled framework that stimulates\nstep-skipping behavior by iteratively refining models to generate shorter and\naccurate reasoning paths. Empirical results indicate that models can develop\nthe step skipping ability under our guidance. Moreover, after fine-tuning on\nexpanded datasets that include both complete and skipped reasoning sequences,\nthe models can not only resolve tasks with increased efficiency without\nsacrificing accuracy, but also exhibit comparable and even enhanced\ngeneralization capabilities in out-of-domain scenarios. Our work presents the\nfirst exploration into human-like step-skipping ability and provides fresh\nperspectives on how such cognitive abilities can benefit AI models.\n","authors":["Tengxiao Liu","Qipeng Guo","Xiangkun Hu","Cheng Jiayang","Yue Zhang","Xipeng Qiu","Zheng Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.01855v1.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2402.16733v2","updated":"2024-11-04T06:54:34Z","published":"2024-02-21T09:12:16Z","title":"DREsS: Dataset for Rubric-based Essay Scoring on EFL Writing","summary":"  Automated essay scoring (AES) is a useful tool in English as a Foreign\nLanguage (EFL) writing education, offering real-time essay scores for students\nand instructors. However, previous AES models were trained on essays and scores\nirrelevant to the practical scenarios of EFL writing education and usually\nprovided a single holistic score due to the lack of appropriate datasets. In\nthis paper, we release DREsS, a large-scale, standard dataset for rubric-based\nautomated essay scoring. DREsS comprises three sub-datasets: DREsS_New,\nDREsS_Std., and DREsS_CASE. We collect DREsS_New, a real-classroom dataset with\n2.3K essays authored by EFL undergraduate students and scored by English\neducation experts. We also standardize existing rubric-based essay scoring\ndatasets as DREsS_Std. We suggest CASE, a corruption-based augmentation\nstrategy for essays, which generates 40.1K synthetic samples of DREsS_CASE and\nimproves the baseline results by 45.44%. DREsS will enable further research to\nprovide a more accurate and practical AES system for EFL writing education.\n","authors":["Haneul Yoo","Jieun Han","So-Yeon Ahn","Alice Oh"],"pdf_url":"https://arxiv.org/pdf/2402.16733v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01841v1","updated":"2024-11-04T06:27:14Z","published":"2024-11-04T06:27:14Z","title":"Leveraging Label Semantics and Meta-Label Refinement for Multi-Label\n  Question Classification","summary":"  Accurate annotation of educational resources is critical in the rapidly\nadvancing field of online education due to the complexity and volume of\ncontent. Existing classification methods face challenges with semantic overlap\nand distribution imbalance of labels in the multi-label context, which impedes\neffective personalized learning and resource recommendation. This paper\nintroduces RR2QC, a novel Retrieval Reranking method To multi-label Question\nClassification by leveraging label semantics and meta-label refinement.\nFirstly, RR2QC leverages semantic relationships within and across label groups\nto enhance pre-training strategie in multi-label context. Next, a class center\nlearning task is introduced, integrating label texts into downstream training\nto ensure questions consistently align with label semantics, retrieving the\nmost relevant label sequences. Finally, this method decomposes labels into\nmeta-labels and trains a meta-label classifier to rerank the retrieved label\nsequences. In doing so, RR2QC enhances the understanding and prediction\ncapability of long-tail labels by learning from meta-labels frequently\nappearing in other labels. Addtionally, a Math LLM is used to generate\nsolutions for questions, extracting latent information to further refine the\nmodel's insights. Experimental results demonstrate that RR2QC outperforms\nexisting classification methods in Precision@k and F1 scores across multiple\neducational datasets, establishing it as a potent enhancement for online\neducational content utilization.\n","authors":["Shi Dong","Xiaobei Niu","Rui Zhong","Zhifeng Wang","Mingzhang Zuo"],"pdf_url":"https://arxiv.org/pdf/2411.01841v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01839v1","updated":"2024-11-04T06:26:09Z","published":"2024-11-04T06:26:09Z","title":"TriG-NER: Triplet-Grid Framework for Discontinuous Named Entity\n  Recognition","summary":"  Discontinuous Named Entity Recognition (DNER) presents a challenging problem\nwhere entities may be scattered across multiple non-adjacent tokens, making\ntraditional sequence labelling approaches inadequate. Existing methods\npredominantly rely on custom tagging schemes to handle these discontinuous\nentities, resulting in models tightly coupled to specific tagging strategies\nand lacking generalisability across diverse datasets. To address these\nchallenges, we propose TriG-NER, a novel Triplet-Grid Framework that introduces\na generalisable approach to learning robust token-level representations for\ndiscontinuous entity extraction. Our framework applies triplet loss at the\ntoken level, where similarity is defined by word pairs existing within the same\nentity, effectively pulling together similar and pushing apart dissimilar ones.\nThis approach enhances entity boundary detection and reduces the dependency on\nspecific tagging schemes by focusing on word-pair relationships within a\nflexible grid structure. We evaluate TriG-NER on three benchmark DNER datasets\nand demonstrate significant improvements over existing grid-based\narchitectures. These results underscore our framework's effectiveness in\ncapturing complex entity structures and its adaptability to various tagging\nschemes, setting a new benchmark for discontinuous entity extraction.\n","authors":["Rina Carines Cabral","Soyeon Caren Han","Areej Alhassan","Riza Batista-Navarro","Goran Nenadic","Josiah Poon"],"pdf_url":"https://arxiv.org/pdf/2411.01839v1.pdf","comment":"Code will be made available upon publication"},{"id":"http://arxiv.org/abs/2411.01834v1","updated":"2024-11-04T06:07:53Z","published":"2024-11-04T06:07:53Z","title":"Align-SLM: Textless Spoken Language Models with Reinforcement Learning\n  from AI Feedback","summary":"  While textless Spoken Language Models (SLMs) have shown potential in\nend-to-end speech-to-speech modeling, they still lag behind text-based Large\nLanguage Models (LLMs) in terms of semantic coherence and relevance. This work\nintroduces the Align-SLM framework, which leverages preference optimization\ninspired by Reinforcement Learning with AI Feedback (RLAIF) to enhance the\nsemantic understanding of SLMs. Our approach generates multiple speech\ncontinuations from a given prompt and uses semantic metrics to create\npreference data for Direct Preference Optimization (DPO). We evaluate the\nframework using ZeroSpeech 2021 benchmarks for lexical and syntactic modeling,\nthe spoken version of the StoryCloze dataset for semantic coherence, and other\nspeech generation metrics, including the GPT4-o score and human evaluation.\nExperimental results show that our method achieves state-of-the-art performance\nfor SLMs on most benchmarks, highlighting the importance of preference\noptimization to improve the semantics of SLMs.\n","authors":["Guan-Ting Lin","Prashanth Gurunath Shivakumar","Aditya Gourav","Yile Gu","Ankur Gandhe","Hung-yi Lee","Ivan Bulyko"],"pdf_url":"https://arxiv.org/pdf/2411.01834v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.12902v2","updated":"2024-11-04T05:11:46Z","published":"2023-10-19T16:54:38Z","title":"Experimental Narratives: A Comparison of Human Crowdsourced Storytelling\n  and AI Storytelling","summary":"  The paper proposes a framework that combines behavioral and computational\nexperiments employing fictional prompts as a novel tool for investigating\ncultural artifacts and social biases in storytelling both by humans and\ngenerative AI. The study analyzes 250 stories authored by crowdworkers in June\n2019 and 80 stories generated by GPT-3.5 and GPT-4 in March 2023 by merging\nmethods from narratology and inferential statistics. Both crowdworkers and\nlarge language models responded to identical prompts about creating and falling\nin love with an artificial human. The proposed experimental paradigm allows a\ndirect and controlled comparison between human and LLM-generated storytelling.\nResponses to the Pygmalionesque prompts confirm the pervasive presence of the\nPygmalion myth in the collective imaginary of both humans and large language\nmodels. All solicited narratives present a scientific or technological pursuit.\nThe analysis reveals that narratives from GPT-3.5 and particularly GPT-4 are\nmore progressive in terms of gender roles and sexuality than those written by\nhumans. While AI narratives with default settings and no additional prompting\ncan occasionally provide innovative plot twists, they offer less imaginative\nscenarios and rhetoric than human-authored texts. The proposed framework argues\nthat fiction can be used as a window into human and AI-based collective\nimaginary and social dimensions.\n","authors":["Nina Begus"],"pdf_url":"https://arxiv.org/pdf/2310.12902v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.04172v2","updated":"2024-11-04T04:59:45Z","published":"2024-07-04T22:16:40Z","title":"ChartGemma: Visual Instruction-tuning for Chart Reasoning in the Wild","summary":"  Given the ubiquity of charts as a data analysis, visualization, and\ndecision-making tool across industries and sciences, there has been a growing\ninterest in developing pre-trained foundation models as well as general purpose\ninstruction-tuned models for chart understanding and reasoning. However,\nexisting methods suffer crucial drawbacks across two critical axes affecting\nthe performance of chart representation models: they are trained on data\ngenerated from underlying data tables of the charts, ignoring the visual trends\nand patterns in chart images, and use weakly aligned vision-language backbone\nmodels for domain-specific training, limiting their generalizability when\nencountering charts in the wild. We address these important drawbacks and\nintroduce ChartGemma, a novel chart understanding and reasoning model developed\nover PaliGemma. Rather than relying on underlying data tables, ChartGemma is\ntrained on instruction-tuning data generated directly from chart images, thus\ncapturing both high-level trends and low-level visual information from a\ndiverse set of charts. Our simple approach achieves state-of-the-art results\nacross $5$ benchmarks spanning chart summarization, question answering, and\nfact-checking, and our elaborate qualitative studies on real-world charts show\nthat ChartGemma generates more realistic and factually correct summaries\ncompared to its contemporaries. We release the code, model checkpoints,\ndataset, and demos at https://github.com/vis-nlp/ChartGemma.\n","authors":["Ahmed Masry","Megh Thakkar","Aayush Bajaj","Aaryaman Kartha","Enamul Hoque","Shafiq Joty"],"pdf_url":"https://arxiv.org/pdf/2407.04172v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.00038v2","updated":"2024-11-04T03:40:54Z","published":"2024-10-29T13:55:17Z","title":"Topic-Conversation Relevance (TCR) Dataset and Benchmarks","summary":"  Workplace meetings are vital to organizational collaboration, yet a large\npercentage of meetings are rated as ineffective. To help improve meeting\neffectiveness by understanding if the conversation is on topic, we create a\ncomprehensive Topic-Conversation Relevance (TCR) dataset that covers a variety\nof domains and meeting styles. The TCR dataset includes 1,500 unique meetings,\n22 million words in transcripts, and over 15,000 meeting topics, sourced from\nboth newly collected Speech Interruption Meeting (SIM) data and existing public\ndatasets. Along with the text data, we also open source scripts to generate\nsynthetic meetings or create augmented meetings from the TCR dataset to enhance\ndata diversity. For each data source, benchmarks are created using GPT-4 to\nevaluate the model accuracy in understanding transcription-topic relevance.\n","authors":["Yaran Fan","Jamie Pool","Senja Filipi","Ross Cutler"],"pdf_url":"https://arxiv.org/pdf/2411.00038v2.pdf","comment":"To be published in 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024) Track on Datasets and Benchmarks"},{"id":"http://arxiv.org/abs/2411.01765v1","updated":"2024-11-04T03:20:00Z","published":"2024-11-04T03:20:00Z","title":"Towards Pedagogical LLMs with Supervised Fine Tuning for Computing\n  Education","summary":"  This paper investigates supervised fine-tuning of large language models\n(LLMs) to improve their pedagogical alignment in computing education,\naddressing concerns that LLMs may hinder learning outcomes. The project\nutilised a proprietary dataset of 2,500 high quality question/answer pairs from\nprogramming course forums, and explores two research questions: the suitability\nof university course forums in contributing to fine-tuning datasets, and how\nsupervised fine-tuning can improve LLMs' alignment with educational principles\nsuch as constructivism. Initial findings suggest benefits in pedagogical\nalignment of LLMs, with deeper evaluations required.\n","authors":["Alexandra Vassar","Jake Renzella","Emily Ross","Andrew Taylor"],"pdf_url":"https://arxiv.org/pdf/2411.01765v1.pdf","comment":"3 pages, 1 table, conference"},{"id":"http://arxiv.org/abs/2405.15362v4","updated":"2024-11-04T03:18:13Z","published":"2024-05-24T08:54:36Z","title":"Pipeline Parallelism with Controllable Memory","summary":"  Pipeline parallelism has been widely explored, but most existing schedules\nlack a systematic methodology. In this paper, we propose a framework to\ndecompose pipeline schedules as repeating a building block, and show that the\nlifespan of the building block decides the peak activation memory of the\npipeline schedule. Guided by the observations, we find that almost all existing\npipeline schedules, to the best of our knowledge, are memory inefficient. To\naddress this, we introduce a family of memory efficient building blocks with\ncontrollable activation memory, which can reduce the peak activation memory to\n1/2 of 1F1B without sacrificing efficiency, and even to 1/3 with comparable\nthroughput. We can also achieve almost zero pipeline bubbles while maintaining\nthe same activation memory as 1F1B. Our evaluations demonstrate that in pure\npipeline parallelism settings, our methods outperform 1F1B by from 7% to 55% in\nterms of throughput. When employing a grid search over hybrid parallelism\nhyperparameters in practical scenarios, our methods demonstrate a 16%\nthroughput improvement over the 1F1B baseline for large language models. The\nimplementation is open-sourced at\nhttps://github.com/sail-sg/zero-bubble-pipeline-parallelism.\n","authors":["Penghui Qi","Xinyi Wan","Nyamdavaa Amar","Min Lin"],"pdf_url":"https://arxiv.org/pdf/2405.15362v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.00801v2","updated":"2024-11-04T03:07:30Z","published":"2024-02-23T18:45:35Z","title":"Self-Retrieval: End-to-End Information Retrieval with One Large Language\n  Model","summary":"  The rise of large language models (LLMs) has significantly transformed both\nthe construction and application of information retrieval (IR) systems.\nHowever, current interactions between IR systems and LLMs remain limited, with\nLLMs merely serving as part of components within IR systems, and IR systems\nbeing constructed independently of LLMs. This separated architecture restricts\nknowledge sharing and deep collaboration between them. In this paper, we\nintroduce Self-Retrieval, a novel end-to-end LLM-driven information retrieval\narchitecture. Self-Retrieval unifies all essential IR functions within a single\nLLM, leveraging the inherent capabilities of LLMs throughout the IR process.\nSpecifically, Self-Retrieval internalizes the retrieval corpus through\nself-supervised learning, transforms the retrieval process into sequential\npassage generation, and performs relevance assessment for reranking.\nExperimental results demonstrate that Self-Retrieval not only outperforms\nexisting retrieval approaches by a significant margin, but also substantially\nenhances the performance of LLM-driven downstream applications like\nretrieval-augmented generation.\n","authors":["Qiaoyu Tang","Jiawei Chen","Zhuoqun Li","Bowen Yu","Yaojie Lu","Cheng Fu","Haiyang Yu","Hongyu Lin","Fei Huang","Ben He","Xianpei Han","Le Sun","Yongbin Li"],"pdf_url":"https://arxiv.org/pdf/2403.00801v2.pdf","comment":"NeurIPS 2024 Camera-ready Version. Code:\n  https://github.com/icip-cas/SelfRetrieval"},{"id":"http://arxiv.org/abs/2310.12821v5","updated":"2024-11-04T02:48:42Z","published":"2023-10-19T15:17:34Z","title":"GestureGPT: Toward Zero-Shot Free-Form Hand Gesture Understanding with\n  Large Language Model Agents","summary":"  Existing gesture interfaces only work with a fixed set of gestures defined\neither by interface designers or by users themselves, which introduces learning\nor demonstration efforts that diminish their naturalness. Humans, on the other\nhand, understand free-form gestures by synthesizing the gesture, context,\nexperience, and common sense. In this way, the user does not need to learn,\ndemonstrate, or associate gestures. We introduce GestureGPT, a free-form hand\ngesture understanding framework that mimics human gesture understanding\nprocedures to enable a natural free-form gestural interface. Our framework\nleverages multiple Large Language Model agents to manage and synthesize gesture\nand context information, then infers the interaction intent by associating the\ngesture with an interface function. More specifically, our triple-agent\nframework includes a Gesture Description Agent that automatically segments and\nformulates natural language descriptions of hand poses and movements based on\nhand landmark coordinates. The description is deciphered by a Gesture Inference\nAgent through self-reasoning and querying about the interaction context (e.g.,\ninteraction history, gaze data), which is managed by a Context Management\nAgent. Following iterative exchanges, the Gesture Inference Agent discerns the\nuser's intent by grounding it to an interactive function. We validated our\nframework offline under two real-world scenarios: smart home control and online\nvideo streaming. The average zero-shot Top-1/Top-5 grounding accuracies are\n44.79%/83.59% for smart home tasks and 37.50%/73.44% for video streaming tasks.\nWe also provide an extensive discussion that includes rationale for model\nselection, generalizability, and future research directions for a practical\nsystem etc.\n","authors":["Xin Zeng","Xiaoyu Wang","Tengxiang Zhang","Chun Yu","Shengdong Zhao","Yiqiang Chen"],"pdf_url":"https://arxiv.org/pdf/2310.12821v5.pdf","comment":"This paper has been accepted to the ISS 2024 track of the Proceedings\n  of the ACM on Human-Computer Interaction"},{"id":"http://arxiv.org/abs/2411.01751v1","updated":"2024-11-04T02:30:05Z","published":"2024-11-04T02:30:05Z","title":"RAGViz: Diagnose and Visualize Retrieval-Augmented Generation","summary":"  Retrieval-augmented generation (RAG) combines knowledge from domain-specific\nsources into large language models to ground answer generation. Current RAG\nsystems lack customizable visibility on the context documents and the model's\nattentiveness towards such documents. We propose RAGViz, a RAG diagnosis tool\nthat visualizes the attentiveness of the generated tokens in retrieved\ndocuments. With a built-in user interface, retrieval index, and Large Language\nModel (LLM) backbone, RAGViz provides two main functionalities: (1) token and\ndocument-level attention visualization, and (2) generation comparison upon\ncontext document addition and removal. As an open-source toolkit, RAGViz can be\neasily hosted with a custom embedding model and HuggingFace-supported LLM\nbackbone. Using a hybrid ANN (Approximate Nearest Neighbor) index,\nmemory-efficient LLM inference tool, and custom context snippet method, RAGViz\noperates efficiently with a median query time of about 5 seconds on a moderate\nGPU node. Our code is available at https://github.com/cxcscmu/RAGViz. A demo\nvideo of RAGViz can be found at https://youtu.be/cTAbuTu6ur4.\n","authors":["Tevin Wang","Jingyuan He","Chenyan Xiong"],"pdf_url":"https://arxiv.org/pdf/2411.01751v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.14826v3","updated":"2024-11-04T02:29:32Z","published":"2024-09-23T08:58:48Z","title":"ToolPlanner: A Tool Augmented LLM for Multi Granularity Instructions\n  with Path Planning and Feedback","summary":"  Recently, tool-augmented LLMs have gained increasing attention. Given an\ninstruction, tool-augmented LLMs can interact with various external tools in\nmultiple rounds and provide a final answer. However, previous LLMs were trained\non overly detailed instructions, which included API names or parameters, while\nreal users would not explicitly mention these API details. This leads to a gap\nbetween trained LLMs and real-world scenarios. In addition, most works ignore\nwhether the interaction process follows the instruction. To address these\nissues, we constructed a training dataset called MGToolBench, which contains\nstatement and category-level instructions to better reflect real-world\nscenarios. In addition, we propose ToolPlanner, a two-stage reinforcement\nlearning framework that utilizes path planning and two feedback mechanisms to\nenhance the LLM's task completion and instruction-following capabilities.\nExperimental results show that ToolPlanner significantly improves the Match\nRate, Pass Rate and Win Rate by 26.8%, 20.2%, and 5.6% compared to the SOTA\nmodel. Human evaluation verifies that the multi-granularity instructions can\nbetter align with users' usage habits. Our data and code will be released upon\nacceptance.\n","authors":["Qinzhuo Wu","Wei Liu","Jian Luan","Bin Wang"],"pdf_url":"https://arxiv.org/pdf/2409.14826v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11988v2","updated":"2024-11-04T02:28:13Z","published":"2024-10-15T18:51:18Z","title":"DISP-LLM: Dimension-Independent Structural Pruning for Large Language\n  Models","summary":"  Large Language Models (LLMs) have achieved remarkable success in various\nnatural language processing tasks, including language modeling, understanding,\nand generation. However, the increased memory and computational costs\nassociated with these models pose significant challenges for deployment on\nresource-limited devices. Structural pruning has emerged as a promising\nsolution to reduce the costs of LLMs without requiring post-processing steps.\nPrior structural pruning methods either follow the dependence of structures at\nthe cost of limiting flexibility, or introduce non-trivial additional\nparameters by incorporating different projection matrices. In this work, we\npropose a novel approach that relaxes the constraint imposed by regular\nstructural pruning methods and eliminates the structural dependence along the\nembedding dimension. Our dimension-independent structural pruning method offers\nseveral benefits. Firstly, our method enables different blocks to utilize\ndifferent subsets of the feature maps. Secondly, by removing structural\ndependence, we facilitate each block to possess varying widths along its input\nand output dimensions, thereby significantly enhancing the flexibility of\nstructural pruning. We evaluate our method on various LLMs, including OPT,\nLLaMA, LLaMA-2, Phi-1.5, and Phi-2. Experimental results demonstrate that our\napproach outperforms other state-of-the-art methods, showing for the first time\nthat structural pruning can achieve an accuracy similar to semi-structural\npruning.\n","authors":["Shangqian Gao","Chi-Heng Lin","Ting Hua","Tang Zheng","Yilin Shen","Hongxia Jin","Yen-Chang Hsu"],"pdf_url":"https://arxiv.org/pdf/2410.11988v2.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.01747v1","updated":"2024-11-04T02:08:59Z","published":"2024-11-04T02:08:59Z","title":"DynaSaur: Large Language Agents Beyond Predefined Actions","summary":"  Existing LLM agent systems typically select actions from a fixed and\npredefined set at every step. While this approach is effective in closed,\nnarrowly-scoped environments, we argue that it presents two major challenges\nwhen deploying LLM agents in real-world scenarios: (1) selecting from a fixed\nset of actions significantly restricts the planning and acting capabilities of\nLLM agents, and (2) this approach requires substantial human effort to\nenumerate and implement all possible actions, which becomes impractical in\ncomplex environments with a vast number of potential actions. In this work, we\npropose an LLM agent framework that enables the dynamic creation and\ncomposition of actions in an online manner. In this framework, the agent\ninteracts with the environment by generating and executing programs written in\na general-purpose programming language at each step. Furthermore, generated\nactions are accumulated over time for future reuse. Our extensive experiments\non the GAIA benchmark demonstrate that this framework offers significantly\ngreater flexibility and outperforms previous methods. Notably, it allows an LLM\nagent to recover in scenarios where no relevant action exists in the predefined\nset or when existing actions fail due to unforeseen edge cases. At the time of\nwriting, we hold the top position on the GAIA public leaderboard. Our code can\nbe found in\n\\href{https://github.com/adobe-research/dynasaur}{https://github.com/adobe-research/dynasaur}.\n","authors":["Dang Nguyen","Viet Dac Lai","Seunghyun Yoon","Ryan A. Rossi","Handong Zhao","Ruiyi Zhang","Puneet Mathur","Nedim Lipka","Yu Wang","Trung Bui","Franck Dernoncourt","Tianyi Zhou"],"pdf_url":"https://arxiv.org/pdf/2411.01747v1.pdf","comment":"15 pages, 8 figures"},{"id":"http://arxiv.org/abs/2409.03171v2","updated":"2024-11-04T00:44:32Z","published":"2024-09-05T01:58:29Z","title":"MARAGS: A Multi-Adapter System for Multi-Task Retrieval Augmented\n  Generation Question Answering","summary":"  In this paper we present a multi-adapter retrieval augmented generation\nsystem (MARAGS) for Meta's Comprehensive RAG (CRAG) competition for KDD CUP\n2024. CRAG is a question answering dataset contains 3 different subtasks aimed\nat realistic question and answering RAG related tasks, with a diverse set of\nquestion topics, question types, time dynamic answers, and questions featuring\nentities of varying popularity.\n  Our system follows a standard setup for web based RAG, which uses processed\nweb pages to provide context for an LLM to produce generations, while also\nquerying API endpoints for additional information. MARAGS also utilizes\nmultiple different adapters to solve the various requirements for these tasks\nwith a standard cross-encoder model for ranking candidate passages relevant for\nanswering the question. Our system achieved 2nd place for Task 1 as well as 3rd\nplace on Task 2.\n","authors":["Mitchell DeHaven"],"pdf_url":"https://arxiv.org/pdf/2409.03171v2.pdf","comment":"Accepted to CRAG KDD Cup 24 Workshop"},{"id":"http://arxiv.org/abs/2411.02674v1","updated":"2024-11-04T23:21:12Z","published":"2024-11-04T23:21:12Z","title":"Wave Network: An Ultra-Small Language Model","summary":"  We propose an innovative token representation and update method in a new\nultra-small language model: the Wave network. Specifically, we use a\n\\textbf{complex vector} to represent each token, encoding both global and local\nsemantics of the input text. A \\textbf{complex vector} consists of two\ncomponents: a magnitude vector representing the \\textit{global semantics} of\nthe input text, and a phase vector capturing the \\textit{relationships between\nindividual tokens and global semantics}. Experiments on the AG News text\nclassification task demonstrate that, when generating complex vectors from\nrandomly initialized token embeddings, our single-layer Wave Network achieves\n90.91\\% accuracy with wave interference and 91.66\\% with wave modulation --\noutperforming a single Transformer layer using BERT pre-trained embeddings by\n19.23\\% and 19.98\\%, respectively, and approaching the accuracy of the\npre-trained and fine-tuned BERT base model (94.64\\%). Additionally, compared to\nBERT base, the Wave Network reduces video memory usage and training time by\n77.34\\% and 85.62\\% during wave modulation. In summary, we used a\n2.4-million-parameter small language model to achieve accuracy comparable to a\n100-million-parameter BERT model in text classification.\n","authors":["Xin Zhang","Victor S. Sheng"],"pdf_url":"https://arxiv.org/pdf/2411.02674v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02657v1","updated":"2024-11-04T22:45:52Z","published":"2024-11-04T22:45:52Z","title":"Zebra-Llama: A Context-Aware Large Language Model for Democratizing Rare\n  Disease Knowledge","summary":"  Rare diseases present unique challenges in healthcare, often suffering from\ndelayed diagnosis and fragmented information landscapes. The scarcity of\nreliable knowledge in these conditions poses a distinct challenge for Large\nLanguage Models (LLMs) in supporting clinical management and delivering precise\npatient information underscoring the need for focused training on these 'zebra'\ncases. We present Zebra-Llama, a specialized context-aware language model with\nhigh precision Retrieval Augmented Generation (RAG) capability, focusing on\nEhlers-Danlos Syndrome (EDS) as our case study. EDS, affecting 1 in 5,000\nindividuals, exemplifies the complexities of rare diseases with its diverse\nsymptoms, multiple subtypes, and evolving diagnostic criteria. By implementing\na novel context-aware fine-tuning methodology trained on questions derived from\nmedical literature, patient experiences, and clinical resources, along with\nexpertly curated responses, Zebra-Llama demonstrates unprecedented capabilities\nin handling EDS-related queries. On a test set of real-world questions\ncollected from EDS patients and clinicians, medical experts evaluated the\nresponses generated by both models, revealing Zebra-Llama's substantial\nimprovements over base model (Llama 3.1-8B-Instruct) in thoroughness (77.5% vs.\n70.1%), accuracy (83.0% vs. 78.8%), clarity (74.7% vs. 72.0%) and citation\nreliability (70.6% vs. 52.3%). Released as an open-source resource, Zebra-Llama\nnot only provides more accessible and reliable EDS information but also\nestablishes a framework for developing specialized AI solutions for other rare\nconditions. This work represents a crucial step towards democratizing\nexpert-level knowledge in rare disease management, potentially transforming how\nhealthcare providers and patients navigate the complex landscape of rare\ndiseases.\n","authors":["Karthik Soman","Andrew Langdon","Catalina Villouta","Chinmay Agrawal","Lashaw Salta","Braian Peetoom","Gianmarco Bellucci","Orion J Buske"],"pdf_url":"https://arxiv.org/pdf/2411.02657v1.pdf","comment":"26 pages, 4 figures, 1 supplementary figure"},{"id":"http://arxiv.org/abs/2407.11606v3","updated":"2024-11-04T22:42:38Z","published":"2024-07-16T11:12:28Z","title":"The Foundations of Tokenization: Statistical and Computational Concerns","summary":"  Tokenization - the practice of converting strings of characters from an\nalphabet into sequences of tokens over a vocabulary - is a critical step in the\nNLP pipeline. The use of token representations is widely credited with\nincreased model performance but is also the source of many undesirable\nbehaviors, such as spurious ambiguity or inconsistency. Despite its recognized\nimportance as a standard representation method in NLP, the theoretical\nunderpinnings of tokenization are not yet fully understood. In particular, the\nimpact of tokenization on statistical estimation has been investigated mostly\nthrough empirical means. The present paper contributes to addressing this\ntheoretical gap by proposing a unified formal framework for representing and\nanalyzing tokenizer models. Based on the category of stochastic maps, this\nframework enables us to establish general conditions for a principled use of\ntokenizers, and most importantly, the necessary and sufficient conditions for a\ntokenizer model to preserve the consistency of statistical estimators.\nAdditionally, we discuss statistical and computational concerns crucial for\ndesigning and implementing tokenizer models, such as inconsistency, ambiguity,\ntractability, and boundedness. The framework and results advanced in this paper\ncontribute to building robust theoretical foundations for representations in\nneural language modeling that can inform future empirical research.\n","authors":["Juan Luis Gastaldi","John Terilla","Luca Malagutti","Brian DuSell","Tim Vieira","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2407.11606v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.21047v2","updated":"2024-11-04T22:04:00Z","published":"2024-05-31T17:39:15Z","title":"Grammar-Aligned Decoding","summary":"  Large Language Models (LLMs) struggle with reliably generating highly\nstructured outputs, such as program code, mathematical formulas, or well-formed\nmarkup. Constrained decoding approaches mitigate this problem by greedily\nrestricting what tokens an LLM can output at each step to guarantee that the\noutput matches a given constraint. Specifically, in grammar-constrained\ndecoding (GCD), the LLM's output must follow a given grammar. In this paper, we\ndemonstrate that GCD techniques (and in general constrained decoding\ntechniques) can distort the LLM's distribution, leading to outputs that are\ngrammatical but appear with likelihoods that are not proportional to the ones\ngiven by the LLM, and so ultimately are low-quality. We call the problem of\naligning sampling with a grammar constraint, grammar-aligned decoding (GAD),\nand propose adaptive sampling with approximate expected futures (ASAp), a\ndecoding algorithm that guarantees the output to be grammatical while provably\nproducing outputs that match the conditional probability of the LLM's\ndistribution conditioned on the given grammar constraint. Our algorithm uses\nprior sample outputs to soundly overapproximate the future grammaticality of\ndifferent output prefixes. Our evaluation on code generation and structured NLP\ntasks shows how ASAp often produces outputs with higher likelihood (according\nto the LLM's distribution) than existing GCD techniques, while still enforcing\nthe desired grammatical constraints.\n","authors":["Kanghee Park","Jiayu Wang","Taylor Berg-Kirkpatrick","Nadia Polikarpova","Loris D'Antoni"],"pdf_url":"https://arxiv.org/pdf/2405.21047v2.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.02643v1","updated":"2024-11-04T22:01:52Z","published":"2024-11-04T22:01:52Z","title":"A Comparative Analysis of Counterfactual Explanation Methods for Text\n  Classifiers","summary":"  Counterfactual explanations can be used to interpret and debug text\nclassifiers by producing minimally altered text inputs that change a\nclassifier's output. In this work, we evaluate five methods for generating\ncounterfactual explanations for a BERT text classifier on two datasets using\nthree evaluation metrics. The results of our experiments suggest that\nestablished white-box substitution-based methods are effective at generating\nvalid counterfactuals that change the classifier's output. In contrast, newer\nmethods based on large language models (LLMs) excel at producing natural and\nlinguistically plausible text counterfactuals but often fail to generate valid\ncounterfactuals that alter the classifier's output. Based on these results, we\nrecommend developing new counterfactual explanation methods that combine the\nstrengths of established gradient-based approaches and newer LLM-based\ntechniques to generate high-quality, valid, and plausible text counterfactual\nexplanations.\n","authors":["Stephen McAleese","Mark Keane"],"pdf_url":"https://arxiv.org/pdf/2411.02643v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2411.02631v1","updated":"2024-11-04T21:42:56Z","published":"2024-11-04T21:42:56Z","title":"Extracting Unlearned Information from LLMs with Activation Steering","summary":"  An unintended consequence of the vast pretraining of Large Language Models\n(LLMs) is the verbatim memorization of fragments of their training data, which\nmay contain sensitive or copyrighted information. In recent years, unlearning\nhas emerged as a solution to effectively remove sensitive knowledge from models\nafter training. Yet, recent work has shown that supposedly deleted information\ncan still be extracted by malicious actors through various attacks. Still,\ncurrent attacks retrieve sets of possible candidate generations and are unable\nto pinpoint the output that contains the actual target information. We propose\nactivation steering as a method for exact information retrieval from unlearned\nLLMs. We introduce a novel approach to generating steering vectors, named\nAnonymized Activation Steering. Additionally, we develop a simple word\nfrequency method to pinpoint the correct answer among a set of candidates when\nretrieving unlearned information. Our evaluation across multiple unlearning\ntechniques and datasets demonstrates that activation steering successfully\nrecovers general knowledge (e.g., widely known fictional characters) while\nrevealing limitations in retrieving specific information (e.g., details about\nnon-public individuals). Overall, our results demonstrate that exact\ninformation retrieval from unlearned models is possible, highlighting a severe\nvulnerability of current unlearning techniques.\n","authors":["Atakan Seyitoğlu","Aleksei Kuvshinov","Leo Schwinn","Stephan Günnemann"],"pdf_url":"https://arxiv.org/pdf/2411.02631v1.pdf","comment":"Accepted at NeurIPS 2024 Workshop Safe Generative AI"},{"id":"http://arxiv.org/abs/2309.07601v3","updated":"2024-11-04T21:33:00Z","published":"2023-09-14T11:06:51Z","title":"Weakly Supervised Veracity Classification with LLM-Predicted Credibility\n  Signals","summary":"  Credibility signals represent a wide range of heuristics typically used by\njournalists and fact-checkers to assess the veracity of online content.\nAutomating the extraction of credibility signals presents significant\nchallenges due to the necessity of training high-accuracy, signal-specific\nextractors, coupled with the lack of sufficiently large annotated datasets.\nThis paper introduces Pastel (Prompted weAk Supervision wiTh crEdibility\nsignaLs), a weakly supervised approach that leverages large language models\n(LLMs) to extract credibility signals from web content, and subsequently\ncombines them to predict the veracity of content without relying on human\nsupervision. We validate our approach using four article-level misinformation\ndetection datasets, demonstrating that Pastel outperforms zero-shot veracity\ndetection by 38.3% and achieves 86.7% of the performance of the\nstate-of-the-art system trained with human supervision. Moreover, in\ncross-domain settings where training and testing datasets originate from\ndifferent domains, Pastel significantly outperforms the state-of-the-art\nsupervised model by 63%. We further study the association between credibility\nsignals and veracity, and perform an ablation study showing the impact of each\nsignal on model performance. Our findings reveal that 12 out of the 19 proposed\nsignals exhibit strong associations with veracity across all datasets, while\nsome signals show domain-specific strengths.\n","authors":["João A. Leite","Olesya Razuvayevskaya","Kalina Bontcheva","Carolina Scarton"],"pdf_url":"https://arxiv.org/pdf/2309.07601v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02617v1","updated":"2024-11-04T21:12:08Z","published":"2024-11-04T21:12:08Z","title":"TeleOracle: Fine-Tuned Retrieval-Augmented Generation with Long-Context\n  Support for Network","summary":"  The telecommunications industry's rapid evolution demands intelligent systems\ncapable of managing complex networks and adapting to emerging technologies.\nWhile large language models (LLMs) show promise in addressing these challenges,\ntheir deployment in telecom environments faces significant constraints due to\nedge device limitations and inconsistent documentation. To bridge this gap, we\npresent TeleOracle, a telecom-specialized retrieval-augmented generation (RAG)\nsystem built on the Phi-2 small language model (SLM). To improve context\nretrieval, TeleOracle employs a two-stage retriever that incorporates semantic\nchunking and hybrid keyword and semantic search. Additionally, we expand the\ncontext window during inference to enhance the model's performance on\nopen-ended queries. We also employ low-rank adaption for efficient fine-tuning.\nA thorough analysis of the model's performance indicates that our RAG framework\nis effective in aligning Phi-2 to the telecom domain in a downstream question\nand answer (QnA) task, achieving a 30% improvement in accuracy over the base\nPhi-2 model, reaching an overall accuracy of 81.20%. Notably, we show that our\nmodel not only performs on par with the much larger LLMs but also achieves a\nhigher faithfulness score, indicating higher adherence to the retrieved\ncontext.\n","authors":["Nouf Alabbasi","Omar Erak","Omar Alhussein","Ismail Lotfi","Sami Muhaidat","Merouane Debbah"],"pdf_url":"https://arxiv.org/pdf/2411.02617v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02610v1","updated":"2024-11-04T21:05:01Z","published":"2024-11-04T21:05:01Z","title":"Investigating Idiomaticity in Word Representations","summary":"  Idiomatic expressions are an integral part of human languages, often used to\nexpress complex ideas in compressed or conventional ways (e.g. eager beaver as\na keen and enthusiastic person). However, their interpretations may not be\nstraightforwardly linked to the meanings of their individual components in\nisolation and this may have an impact for compositional approaches. In this\npaper, we investigate to what extent word representation models are able to go\nbeyond compositional word combinations and capture multiword expression\nidiomaticity and some of the expected properties related to idiomatic meanings.\nWe focus on noun compounds of varying levels of idiomaticity in two languages\n(English and Portuguese), presenting a dataset of minimal pairs containing\nhuman idiomaticity judgments for each noun compound at both type and token\nlevels, their paraphrases and their occurrences in naturalistic and\nsense-neutral contexts, totalling 32,200 sentences. We propose this set of\nminimal pairs for evaluating how well a model captures idiomatic meanings, and\ndefine a set of fine-grained metrics of Affinity and Scaled Similarity, to\ndetermine how sensitive the models are to perturbations that may lead to\nchanges in idiomaticity. The results obtained with a variety of representative\nand widely used models indicate that, despite superficial indications to the\ncontrary in the form of high similarities, idiomaticity is not yet accurately\nrepresented in current models. Moreover, the performance of models with\ndifferent levels of contextualisation suggests that their ability to capture\ncontext is not yet able to go beyond more superficial lexical clues provided by\nthe words and to actually incorporate the relevant semantic clues needed for\nidiomaticity.\n","authors":["Wei He","Tiago Kramer Vieira","Marcos Garcia","Carolina Scarton","Marco Idiart","Aline Villavicencio"],"pdf_url":"https://arxiv.org/pdf/2411.02610v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.06458v2","updated":"2024-11-04T21:04:31Z","published":"2024-08-12T19:18:05Z","title":"Towards Autonomous Agents: Adaptive-planning, Reasoning, and Acting in\n  Language Models","summary":"  We propose a novel in-context learning algorithm for building autonomous\ndecision-making language agents. The language agent continuously attempts to\nsolve the same task by self-correcting each time the task fails. Our selected\nlanguage agent demonstrates the ability to solve tasks in a text-based game\nenvironment. Our results show that the gemma-2-9b-it language model, using our\nproposed method, can successfully complete two of six tasks that failed in the\nfirst attempt. This highlights the effectiveness of our approach in enhancing\nthe problem-solving capabilities of a single language model through\nself-correction, paving the way for more advanced autonomous agents. The code\nis publicly available at\nhttps://github.com/YenCheHsiao/AutonomousLLMAgentwithAdaptingPlanning.\n","authors":["Abhishek Dutta","Yen-Che Hsiao"],"pdf_url":"https://arxiv.org/pdf/2408.06458v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02603v1","updated":"2024-11-04T20:53:04Z","published":"2024-11-04T20:53:04Z","title":"FactTest: Factuality Testing in Large Language Models with Statistical\n  Guarantees","summary":"  The propensity of Large Language Models (LLMs) to generate hallucinations and\nnon-factual content undermines their reliability in high-stakes domains, where\nrigorous control over Type I errors (the conditional probability of incorrectly\nclassifying hallucinations as truthful content) is essential. Despite its\nimportance, formal verification of LLM factuality with such guarantees remains\nlargely unexplored. In this paper, we introduce FactTest, a novel framework\nthat statistically assesses whether an LLM can confidently provide correct\nanswers to given questions with high-probability correctness guarantees. We\nformulate factuality testing as hypothesis testing problem to enforce an upper\nbound of Type I errors at user-specified significance levels. Notably, we prove\nthat our framework also ensures strong Type II error control under mild\nconditions and can be extended to maintain its effectiveness when covariate\nshifts exist. %These analyses are amenable to the principled NP framework. Our\napproach is distribution-free and works for any number of human-annotated\nsamples. It is model-agnostic and applies to any black-box or white-box LM.\nExtensive experiments on question-answering (QA) and multiple-choice benchmarks\ndemonstrate that \\approach effectively detects hallucinations and improves the\nmodel's ability to abstain from answering unknown questions, leading to an over\n40% accuracy improvement.\n","authors":["Fan Nie","Xiaotian Hou","Shuhang Lin","James Zou","Huaxiu Yao","Linjun Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.02603v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02599v1","updated":"2024-11-04T20:44:40Z","published":"2024-11-04T20:44:40Z","title":"Vocal Sandbox: Continual Learning and Adaptation for Situated\n  Human-Robot Collaboration","summary":"  We introduce Vocal Sandbox, a framework for enabling seamless human-robot\ncollaboration in situated environments. Systems in our framework are\ncharacterized by their ability to adapt and continually learn at multiple\nlevels of abstraction from diverse teaching modalities such as spoken dialogue,\nobject keypoints, and kinesthetic demonstrations. To enable such adaptation, we\ndesign lightweight and interpretable learning algorithms that allow users to\nbuild an understanding and co-adapt to a robot's capabilities in real-time, as\nthey teach new behaviors. For example, after demonstrating a new low-level\nskill for \"tracking around\" an object, users are provided with trajectory\nvisualizations of the robot's intended motion when asked to track a new object.\nSimilarly, users teach high-level planning behaviors through spoken dialogue,\nusing pretrained language models to synthesize behaviors such as \"packing an\nobject away\" as compositions of low-level skills $-$ concepts that can be\nreused and built upon. We evaluate Vocal Sandbox in two settings: collaborative\ngift bag assembly and LEGO stop-motion animation. In the first setting, we run\nsystematic ablations and user studies with 8 non-expert participants,\nhighlighting the impact of multi-level teaching. Across 23 hours of total robot\ninteraction time, users teach 17 new high-level behaviors with an average of 16\nnovel low-level skills, requiring 22.1% less active supervision compared to\nbaselines and yielding more complex autonomous performance (+19.7%) with fewer\nfailures (-67.1%). Qualitatively, users strongly prefer Vocal Sandbox systems\ndue to their ease of use (+20.6%) and overall performance (+13.9%). Finally, we\npair an experienced system-user with a robot to film a stop-motion animation;\nover two hours of continuous collaboration, the user teaches progressively more\ncomplex motion skills to shoot a 52 second (232 frame) movie.\n","authors":["Jennifer Grannen","Siddharth Karamcheti","Suvir Mirchandani","Percy Liang","Dorsa Sadigh"],"pdf_url":"https://arxiv.org/pdf/2411.02599v1.pdf","comment":"Published at CoRL 2024. 24 pages, 8 figures. Project Page:\n  https://vocal-sandbox.github.io"},{"id":"http://arxiv.org/abs/2411.02594v1","updated":"2024-11-04T20:35:10Z","published":"2024-11-04T20:35:10Z","title":"\"It's a conversation, not a quiz\": A Risk Taxonomy and Reflection Tool\n  for LLM Adoption in Public Health","summary":"  Recent breakthroughs in large language models (LLMs) have generated both\ninterest and concern about their potential adoption as accessible information\nsources or communication tools across different domains. In public health --\nwhere stakes are high and impacts extend across populations -- adopting LLMs\nposes unique challenges that require thorough evaluation. However, structured\napproaches for assessing potential risks in public health remain\nunder-explored. To address this gap, we conducted focus groups with health\nprofessionals and health issue experiencers to unpack their concerns, situated\nacross three distinct and critical public health issues that demand\nhigh-quality information: vaccines, opioid use disorder, and intimate partner\nviolence. We synthesize participants' perspectives into a risk taxonomy,\ndistinguishing and contextualizing the potential harms LLMs may introduce when\npositioned alongside traditional health communication. This taxonomy highlights\nfour dimensions of risk in individual behaviors, human-centered care,\ninformation ecosystem, and technology accountability. For each dimension, we\ndiscuss specific risks and example reflection questions to help practitioners\nadopt a risk-reflexive approach. This work offers a shared vocabulary and\nreflection tool for experts in both computing and public health to\ncollaboratively anticipate, evaluate, and mitigate risks in deciding when to\nemploy LLM capabilities (or not) and how to mitigate harm when they are used.\n","authors":["Jiawei Zhou","Amy Z. Chen","Darshi Shah","Laura Schwab Reese","Munmun De Choudhury"],"pdf_url":"https://arxiv.org/pdf/2411.02594v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02591v1","updated":"2024-11-04T20:31:22Z","published":"2024-11-04T20:31:22Z","title":"Geometry of orofacial neuromuscular signals: speech articulation\n  decoding using surface electromyography","summary":"  Each year, millions of individuals lose the ability to speak intelligibly due\nto causes such as neuromuscular disease, stroke, trauma, and head/neck cancer\nsurgery (e.g. laryngectomy) or treatment (e.g. radiotherapy toxicity to the\nspeech articulators). Effective communication is crucial for daily activities,\nand losing the ability to speak leads to isolation, depression, anxiety, and a\nhost of detrimental sequelae. Noninvasive surface electromyography (sEMG) has\nshown promise to restore speech output in these individuals. The goal is to\ncollect sEMG signals from multiple articulatory sites as people silently\nproduce speech and then decode the signals to enable fluent and natural\ncommunication. Currently, many fundamental properties of orofacial\nneuromuscular signals relating to speech articulation remain unanswered. They\ninclude questions relating to 1) the data structure of the orofacial sEMG\nsignals, 2)the signal distribution shift of sEMG across individuals, 3) ability\nof sEMG signals to span the entire English language phonetic space during\nsilent speech articulations, and 4) the generalization capability of\nnon-invasive sEMG based silent speech interfaces. We address these questions\nthrough a series of experiments involving healthy human subjects. We show that\nsEMG signals evince graph data structure and that the signal distribution shift\nis given by a change of basis. Furthermore, we show that silently voiced\narticulations spanning the entire English language phonetic space can be\ndecoded using small neural networks which can be trained with little data and\nthat such architectures work well across individuals. To ensure transparency\nand reproducibility, we open-source all the data and codes used in this study.\n","authors":["Harshavardhana T. Gowda","Zachary D. McNaughton","Lee M. Miller"],"pdf_url":"https://arxiv.org/pdf/2411.02591v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02589v1","updated":"2024-11-04T20:29:35Z","published":"2024-11-04T20:29:35Z","title":"Context-Informed Machine Translation of Manga using Multimodal Large\n  Language Models","summary":"  Due to the significant time and effort required for handcrafting\ntranslations, most manga never leave the domestic Japanese market. Automatic\nmanga translation is a promising potential solution. However, it is a budding\nand underdeveloped field and presents complexities even greater than those\nfound in standard translation due to the need to effectively incorporate visual\nelements into the translation process to resolve ambiguities. In this work, we\ninvestigate to what extent multimodal large language models (LLMs) can provide\neffective manga translation, thereby assisting manga authors and publishers in\nreaching wider audiences. Specifically, we propose a methodology that leverages\nthe vision component of multimodal LLMs to improve translation quality and\nevaluate the impact of translation unit size, context length, and propose a\ntoken efficient approach for manga translation. Moreover, we introduce a new\nevaluation dataset -- the first parallel Japanese-Polish manga translation\ndataset -- as part of a benchmark to be used in future research. Finally, we\ncontribute an open-source software suite, enabling others to benchmark LLMs for\nmanga translation. Our findings demonstrate that our proposed methods achieve\nstate-of-the-art results for Japanese-English translation and set a new\nstandard for Japanese-Polish.\n","authors":["Philip Lippmann","Konrad Skublicki","Joshua Tanner","Shonosuke Ishiwatari","Jie Yang"],"pdf_url":"https://arxiv.org/pdf/2411.02589v1.pdf","comment":"Preprint. Under review"},{"id":"http://arxiv.org/abs/2411.02580v1","updated":"2024-11-04T20:23:03Z","published":"2024-11-04T20:23:03Z","title":"Social Support Detection from Social Media Texts","summary":"  Social support, conveyed through a multitude of interactions and platforms\nsuch as social media, plays a pivotal role in fostering a sense of belonging,\naiding resilience in the face of challenges, and enhancing overall well-being.\nThis paper introduces Social Support Detection (SSD) as a Natural language\nprocessing (NLP) task aimed at identifying supportive interactions within\nonline communities. The study presents the task of Social Support Detection\n(SSD) in three subtasks: two binary classification tasks and one multiclass\ntask, with labels detailed in the dataset section. We conducted experiments on\na dataset comprising 10,000 YouTube comments. Traditional machine learning\nmodels were employed, utilizing various feature combinations that encompass\nlinguistic, psycholinguistic, emotional, and sentiment information.\nAdditionally, we experimented with neural network-based models using various\nword embeddings to enhance the performance of our models across these\nsubtasks.The results reveal a prevalence of group-oriented support in online\ndialogues, reflecting broader societal patterns. The findings demonstrate the\neffectiveness of integrating psycholinguistic, emotional, and sentiment\nfeatures with n-grams in detecting social support and distinguishing whether it\nis directed toward an individual or a group. The best results for different\nsubtasks across all experiments range from 0.72 to 0.82.\n","authors":["Zahra Ahani","Moein Shahiki Tash","Fazlourrahman Balouchzahi","Luis Ramos","Grigori Sidorov","Alexander Gelbukh"],"pdf_url":"https://arxiv.org/pdf/2411.02580v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02571v1","updated":"2024-11-04T20:06:34Z","published":"2024-11-04T20:06:34Z","title":"MM-Embed: Universal Multimodal Retrieval with Multimodal LLMs","summary":"  State-of-the-art retrieval models typically address a straightforward search\nscenario, where retrieval tasks are fixed (e.g., finding a passage to answer a\nspecific question) and only a single modality is supported for both queries and\nretrieved results. This paper introduces techniques for advancing information\nretrieval with multimodal large language models (MLLMs), enabling a broader\nsearch scenario, termed universal multimodal retrieval, where multiple\nmodalities and diverse retrieval tasks are accommodated. To this end, we first\nstudy fine-tuning an MLLM as a bi-encoder retriever on 10 datasets with 16\nretrieval tasks. Our empirical results show that the fine-tuned MLLM retriever\nis capable of understanding challenging queries, composed of both text and\nimage, but underperforms a smaller CLIP retriever in cross-modal retrieval\ntasks due to modality bias from MLLMs. To address the issue, we propose\nmodality-aware hard negative mining to mitigate the modality bias exhibited by\nMLLM retrievers. Second, we propose to continually fine-tune the universal\nmultimodal retriever to enhance its text retrieval capability while maintaining\nmultimodal retrieval capability. As a result, our model, MM-Embed, achieves\nstate-of-the-art performance on the multimodal retrieval benchmark M-BEIR,\nwhich spans multiple domains and tasks, while also surpassing the\nstate-of-the-art text retrieval model, NV-Embed-v1, on MTEB retrieval\nbenchmark. Finally, we explore to prompt the off-the-shelf MLLMs as the\nzero-shot rerankers to refine the ranking of the candidates from the multimodal\nretriever. We find that through prompt-and-reranking, MLLMs can further improve\nmultimodal retrieval when the user queries (e.g., text-image composed queries)\nare more complex and challenging to understand. These findings also pave the\nway to advance universal multimodal retrieval in the future.\n","authors":["Sheng-Chieh Lin","Chankyu Lee","Mohammad Shoeybi","Jimmy Lin","Bryan Catanzaro","Wei Ping"],"pdf_url":"https://arxiv.org/pdf/2411.02571v1.pdf","comment":"We release the model weights at:\n  https://huggingface.co/nvidia/MM-Embed"},{"id":"http://arxiv.org/abs/2411.00023v2","updated":"2024-11-04T19:57:55Z","published":"2024-10-28T19:43:43Z","title":"Device-Directed Speech Detection for Follow-up Conversations Using Large\n  Language Models","summary":"  Follow-up conversations with virtual assistants (VAs) enable a user to\nseamlessly interact with a VA without the need to repeatedly invoke it using a\nkeyword (after the first query). Therefore, accurate Device-directed Speech\nDetection (DDSD) from the follow-up queries is critical for enabling\nnaturalistic user experience. To this end, we explore the notion of Large\nLanguage Models (LLMs) and model the first query when making inference about\nthe follow-ups (based on the ASR-decoded text), via prompting of a pretrained\nLLM, or by adapting a binary classifier on top of the LLM. In doing so, we also\nexploit the ASR uncertainty when designing the LLM prompts. We show on the\nreal-world dataset of follow-up conversations that this approach yields large\ngains (20-40% reduction in false alarms at 10% fixed false rejects) due to the\njoint modeling of the previous speech context and ASR uncertainty, compared to\nwhen follow-ups are modeled alone.\n","authors":[" Ognjen"," Rudovic","Pranay Dighe","Yi Su","Vineet Garg","Sameer Dharur","Xiaochuan Niu","Ahmed H. Abdelaziz","Saurabh Adya","Ahmed Tewfik"],"pdf_url":"https://arxiv.org/pdf/2411.00023v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.02083v7","updated":"2024-11-04T19:51:53Z","published":"2023-02-04T03:50:01Z","title":"Evaluating Large Language Models in Theory of Mind Tasks","summary":"  Eleven Large Language Models (LLMs) were assessed using a custom-made battery\nof false-belief tasks, considered a gold standard in testing Theory of Mind\n(ToM) in humans. The battery included 640 prompts spread across 40 diverse\ntasks, each one including a false-belief scenario, three closely matched\ntrue-belief control scenarios, and the reversed versions of all four. To solve\na single task, a model needed to correctly answer 16 prompts across all eight\nscenarios. Smaller and older models solved no tasks; GPT-3-davinci-003 (from\nNovember 2022) and ChatGPT-3.5-turbo (from March 2023) solved 20% of the tasks;\nChatGPT-4 (from June 2023) solved 75% of the tasks, matching the performance of\nsix-year-old children observed in past studies. We explore the potential\ninterpretation of these findings, including the intriguing possibility that\nToM, previously considered exclusive to humans, may have spontaneously emerged\nas a byproduct of LLMs' improving language skills.\n","authors":["Michal Kosinski"],"pdf_url":"https://arxiv.org/pdf/2302.02083v7.pdf","comment":"TRY RUNNING ToM EXPERIMENTS ON YOUR OWN: The code and tasks used in\n  this study are available at Colab\n  (https://colab.research.google.com/drive/1ZRtmw87CdA4xp24DNS_Ik_uA2ypaRnoU).\n  Don't worry if you are not an expert coder, you should be able to run this\n  code with no-to-minimum Python skills. Or copy-paste the tasks to ChatGPT's\n  web interface. Proceedings of the National Academy of Sciences (PNAS) 2024"},{"id":"http://arxiv.org/abs/2411.02558v1","updated":"2024-11-04T19:44:43Z","published":"2024-11-04T19:44:43Z","title":"Enhancing Risk Assessment in Transformers with Loss-at-Risk Functions","summary":"  In the financial field, precise risk assessment tools are essential for\ndecision-making. Recent studies have challenged the notion that traditional\nnetwork loss functions like Mean Square Error (MSE) are adequate, especially\nunder extreme risk conditions that can lead to significant losses during market\nupheavals. Transformers and Transformer-based models are now widely used in\nfinancial forecasting according to their outstanding performance in\ntime-series-related predictions. However, these models typically lack\nsensitivity to extreme risks and often underestimate great financial losses. To\naddress this problem, we introduce a novel loss function, the Loss-at-Risk,\nwhich incorporates Value at Risk (VaR) and Conditional Value at Risk (CVaR)\ninto Transformer models. This integration allows Transformer models to\nrecognize potential extreme losses and further improves their capability to\nhandle high-stakes financial decisions. Moreover, we conduct a series of\nexperiments with highly volatile financial datasets to demonstrate that our\nLoss-at-Risk function improves the Transformers' risk prediction and management\ncapabilities without compromising their decision-making accuracy or efficiency.\nThe results demonstrate that integrating risk-aware metrics during training\nenhances the Transformers' risk assessment capabilities while preserving their\ncore strengths in decision-making and reasoning across diverse scenarios.\n","authors":["Jinghan Zhang","Henry Xie","Xinhao Zhang","Kunpeng Liu"],"pdf_url":"https://arxiv.org/pdf/2411.02558v1.pdf","comment":"Accepted by ICKG 2024"},{"id":"http://arxiv.org/abs/2411.02556v1","updated":"2024-11-04T19:41:16Z","published":"2024-11-04T19:41:16Z","title":"Leveraging Transformer-Based Models for Predicting Inflection Classes of\n  Words in an Endangered Sami Language","summary":"  This paper presents a methodology for training a transformer-based model to\nclassify lexical and morphosyntactic features of Skolt Sami, an endangered\nUralic language characterized by complex morphology. The goal of our approach\nis to create an effective system for understanding and analyzing Skolt Sami,\ngiven the limited data availability and linguistic intricacies inherent to the\nlanguage. Our end-to-end pipeline includes data extraction, augmentation, and\ntraining a transformer-based model capable of predicting inflection classes.\nThe motivation behind this work is to support language preservation and\nrevitalization efforts for minority languages like Skolt Sami. Accurate\nclassification not only helps improve the state of Finite-State Transducers\n(FSTs) by providing greater lexical coverage but also contributes to systematic\nlinguistic documentation for researchers working with newly discovered words\nfrom literature and native speakers. Our model achieves an average weighted F1\nscore of 1.00 for POS classification and 0.81 for inflection class\nclassification. The trained model and code will be released publicly to\nfacilitate future research in endangered NLP.\n","authors":["Khalid Alnajjar","Mika Hämäläinen","Jack Rueter"],"pdf_url":"https://arxiv.org/pdf/2411.02556v1.pdf","comment":"IWCLUL 2024"},{"id":"http://arxiv.org/abs/2410.18163v2","updated":"2024-11-04T19:29:40Z","published":"2024-10-23T17:51:58Z","title":"Gazelle: An Instruction Dataset for Arabic Writing Assistance","summary":"  Writing has long been considered a hallmark of human intelligence and remains\na pinnacle task for artificial intelligence (AI) due to the intricate cognitive\nprocesses involved. Recently, rapid advancements in generative AI, particularly\nthrough the development of Large Language Models (LLMs), have significantly\ntransformed the landscape of writing assistance. However, underrepresented\nlanguages like Arabic encounter significant challenges in the development of\nadvanced AI writing tools, largely due to the limited availability of data.\nThis scarcity constrains the training of effective models, impeding the\ncreation of sophisticated writing assistance technologies. To address these\nissues, we present Gazelle, a comprehensive dataset for Arabic writing\nassistance. In addition, we offer an evaluation framework designed to enhance\nArabic writing assistance tools. Our human evaluation of leading LLMs,\nincluding GPT-4, GPT-4o, Cohere Command R+, and Gemini 1.5 Pro, highlights\ntheir respective strengths and limitations in addressing the challenges of\nArabic writing. Our findings underscore the need for continuous model training\nand dataset enrichment to manage the complexities of Arabic language\nprocessing, paving the way for more effective AI-powered Arabic writing tools.\n","authors":["Samar M. Magdy","Fakhraddin Alwajih","Sang Yun Kwon","Reem Abdel-Salam","Muhammad Abdul-Mageed"],"pdf_url":"https://arxiv.org/pdf/2410.18163v2.pdf","comment":"EMNLP2024 Finding Camara-ready version"},{"id":"http://arxiv.org/abs/2411.02545v1","updated":"2024-11-04T19:24:59Z","published":"2024-11-04T19:24:59Z","title":"TripletCLIP: Improving Compositional Reasoning of CLIP via Synthetic\n  Vision-Language Negatives","summary":"  Contrastive Language-Image Pretraining (CLIP) models maximize the mutual\ninformation between text and visual modalities to learn representations. This\nmakes the nature of the training data a significant factor in the efficacy of\nCLIP for downstream tasks. However, the lack of compositional diversity in\ncontemporary image-text datasets limits the compositional reasoning ability of\nCLIP. We show that generating ``hard'' negative captions via in-context\nlearning and synthesizing corresponding negative images with text-to-image\ngenerators offers a solution. We introduce a novel contrastive pre-training\nstrategy that leverages these hard negative captions and images in an\nalternating fashion to train CLIP. We demonstrate that our method, named\nTripletCLIP, when applied to existing datasets such as CC3M and CC12M, enhances\nthe compositional capabilities of CLIP, resulting in an absolute improvement of\nover 9% on the SugarCrepe benchmark on an equal computational budget, as well\nas improvements in zero-shot image classification and image retrieval. Our\ncode, models, and data are available at: https://tripletclip.github.io\n","authors":["Maitreya Patel","Abhiram Kusumba","Sheng Cheng","Changhoon Kim","Tejas Gokhale","Chitta Baral","Yezhou Yang"],"pdf_url":"https://arxiv.org/pdf/2411.02545v1.pdf","comment":"Accepted at: NeurIPS 2024 | Project Page:\n  https://tripletclip.github.io"},{"id":"http://arxiv.org/abs/2411.02538v1","updated":"2024-11-04T19:17:17Z","published":"2024-11-04T19:17:17Z","title":"MILU: A Multi-task Indic Language Understanding Benchmark","summary":"  Evaluating Large Language Models (LLMs) in low-resource and linguistically\ndiverse languages remains a significant challenge in NLP, particularly for\nlanguages using non-Latin scripts like those spoken in India. Existing\nbenchmarks predominantly focus on English, leaving substantial gaps in\nassessing LLM capabilities in these languages. We introduce MILU, a Multi task\nIndic Language Understanding Benchmark, a comprehensive evaluation benchmark\ndesigned to address this gap. MILU spans 8 domains and 42 subjects across 11\nIndic languages, reflecting both general and culturally specific knowledge.\nWith an India-centric design, incorporates material from regional and\nstate-level examinations, covering topics such as local history, arts,\nfestivals, and laws, alongside standard subjects like science and mathematics.\nWe evaluate over 42 LLMs, and find that current LLMs struggle with MILU, with\nGPT-4o achieving the highest average accuracy at 72 percent. Open multilingual\nmodels outperform language-specific fine-tuned models, which perform only\nslightly better than random baselines. Models also perform better in high\nresource languages as compared to low resource ones. Domain-wise analysis\nindicates that models perform poorly in culturally relevant areas like Arts and\nHumanities, Law and Governance compared to general fields like STEM. To the\nbest of our knowledge, MILU is the first of its kind benchmark focused on Indic\nlanguages, serving as a crucial step towards comprehensive cultural evaluation.\nAll code, benchmarks, and artifacts will be made publicly available to foster\nopen research.\n","authors":["Sshubam Verma","Mohammed Safi Ur Rahman Khan","Vishwajeet Kumar","Rudra Murthy","Jaydeep Sen"],"pdf_url":"https://arxiv.org/pdf/2411.02538v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02537v1","updated":"2024-11-04T19:16:53Z","published":"2024-11-04T19:16:53Z","title":"INQUIRE: A Natural World Text-to-Image Retrieval Benchmark","summary":"  We introduce INQUIRE, a text-to-image retrieval benchmark designed to\nchallenge multimodal vision-language models on expert-level queries. INQUIRE\nincludes iNaturalist 2024 (iNat24), a new dataset of five million natural world\nimages, along with 250 expert-level retrieval queries. These queries are paired\nwith all relevant images comprehensively labeled within iNat24, comprising\n33,000 total matches. Queries span categories such as species identification,\ncontext, behavior, and appearance, emphasizing tasks that require nuanced image\nunderstanding and domain expertise. Our benchmark evaluates two core retrieval\ntasks: (1) INQUIRE-Fullrank, a full dataset ranking task, and (2)\nINQUIRE-Rerank, a reranking task for refining top-100 retrievals. Detailed\nevaluation of a range of recent multimodal models demonstrates that INQUIRE\nposes a significant challenge, with the best models failing to achieve an\nmAP@50 above 50%. In addition, we show that reranking with more powerful\nmultimodal models can enhance retrieval performance, yet there remains a\nsignificant margin for improvement. By focusing on scientifically-motivated\necological challenges, INQUIRE aims to bridge the gap between AI capabilities\nand the needs of real-world scientific inquiry, encouraging the development of\nretrieval systems that can assist with accelerating ecological and biodiversity\nresearch. Our dataset and code are available at\nhttps://inquire-benchmark.github.io\n","authors":["Edward Vendrow","Omiros Pantazis","Alexander Shepard","Gabriel Brostow","Kate E. Jones","Oisin Mac Aodha","Sara Beery","Grant Van Horn"],"pdf_url":"https://arxiv.org/pdf/2411.02537v1.pdf","comment":"Published in NeurIPS 2024, Datasets and Benchmarks Track"},{"id":"http://arxiv.org/abs/2408.02248v2","updated":"2024-11-04T19:16:19Z","published":"2024-08-05T05:43:23Z","title":"ReDel: A Toolkit for LLM-Powered Recursive Multi-Agent Systems","summary":"  Recently, there has been increasing interest in using Large Language Models\n(LLMs) to construct complex multi-agent systems to perform tasks such as\ncompiling literature reviews, drafting consumer reports, and planning\nvacations. Many tools and libraries exist for helping create such systems,\nhowever none support recursive multi-agent systems -- where the models\nthemselves flexibly decide when to delegate tasks and how to organize their\ndelegation structure. In this work, we introduce ReDel: a toolkit for recursive\nmulti-agent systems that supports custom tool-use, delegation schemes,\nevent-based logging, and interactive replay in an easy-to-use web interface. We\nshow that, using ReDel, we are able to easily identify potential areas of\nimprovements through the visualization and debugging tools. Our code,\ndocumentation, and PyPI package are open-source and free to use under the MIT\nlicense at https://github.com/zhudotexe/redel.\n","authors":["Andrew Zhu","Liam Dugan","Chris Callison-Burch"],"pdf_url":"https://arxiv.org/pdf/2408.02248v2.pdf","comment":"EMNLP 2024 (Demo Track)"},{"id":"http://arxiv.org/abs/2410.21314v2","updated":"2024-11-04T19:12:54Z","published":"2024-10-25T21:44:51Z","title":"Decoding Diffusion: A Scalable Framework for Unsupervised Analysis of\n  Latent Space Biases and Representations Using Natural Language Prompts","summary":"  Recent advances in image generation have made diffusion models powerful tools\nfor creating high-quality images. However, their iterative denoising process\nmakes understanding and interpreting their semantic latent spaces more\nchallenging than other generative models, such as GANs. Recent methods have\nattempted to address this issue by identifying semantically meaningful\ndirections within the latent space. However, they often need manual\ninterpretation or are limited in the number of vectors that can be trained,\nrestricting their scope and utility. This paper proposes a novel framework for\nunsupervised exploration of diffusion latent spaces. We directly leverage\nnatural language prompts and image captions to map latent directions. This\nmethod allows for the automatic understanding of hidden features and supports a\nbroader range of analysis without the need to train specific vectors. Our\nmethod provides a more scalable and interpretable understanding of the semantic\nknowledge encoded within diffusion models, facilitating comprehensive analysis\nof latent biases and the nuanced representations these models learn.\nExperimental results show that our framework can uncover hidden patterns and\nassociations in various domains, offering new insights into the\ninterpretability of diffusion model latent spaces.\n","authors":["E. Zhixuan Zeng","Yuhao Chen","Alexander Wong"],"pdf_url":"https://arxiv.org/pdf/2410.21314v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02536v1","updated":"2024-11-04T19:12:27Z","published":"2024-11-04T19:12:27Z","title":"Towards Leveraging News Media to Support Impact Assessment of AI\n  Technologies","summary":"  Expert-driven frameworks for impact assessments (IAs) may inadvertently\noverlook the effects of AI technologies on the public's social behavior,\npolicy, and the cultural and geographical contexts shaping the perception of AI\nand the impacts around its use. This research explores the potentials of\nfine-tuning LLMs on negative impacts of AI reported in a diverse sample of\narticles from 266 news domains spanning 30 countries around the world to\nincorporate more diversity into IAs. Our findings highlight (1) the potential\nof fine-tuned open-source LLMs in supporting IA of AI technologies by\ngenerating high-quality negative impacts across four qualitative dimensions:\ncoherence, structure, relevance, and plausibility, and (2) the efficacy of\nsmall open-source LLM (Mistral-7B) fine-tuned on impacts from news media in\ncapturing a wider range of categories of impacts that GPT-4 had gaps in\ncovering.\n","authors":["Mowafak Allaham","Kimon Kieslich","Nicholas Diakopoulos"],"pdf_url":"https://arxiv.org/pdf/2411.02536v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2401.18028"},{"id":"http://arxiv.org/abs/2411.02528v1","updated":"2024-11-04T19:05:49Z","published":"2024-11-04T19:05:49Z","title":"What Goes Into a LM Acceptability Judgment? Rethinking the Impact of\n  Frequency and Length","summary":"  When comparing the linguistic capabilities of language models (LMs) with\nhumans using LM probabilities, factors such as the length of the sequence and\nthe unigram frequency of lexical items have a significant effect on LM\nprobabilities in ways that humans are largely robust to. Prior works in\ncomparing LM and human acceptability judgments treat these effects uniformly\nacross models, making a strong assumption that models require the same degree\nof adjustment to control for length and unigram frequency effects. We propose\nMORCELA, a new linking theory between LM scores and acceptability judgments\nwhere the optimal level of adjustment for these effects is estimated from data\nvia learned parameters for length and unigram frequency. We first show that\nMORCELA outperforms a commonly used linking theory for acceptability--SLOR\n(Pauls and Klein, 2012; Lau et al. 2017)--across two families of transformer\nLMs (Pythia and OPT). Furthermore, we demonstrate that the assumed degrees of\nadjustment in SLOR for length and unigram frequency overcorrect for these\nconfounds, and that larger models require a lower relative degree of adjustment\nfor unigram frequency, though a significant amount of adjustment is still\nnecessary for all models. Finally, our subsequent analysis shows that larger\nLMs' lower susceptibility to frequency effects can be explained by an ability\nto better predict rarer words in context.\n","authors":["Lindia Tjuatja","Graham Neubig","Tal Linzen","Sophie Hao"],"pdf_url":"https://arxiv.org/pdf/2411.02528v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02481v1","updated":"2024-11-04T18:54:39Z","published":"2024-11-04T18:54:39Z","title":"Fantastic LLMs for Preference Data Annotation and How to (not) Find Them","summary":"  Preference tuning of large language models (LLMs) relies on high-quality\nhuman preference data, which is often expensive and time-consuming to gather.\nWhile existing methods can use trained reward models or proprietary model as\njudges for preference annotation, they have notable drawbacks: training reward\nmodels remain dependent on initial human data, and using proprietary model\nimposes license restrictions that inhibits commercial usage. In this paper, we\nintroduce customized density ratio (CDR) that leverages open-source LLMs for\ndata annotation, offering an accessible and effective solution. Our approach\nuses the log-density ratio between a well-aligned LLM and a less aligned LLM as\na reward signal. We explores 221 different LLMs pairs and empirically\ndemonstrate that increasing the performance gap between paired LLMs correlates\nwith better reward generalization. Furthermore, we show that tailoring the\ndensity ratio reward function with specific criteria and preference exemplars\nenhances performance across domains and within target areas.\n  In our experiment using density ratio from a pair of Mistral-7B models, CDR\nachieves a RewardBench score of 82.6, outperforming the best in-class trained\nreward functions and demonstrating competitive performance against SoTA models\nin Safety (91.0) and Reasoning (88.0) domains. We use CDR to annotate an\non-policy preference dataset with which we preference tune Llama-3-8B-Instruct\nwith SimPO. The final model achieves a 37.4% (+15.1%) win rate on ArenaHard and\na 40.7% (+17.8%) win rate on Length-Controlled AlpacaEval 2.0, along with a\nscore of 8.0 on MT-Bench.\n","authors":["Guangxuan Xu","Kai Xu","Shivchander Sudalairaj","Hao Wang","Akash Srivastava"],"pdf_url":"https://arxiv.org/pdf/2411.02481v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08986v3","updated":"2024-11-04T18:53:29Z","published":"2022-12-18T01:57:30Z","title":"Low-Resource Authorship Style Transfer: Can Non-Famous Authors Be\n  Imitated?","summary":"  Authorship style transfer involves altering text to match the style of a\ntarget author whilst preserving the original meaning. Existing unsupervised\napproaches like STRAP have largely focused on style transfer to target authors\nwith many examples of their writing style in books, speeches, or other\npublished works. This high-resource training data requirement (often greater\nthan 100,000 words) makes these approaches primarily useful for style transfer\nto published authors, politicians, or other well-known figures and authorship\nstyles, while style transfer to non-famous authors has not been well-studied.\nWe introduce the low-resource authorship style transfer task, a more\nchallenging class of authorship style transfer where only a limited amount of\ntext in the target author's style may exist. In our experiments, we\nspecifically choose source and target authors from Reddit and style transfer\ntheir Reddit posts, limiting ourselves to just 16 posts (on average ~500 words)\nof the target author's style. Style transfer accuracy is typically measured by\nhow often a classifier or human judge will classify an output as written by the\ntarget author. Recent authorship representations models excel at authorship\nidentification even with just a few writing samples, making automatic\nevaluation of this task possible for the first time through evaluation metrics\nwe propose. Our results establish an in-context learning technique we develop\nas the strongest baseline, though we find current approaches do not yet achieve\nmastery of this challenging task. We release our data and implementations to\nencourage further investigation.\n","authors":["Ajay Patel","Nicholas Andrews","Chris Callison-Burch"],"pdf_url":"https://arxiv.org/pdf/2212.08986v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.08030v5","updated":"2024-11-04T18:25:12Z","published":"2023-09-14T21:07:53Z","title":"AV2Wav: Diffusion-Based Re-synthesis from Continuous Self-supervised\n  Features for Audio-Visual Speech Enhancement","summary":"  Speech enhancement systems are typically trained using pairs of clean and\nnoisy speech. In audio-visual speech enhancement (AVSE), there is not as much\nground-truth clean data available; most audio-visual datasets are collected in\nreal-world environments with background noise and reverberation, hampering the\ndevelopment of AVSE. In this work, we introduce AV2Wav, a resynthesis-based\naudio-visual speech enhancement approach that can generate clean speech despite\nthe challenges of real-world training data. We obtain a subset of nearly clean\nspeech from an audio-visual corpus using a neural quality estimator, and then\ntrain a diffusion model on this subset to generate waveforms conditioned on\ncontinuous speech representations from AV-HuBERT with noise-robust training. We\nuse continuous rather than discrete representations to retain prosody and\nspeaker information. With this vocoding task alone, the model can perform\nspeech enhancement better than a masking-based baseline. We further fine-tune\nthe diffusion model on clean/noisy utterance pairs to improve the performance.\nOur approach outperforms a masking-based baseline in terms of both automatic\nmetrics and a human listening test and is close in quality to the target speech\nin the listening test. Audio samples can be found at\nhttps://home.ttic.edu/~jcchou/demo/avse/avse_demo.html.\n","authors":["Ju-Chieh Chou","Chung-Ming Chien","Karen Livescu"],"pdf_url":"https://arxiv.org/pdf/2309.08030v5.pdf","comment":"extended version for the accepted paper at ICASSP 2024"},{"id":"http://arxiv.org/abs/2411.02476v1","updated":"2024-11-04T18:06:36Z","published":"2024-11-04T18:06:36Z","title":"A Comparative Analysis of Instruction Fine-Tuning LLMs for Financial\n  Text Classification","summary":"  Large Language Models (LLMs) have demonstrated impressive capabilities across\ndiverse Natural Language Processing (NLP) tasks, including language\nunderstanding, reasoning, and generation. However, general-domain LLMs often\nstruggle with financial tasks due to the technical and specialized nature of\nfinancial texts. This study investigates the efficacy of instruction\nfine-tuning smaller-scale LLMs, including Mistral-7B, Llama3-8B, and Phi3-mini,\nto enhance their performance in financial text classification tasks. We\nfine-tuned both instruction-tuned and base models across four financial\nclassification tasks, achieving significant improvements in task-specific\nperformance. Furthermore, we evaluated the zero-shot capabilities of these\nfine-tuned models on three unseen complex financial tasks, including argument\nclassification, deal completeness classification, and causal classification.\nOur results indicate while base model fine-tuning led to greater degradation,\ninstruction-tuned models maintained more robust performance. To address this\ndegradation, we employed model merging techniques, integrating single-task\ndomain-specific fine-tuned models with the base model. Using this merging\nmethod resulted in significant enhancements in zero-shot performance, even\nexceeding the original model's accuracy on certain datasets. Our findings\nunderscore the effectiveness of instruction fine-tuning and model merging for\nadapting LLMs to specialized financial text classification tasks.\n","authors":["Sorouralsadat Fatemi","Yuheng Hu","Maryam Mousavi"],"pdf_url":"https://arxiv.org/pdf/2411.02476v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.15014v2","updated":"2024-11-04T16:00:39Z","published":"2023-05-24T10:57:53Z","title":"Unlocking Temporal Question Answering for Large Language Models with\n  Tailor-Made Reasoning Logic","summary":"  The temporal aspect is a significant dimension of our reality. We notice the\nchallenge that large language models (LLMs) face when engaging in temporal\nreasoning. Our preliminary experiments show that methods involving the\ngeneration of intermediate reasoning steps, such as chain-of-thought and\nprogram-aided language models, do not consistently boost the performance of\ncomplex temporal question-answering tasks. This limitation can be attributed to\nthe LLMs' inadequate understanding of temporal information. To address this\nproblem, we propose TempLogic, a novel framework designed specifically for\ntemporal question-answering tasks across three levels of reasoning. TempLogic\nincorporates retrieval-guided context distillation, temporal data extraction,\nand tailor-made logic reasoning. Extensive experiments and analysis demonstrate\nthe effectiveness of our framework in solving intricate time-bound reasoning\ntasks.\n","authors":["Xingxuan Li","Liying Cheng","Qingyu Tan","Hwee Tou Ng","Shafiq Joty","Lidong Bing"],"pdf_url":"https://arxiv.org/pdf/2305.15014v2.pdf","comment":"Work in progress"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2411.02397v1","updated":"2024-11-04T18:59:44Z","published":"2024-11-04T18:59:44Z","title":"Adaptive Caching for Faster Video Generation with Diffusion Transformers","summary":"  Generating temporally-consistent high-fidelity videos can be computationally\nexpensive, especially over longer temporal spans. More-recent Diffusion\nTransformers (DiTs) -- despite making significant headway in this context --\nhave only heightened such challenges as they rely on larger models and heavier\nattention mechanisms, resulting in slower inference speeds. In this paper, we\nintroduce a training-free method to accelerate video DiTs, termed Adaptive\nCaching (AdaCache), which is motivated by the fact that \"not all videos are\ncreated equal\": meaning, some videos require fewer denoising steps to attain a\nreasonable quality than others. Building on this, we not only cache\ncomputations through the diffusion process, but also devise a caching schedule\ntailored to each video generation, maximizing the quality-latency trade-off. We\nfurther introduce a Motion Regularization (MoReg) scheme to utilize video\ninformation within AdaCache, essentially controlling the compute allocation\nbased on motion content. Altogether, our plug-and-play contributions grant\nsignificant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video\ngeneration) without sacrificing the generation quality, across multiple video\nDiT baselines.\n","authors":["Kumara Kahatapitiya","Haozhe Liu","Sen He","Ding Liu","Menglin Jia","Michael S. Ryoo","Tian Xie"],"pdf_url":"https://arxiv.org/pdf/2411.02397v1.pdf","comment":"Project-page is available at https://adacache-dit.github.io"},{"id":"http://arxiv.org/abs/2411.02394v1","updated":"2024-11-04T18:59:05Z","published":"2024-11-04T18:59:05Z","title":"AutoVFX: Physically Realistic Video Editing from Natural Language\n  Instructions","summary":"  Modern visual effects (VFX) software has made it possible for skilled artists\nto create imagery of virtually anything. However, the creation process remains\nlaborious, complex, and largely inaccessible to everyday users. In this work,\nwe present AutoVFX, a framework that automatically creates realistic and\ndynamic VFX videos from a single video and natural language instructions. By\ncarefully integrating neural scene modeling, LLM-based code generation, and\nphysical simulation, AutoVFX is able to provide physically-grounded,\nphotorealistic editing effects that can be controlled directly using natural\nlanguage instructions. We conduct extensive experiments to validate AutoVFX's\nefficacy across a diverse spectrum of videos and instructions. Quantitative and\nqualitative results suggest that AutoVFX outperforms all competing methods by a\nlarge margin in generative quality, instruction alignment, editing versatility,\nand physical plausibility.\n","authors":["Hao-Yu Hsu","Zhi-Hao Lin","Albert Zhai","Hongchi Xia","Shenlong Wang"],"pdf_url":"https://arxiv.org/pdf/2411.02394v1.pdf","comment":"Project page: https://haoyuhsu.github.io/autovfx-website/"},{"id":"http://arxiv.org/abs/2411.02395v1","updated":"2024-11-04T18:59:05Z","published":"2024-11-04T18:59:05Z","title":"Training-free Regional Prompting for Diffusion Transformers","summary":"  Diffusion models have demonstrated excellent capabilities in text-to-image\ngeneration. Their semantic understanding (i.e., prompt following) ability has\nalso been greatly improved with large language models (e.g., T5, Llama).\nHowever, existing models cannot perfectly handle long and complex text prompts,\nespecially when the text prompts contain various objects with numerous\nattributes and interrelated spatial relationships. While many regional\nprompting methods have been proposed for UNet-based models (SD1.5, SDXL), but\nthere are still no implementations based on the recent Diffusion Transformer\n(DiT) architecture, such as SD3 and FLUX.1.In this report, we propose and\nimplement regional prompting for FLUX.1 based on attention manipulation, which\nenables DiT with fined-grained compositional text-to-image generation\ncapability in a training-free manner. Code is available at\nhttps://github.com/antonioo-c/Regional-Prompting-FLUX.\n","authors":["Anthony Chen","Jianjin Xu","Wenzhao Zheng","Gaole Dai","Yida Wang","Renrui Zhang","Haofan Wang","Shanghang Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.02395v1.pdf","comment":"Code is available at\n  https://github.com/antonioo-c/Regional-Prompting-FLUX"},{"id":"http://arxiv.org/abs/2411.02393v1","updated":"2024-11-04T18:58:01Z","published":"2024-11-04T18:58:01Z","title":"Adaptive Length Image Tokenization via Recurrent Allocation","summary":"  Current vision systems typically assign fixed-length representations to\nimages, regardless of the information content. This contrasts with human\nintelligence - and even large language models - which allocate varying\nrepresentational capacities based on entropy, context and familiarity. Inspired\nby this, we propose an approach to learn variable-length token representations\nfor 2D images. Our encoder-decoder architecture recursively processes 2D image\ntokens, distilling them into 1D latent tokens over multiple iterations of\nrecurrent rollouts. Each iteration refines the 2D tokens, updates the existing\n1D latent tokens, and adaptively increases representational capacity by adding\nnew tokens. This enables compression of images into a variable number of\ntokens, ranging from 32 to 256. We validate our tokenizer using reconstruction\nloss and FID metrics, demonstrating that token count aligns with image entropy,\nfamiliarity and downstream task requirements. Recurrent token processing with\nincreasing representational capacity in each iteration shows signs of token\nspecialization, revealing potential for object / part discovery.\n","authors":["Shivam Duggal","Phillip Isola","Antonio Torralba","William T. Freeman"],"pdf_url":"https://arxiv.org/pdf/2411.02393v1.pdf","comment":"Code at: https://github.com/ShivamDuggal4/adaptive-length-tokenizer"},{"id":"http://arxiv.org/abs/2411.02385v1","updated":"2024-11-04T18:53:05Z","published":"2024-11-04T18:53:05Z","title":"How Far is Video Generation from World Model: A Physical Law Perspective","summary":"  OpenAI's Sora highlights the potential of video generation for developing\nworld models that adhere to fundamental physical laws. However, the ability of\nvideo generation models to discover such laws purely from visual data without\nhuman priors can be questioned. A world model learning the true law should give\npredictions robust to nuances and correctly extrapolate on unseen scenarios. In\nthis work, we evaluate across three key scenarios: in-distribution,\nout-of-distribution, and combinatorial generalization. We developed a 2D\nsimulation testbed for object movement and collisions to generate videos\ndeterministically governed by one or more classical mechanics laws. This\nprovides an unlimited supply of data for large-scale experimentation and\nenables quantitative evaluation of whether the generated videos adhere to\nphysical laws. We trained diffusion-based video generation models to predict\nobject movements based on initial frames. Our scaling experiments show perfect\ngeneralization within the distribution, measurable scaling behavior for\ncombinatorial generalization, but failure in out-of-distribution scenarios.\nFurther experiments reveal two key insights about the generalization mechanisms\nof these models: (1) the models fail to abstract general physical rules and\ninstead exhibit \"case-based\" generalization behavior, i.e., mimicking the\nclosest training example; (2) when generalizing to new cases, models are\nobserved to prioritize different factors when referencing training data: color\n> size > velocity > shape. Our study suggests that scaling alone is\ninsufficient for video generation models to uncover fundamental physical laws,\ndespite its role in Sora's broader success. See our project page at\nhttps://phyworld.github.io\n","authors":["Bingyi Kang","Yang Yue","Rui Lu","Zhijie Lin","Yang Zhao","Kaixin Wang","Gao Huang","Jiashi Feng"],"pdf_url":"https://arxiv.org/pdf/2411.02385v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2410.23262v2","updated":"2024-11-04T18:44:20Z","published":"2024-10-30T17:46:31Z","title":"EMMA: End-to-End Multimodal Model for Autonomous Driving","summary":"  We introduce EMMA, an End-to-end Multimodal Model for Autonomous driving.\nBuilt on a multi-modal large language model foundation, EMMA directly maps raw\ncamera sensor data into various driving-specific outputs, including planner\ntrajectories, perception objects, and road graph elements. EMMA maximizes the\nutility of world knowledge from the pre-trained large language models, by\nrepresenting all non-sensor inputs (e.g. navigation instructions and ego\nvehicle status) and outputs (e.g. trajectories and 3D locations) as natural\nlanguage text. This approach allows EMMA to jointly process various driving\ntasks in a unified language space, and generate the outputs for each task using\ntask-specific prompts. Empirically, we demonstrate EMMA's effectiveness by\nachieving state-of-the-art performance in motion planning on nuScenes as well\nas competitive results on the Waymo Open Motion Dataset (WOMD). EMMA also\nyields competitive results for camera-primary 3D object detection on the Waymo\nOpen Dataset (WOD). We show that co-training EMMA with planner trajectories,\nobject detection, and road graph tasks yields improvements across all three\ndomains, highlighting EMMA's potential as a generalist model for autonomous\ndriving applications. However, EMMA also exhibits certain limitations: it can\nprocess only a small amount of image frames, does not incorporate accurate 3D\nsensing modalities like LiDAR or radar and is computationally expensive. We\nhope that our results will inspire further research to mitigate these issues\nand to further evolve the state of the art in autonomous driving model\narchitectures.\n","authors":["Jyh-Jing Hwang","Runsheng Xu","Hubert Lin","Wei-Chih Hung","Jingwei Ji","Kristy Choi","Di Huang","Tong He","Paul Covington","Benjamin Sapp","Yin Zhou","James Guo","Dragomir Anguelov","Mingxing Tan"],"pdf_url":"https://arxiv.org/pdf/2410.23262v2.pdf","comment":"Blog post: https://waymo.com/blog/2024/10/introducing-emma/"},{"id":"http://arxiv.org/abs/2411.02372v1","updated":"2024-11-04T18:40:46Z","published":"2024-11-04T18:40:46Z","title":"Learning General-Purpose Biomedical Volume Representations using\n  Randomized Synthesis","summary":"  Current volumetric biomedical foundation models struggle to generalize as\npublic 3D datasets are small and do not cover the broad diversity of medical\nprocedures, conditions, anatomical regions, and imaging protocols. We address\nthis by creating a representation learning method that instead anticipates\nstrong domain shifts at training time itself. We first propose a data engine\nthat synthesizes highly variable training samples that enable generalization to\nnew biomedical contexts. To then train a single 3D network for any voxel-level\ntask, we develop a contrastive learning method that pretrains the network to be\nstable against nuisance imaging variation simulated by the data engine, a key\ninductive bias for generalization. This network's features can be used as\nrobust representations of input images for downstream tasks and its weights\nprovide a strong, dataset-agnostic initialization for finetuning on new\ndatasets. As a result, we set new standards across both multimodality\nregistration and few-shot segmentation, a first for any 3D biomedical vision\nmodel, all without (pre-)training on any existing dataset of real images.\n","authors":["Neel Dey","Benjamin Billot","Hallee E. Wong","Clinton J. Wang","Mengwei Ren","P. Ellen Grant","Adrian V. Dalca","Polina Golland"],"pdf_url":"https://arxiv.org/pdf/2411.02372v1.pdf","comment":"Code and model weights available at\n  https://github.com/neel-dey/anatomix"},{"id":"http://arxiv.org/abs/2411.02354v1","updated":"2024-11-04T18:21:56Z","published":"2024-11-04T18:21:56Z","title":"Machine learning identification of maternal inflammatory response and\n  histologic choroamnionitis from placental membrane whole slide images","summary":"  The placenta forms a critical barrier to infection through pregnancy, labor\nand, delivery. Inflammatory processes in the placenta have short-term, and\nlong-term consequences for offspring health. Digital pathology and machine\nlearning can play an important role in understanding placental inflammation,\nand there have been very few investigations into methods for predicting and\nunderstanding Maternal Inflammatory Response (MIR). This work intends to\ninvestigate the potential of using machine learning to understand MIR based on\nwhole slide images (WSI), and establish early benchmarks. To that end, we use\nMultiple Instance Learning framework with 3 feature extractors: ImageNet-based\nEfficientNet-v2s, and 2 histopathology foundation models, UNI and Phikon to\ninvestigate predictability of MIR stage from histopathology WSIs. We also\ninterpret predictions from these models using the learned attention maps from\nthese models. We also use the MIL framework for predicting white blood cells\ncount (WBC) and maximum fever temperature ($T_{max}$). Attention-based MIL\nmodels are able to classify MIR with a balanced accuracy of up to 88.5% with a\nCohen's Kappa ($\\kappa$) of up to 0.772. Furthermore, we found that the\npathology foundation models (UNI and Phikon) are both able to achieve higher\nperformance with balanced accuracy and $\\kappa$, compared to ImageNet-based\nfeature extractor (EfficientNet-v2s). For WBC and $T_{max}$ prediction, we\nfound mild correlation between actual values and those predicted from\nhistopathology WSIs. We used MIL framework for predicting MIR stage from WSIs,\nand compared effectiveness of foundation models as feature extractors, with\nthat of an ImageNet-based model. We further investigated model failure cases\nand found them to be either edge cases prone to interobserver variability,\nexamples of pathologist's overreach, or mislabeled due to processing errors.\n","authors":["Abhishek Sharma","Ramin Nateghi","Marina Ayad","Lee A. D. Cooper","Jeffery A. Goldstein"],"pdf_url":"https://arxiv.org/pdf/2411.02354v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02347v1","updated":"2024-11-04T18:17:44Z","published":"2024-11-04T18:17:44Z","title":"Physically Based Neural Bidirectional Reflectance Distribution Function","summary":"  We introduce the physically based neural bidirectional reflectance\ndistribution function (PBNBRDF), a novel, continuous representation for\nmaterial appearance based on neural fields. Our model accurately reconstructs\nreal-world materials while uniquely enforcing physical properties for realistic\nBRDFs, specifically Helmholtz reciprocity via reparametrization and energy\npassivity via efficient analytical integration. We conduct a systematic\nanalysis demonstrating the benefits of adhering to these physical laws on the\nvisual quality of reconstructed materials. Additionally, we enhance the color\naccuracy of neural BRDFs by introducing chromaticity enforcement supervising\nthe norms of RGB channels. Through both qualitative and quantitative\nexperiments on multiple databases of measured real-world BRDFs, we show that\nadhering to these physical constraints enables neural fields to more faithfully\nand stably represent the original data and achieve higher rendering quality.\n","authors":["Chenliang Zhou","Alejandro Sztrajman","Gilles Rainer","Fangcheng Zhong","Fazilet Gokbudak","Zhilin Guo","Weihao Xia","Rafal Mantiuk","Cengiz Oztireli"],"pdf_url":"https://arxiv.org/pdf/2411.02347v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02336v1","updated":"2024-11-04T17:59:39Z","published":"2024-11-04T17:59:39Z","title":"MVPaint: Synchronized Multi-View Diffusion for Painting Anything 3D","summary":"  Texturing is a crucial step in the 3D asset production workflow, which\nenhances the visual appeal and diversity of 3D assets. Despite recent\nadvancements in Text-to-Texture (T2T) generation, existing methods often yield\nsubpar results, primarily due to local discontinuities, inconsistencies across\nmultiple views, and their heavy dependence on UV unwrapping outcomes. To tackle\nthese challenges, we propose a novel generation-refinement 3D texturing\nframework called MVPaint, which can generate high-resolution, seamless textures\nwhile emphasizing multi-view consistency. MVPaint mainly consists of three key\nmodules. 1) Synchronized Multi-view Generation (SMG). Given a 3D mesh model,\nMVPaint first simultaneously generates multi-view images by employing an SMG\nmodel, which leads to coarse texturing results with unpainted parts due to\nmissing observations. 2) Spatial-aware 3D Inpainting (S3I). To ensure complete\n3D texturing, we introduce the S3I method, specifically designed to effectively\ntexture previously unobserved areas. 3) UV Refinement (UVR). Furthermore,\nMVPaint employs a UVR module to improve the texture quality in the UV space,\nwhich first performs a UV-space Super-Resolution, followed by a Spatial-aware\nSeam-Smoothing algorithm for revising spatial texturing discontinuities caused\nby UV unwrapping. Moreover, we establish two T2T evaluation benchmarks: the\nObjaverse T2T benchmark and the GSO T2T benchmark, based on selected\nhigh-quality 3D meshes from the Objaverse dataset and the entire GSO dataset,\nrespectively. Extensive experimental results demonstrate that MVPaint surpasses\nexisting state-of-the-art methods. Notably, MVPaint could generate\nhigh-fidelity textures with minimal Janus issues and highly enhanced cross-view\nconsistency.\n","authors":["Wei Cheng","Juncheng Mu","Xianfang Zeng","Xin Chen","Anqi Pang","Chi Zhang","Zhibin Wang","Bin Fu","Gang Yu","Ziwei Liu","Liang Pan"],"pdf_url":"https://arxiv.org/pdf/2411.02336v1.pdf","comment":"Project Page: https://mvpaint.github.io"},{"id":"http://arxiv.org/abs/2411.02334v1","updated":"2024-11-04T17:58:54Z","published":"2024-11-04T17:58:54Z","title":"Diffusion-based Generative Multicasting with Intent-aware Semantic\n  Decomposition","summary":"  Generative diffusion models (GDMs) have recently shown great success in\nsynthesizing multimedia signals with high perceptual quality enabling highly\nefficient semantic communications in future wireless networks. In this paper,\nwe develop an intent-aware generative semantic multicasting framework utilizing\npre-trained diffusion models. In the proposed framework, the transmitter\ndecomposes the source signal to multiple semantic classes based on the\nmulti-user intent, i.e. each user is assumed to be interested in details of\nonly a subset of the semantic classes. The transmitter then sends to each user\nonly its intended classes, and multicasts a highly compressed semantic map to\nall users over shared wireless resources that allows them to locally synthesize\nthe other classes, i.e. non-intended classes, utilizing pre-trained diffusion\nmodels. The signal retrieved at each user is thereby partially reconstructed\nand partially synthesized utilizing the received semantic map. This improves\nutilization of the wireless resources, with better preserving privacy of the\nnon-intended classes. We design a communication/computation-aware scheme for\nper-class adaptation of the communication parameters, such as the transmission\npower and compression rate to minimize the total latency of retrieving signals\nat multiple receivers, tailored to the prevailing channel conditions as well as\nthe users reconstruction/synthesis distortion/perception requirements. The\nsimulation results demonstrate significantly reduced per-user latency compared\nwith non-generative and intent-unaware multicasting benchmarks while\nmaintaining high perceptual quality of the signals retrieved at the users.\n","authors":["Xinkai Liu","Mahdi Boloursaz Mashhadi","Li Qiao","Yi Ma","Rahim Tafazolli","Mehdi Bennis"],"pdf_url":"https://arxiv.org/pdf/2411.02334v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02327v1","updated":"2024-11-04T17:50:36Z","published":"2024-11-04T17:50:36Z","title":"PPLLaVA: Varied Video Sequence Understanding With Prompt Guidance","summary":"  The past year has witnessed the significant advancement of video-based large\nlanguage models. However, the challenge of developing a unified model for both\nshort and long video understanding remains unresolved. Most existing video LLMs\ncannot handle hour-long videos, while methods custom for long videos tend to be\nineffective for shorter videos and images. In this paper, we identify the key\nissue as the redundant content in videos. To address this, we propose a novel\npooling strategy that simultaneously achieves token compression and\ninstruction-aware visual feature aggregation. Our model is termed Prompt-guided\nPooling LLaVA, or PPLLaVA for short. Specifically, PPLLaVA consists of three\ncore components: the CLIP-based visual-prompt alignment that extracts visual\ninformation relevant to the user's instructions, the prompt-guided pooling that\ncompresses the visual sequence to arbitrary scales using convolution-style\npooling, and the clip context extension designed for lengthy prompt common in\nvisual dialogue. Moreover, our codebase also integrates the most advanced video\nDirect Preference Optimization (DPO) and visual interleave training. Extensive\nexperiments have validated the performance of our model. With superior\nthroughput and only 1024 visual context, PPLLaVA achieves better results on\nimage benchmarks as a video LLM, while achieving state-of-the-art performance\nacross various video benchmarks, excelling in tasks ranging from caption\ngeneration to multiple-choice questions, and handling video lengths from\nseconds to hours. Codes have been available at\nhttps://github.com/farewellthree/PPLLaVA.\n","authors":["Ruyang Liu","Haoran Tang","Haibo Liu","Yixiao Ge","Ying Shan","Chen Li","Jiankun Yang"],"pdf_url":"https://arxiv.org/pdf/2411.02327v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02319v1","updated":"2024-11-04T17:45:44Z","published":"2024-11-04T17:45:44Z","title":"GenXD: Generating Any 3D and 4D Scenes","summary":"  Recent developments in 2D visual generation have been remarkably successful.\nHowever, 3D and 4D generation remain challenging in real-world applications due\nto the lack of large-scale 4D data and effective model design. In this paper,\nwe propose to jointly investigate general 3D and 4D generation by leveraging\ncamera and object movements commonly observed in daily life. Due to the lack of\nreal-world 4D data in the community, we first propose a data curation pipeline\nto obtain camera poses and object motion strength from videos. Based on this\npipeline, we introduce a large-scale real-world 4D scene dataset: CamVid-30K.\nBy leveraging all the 3D and 4D data, we develop our framework, GenXD, which\nallows us to produce any 3D or 4D scene. We propose multiview-temporal modules,\nwhich disentangle camera and object movements, to seamlessly learn from both 3D\nand 4D data. Additionally, GenXD employs masked latent conditions to support a\nvariety of conditioning views. GenXD can generate videos that follow the camera\ntrajectory as well as consistent 3D views that can be lifted into 3D\nrepresentations. We perform extensive evaluations across various real-world and\nsynthetic datasets, demonstrating GenXD's effectiveness and versatility\ncompared to previous methods in 3D and 4D generation.\n","authors":["Yuyang Zhao","Chung-Ching Lin","Kevin Lin","Zhiwen Yan","Linjie Li","Zhengyuan Yang","Jianfeng Wang","Gim Hee Lee","Lijuan Wang"],"pdf_url":"https://arxiv.org/pdf/2411.02319v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.18145v2","updated":"2024-11-04T17:31:40Z","published":"2024-07-25T15:49:26Z","title":"Taxonomy-Aware Continual Semantic Segmentation in Hyperbolic Spaces for\n  Open-World Perception","summary":"  Semantic segmentation models are typically trained on a fixed set of classes,\nlimiting their applicability in open-world scenarios. Class-incremental\nsemantic segmentation aims to update models with emerging new classes while\npreventing catastrophic forgetting of previously learned ones. However,\nexisting methods impose strict rigidity on old classes, reducing their\neffectiveness in learning new incremental classes. In this work, we propose\nTaxonomy-Oriented Poincar\\'e-regularized Incremental-Class Segmentation\n(TOPICS) that learns feature embeddings in hyperbolic space following explicit\ntaxonomy-tree structures. This supervision provides plasticity for old classes,\nupdating ancestors based on new classes while integrating new classes at\nfitting positions. Additionally, we maintain implicit class relational\nconstraints on the geometric basis of the Poincar\\'e ball. This ensures that\nthe latent space can continuously adapt to new constraints while maintaining a\nrobust structure to combat catastrophic forgetting. We also establish eight\nrealistic incremental learning protocols for autonomous driving scenarios,\nwhere novel classes can originate from known classes or the background.\nExtensive evaluations of TOPICS on the Cityscapes and Mapillary Vistas 2.0\nbenchmarks demonstrate that it achieves state-of-the-art performance. We make\nthe code and trained models publicly available at\nhttp://topics.cs.uni-freiburg.de.\n","authors":["Julia Hindel","Daniele Cattaneo","Abhinav Valada"],"pdf_url":"https://arxiv.org/pdf/2407.18145v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.10376v2","updated":"2024-11-04T17:28:54Z","published":"2024-02-16T00:04:36Z","title":"Interpreting CLIP with Sparse Linear Concept Embeddings (SpLiCE)","summary":"  CLIP embeddings have demonstrated remarkable performance across a wide range\nof multimodal applications. However, these high-dimensional, dense vector\nrepresentations are not easily interpretable, limiting our understanding of the\nrich structure of CLIP and its use in downstream applications that require\ntransparency. In this work, we show that the semantic structure of CLIP's\nlatent space can be leveraged to provide interpretability, allowing for the\ndecomposition of representations into semantic concepts. We formulate this\nproblem as one of sparse recovery and propose a novel method, Sparse Linear\nConcept Embeddings, for transforming CLIP representations into sparse linear\ncombinations of human-interpretable concepts. Distinct from previous work,\nSpLiCE is task-agnostic and can be used, without training, to explain and even\nreplace traditional dense CLIP representations, maintaining high downstream\nperformance while significantly improving their interpretability. We also\ndemonstrate significant use cases of SpLiCE representations including detecting\nspurious correlations and model editing.\n","authors":["Usha Bhalla","Alex Oesterling","Suraj Srinivas","Flavio P. Calmon","Himabindu Lakkaraju"],"pdf_url":"https://arxiv.org/pdf/2402.10376v2.pdf","comment":"25 pages, 15 figures, NeurIPS 2024. Code is provided at\n  https://github.com/AI4LIFE-GROUP/SpLiCE"},{"id":"http://arxiv.org/abs/2411.02299v1","updated":"2024-11-04T17:25:10Z","published":"2024-11-04T17:25:10Z","title":"Grouped Discrete Representation for Object-Centric Learning","summary":"  Object-Centric Learning (OCL) can discover objects in images or videos by\nsimply reconstructing the input. For better object discovery, representative\nOCL methods reconstruct the input as its Variational Autoencoder (VAE)\nintermediate representation, which suppresses pixel noises and promotes object\nseparability by discretizing continuous super-pixels with template features.\nHowever, treating features as units overlooks their composing attributes, thus\nimpeding model generalization; indexing features with scalar numbers loses\nattribute-level similarities and differences, thus hindering model convergence.\nWe propose \\textit{Grouped Discrete Representation} (GDR) for OCL. We decompose\nfeatures into combinatorial attributes via organized channel grouping, and\ncompose these attributes into discrete representation via tuple indexes.\nExperiments show that our GDR improves both Transformer- and Diffusion-based\nOCL methods consistently on various datasets. Visualizations show that our GDR\ncaptures better object separability.\n","authors":["Rongzhen Zhao","Vivienne Wang","Juho Kannala","Joni Pajarinen"],"pdf_url":"https://arxiv.org/pdf/2411.02299v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02293v1","updated":"2024-11-04T17:21:42Z","published":"2024-11-04T17:21:42Z","title":"Hunyuan3D-1.0: A Unified Framework for Text-to-3D and Image-to-3D\n  Generation","summary":"  While 3D generative models have greatly improved artists' workflows, the\nexisting diffusion models for 3D generation suffer from slow generation and\npoor generalization. To address this issue, we propose a two-stage approach\nnamed Hunyuan3D-1.0 including a lite version and a standard version, that both\nsupport text- and image-conditioned generation. In the first stage, we employ a\nmulti-view diffusion model that efficiently generates multi-view RGB in\napproximately 4 seconds. These multi-view images capture rich details of the 3D\nasset from different viewpoints, relaxing the tasks from single-view to\nmulti-view reconstruction. In the second stage, we introduce a feed-forward\nreconstruction model that rapidly and faithfully reconstructs the 3D asset\ngiven the generated multi-view images in approximately 7 seconds. The\nreconstruction network learns to handle noises and in-consistency introduced by\nthe multi-view diffusion and leverages the available information from the\ncondition image to efficiently recover the 3D structure. % Extensive\nexperimental results demonstrate the effectiveness of Hunyuan3D-1.0 in\ngenerating high-quality 3D assets. Our framework involves the text-to-image\nmodel ~\\ie, Hunyuan-DiT, making it a unified framework to support both text-\nand image-conditioned 3D generation. Our standard version has $10\\times$ more\nparameters than our lite and other existing model. Our Hunyuan3D-1.0 achieves\nan impressive balance between speed and quality, significantly reducing\ngeneration time while maintaining the quality and diversity of the produced\nassets.\n","authors":["Xianghui Yang","Huiwen Shi","Bowen Zhang","Fan Yang","Jiacheng Wang","Hongxu Zhao","Xinhai Liu","Xinzhou Wang","Qingxiang Lin","Jiaao Yu","Lifu Wang","Zhuo Chen","Sicong Liu","Yuhong Liu","Yong Yang","Di Wang","Jie Jiang","Chunchao Guo"],"pdf_url":"https://arxiv.org/pdf/2411.02293v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02281v1","updated":"2024-11-04T17:09:58Z","published":"2024-11-04T17:09:58Z","title":"Conformal-in-the-Loop for Learning with Imbalanced Noisy Data","summary":"  Class imbalance and label noise are pervasive in large-scale datasets, yet\nmuch of machine learning research assumes well-labeled, balanced data, which\nrarely reflects real world conditions. Existing approaches typically address\neither label noise or class imbalance in isolation, leading to suboptimal\nresults when both issues coexist. In this work, we propose\nConformal-in-the-Loop (CitL), a novel training framework that addresses both\nchallenges with a conformal prediction-based approach. CitL evaluates sample\nuncertainty to adjust weights and prune unreliable examples, enhancing model\nresilience and accuracy with minimal computational cost. Our extensive\nexperiments include a detailed analysis showing how CitL effectively emphasizes\nimpactful data in noisy, imbalanced datasets. Our results show that CitL\nconsistently boosts model performance, achieving up to a 6.1% increase in\nclassification accuracy and a 5.0 mIoU improvement in segmentation. Our code is\npublicly available: CitL.\n","authors":["John Brandon Graham-Knight","Jamil Fayyad","Nourhan Bayasi","Patricia Lasserre","Homayoun Najjaran"],"pdf_url":"https://arxiv.org/pdf/2411.02281v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2406.04280v2","updated":"2024-11-04T17:02:45Z","published":"2024-06-06T17:26:40Z","title":"xMIL: Insightful Explanations for Multiple Instance Learning in\n  Histopathology","summary":"  Multiple instance learning (MIL) is an effective and widely used approach for\nweakly supervised machine learning. In histopathology, MIL models have achieved\nremarkable success in tasks like tumor detection, biomarker prediction, and\noutcome prognostication. However, MIL explanation methods are still lagging\nbehind, as they are limited to small bag sizes or disregard instance\ninteractions. We revisit MIL through the lens of explainable AI (XAI) and\nintroduce xMIL, a refined framework with more general assumptions. We\ndemonstrate how to obtain improved MIL explanations using layer-wise relevance\npropagation (LRP) and conduct extensive evaluation experiments on three toy\nsettings and four real-world histopathology datasets. Our approach consistently\noutperforms previous explanation attempts with particularly improved\nfaithfulness scores on challenging biomarker prediction tasks. Finally, we\nshowcase how xMIL explanations enable pathologists to extract insights from MIL\nmodels, representing a significant advance for knowledge discovery and model\ndebugging in digital histopathology. Codes are available at:\nhttps://github.com/tubml-pathology/xMIL.\n","authors":["Julius Hense","Mina Jamshidi Idaji","Oliver Eberle","Thomas Schnake","Jonas Dippel","Laure Ciernik","Oliver Buchstab","Andreas Mock","Frederick Klauschen","Klaus-Robert Müller"],"pdf_url":"https://arxiv.org/pdf/2406.04280v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02256v1","updated":"2024-11-04T16:46:53Z","published":"2024-11-04T16:46:53Z","title":"Unified Speech Recognition: A Single Model for Auditory, Visual, and\n  Audiovisual Inputs","summary":"  Research in auditory, visual, and audiovisual speech recognition (ASR, VSR,\nand AVSR, respectively) has traditionally been conducted independently. Even\nrecent self-supervised studies addressing two or all three tasks simultaneously\ntend to yield separate models, leading to disjoint inference pipelines with\nincreased memory requirements and redundancies. This paper proposes unified\ntraining strategies for these systems. We demonstrate that training a single\nmodel for all three tasks enhances VSR and AVSR performance, overcoming typical\noptimisation challenges when training from scratch. Moreover, we introduce a\ngreedy pseudo-labelling approach to more effectively leverage unlabelled\nsamples, addressing shortcomings in related self-supervised methods. Finally,\nwe develop a self-supervised pre-training method within our framework, proving\nits effectiveness alongside our semi-supervised approach. Despite using a\nsingle model for all tasks, our unified approach achieves state-of-the-art\nperformance compared to recent methods on LRS3 and LRS2 for ASR, VSR, and AVSR,\nas well as on the newly released WildVSR dataset. Code and models are available\nat https://github.com/ahaliassos/usr.\n","authors":["Alexandros Haliassos","Rodrigo Mira","Honglie Chen","Zoe Landgraf","Stavros Petridis","Maja Pantic"],"pdf_url":"https://arxiv.org/pdf/2411.02256v1.pdf","comment":"NeurIPS 2024. Code: https://github.com/ahaliassos/usr"},{"id":"http://arxiv.org/abs/2411.00225v2","updated":"2024-11-04T16:46:01Z","published":"2024-10-31T21:52:33Z","title":"Fashion-VDM: Video Diffusion Model for Virtual Try-On","summary":"  We present Fashion-VDM, a video diffusion model (VDM) for generating virtual\ntry-on videos. Given an input garment image and person video, our method aims\nto generate a high-quality try-on video of the person wearing the given\ngarment, while preserving the person's identity and motion. Image-based virtual\ntry-on has shown impressive results; however, existing video virtual try-on\n(VVT) methods are still lacking garment details and temporal consistency. To\naddress these issues, we propose a diffusion-based architecture for video\nvirtual try-on, split classifier-free guidance for increased control over the\nconditioning inputs, and a progressive temporal training strategy for\nsingle-pass 64-frame, 512px video generation. We also demonstrate the\neffectiveness of joint image-video training for video try-on, especially when\nvideo data is limited. Our qualitative and quantitative experiments show that\nour approach sets the new state-of-the-art for video virtual try-on. For\nadditional results, visit our project page:\nhttps://johannakarras.github.io/Fashion-VDM.\n","authors":["Johanna Karras","Yingwei Li","Nan Liu","Luyang Zhu","Innfarn Yoo","Andreas Lugmayr","Chris Lee","Ira Kemelmacher-Shlizerman"],"pdf_url":"https://arxiv.org/pdf/2411.00225v2.pdf","comment":"Accepted to SIGGRAPH Asia 2024"},{"id":"http://arxiv.org/abs/2405.07257v3","updated":"2024-11-04T16:42:38Z","published":"2024-05-12T11:41:44Z","title":"SPEAK: Speech-Driven Pose and Emotion-Adjustable Talking Head Generation","summary":"  Most earlier researches on talking face generation have focused on the\nsynchronization of lip motion and speech content. However, head pose and facial\nemotions are equally important characteristics of natural faces. While\naudio-driven talking face generation has seen notable advancements, existing\nmethods either overlook facial emotions or are limited to specific individuals\nand cannot be applied to arbitrary subjects. In this paper, we propose a novel\none-shot Talking Head Generation framework (SPEAK) that distinguishes itself\nfrom the general Talking Face Generation by enabling emotional and postural\ncontrol. Specifically, we introduce Inter-Reconstructed Feature Disentanglement\n(IRFD) module to decouple facial features into three latent spaces. Then we\ndesign a face editing module that modifies speech content and facial latent\ncodes into a single latent space. Subsequently, we present a novel generator\nthat employs modified latent codes derived from the editing module to regulate\nemotional expression, head poses, and speech content in synthesizing facial\nanimations. Extensive trials demonstrate that our method ensures lip\nsynchronization with the audio while enabling decoupled control of facial\nfeatures, it can generate realistic talking head with coordinated lip motions,\nauthentic facial emotions, and smooth head movements. The demo video is\navailable: https://anonymous.4open.science/r/SPEAK-8A22\n","authors":["Changpeng Cai","Guinan Guo","Jiao Li","Junhao Su","Fei Shen","Chenghao He","Jing Xiao","Yuanxu Chen","Lei Dai","Feiyu Zhu"],"pdf_url":"https://arxiv.org/pdf/2405.07257v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02236v1","updated":"2024-11-04T16:30:14Z","published":"2024-11-04T16:30:14Z","title":"3D Audio-Visual Segmentation","summary":"  Recognizing the sounding objects in scenes is a longstanding objective in\nembodied AI, with diverse applications in robotics and AR/VR/MR. To that end,\nAudio-Visual Segmentation (AVS), taking as condition an audio signal to\nidentify the masks of the target sounding objects in an input image with\nsynchronous camera and microphone sensors, has been recently advanced. However,\nthis paradigm is still insufficient for real-world operation, as the mapping\nfrom 2D images to 3D scenes is missing. To address this fundamental limitation,\nwe introduce a novel research problem, 3D Audio-Visual Segmentation, extending\nthe existing AVS to the 3D output space. This problem poses more challenges due\nto variations in camera extrinsics, audio scattering, occlusions, and diverse\nacoustics across sounding object categories. To facilitate this research, we\ncreate the very first simulation based benchmark, 3DAVS-S34-O7, providing\nphotorealistic 3D scene environments with grounded spatial audio under\nsingle-instance and multi-instance settings, across 34 scenes and 7 object\ncategories. This is made possible by re-purposing the Habitat simulator to\ngenerate comprehensive annotations of sounding object locations and\ncorresponding 3D masks. Subsequently, we propose a new approach, EchoSegnet,\ncharacterized by integrating the ready-to-use knowledge from pretrained 2D\naudio-visual foundation models synergistically with 3D visual scene\nrepresentation through spatial audio-aware mask alignment and refinement.\nExtensive experiments demonstrate that EchoSegnet can effectively segment\nsounding objects in 3D space on our new benchmark, representing a significant\nadvancement in the field of embodied AI. Project page:\nhttps://surrey-uplab.github.io/research/3d-audio-visual-segmentation/\n","authors":["Artem Sokolov","Swapnil Bhosale","Xiatian Zhu"],"pdf_url":"https://arxiv.org/pdf/2411.02236v1.pdf","comment":"Accepted at the NeurIPS 2024 Workshop on Audio Imagination"},{"id":"http://arxiv.org/abs/2411.02229v1","updated":"2024-11-04T16:21:00Z","published":"2024-11-04T16:21:00Z","title":"FewViewGS: Gaussian Splatting with Few View Matching and Multi-stage\n  Training","summary":"  The field of novel view synthesis from images has seen rapid advancements\nwith the introduction of Neural Radiance Fields (NeRF) and more recently with\n3D Gaussian Splatting. Gaussian Splatting became widely adopted due to its\nefficiency and ability to render novel views accurately. While Gaussian\nSplatting performs well when a sufficient amount of training images are\navailable, its unstructured explicit representation tends to overfit in\nscenarios with sparse input images, resulting in poor rendering performance. To\naddress this, we present a 3D Gaussian-based novel view synthesis method using\nsparse input images that can accurately render the scene from the viewpoints\nnot covered by the training images. We propose a multi-stage training scheme\nwith matching-based consistency constraints imposed on the novel views without\nrelying on pre-trained depth estimation or diffusion models. This is achieved\nby using the matches of the available training images to supervise the\ngeneration of the novel views sampled between the training frames with color,\ngeometry, and semantic losses. In addition, we introduce a locality preserving\nregularization for 3D Gaussians which removes rendering artifacts by preserving\nthe local color structure of the scene. Evaluation on synthetic and real-world\ndatasets demonstrates competitive or superior performance of our method in\nfew-shot novel view synthesis compared to existing state-of-the-art methods.\n","authors":["Ruihong Yin","Vladimir Yugay","Yue Li","Sezer Karaoglu","Theo Gevers"],"pdf_url":"https://arxiv.org/pdf/2411.02229v1.pdf","comment":"Accepted by NeurIPS2024"},{"id":"http://arxiv.org/abs/2411.02220v1","updated":"2024-11-04T16:14:35Z","published":"2024-11-04T16:14:35Z","title":"SIRA: Scalable Inter-frame Relation and Association for Radar Perception","summary":"  Conventional radar feature extraction faces limitations due to low spatial\nresolution, noise, multipath reflection, the presence of ghost targets, and\nmotion blur. Such limitations can be exacerbated by nonlinear object motion,\nparticularly from an ego-centric viewpoint. It becomes evident that to address\nthese challenges, the key lies in exploiting temporal feature relation over an\nextended horizon and enforcing spatial motion consistency for effective\nassociation. To this end, this paper proposes SIRA (Scalable Inter-frame\nRelation and Association) with two designs. First, inspired by Swin\nTransformer, we introduce extended temporal relation, generalizing the existing\ntemporal relation layer from two consecutive frames to multiple inter-frames\nwith temporally regrouped window attention for scalability. Second, we propose\nmotion consistency track with the concept of a pseudo-tracklet generated from\nobservational data for better trajectory prediction and subsequent object\nassociation. Our approach achieves 58.11 mAP@0.5 for oriented object detection\nand 47.79 MOTA for multiple object tracking on the Radiate dataset, surpassing\nprevious state-of-the-art by a margin of +4.11 mAP@0.5 and +9.94 MOTA,\nrespectively.\n","authors":["Ryoma Yataka","Pu Perry Wang","Petros Boufounos","Ryuhei Takahashi"],"pdf_url":"https://arxiv.org/pdf/2411.02220v1.pdf","comment":"25 pages, Accepted to CVPR2024"},{"id":"http://arxiv.org/abs/2411.02210v1","updated":"2024-11-04T16:04:59Z","published":"2024-11-04T16:04:59Z","title":"One VLM to Keep it Learning: Generation and Balancing for Data-free\n  Continual Visual Question Answering","summary":"  Vision-Language Models (VLMs) have shown significant promise in Visual\nQuestion Answering (VQA) tasks by leveraging web-scale multimodal datasets.\nHowever, these models often struggle with continual learning due to\ncatastrophic forgetting when adapting to new tasks. As an effective remedy to\nmitigate catastrophic forgetting, rehearsal strategy uses the data of past\ntasks upon learning new task. However, such strategy incurs the need of storing\npast data, which might not be feasible due to hardware constraints or privacy\nconcerns. In this work, we propose the first data-free method that leverages\nthe language generation capability of a VLM, instead of relying on external\nmodels, to produce pseudo-rehearsal data for addressing continual VQA. Our\nproposal, named as GaB, generates pseudo-rehearsal data by posing previous task\nquestions on new task data. Yet, despite being effective, the distribution of\ngenerated questions skews towards the most frequently posed questions due to\nthe limited and task-specific training data. To mitigate this issue, we\nintroduce a pseudo-rehearsal balancing module that aligns the generated data\ntowards the ground-truth data distribution using either the question\nmeta-statistics or an unsupervised clustering method. We evaluate our proposed\nmethod on two recent benchmarks, \\ie VQACL-VQAv2 and CLOVE-function benchmarks.\nGaB outperforms all the data-free baselines with substantial improvement in\nmaintaining VQA performance across evolving tasks, while being on-par with\nmethods with access to the past data.\n","authors":["Deepayan Das","Davide Talon","Massimiliano Mancini","Yiming Wang","Elisa Ricci"],"pdf_url":"https://arxiv.org/pdf/2411.02210v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.15127v2","updated":"2024-11-04T15:54:21Z","published":"2024-04-23T15:27:19Z","title":"GSCo: Towards Generalizable AI in Medicine via Generalist-Specialist\n  Collaboration","summary":"  Generalist foundation models (GFMs) are renowned for their exceptional\ncapability and flexibility in effectively generalizing across diverse tasks and\nmodalities. In the field of medicine, while GFMs exhibit superior\ngeneralizability based on their extensive intrinsic knowledge as well as\nproficiency in instruction following and in-context learning, specialist models\nexcel in precision due to their domain knowledge. In this work, for the first\ntime, we explore the synergy between the GFM and specialist models, to enable\nprecise medical image analysis on a broader scope. Specifically, we propose a\ncooperative framework, Generalist-Specialist Collaboration (GSCo), which\nconsists of two stages, namely the construction of GFM and specialists, and\ncollaborative inference on downstream tasks. In the construction stage, we\ndevelop MedDr, the largest open-source GFM tailored for medicine, showcasing\nexceptional instruction-following and in-context learning capabilities.\nMeanwhile, a series of lightweight specialists are crafted for downstream tasks\nwith low computational cost. In the collaborative inference stage, we introduce\ntwo cooperative mechanisms, Mixture-of-Expert Diagnosis and Retrieval-Augmented\nDiagnosis, to harvest the generalist's in-context learning abilities alongside\nthe specialists' domain expertise. For a comprehensive evaluation, we curate a\nlarge-scale benchmark featuring 28 datasets and about 250,000 images. Extensive\nresults demonstrate that MedDr consistently outperforms state-of-the-art GFMs\non downstream datasets. Furthermore, GSCo exceeds both GFMs and specialists\nacross all out-of-domain disease diagnosis datasets. These findings indicate a\nsignificant paradigm shift in the application of GFMs, transitioning from\nseparate models for specific tasks to a collaborative approach between GFMs and\nspecialists, thereby advancing the frontiers of generalizable AI in medicine.\n","authors":["Sunan He","Yuxiang Nie","Hongmei Wang","Shu Yang","Yihui Wang","Zhiyuan Cai","Zhixuan Chen","Yingxue Xu","Luyang Luo","Huiling Xiang","Xi Lin","Mingxiang Wu","Yifan Peng","George Shih","Ziyang Xu","Xian Wu","Qiong Wang","Ronald Cheong Kin Chan","Varut Vardhanabhuti","Winnie Chiu Wing Chu","Yefeng Zheng","Pranav Rajpurkar","Kang Zhang","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2404.15127v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.20915v2","updated":"2024-11-04T15:48:10Z","published":"2024-05-31T15:21:44Z","title":"Fast yet Safe: Early-Exiting with Risk Control","summary":"  Scaling machine learning models significantly improves their performance.\nHowever, such gains come at the cost of inference being slow and\nresource-intensive. Early-exit neural networks (EENNs) offer a promising\nsolution: they accelerate inference by allowing intermediate layers to exit and\nproduce a prediction early. Yet a fundamental issue with EENNs is how to\ndetermine when to exit without severely degrading performance. In other words,\nwhen is it 'safe' for an EENN to go 'fast'? To address this issue, we\ninvestigate how to adapt frameworks of risk control to EENNs. Risk control\noffers a distribution-free, post-hoc solution that tunes the EENN's exiting\nmechanism so that exits only occur when the output is of sufficient quality. We\nempirically validate our insights on a range of vision and language tasks,\ndemonstrating that risk control can produce substantial computational savings,\nall the while preserving user-specified performance goals.\n","authors":["Metod Jazbec","Alexander Timans","Tin Hadži Veljković","Kaspar Sakmann","Dan Zhang","Christian A. Naesseth","Eric Nalisnick"],"pdf_url":"https://arxiv.org/pdf/2405.20915v2.pdf","comment":"27 pages, 13 figures, 4 tables (incl. appendix)"},{"id":"http://arxiv.org/abs/2411.02188v1","updated":"2024-11-04T15:42:22Z","published":"2024-11-04T15:42:22Z","title":"Digi2Real: Bridging the Realism Gap in Synthetic Data Face Recognition\n  via Foundation Models","summary":"  The accuracy of face recognition systems has improved significantly in the\npast few years, thanks to the large amount of data collected and the\nadvancement in neural network architectures. However, these large-scale\ndatasets are often collected without explicit consent, raising ethical and\nprivacy concerns. To address this, there have been proposals to use synthetic\ndatasets for training face recognition models. Yet, such models still rely on\nreal data to train the generative models and generally exhibit inferior\nperformance compared to those trained on real datasets. One of these datasets,\nDigiFace, uses a graphics pipeline to generate different identities and\ndifferent intra-class variations without using real data in training the\nmodels. However, the performance of this approach is poor on face recognition\nbenchmarks, possibly due to the lack of realism in the images generated from\nthe graphics pipeline. In this work, we introduce a novel framework for realism\ntransfer aimed at enhancing the realism of synthetically generated face images.\nOur method leverages the large-scale face foundation model, and we adapt the\npipeline for realism enhancement. By integrating the controllable aspects of\nthe graphics pipeline with our realism enhancement technique, we generate a\nlarge amount of realistic variations-combining the advantages of both\napproaches. Our empirical evaluations demonstrate that models trained using our\nenhanced dataset significantly improve the performance of face recognition\nsystems over the baseline. The source code and datasets will be made available\npublicly.\n","authors":["Anjith George","Sebastien Marcel"],"pdf_url":"https://arxiv.org/pdf/2411.02188v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2411.02184v1","updated":"2024-11-04T15:39:12Z","published":"2024-11-04T15:39:12Z","title":"Double Descent Meets Out-of-Distribution Detection: Theoretical Insights\n  and Empirical Analysis on the role of model complexity","summary":"  While overparameterization is known to benefit generalization, its impact on\nOut-Of-Distribution (OOD) detection is less understood. This paper investigates\nthe influence of model complexity in OOD detection. We propose an expected OOD\nrisk metric to evaluate classifiers confidence on both training and OOD\nsamples. Leveraging Random Matrix Theory, we derive bounds for the expected OOD\nrisk of binary least-squares classifiers applied to Gaussian data. We show that\nthe OOD risk depicts an infinite peak, when the number of parameters is equal\nto the number of samples, which we associate with the double descent\nphenomenon. Our experimental study on different OOD detection methods across\nmultiple neural architectures extends our theoretical insights and highlights a\ndouble descent curve. Our observations suggest that overparameterization does\nnot necessarily lead to better OOD detection. Using the Neural Collapse\nframework, we provide insights to better understand this behavior. To\nfacilitate reproducibility, our code will be made publicly available upon\npublication.\n","authors":["Mouïn Ben Ammar","David Brellmann","Arturo Mendoza","Antoine Manzanera","Gianni Franchi"],"pdf_url":"https://arxiv.org/pdf/2411.02184v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02181v1","updated":"2024-11-04T15:38:32Z","published":"2024-11-04T15:38:32Z","title":"Detect an Object At Once without Fine-tuning","summary":"  When presented with one or a few photos of a previously unseen object, humans\ncan instantly recognize it in different scenes. Although the human brain\nmechanism behind this phenomenon is still not fully understood, this work\nintroduces a novel technical realization of this task. It consists of two\nphases: (1) generating a Similarity Density Map (SDM) by convolving the scene\nimage with the given object image patch(es) so that the highlight areas in the\nSDM indicate the possible locations; (2) obtaining the object occupied areas in\nthe scene through a Region Alignment Network (RAN). The RAN is constructed on a\nbackbone of Deep Siamese Network (DSN), and different from the traditional\nDSNs, it aims to obtain the object accurate regions by regressing the location\nand area differences between the ground truths and the predicted ones indicated\nby the highlight areas in SDM. By pre-learning from labels annotated in\ntraditional datasets, the SDM-RAN can detect previously unknown objects without\nfine-tuning. Experiments were conducted on the MS COCO, PASCAL VOC datasets.\nThe results indicate that the proposed method outperforms state-of-the-art\nmethods on the same task.\n","authors":["Junyu Hao","Jianheng Liu","Yongjia Zhao","Zuofan Chen","Qi Sun","Jinlong Chen","Jianguo Wei","Minghao Yang"],"pdf_url":"https://arxiv.org/pdf/2411.02181v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02179v1","updated":"2024-11-04T15:37:18Z","published":"2024-11-04T15:37:18Z","title":"CleAR: Robust Context-Guided Generative Lighting Estimation for Mobile\n  Augmented Reality","summary":"  High-quality environment lighting is the foundation of creating immersive\nuser experiences in mobile augmented reality (AR) applications. However,\nachieving visually coherent environment lighting estimation for Mobile AR is\nchallenging due to several key limitations associated with AR device sensing\ncapabilities, including limitations in device camera FoV and pixel dynamic\nranges. Recent advancements in generative AI, which can generate high-quality\nimages from different types of prompts, including texts and images, present a\npotential solution for high-quality lighting estimation. Still, to effectively\nuse generative image diffusion models, we must address their key limitations of\ngeneration hallucination and slow inference process. To do so, in this work, we\ndesign and implement a generative lighting estimation system called CleAR that\ncan produce high-quality and diverse environment maps in the format of\n360$^\\circ$ images. Specifically, we design a two-step generation pipeline\nguided by AR environment context data to ensure the results follow physical\nenvironment visual context and color appearances. To improve the estimation\nrobustness under different lighting conditions, we design a real-time\nrefinement component to adjust lighting estimation results on AR devices. To\ntrain and test our generative models, we curate a large-scale environment\nlighting estimation dataset with diverse lighting conditions. Through\nquantitative evaluation and user study, we show that CleAR outperforms\nstate-of-the-art lighting estimation methods on both estimation accuracy and\nrobustness. Moreover, CleAR supports real-time refinement of lighting\nestimation results, ensuring robust and timely environment lighting updates for\nAR applications. Our end-to-end generative estimation takes as fast as 3.2\nseconds, outperforming state-of-the-art methods by 110x.\n","authors":["Yiqin Zhao","Mallesham Dasari","Tian Guo"],"pdf_url":"https://arxiv.org/pdf/2411.02179v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02175v1","updated":"2024-11-04T15:34:30Z","published":"2024-11-04T15:34:30Z","title":"SAFE: Slow and Fast Parameter-Efficient Tuning for Continual Learning\n  with Pre-Trained Models","summary":"  Continual learning aims to incrementally acquire new concepts in data streams\nwhile resisting forgetting previous knowledge. With the rise of powerful\npre-trained models (PTMs), there is a growing interest in training incremental\nlearning systems using these foundation models, rather than learning from\nscratch. Existing works often view PTMs as a strong initial point and directly\napply parameter-efficient tuning (PET) in the first session for adapting to\ndownstream tasks. In the following sessions, most methods freeze model\nparameters for tackling forgetting issues. However, applying PET directly to\ndownstream data cannot fully explore the inherent knowledge in PTMs.\nAdditionally, freezing the parameters in incremental sessions hinders models'\nplasticity to novel concepts not covered in the first session. To solve the\nabove issues, we propose a Slow And Fast parameter-Efficient tuning (SAFE)\nframework. In particular, to inherit general knowledge from foundation models,\nwe include a transfer loss function by measuring the correlation between the\nPTM and the PET-applied model. After calibrating in the first session, the slow\nefficient tuning parameters can capture more informative features, improving\ngeneralization to incoming classes. Moreover, to further incorporate novel\nconcepts, we strike a balance between stability and plasticity by fixing slow\nefficient tuning parameters and continuously updating the fast ones.\nSpecifically, a cross-classification loss with feature alignment is proposed to\ncircumvent catastrophic forgetting. During inference, we introduce an\nentropy-based aggregation strategy to dynamically utilize the complementarity\nin the slow and fast learners. Extensive experiments on seven benchmark\ndatasets verify the effectiveness of our method by significantly surpassing the\nstate-of-the-art.\n","authors":["Linglan Zhao","Xuerui Zhang","Ke Yan","Shouhong Ding","Weiran Huang"],"pdf_url":"https://arxiv.org/pdf/2411.02175v1.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.02149v1","updated":"2024-11-04T15:06:57Z","published":"2024-11-04T15:06:57Z","title":"Improving Domain Generalization in Self-supervised Monocular Depth\n  Estimation via Stabilized Adversarial Training","summary":"  Learning a self-supervised Monocular Depth Estimation (MDE) model with great\ngeneralization remains significantly challenging. Despite the success of\nadversarial augmentation in the supervised learning generalization, naively\nincorporating it into self-supervised MDE models potentially causes\nover-regularization, suffering from severe performance degradation. In this\npaper, we conduct qualitative analysis and illuminate the main causes: (i)\ninherent sensitivity in the UNet-alike depth network and (ii) dual optimization\nconflict caused by over-regularization. To tackle these issues, we propose a\ngeneral adversarial training framework, named Stabilized Conflict-optimization\nAdversarial Training (SCAT), integrating adversarial data augmentation into\nself-supervised MDE methods to achieve a balance between stability and\ngeneralization. Specifically, we devise an effective scaling depth network that\ntunes the coefficients of long skip connection and effectively stabilizes the\ntraining process. Then, we propose a conflict gradient surgery strategy, which\nprogressively integrates the adversarial gradient and optimizes the model\ntoward a conflict-free direction. Extensive experiments on five benchmarks\ndemonstrate that SCAT can achieve state-of-the-art performance and\nsignificantly improve the generalization capability of existing self-supervised\nMDE methods.\n","authors":["Yuanqi Yao","Gang Wu","Kui Jiang","Siao Liu","Jian Kuai","Xianming Liu","Junjun Jiang"],"pdf_url":"https://arxiv.org/pdf/2411.02149v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02136v1","updated":"2024-11-04T14:49:01Z","published":"2024-11-04T14:49:01Z","title":"Advanced computer vision for extracting georeferenced vehicle\n  trajectories from drone imagery","summary":"  This paper presents a framework for extracting georeferenced vehicle\ntrajectories from high-altitude drone footage, addressing key challenges in\nurban traffic monitoring and limitations of traditional ground-based systems.\nWe employ state-of-the-art computer vision and deep learning to create an\nend-to-end pipeline that enhances vehicle detection, tracking, and trajectory\nstabilization. Conducted in the Songdo International Business District, South\nKorea, the study used a multi-drone experiment over 20 intersections, capturing\napproximately 12TB of 4K video data over four days. We developed a novel track\nstabilization method that uses detected vehicle bounding boxes as exclusion\nmasks during image registration, which, combined with advanced georeferencing\ntechniques, accurately transforms vehicle coordinates into real-world\ngeographical data. Additionally, our framework includes robust vehicle\ndimension estimation and detailed road segmentation for in-depth traffic\nanalysis. The framework produced two high-quality datasets: the Songdo Traffic\ndataset, comprising nearly 1 million unique vehicle trajectories, and the\nSongdo Vision dataset, containing over 5,000 human-annotated frames with about\n300,000 vehicle instances in four classes. Comparisons between drone-derived\ndata and high-precision sensor data from an instrumented probe vehicle\nhighlight the accuracy and consistency of our framework's extraction in dense\nurban settings. By publicly releasing these datasets and the pipeline source\ncode, this work sets new benchmarks for data quality, reproducibility, and\nscalability in traffic research. Results demonstrate the potential of\nintegrating drone technology with advanced computer vision for precise,\ncost-effective urban traffic monitoring, providing valuable resources for the\nresearch community to develop intelligent transportation systems and improve\ntraffic management strategies.\n","authors":["Robert Fonod","Haechan Cho","Hwasoo Yeo","Nikolas Geroliminis"],"pdf_url":"https://arxiv.org/pdf/2411.02136v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.13174v2","updated":"2024-11-04T14:48:23Z","published":"2024-09-20T03:02:05Z","title":"Manipulation Facing Threats: Evaluating Physical Vulnerabilities in\n  End-to-End Vision Language Action Models","summary":"  Recently, driven by advancements in Multimodal Large Language Models (MLLMs),\nVision Language Action Models (VLAMs) are being proposed to achieve better\nperformance in open-vocabulary scenarios for robotic manipulation tasks. Since\nmanipulation tasks involve direct interaction with the physical world, ensuring\nrobustness and safety during the execution of this task is always a very\ncritical issue. In this paper, by synthesizing current safety research on MLLMs\nand the specific application scenarios of the manipulation task in the physical\nworld, we comprehensively evaluate VLAMs in the face of potential physical\nthreats. Specifically, we propose the Physical Vulnerability Evaluating\nPipeline (PVEP) that can incorporate as many visual modal physical threats as\npossible for evaluating the physical robustness of VLAMs. The physical threats\nin PVEP specifically include Out-of-Distribution, Typography-based Visual\nPrompts, and Adversarial Patch Attacks. By comparing the performance\nfluctuations of VLAMs before and after being attacked, we provide generalizable\nAnalyses of how VLAMs respond to different physical security threats. Our\nproject page is in this link:\nhttps://chaducheng.github.io/Manipulat-Facing-Threats/.\n","authors":["Hao Cheng","Erjia Xiao","Chengyuan Yu","Zhao Yao","Jiahang Cao","Qiang Zhang","Jiaxu Wang","Mengshu Sun","Kaidi Xu","Jindong Gu","Renjing Xu"],"pdf_url":"https://arxiv.org/pdf/2409.13174v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.17360v2","updated":"2024-11-04T14:47:12Z","published":"2024-04-26T12:21:57Z","title":"UniRGB-IR: A Unified Framework for RGB-Infrared Semantic Tasks via\n  Adapter Tuning","summary":"  Semantic analysis on visible (RGB) and infrared (IR) images has gained\nattention for its ability to be more accurate and robust under low-illumination\nand complex weather conditions. Due to the lack of pre-trained foundation\nmodels on the large-scale infrared image datasets, existing methods prefer to\ndesign task-specific frameworks and directly fine-tune them with pre-trained\nfoundation models on their RGB-IR semantic relevance datasets, which results in\npoor scalability and limited generalization. In this work, we propose a general\nand efficient framework called UniRGB-IR to unify RGB-IR semantic tasks, in\nwhich a novel adapter is developed to efficiently introduce richer RGB-IR\nfeatures into the pre-trained RGB-based foundation model. Specifically, our\nframework consists of a RGB-based foundation model, a Multi-modal Feature Pool\n(MFP) module and a Supplementary Feature Injector (SFI) module. The MFP and SFI\nmodules cooperate with each other as an adapter to effectively complement the\nRGB-based features with the rich RGB-IR features. During training process, we\nfreeze the entire foundation model to inherit prior knowledge and only optimize\nthe proposed adapter. Furthermore, to verify the effectiveness of our\nframework, we utilize the vanilla vision transformer (ViT-Base) as the\npre-trained foundation model to perform extensive experiments. Experimental\nresults on various RGB-IR downstream tasks demonstrate that our method can\nachieve state-of-the-art performance. The source code and results are available\nat https://github.com/PoTsui99/UniRGB-IR.git.\n","authors":["Maoxun Yuan","Bo Cui","Tianyi Zhao","Jiayi Wang","Shan Fu","Xingxing Wei"],"pdf_url":"https://arxiv.org/pdf/2404.17360v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06753v3","updated":"2024-11-04T14:32:02Z","published":"2023-03-12T21:01:54Z","title":"Modular Quantization-Aware Training for 6D Object Pose Estimation","summary":"  Edge applications, such as collaborative robotics and spacecraft rendezvous,\ndemand efficient 6D object pose estimation on resource-constrained embedded\nplatforms. Existing 6D pose estimation networks are often too large for such\ndeployments, necessitating compression while maintaining reliable performance.\nTo address this challenge, we introduce Modular Quantization-Aware Training\n(MQAT), an adaptive and mixed-precision quantization-aware training strategy\nthat exploits the modular structure of modern 6D pose estimation architectures.\nMQAT guides a systematic gradated modular quantization sequence and determines\nmodule-specific bit precisions, leading to quantized models that outperform\nthose produced by state-of-the-art uniform and mixed-precision quantization\ntechniques. Our experiments showcase the generality of MQAT across datasets,\narchitectures, and quantization algorithms. Remarkably, MQAT-trained quantized\nmodels achieve a significant accuracy boost (>7%) over the baseline\nfull-precision network while reducing model size by a factor of 4x or more. Our\nproject website is at: https://saqibjaved1.github.io/MQAT_/\n","authors":["Saqib Javed","Chengkun Li","Andrew Price","Yinlin Hu","Mathieu Salzmann"],"pdf_url":"https://arxiv.org/pdf/2303.06753v3.pdf","comment":"Accepted to Transactions on Machine Learning Research (TMLR), 2024"},{"id":"http://arxiv.org/abs/2408.05500v2","updated":"2024-11-04T14:30:03Z","published":"2024-08-10T09:31:58Z","title":"PointNCBW: Towards Dataset Ownership Verification for Point Clouds via\n  Negative Clean-label Backdoor Watermark","summary":"  Recently, point clouds have been widely used in computer vision, whereas\ntheir collection is time-consuming and expensive. As such, point cloud datasets\nare the valuable intellectual property of their owners and deserve protection.\nTo detect and prevent unauthorized use of these datasets, especially for\ncommercial or open-sourced ones that cannot be sold again or used commercially\nwithout permission, we intend to identify whether a suspicious third-party\nmodel is trained on our protected dataset under the black-box setting. We\nachieve this goal by designing a scalable clean-label backdoor-based dataset\nwatermark for point clouds that ensures both effectiveness and stealthiness.\nUnlike existing clean-label watermark schemes, which are susceptible to the\nnumber of categories, our method could watermark samples from all classes\ninstead of only from the target one. Accordingly, it can still preserve high\neffectiveness even on large-scale datasets with many classes. Specifically, we\nperturb selected point clouds with non-target categories in both shape-wise and\npoint-wise manners before inserting trigger patterns without changing their\nlabels. The features of perturbed samples are similar to those of benign\nsamples from the target class. As such, models trained on the watermarked\ndataset will have a distinctive yet stealthy backdoor behavior, i.e.,\nmisclassifying samples from the target class whenever triggers appear, since\nthe trained DNNs will treat the inserted trigger pattern as a signal to deny\npredicting the target label. We also design a hypothesis-test-guided dataset\nownership verification based on the proposed watermark. Extensive experiments\non benchmark datasets are conducted, verifying the effectiveness of our method\nand its resistance to potential removal methods.\n","authors":["Cheng Wei","Yang Wang","Kuofeng Gao","Shuo Shao","Yiming Li","Zhibo Wang","Zhan Qin"],"pdf_url":"https://arxiv.org/pdf/2408.05500v2.pdf","comment":"This paper was accepted by IEEE Transactions on Information Forensics\n  and Security (TIFS), 2024. 16 pages"},{"id":"http://arxiv.org/abs/2411.02116v1","updated":"2024-11-04T14:29:28Z","published":"2024-11-04T14:29:28Z","title":"Advancements and limitations of LLMs in replicating human color-word\n  associations","summary":"  Color-word associations play a fundamental role in human cognition and design\napplications. Large Language Models (LLMs) have become widely available and\ndemonstrated intelligent behaviors in various benchmarks with natural\nconversation skills. However, their ability to replicate human color-word\nassociations remains understudied. We compared multiple generations of LLMs\n(from GPT-3 to GPT- 4o) against human color-word associations using data\ncollected from over 10,000 Japanese participants, involving 17 colors and words\nfrom eight categories in Japanese. Our findings reveal a clear progression in\nLLM performance across generations, with GPT-4o achieving the highest accuracy\nin predicting the best voted word for each color and category, particularly\nwhen using visual inputs rather than text-based color codes. However, the\nhighest median performance was approximately 50% even for GPT4-o with visual\ninputs (chance level is 10%), and the performance levels varied significantly\nacross word categories and colors, indicating a failure to fully replicate\nhuman color-word associations. On the other hand, color discrimination ability\nestimated from our color-word association data showed that LLMs demonstrated\nhigh correlation with human color discrimination patterns, similarly to\nprevious studies. Our study highlights both the advancements in LLM\ncapabilities and their persistent limitations, suggesting differences in\nsemantic memory structures between humans and LLMs in representing color-word\nassociations.\n","authors":["Makoto Fukushima","Shusuke Eshita","Hiroshige Fukuhara"],"pdf_url":"https://arxiv.org/pdf/2411.02116v1.pdf","comment":"20 pages, 7 figures, 3 tables"},{"id":"http://arxiv.org/abs/2411.02112v1","updated":"2024-11-04T14:27:10Z","published":"2024-11-04T14:27:10Z","title":"Multi-modal biometric authentication: Leveraging shared layer\n  architectures for enhanced security","summary":"  In this study, we introduce a novel multi-modal biometric authentication\nsystem that integrates facial, vocal, and signature data to enhance security\nmeasures. Utilizing a combination of Convolutional Neural Networks (CNNs) and\nRecurrent Neural Networks (RNNs), our model architecture uniquely incorporates\ndual shared layers alongside modality-specific enhancements for comprehensive\nfeature extraction. The system undergoes rigorous training with a joint loss\nfunction, optimizing for accuracy across diverse biometric inputs.\nFeature-level fusion via Principal Component Analysis (PCA) and classification\nthrough Gradient Boosting Machines (GBM) further refine the authentication\nprocess. Our approach demonstrates significant improvements in authentication\naccuracy and robustness, paving the way for advanced secure identity\nverification solutions.\n","authors":["Vatchala S","Yogesh C","Yeshwanth Govindarajan","Krithik Raja M","Vishal Pramav Amirtha Ganesan","Aashish Vinod A","Dharun Ramesh"],"pdf_url":"https://arxiv.org/pdf/2411.02112v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02104v1","updated":"2024-11-04T14:15:26Z","published":"2024-11-04T14:15:26Z","title":"Deep Learning on 3D Semantic Segmentation: A Detailed Review","summary":"  In this paper an exhaustive review and comprehensive analysis of recent and\nformer deep learning methods in 3D Semantic Segmentation (3DSS) is presented.\nIn the related literature, the taxonomy scheme used for the classification of\nthe 3DSS deep learning methods is ambiguous. Based on the taxonomy schemes of 9\nexisting review papers, a new taxonomy scheme of the 3DSS deep learning methods\nis proposed, aiming to standardize it and improve the comparability and clarity\nacross related studies. Furthermore, an extensive overview of the available\n3DSS indoor and outdoor datasets is provided along with their links. The core\npart of the review is the detailed presentation of recent and former 3DSS deep\nlearning methods and their classification using the proposed taxonomy scheme\nalong with their GitHub repositories. Additionally, a brief but informative\nanalysis of the evaluation metrics and loss functions used in 3DSS is included.\nFinally, a fruitful discussion of the examined 3DSS methods and datasets, is\npresented to foster new research directions and applications in the field of\n3DSS. Supplementary, to this review a GitHub repository is provided\n(https://github.com/thobet/Deep-Learning-on-3D-Semantic-Segmentation-a-\nDetailed-Review) including a quick classification of over 400 3DSS methods,\nusing the proposed taxonomy scheme.\n","authors":["Thodoris Betsas","Andreas Georgopoulos","Anastasios Doulamis","Pierre Grussenmeyer"],"pdf_url":"https://arxiv.org/pdf/2411.02104v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02099v1","updated":"2024-11-04T14:08:26Z","published":"2024-11-04T14:08:26Z","title":"Differentially Private Integrated Decision Gradients (IDG-DP) for\n  Radar-based Human Activity Recognition","summary":"  Human motion analysis offers significant potential for healthcare monitoring\nand early detection of diseases. The advent of radar-based sensing systems has\ncaptured the spotlight for they are able to operate without physical contact\nand they can integrate with pre-existing Wi-Fi networks. They are also seen as\nless privacy-invasive compared to camera-based systems. However, recent\nresearch has shown high accuracy in recognizing subjects or gender from radar\ngait patterns, raising privacy concerns. This study addresses these issues by\ninvestigating privacy vulnerabilities in radar-based Human Activity Recognition\n(HAR) systems and proposing a novel method for privacy preservation using\nDifferential Privacy (DP) driven by attributions derived with Integrated\nDecision Gradient (IDG) algorithm. We investigate Black-box Membership\nInference Attack (MIA) Models in HAR settings across various levels of\nattacker-accessible information. We extensively evaluated the effectiveness of\nthe proposed IDG-DP method by designing a CNN-based HAR model and rigorously\nassessing its resilience against MIAs. Experimental results demonstrate the\npotential of IDG-DP in mitigating privacy attacks while maintaining utility\nacross all settings, particularly excelling against label-only and shadow model\nblack-box MIA attacks. This work represents a crucial step towards balancing\nthe need for effective radar-based HAR with robust privacy protection in\nhealthcare environments.\n","authors":["Idris Zakariyya","Linda Tran","Kaushik Bhargav Sivangi","Paul Henderson","Fani Deligianni"],"pdf_url":"https://arxiv.org/pdf/2411.02099v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22709v2","updated":"2024-11-04T14:06:19Z","published":"2024-10-30T05:38:03Z","title":"FilterViT and DropoutViT: Lightweight Vision Transformer Models for\n  Efficient Attention Mechanisms","summary":"  In this study, we introduce FilterViT, an enhanced version of MobileViT,\nwhich leverages an attention-based mechanism for early-stage downsampling.\nTraditional QKV operations on high-resolution feature maps are computationally\nintensive due to the abundance of tokens. To address this, we propose a filter\nattention mechanism using a convolutional neural network (CNN) to generate an\nimportance mask, focusing attention on key image regions. The method\nsignificantly reduces computational complexity while maintaining\ninterpretability, as it highlights essential image areas. Experimental results\nshow that FilterViT achieves substantial gains in both efficiency and accuracy\ncompared to other models. We also introduce DropoutViT, a variant that uses a\nstochastic approach for pixel selection, further enhancing robustness.\n","authors":["Bohang Sun"],"pdf_url":"https://arxiv.org/pdf/2410.22709v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02095v1","updated":"2024-11-04T13:59:01Z","published":"2024-11-04T13:59:01Z","title":"The evolution of volumetric video: A survey of smart transcoding and\n  compression approaches","summary":"  Volumetric video, the capture and display of three-dimensional (3D) imagery,\nhas emerged as a revolutionary technology poised to transform the media\nlandscape, enabling immersive experiences that transcend the limitations of\ntraditional 2D video. One of the key challenges in this domain is the efficient\ndelivery of these high-bandwidth, data-intensive volumetric video streams,\nwhich requires innovative transcoding and compression techniques. This research\npaper explores the state-of-the-art in volumetric video compression and\ndelivery, with a focus on the potential of AI-driven solutions to address the\nunique challenges posed by this emerging medium.\n","authors":["Preetish Kakkar","Hariharan Ragothaman"],"pdf_url":"https://arxiv.org/pdf/2411.02095v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.06457v2","updated":"2024-11-04T13:43:15Z","published":"2024-08-12T19:17:57Z","title":"Advanced Vision Transformers and Open-Set Learning for Robust Mosquito\n  Classification: A Novel Approach to Entomological Studies","summary":"  Mosquito-related diseases pose a significant threat to global public health,\nnecessitating efficient and accurate mosquito classification for effective\nsurveillance and control. This work presents an innovative approach to mosquito\nclassification by leveraging state-of-the-art vision transformers and open-set\nlearning techniques. A novel framework has been introduced that integrates\nTransformer-based deep learning models with comprehensive data augmentation and\npreprocessing methods, enabling robust and precise identification of ten\nmosquito species. The Swin Transformer model achieves the best performance for\ntraditional closed-set learning with 99.80% accuracy and 0.998 F1 score. The\nlightweight MobileViT technique attains an almost similar accuracy of 98.90%\nwith significantly reduced parameters and model complexities. Next, the applied\ndeep learning models' adaptability and generalizability in a static environment\nhave been enhanced by using new classes of data samples during the inference\nstage that have not been included in the training set. The proposed framework's\nability to handle unseen classes like insects similar to mosquitoes, even\nhumans, through open-set learning further enhances its practical applicability\nby employing the OpenMax technique and Weibull distribution. The traditional\nCNN model, Xception, outperforms the latest transformer with higher accuracy\nand F1 score for open-set learning. The study's findings highlight the\ntransformative potential of advanced deep-learning architectures in entomology,\nproviding a strong groundwork for future research and development in mosquito\nsurveillance and vector control. The implications of this work extend beyond\nmosquito classification, offering valuable insights for broader ecological and\nenvironmental monitoring applications.\n","authors":["Ahmed Akib Jawad Karim","Muhammad Zawad Mahmud","Riasat Khan"],"pdf_url":"https://arxiv.org/pdf/2408.06457v2.pdf","comment":"23 pages, 15 figures"},{"id":"http://arxiv.org/abs/2402.18718v2","updated":"2024-11-04T13:40:24Z","published":"2024-02-28T21:29:16Z","title":"Model Pairing Using Embedding Translation for Backdoor Attack Detection\n  on Open-Set Classification Tasks","summary":"  Backdoor attacks allow an attacker to embed a specific vulnerability in a\nmachine learning algorithm, activated when an attacker-chosen pattern is\npresented, causing a specific misprediction. The need to identify backdoors in\nbiometric scenarios has led us to propose a novel technique with different\ntrade-offs. In this paper we propose to use model pairs on open-set\nclassification tasks for detecting backdoors. Using a simple linear operation\nto project embeddings from a probe model's embedding space to a reference\nmodel's embedding space, we can compare both embeddings and compute a\nsimilarity score. We show that this score, can be an indicator for the presence\nof a backdoor despite models being of different architectures, having been\ntrained independently and on different datasets. This technique allows for the\ndetection of backdoors on models designed for open-set classification tasks,\nwhich is little studied in the literature. Additionally, we show that backdoors\ncan be detected even when both models are backdoored. The source code is made\navailable for reproducibility purposes.\n","authors":["Alexander Unnervik","Hatef Otroshi Shahreza","Anjith George","Sébastien Marcel"],"pdf_url":"https://arxiv.org/pdf/2402.18718v2.pdf","comment":"Accepted in NeurIPS 2024 Safe Generative AI Workshop (oral\n  presentation)"},{"id":"http://arxiv.org/abs/2410.18978v2","updated":"2024-11-04T13:37:31Z","published":"2024-10-24T17:59:51Z","title":"Framer: Interactive Frame Interpolation","summary":"  We propose Framer for interactive frame interpolation, which targets\nproducing smoothly transitioning frames between two images as per user\ncreativity. Concretely, besides taking the start and end frames as inputs, our\napproach supports customizing the transition process by tailoring the\ntrajectory of some selected keypoints. Such a design enjoys two clear benefits.\nFirst, incorporating human interaction mitigates the issue arising from\nnumerous possibilities of transforming one image to another, and in turn\nenables finer control of local motions. Second, as the most basic form of\ninteraction, keypoints help establish the correspondence across frames,\nenhancing the model to handle challenging cases (e.g., objects on the start and\nend frames are of different shapes and styles). It is noteworthy that our\nsystem also offers an \"autopilot\" mode, where we introduce a module to estimate\nthe keypoints and refine the trajectory automatically, to simplify the usage in\npractice. Extensive experimental results demonstrate the appealing performance\nof Framer on various applications, such as image morphing, time-lapse video\ngeneration, cartoon interpolation, etc. The code, the model, and the interface\nwill be released to facilitate further research.\n","authors":["Wen Wang","Qiuyu Wang","Kecheng Zheng","Hao Ouyang","Zhekai Chen","Biao Gong","Hao Chen","Yujun Shen","Chunhua Shen"],"pdf_url":"https://arxiv.org/pdf/2410.18978v2.pdf","comment":"Project page: https://aim-uofa.github.io/Framer/"},{"id":"http://arxiv.org/abs/2410.01768v2","updated":"2024-11-04T13:33:16Z","published":"2024-10-02T17:25:31Z","title":"SegEarth-OV: Towards Training-Free Open-Vocabulary Segmentation for\n  Remote Sensing Images","summary":"  Remote sensing image plays an irreplaceable role in fields such as\nagriculture, water resources, military, and disaster relief. Pixel-level\ninterpretation is a critical aspect of remote sensing image applications;\nhowever, a prevalent limitation remains the need for extensive manual\nannotation. For this, we try to introduce open-vocabulary semantic segmentation\n(OVSS) into the remote sensing context. However, due to the sensitivity of\nremote sensing images to low-resolution features, distorted target shapes and\nill-fitting boundaries are exhibited in the prediction mask. To tackle this\nissue, we propose a simple and general upsampler, SimFeatUp, to restore lost\nspatial information in deep features in a training-free style. Further, based\non the observation of the abnormal response of local patch tokens to [CLS]\ntoken in CLIP, we propose to execute a straightforward subtraction operation to\nalleviate the global bias in patch tokens. Extensive experiments are conducted\non 17 remote sensing datasets spanning semantic segmentation, building\nextraction, road detection, and flood detection tasks. Our method achieves an\naverage of 5.8%, 8.2%, 4.0%, and 15.3% improvement over state-of-the-art\nmethods on 4 tasks. All codes are released.\n\\url{https://earth-insights.github.io/SegEarth-OV}\n","authors":["Kaiyu Li","Ruixun Liu","Xiangyong Cao","Xueru Bai","Feng Zhou","Deyu Meng","Zhi Wang"],"pdf_url":"https://arxiv.org/pdf/2410.01768v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.11944v4","updated":"2024-11-04T13:28:48Z","published":"2024-01-22T13:34:34Z","title":"CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding\n  Benchmark","summary":"  As the capabilities of large multimodal models (LMMs) continue to advance,\nevaluating the performance of LMMs emerges as an increasing need. Additionally,\nthere is an even larger gap in evaluating the advanced knowledge and reasoning\nabilities of LMMs in non-English contexts such as Chinese. We introduce CMMMU,\na new Chinese Massive Multi-discipline Multimodal Understanding benchmark\ndesigned to evaluate LMMs on tasks demanding college-level subject knowledge\nand deliberate reasoning in a Chinese context. CMMMU is inspired by and\nstrictly follows the annotation and analysis pattern of MMMU. CMMMU includes\n12k manually collected multimodal questions from college exams, quizzes, and\ntextbooks, covering six core disciplines: Art & Design, Business, Science,\nHealth & Medicine, Humanities & Social Science, and Tech & Engineering, like\nits companion, MMMU. These questions span 30 subjects and comprise 39 highly\nheterogeneous image types, such as charts, diagrams, maps, tables, music\nsheets, and chemical structures. CMMMU focuses on complex perception and\nreasoning with domain-specific knowledge in the Chinese context. We evaluate 11\nopen-source LLMs and one proprietary GPT-4V(ision). Even GPT-4V only achieves\naccuracies of 42%, indicating a large space for improvement. CMMMU will boost\nthe community to build the next-generation LMMs towards expert artificial\nintelligence and promote the democratization of LMMs by providing diverse\nlanguage contexts.\n","authors":["Ge Zhang","Xinrun Du","Bei Chen","Yiming Liang","Tongxu Luo","Tianyu Zheng","Kang Zhu","Yuyang Cheng","Chunpu Xu","Shuyue Guo","Haoran Zhang","Xingwei Qu","Junjie Wang","Ruibin Yuan","Yizhi Li","Zekun Wang","Yudong Liu","Yu-Hsuan Tsai","Fengji Zhang","Chenghua Lin","Wenhao Huang","Jie Fu"],"pdf_url":"https://arxiv.org/pdf/2401.11944v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02074v1","updated":"2024-11-04T13:26:15Z","published":"2024-11-04T13:26:15Z","title":"GraphVL: Graph-Enhanced Semantic Modeling via Vision-Language Models for\n  Generalized Class Discovery","summary":"  Generalized Category Discovery (GCD) aims to cluster unlabeled images into\nknown and novel categories using labeled images from known classes. To address\nthe challenge of transferring features from known to unknown classes while\nmitigating model bias, we introduce GraphVL, a novel approach for\nvision-language modeling in GCD, leveraging CLIP. Our method integrates a graph\nconvolutional network (GCN) with CLIP's text encoder to preserve class\nneighborhood structure. We also employ a lightweight visual projector for image\ndata, ensuring discriminative features through margin-based contrastive losses\nfor image-text mapping. This neighborhood preservation criterion effectively\nregulates the semantic space, making it less sensitive to known classes.\nAdditionally, we learn textual prompts from known classes and align them to\ncreate a more contextually meaningful semantic feature space for the GCN layer\nusing a contextual similarity loss. Finally, we represent unlabeled samples\nbased on their semantic distance to class prompts from the GCN, enabling\nsemi-supervised clustering for class discovery and minimizing errors. Our\nexperiments on seven benchmark datasets consistently demonstrate the\nsuperiority of GraphVL when integrated with the CLIP backbone.\n","authors":["Bhupendra Solanki","Ashwin Nair","Mainak Singha","Souradeep Mukhopadhyay","Ankit Jha","Biplab Banerjee"],"pdf_url":"https://arxiv.org/pdf/2411.02074v1.pdf","comment":"Accepted in ACM ICVGIP 2024"},{"id":"http://arxiv.org/abs/2406.09952v2","updated":"2024-11-04T13:26:07Z","published":"2024-06-14T11:58:49Z","title":"BiVLC: Extending Vision-Language Compositionality Evaluation with\n  Text-to-Image Retrieval","summary":"  Existing Vision-Language Compositionality (VLC) benchmarks like SugarCrepe\nare formulated as image-to-text retrieval problems, where, given an image, the\nmodels need to select between the correct textual description and a synthetic\nhard negative text. In this work, we present the Bidirectional Vision-Language\nCompositionality (BiVLC) dataset. The novelty of BiVLC is to add a synthetic\nhard negative image generated from the synthetic text, resulting in two\nimage-to-text retrieval examples (one for each image) and, more importantly,\ntwo text-to-image retrieval examples (one for each text). Human annotators\nfilter out ill-formed examples ensuring the validity of the benchmark. The\nexperiments on BiVLC uncover a weakness of current multimodal models, as they\nperform poorly in the text-to-image direction. In fact, when considering both\nretrieval directions, the conclusions obtained in previous works change\nsignificantly. In addition to the benchmark, we show that a contrastive model\ntrained using synthetic images and texts significantly improves over the base\nmodel in SugarCrepe and in BiVLC for both retrieval directions. The gap to\nhuman performance in BiVLC confirms that Vision-Language Compositionality is\nstill a challenging problem. BiVLC and code are available at\nhttps://imirandam.github.io/BiVLC_project_page.\n","authors":["Imanol Miranda","Ander Salaberria","Eneko Agirre","Gorka Azkune"],"pdf_url":"https://arxiv.org/pdf/2406.09952v2.pdf","comment":"Accepted to NeurIPS 24 Datasets and Benchmarks Track; Project page\n  at: https://imirandam.github.io/BiVLC_project_page/"},{"id":"http://arxiv.org/abs/2404.13686v3","updated":"2024-11-04T13:24:18Z","published":"2024-04-21T15:16:05Z","title":"Hyper-SD: Trajectory Segmented Consistency Model for Efficient Image\n  Synthesis","summary":"  Recently, a series of diffusion-aware distillation algorithms have emerged to\nalleviate the computational overhead associated with the multi-step inference\nprocess of Diffusion Models (DMs). Current distillation techniques often\ndichotomize into two distinct aspects: i) ODE Trajectory Preservation; and ii)\nODE Trajectory Reformulation. However, these approaches suffer from severe\nperformance degradation or domain shifts. To address these limitations, we\npropose Hyper-SD, a novel framework that synergistically amalgamates the\nadvantages of ODE Trajectory Preservation and Reformulation, while maintaining\nnear-lossless performance during step compression. Firstly, we introduce\nTrajectory Segmented Consistency Distillation to progressively perform\nconsistent distillation within pre-defined time-step segments, which\nfacilitates the preservation of the original ODE trajectory from a higher-order\nperspective. Secondly, we incorporate human feedback learning to boost the\nperformance of the model in a low-step regime and mitigate the performance loss\nincurred by the distillation process. Thirdly, we integrate score distillation\nto further improve the low-step generation capability of the model and offer\nthe first attempt to leverage a unified LoRA to support the inference process\nat all steps. Extensive experiments and user studies demonstrate that Hyper-SD\nachieves SOTA performance from 1 to 8 inference steps for both SDXL and SD1.5.\nFor example, Hyper-SDXL surpasses SDXL-Lightning by +0.68 in CLIP Score and\n+0.51 in Aes Score in the 1-step inference.\n","authors":["Yuxi Ren","Xin Xia","Yanzuo Lu","Jiacheng Zhang","Jie Wu","Pan Xie","Xing Wang","Xuefeng Xiao"],"pdf_url":"https://arxiv.org/pdf/2404.13686v3.pdf","comment":"Accepted by NeurIPS 2024 (Camera-Ready Version). Project Page:\n  https://hyper-sd.github.io/"},{"id":"http://arxiv.org/abs/2309.09875v2","updated":"2024-11-04T13:17:00Z","published":"2023-09-18T15:37:01Z","title":"RaLF: Flow-based Global and Metric Radar Localization in LiDAR Maps","summary":"  Localization is paramount for autonomous robots. While camera and LiDAR-based\napproaches have been extensively investigated, they are affected by adverse\nillumination and weather conditions. Therefore, radar sensors have recently\ngained attention due to their intrinsic robustness to such conditions. In this\npaper, we propose RaLF, a novel deep neural network-based approach for\nlocalizing radar scans in a LiDAR map of the environment, by jointly learning\nto address both place recognition and metric localization. RaLF is composed of\nradar and LiDAR feature encoders, a place recognition head that generates\nglobal descriptors, and a metric localization head that predicts the 3-DoF\ntransformation between the radar scan and the map. We tackle the place\nrecognition task by learning a shared embedding space between the two\nmodalities via cross-modal metric learning. Additionally, we perform metric\nlocalization by predicting pixel-level flow vectors that align the query radar\nscan with the LiDAR map. We extensively evaluate our approach on multiple\nreal-world driving datasets and show that RaLF achieves state-of-the-art\nperformance for both place recognition and metric localization. Moreover, we\ndemonstrate that our approach can effectively generalize to different cities\nand sensor setups than the ones used during training. We make the code and\ntrained models publicly available at http://ralf.cs.uni-freiburg.de.\n","authors":["Abhijeet Nayak","Daniele Cattaneo","Abhinav Valada"],"pdf_url":"https://arxiv.org/pdf/2309.09875v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02068v1","updated":"2024-11-04T13:15:28Z","published":"2024-11-04T13:15:28Z","title":"Model Integrity when Unlearning with T2I Diffusion Models","summary":"  The rapid advancement of text-to-image Diffusion Models has led to their\nwidespread public accessibility. However these models, trained on large\ninternet datasets, can sometimes generate undesirable outputs. To mitigate\nthis, approximate Machine Unlearning algorithms have been proposed to modify\nmodel weights to reduce the generation of specific types of images,\ncharacterized by samples from a ``forget distribution'', while preserving the\nmodel's ability to generate other images, characterized by samples from a\n``retain distribution''. While these methods aim to minimize the influence of\ntraining data in the forget distribution without extensive additional\ncomputation, we point out that they can compromise the model's integrity by\ninadvertently affecting generation for images in the retain distribution.\nRecognizing the limitations of FID and CLIPScore in capturing these effects, we\nintroduce a novel retention metric that directly assesses the perceptual\ndifference between outputs generated by the original and the unlearned models.\nWe then propose unlearning algorithms that demonstrate superior effectiveness\nin preserving model integrity compared to existing baselines. Given their\nstraightforward implementation, these algorithms serve as valuable benchmarks\nfor future advancements in approximate Machine Unlearning for Diffusion Models.\n","authors":["Andrea Schioppa","Emiel Hoogeboom","Jonathan Heek"],"pdf_url":"https://arxiv.org/pdf/2411.02068v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02065v1","updated":"2024-11-04T13:07:22Z","published":"2024-11-04T13:07:22Z","title":"AM Flow: Adapters for Temporal Processing in Action Recognition","summary":"  Deep learning models, in particular \\textit{image} models, have recently\ngained generalisability and robustness. %are becoming more general and robust\nby the day. In this work, we propose to exploit such advances in the realm of\n\\textit{video} classification. Video foundation models suffer from the\nrequirement of extensive pretraining and a large training time. Towards\nmitigating such limitations, we propose \"\\textit{Attention Map (AM) Flow}\" for\nimage models, a method for identifying pixels relevant to motion in each input\nvideo frame. In this context, we propose two methods to compute AM flow,\ndepending on camera motion. AM flow allows the separation of spatial and\ntemporal processing, while providing improved results over combined\nspatio-temporal processing (as in video models). Adapters, one of the popular\ntechniques in parameter efficient transfer learning, facilitate the\nincorporation of AM flow into pretrained image models, mitigating the need for\nfull-finetuning. We extend adapters to \"\\textit{temporal processing adapters}\"\nby incorporating a temporal processing unit into the adapters. Our work\nachieves faster convergence, therefore reducing the number of epochs needed for\ntraining. Moreover, we endow an image model with the ability to achieve\nstate-of-the-art results on popular action recognition datasets. This reduces\ntraining time and simplifies pretraining. We present experiments on\nKinetics-400, Something-Something v2, and Toyota Smarthome datasets, showcasing\nstate-of-the-art or comparable results.\n","authors":["Tanay Agrawal","Abid Ali","Antitza Dantcheva","Francois Bremond"],"pdf_url":"https://arxiv.org/pdf/2411.02065v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02057v1","updated":"2024-11-04T12:59:13Z","published":"2024-11-04T12:59:13Z","title":"Exploiting Unlabeled Data with Multiple Expert Teachers for Open\n  Vocabulary Aerial Object Detection and Its Orientation Adaptation","summary":"  In recent years, aerial object detection has been increasingly pivotal in\nvarious earth observation applications. However, current algorithms are limited\nto detecting a set of pre-defined object categories, demanding sufficient\nannotated training samples, and fail to detect novel object categories. In this\npaper, we put forth a novel formulation of the aerial object detection problem,\nnamely open-vocabulary aerial object detection (OVAD), which can detect objects\nbeyond training categories without costly collecting new labeled data. We\npropose CastDet, a CLIP-activated student-teacher detection framework that\nserves as the first OVAD detector specifically designed for the challenging\naerial scenario, where objects often exhibit weak appearance features and\narbitrary orientations. Our framework integrates a robust localization teacher\nalong with several box selection strategies to generate high-quality proposals\nfor novel objects. Additionally, the RemoteCLIP model is adopted as an\nomniscient teacher, which provides rich knowledge to enhance classification\ncapabilities for novel categories. A dynamic label queue is devised to maintain\nhigh-quality pseudo-labels during training. By doing so, the proposed CastDet\nboosts not only novel object proposals but also classification. Furthermore, we\nextend our approach from horizontal OVAD to oriented OVAD with tailored\nalgorithm designs to effectively manage bounding box representation and\npseudo-label generation. Extensive experiments for both tasks on multiple\nexisting aerial object detection datasets demonstrate the effectiveness of our\napproach. The code is available at https://github.com/lizzy8587/CastDet.\n","authors":["Yan Li","Weiwei Guo","Xue Yang","Ning Liao","Shaofeng Zhang","Yi Yu","Wenxian Yu","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2411.02057v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.14435v2","updated":"2024-11-04T12:48:38Z","published":"2023-11-24T12:22:00Z","title":"Local Concept Embeddings for Analysis of Concept Distributions in DNN\n  Feature Spaces","summary":"  Insights into the learned latent representations are imperative for verifying\ndeep neural networks (DNNs) in critical computer vision (CV) tasks. Therefore,\nstate-of-the-art supervised Concept-based eXplainable Artificial Intelligence\n(C-XAI) methods associate user-defined concepts like ``car'' each with a single\nvector in the DNN latent space (concept embedding vector). In the case of\nconcept segmentation, these linearly separate between activation map pixels\nbelonging to a concept and those belonging to background. Existing methods for\nconcept segmentation, however, fall short of capturing sub-concepts (e.g.,\n``proximate car'' and ``distant car''), and concept overlap (e.g., between\n``bus'' and ``truck''). In other words, they do not capture the full\ndistribution of concept representatives in latent space. For the first time,\nthis work shows that these simplifications are frequently broken and that\ndistribution information can be particularly useful for understanding\nDNN-learned notions of sub-concepts, concept confusion, and concept outliers.\nTo allow exploration of learned concept distributions, we propose a novel local\nconcept analysis framework. Instead of optimizing a single global concept\nvector on the complete dataset, it generates a local concept embedding (LoCE)\nvector for each individual sample. We use the distribution formed by LoCEs to\nexplore the latent concept distribution by fitting Gaussian mixture models\n(GMMs), hierarchical clustering, and concept-level information retrieval and\noutlier detection. Despite its context sensitivity, our method's concept\nsegmentation performance is competitive to global baselines. Analysis results\nare obtained on two datasets and five diverse vision DNN architectures,\nincluding vision transformers (ViTs).\n","authors":["Georgii Mikriukov","Gesina Schwalbe","Korinna Bade"],"pdf_url":"https://arxiv.org/pdf/2311.14435v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02038v1","updated":"2024-11-04T12:40:18Z","published":"2024-11-04T12:40:18Z","title":"Addressing Representation Collapse in Vector Quantized Models with One\n  Linear Layer","summary":"  Vector Quantization (VQ) is a widely used method for converting continuous\nrepresentations into discrete codes, which has become fundamental in\nunsupervised representation learning and latent generative models. However, VQ\nmodels are often hindered by the problem of representation collapse in the\nlatent space, which leads to low codebook utilization and limits the\nscalability of the codebook for large-scale training. Existing methods designed\nto mitigate representation collapse typically reduce the dimensionality of\nlatent space at the expense of model capacity, which do not fully resolve the\ncore issue. In this study, we conduct a theoretical analysis of representation\ncollapse in VQ models and identify its primary cause as the disjoint\noptimization of the codebook, where only a small subset of code vectors are\nupdated through gradient descent. To address this issue, we propose\n\\textbf{SimVQ}, a novel method which reparameterizes the code vectors through a\nlinear transformation layer based on a learnable latent basis. This\ntransformation optimizes the \\textit{entire linear space} spanned by the\ncodebook, rather than merely updating \\textit{the code vector} selected by the\nnearest-neighbor search in vanilla VQ models. Although it is commonly\nunderstood that the multiplication of two linear matrices is equivalent to\napplying a single linear layer, our approach works surprisingly well in\nresolving the collapse issue in VQ models with just one linear layer. We\nvalidate the efficacy of SimVQ through extensive experiments across various\nmodalities, including image and audio data with different model architectures.\nOur code is available at \\url{https://github.com/youngsheen/SimVQ}.\n","authors":["Yongxin Zhu","Bocheng Li","Yifei Xin","Linli Xu"],"pdf_url":"https://arxiv.org/pdf/2411.02038v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03078v2","updated":"2024-11-04T12:19:57Z","published":"2024-08-06T10:13:57Z","title":"BodySLAM: A Generalized Monocular Visual SLAM Framework for Surgical\n  Applications","summary":"  Endoscopic surgery relies on two-dimensional views, posing challenges for\nsurgeons in depth perception and instrument manipulation. While Monocular\nVisual Simultaneous Localization and Mapping (MVSLAM) has emerged as a\npromising solution, its implementation in endoscopic procedures faces\nsignificant challenges due to hardware limitations, such as the use of a\nmonocular camera and the absence of odometry sensors. This study presents\nBodySLAM, a robust deep learning-based MVSLAM approach that addresses these\nchallenges through three key components: CycleVO, a novel unsupervised\nmonocular pose estimation module; the integration of the state-of-the-art Zoe\narchitecture for monocular depth estimation; and a 3D reconstruction module\ncreating a coherent surgical map. The approach is rigorously evaluated using\nthree publicly available datasets (Hamlyn, EndoSLAM, and SCARED) spanning\nlaparoscopy, gastroscopy, and colonoscopy scenarios, and benchmarked against\nfour state-of-the-art methods. Results demonstrate that CycleVO exhibited\ncompetitive performance with the lowest inference time among pose estimation\nmethods, while maintaining robust generalization capabilities, whereas Zoe\nsignificantly outperformed existing algorithms for depth estimation in\nendoscopy. BodySLAM's strong performance across diverse endoscopic scenarios\ndemonstrates its potential as a viable MVSLAM solution for endoscopic\napplications.\n","authors":["G. Manni","C. Lauretti","F. Prata","R. Papalia","L. Zollo","P. Soda"],"pdf_url":"https://arxiv.org/pdf/2408.03078v2.pdf","comment":"16 pages, 7 figures"},{"id":"http://arxiv.org/abs/2411.02009v1","updated":"2024-11-04T11:54:31Z","published":"2024-11-04T11:54:31Z","title":"Tree level change detection over Ahmedabad city using very high\n  resolution satellite images and Deep Learning","summary":"  In this study, 0.5m high resolution satellite datasets over Indian urban\nregion was used to demonstrate the applicability of deep learning models over\nAhmedabad, India. Here, YOLOv7 instance segmentation model was trained on well\ncurated trees canopy dataset (6500 images) in order to carry out the change\ndetection. During training, evaluation metrics such as bounding box regression\nand mask regression loss, mean average precision (mAP) and stochastic gradient\ndescent algorithm were used for evaluating and optimizing the performance of\nmodel. After the 500 epochs, the mAP of 0.715 and 0.699 for individual tree\ndetection and tree canopy mask segmentation were obtained. However, by further\ntuning hyper parameters of the model, maximum accuracy of 80 % of trees\ndetection with false segmentation rate of 2% on data was obtained.\n","authors":["Jai G Singla","Gautam Jaiswal"],"pdf_url":"https://arxiv.org/pdf/2411.02009v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16998v2","updated":"2024-11-04T11:44:29Z","published":"2024-09-25T15:03:22Z","title":"PitRSDNet: Predicting Intra-operative Remaining Surgery Duration in\n  Endoscopic Pituitary Surgery","summary":"  Accurate intra-operative Remaining Surgery Duration (RSD) predictions allow\nfor anaesthetists to more accurately decide when to administer anaesthetic\nagents and drugs, as well as to notify hospital staff to send in the next\npatient. Therefore RSD plays an important role in improving patient care and\nminimising surgical theatre costs via efficient scheduling. In endoscopic\npituitary surgery, it is uniquely challenging due to variable workflow\nsequences with a selection of optional steps contributing to high variability\nin surgery duration. This paper presents PitRSDNet for predicting RSD during\npituitary surgery, a spatio-temporal neural network model that learns from\nhistorical data focusing on workflow sequences. PitRSDNet integrates workflow\nknowledge into RSD prediction in two forms: 1) multi-task learning for\nconcurrently predicting step and RSD; and 2) incorporating prior steps as\ncontext in temporal learning and inference. PitRSDNet is trained and evaluated\non a new endoscopic pituitary surgery dataset with 88 videos to show\ncompetitive performance improvements over previous statistical and machine\nlearning methods. The findings also highlight how PitRSDNet improve RSD\nprecision on outlier cases utilising the knowledge of prior steps.\n","authors":["Anjana Wijekoon","Adrito Das","Roxana R. Herrera","Danyal Z. Khan","John Hanrahan","Eleanor Carter","Valpuri Luoma","Danail Stoyanov","Hani J. Marcus","Sophia Bano"],"pdf_url":"https://arxiv.org/pdf/2409.16998v2.pdf","comment":"Accepted to the Augmented Environments for Computer-Assisted\n  Interventions (AE-CAI) Workshop at the Medical Image Computing and\n  Computer-Assisted Interventions (MICCAI) Conference 2024"},{"id":"http://arxiv.org/abs/2409.18783v2","updated":"2024-11-04T11:39:40Z","published":"2024-09-27T14:30:24Z","title":"DualDn: Dual-domain Denoising via Differentiable ISP","summary":"  Image denoising is a critical component in a camera's Image Signal Processing\n(ISP) pipeline. There are two typical ways to inject a denoiser into the ISP\npipeline: applying a denoiser directly to captured raw frames (raw domain) or\nto the ISP's output sRGB images (sRGB domain). However, both approaches have\ntheir limitations. Residual noise from raw-domain denoising can be amplified by\nthe subsequent ISP processing, and the sRGB domain struggles to handle\nspatially varying noise since it only sees noise distorted by the ISP.\nConsequently, most raw or sRGB domain denoising works only for specific noise\ndistributions and ISP configurations. To address these challenges, we propose\nDualDn, a novel learning-based dual-domain denoising. Unlike previous\nsingle-domain denoising, DualDn consists of two denoising networks: one in the\nraw domain and one in the sRGB domain. The raw domain denoising adapts to\nsensor-specific noise as well as spatially varying noise levels, while the sRGB\ndomain denoising adapts to ISP variations and removes residual noise amplified\nby the ISP. Both denoising networks are connected with a differentiable ISP,\nwhich is trained end-to-end and discarded during the inference stage. With this\ndesign, DualDn achieves greater generalizability compared to most\nlearning-based denoising methods, as it can adapt to different unseen noises,\nISP parameters, and even novel ISP pipelines. Experiments show that DualDn\nachieves state-of-the-art performance and can adapt to different denoising\narchitectures. Moreover, DualDn can be used as a plug-and-play denoising module\nwith real cameras without retraining, and still demonstrate better performance\nthan commercial on-camera denoising. The project website is available at:\nhttps://openimaginglab.github.io/DualDn/\n","authors":["Ruikang Li","Yujin Wang","Shiqi Chen","Fan Zhang","Jinwei Gu","Tianfan Xue"],"pdf_url":"https://arxiv.org/pdf/2409.18783v2.pdf","comment":"Accepted at ECCV 2024, Project page:\n  https://openimaginglab.github.io/DualDn/"},{"id":"http://arxiv.org/abs/2403.00174v4","updated":"2024-11-04T11:38:49Z","published":"2024-02-29T22:58:13Z","title":"A citizen science toolkit to collect human perceptions of urban\n  environments using open street view images","summary":"  Street View Imagery (SVI) is a valuable data source for studies (e.g.,\nenvironmental assessments, green space identification or land cover\nclassification). While commercial SVI is available, such providers commonly\nrestrict copying or reuse in ways necessary for research. Open SVI datasets are\nreadily available from less restrictive sources, such as Mapillary, but due to\nthe heterogeneity of the images, these require substantial preprocessing,\nfiltering, and careful quality checks. We present an efficient method for\nautomated downloading, processing, cropping, and filtering open SVI, to be used\nin a survey of human perceptions of the streets portrayed in these images. We\ndemonstrate our open-source reusable SVI preparation and smartphone-friendly\nperception-survey software with Amsterdam (Netherlands) as the case study.\nUsing a citizen science approach, we collected from 331 people 22,637 ratings\nabout their perceptions for various criteria. We have published our software in\na public repository for future re-use and reproducibility.\n","authors":["Matthew Danish","SM Labib","Britta Ricker","Marco Helbich"],"pdf_url":"https://arxiv.org/pdf/2403.00174v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10853v2","updated":"2024-11-04T11:37:51Z","published":"2024-06-16T08:54:38Z","title":"MV2Cyl: Reconstructing 3D Extrusion Cylinders from Multi-View Images","summary":"  We present MV2Cyl, a novel method for reconstructing 3D from 2D multi-view\nimages, not merely as a field or raw geometry but as a sketch-extrude CAD\nmodel. Extracting extrusion cylinders from raw 3D geometry has been extensively\nresearched in computer vision, while the processing of 3D data through neural\nnetworks has remained a bottleneck. Since 3D scans are generally accompanied by\nmulti-view images, leveraging 2D convolutional neural networks allows these\nimages to be exploited as a rich source for extracting extrusion cylinder\ninformation. However, we observe that extracting only the surface information\nof the extrudes and utilizing it results in suboptimal outcomes due to the\nchallenges in the occlusion and surface segmentation. By synergizing with the\nextracted base curve information, we achieve the optimal reconstruction result\nwith the best accuracy in 2D sketch and extrude parameter estimation. Our\nexperiments, comparing our method with previous work that takes a raw 3D point\ncloud as input, demonstrate the effectiveness of our approach by taking\nadvantage of multi-view images.\n","authors":["Eunji Hong","Minh Hieu Nguyen","Mikaela Angelina Uy","Minhyuk Sung"],"pdf_url":"https://arxiv.org/pdf/2406.10853v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.01988v1","updated":"2024-11-04T11:20:17Z","published":"2024-11-04T11:20:17Z","title":"QCS:Feature Refining from Quadruplet Cross Similarity for Facial\n  Expression Recognition","summary":"  On facial expression datasets with complex and numerous feature types, where\nthe significance and dominance of labeled features are difficult to predict,\nfacial expression recognition(FER) encounters the challenges of inter-class\nsimilarity and intra-class variances, making it difficult to mine effective\nfeatures. We aim to solely leverage the feature similarity among facial samples\nto address this. We introduce the Cross Similarity Attention (CSA), an\ninput-output position-sensitive attention mechanism that harnesses feature\nsimilarity across different images to compute the corresponding global spatial\nattention. Based on this, we propose a four-branch circular framework, called\nQuadruplet Cross Similarity (QCS), to extract discriminative features from the\nsame class and eliminate redundant ones from different classes synchronously to\nrefine cleaner features. The symmetry of the network ensures balanced and\nstable training and reduces the amount of CSA interaction matrix. Contrastive\nresidual distillation is utilized to transfer the information learned in the\ncross module back to the base network. The cross-attention module exists during\ntraining, and only one base branch is retained during inference. our proposed\nQCS model outperforms state-of-the-art methods on several popular FER datasets,\nwithout requiring additional landmark information or other extra training data.\nThe code is available at https://github.com/birdwcp/QCS.\n","authors":["Chengpeng Wang","Li Chen","Lili Wang","Zhaofan Li","Xuebin Lv"],"pdf_url":"https://arxiv.org/pdf/2411.01988v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10618v2","updated":"2024-11-04T11:11:49Z","published":"2024-04-16T14:42:49Z","title":"Private Attribute Inference from Images with Vision-Language Models","summary":"  As large language models (LLMs) become ubiquitous in our daily tasks and\ndigital interactions, associated privacy risks are increasingly in focus. While\nLLM privacy research has primarily focused on the leakage of model training\ndata, it has recently been shown that LLMs can make accurate privacy-infringing\ninferences from previously unseen texts. With the rise of vision-language\nmodels (VLMs), capable of understanding both images and text, a key question is\nwhether this concern transfers to the previously unexplored domain of benign\nimages posted online. To answer this question, we compile an image dataset with\nhuman-annotated labels of the image owner's personal attributes. In order to\nunderstand the privacy risks posed by VLMs beyond traditional human attribute\nrecognition, our dataset consists of images where the inferable private\nattributes do not stem from direct depictions of humans. On this dataset, we\nevaluate 7 state-of-the-art VLMs, finding that they can infer various personal\nattributes at up to 77.6% accuracy. Concerningly, we observe that accuracy\nscales with the general capabilities of the models, implying that future models\ncan be misused as stronger inferential adversaries, establishing an imperative\nfor the development of adequate defenses.\n","authors":["Batuhan Tömekçe","Mark Vero","Robin Staab","Martin Vechev"],"pdf_url":"https://arxiv.org/pdf/2404.10618v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01981v1","updated":"2024-11-04T11:09:47Z","published":"2024-11-04T11:09:47Z","title":"Typicalness-Aware Learning for Failure Detection","summary":"  Deep neural networks (DNNs) often suffer from the overconfidence issue, where\nincorrect predictions are made with high confidence scores, hindering the\napplications in critical systems. In this paper, we propose a novel approach\ncalled Typicalness-Aware Learning (TAL) to address this issue and improve\nfailure detection performance. We observe that, with the cross-entropy loss,\nmodel predictions are optimized to align with the corresponding labels via\nincreasing logit magnitude or refining logit direction. However, regarding\natypical samples, the image content and their labels may exhibit disparities.\nThis discrepancy can lead to overfitting on atypical samples, ultimately\nresulting in the overconfidence issue that we aim to address. To tackle the\nproblem, we have devised a metric that quantifies the typicalness of each\nsample, enabling the dynamic adjustment of the logit magnitude during the\ntraining process. By allowing atypical samples to be adequately fitted while\npreserving reliable logit direction, the problem of overconfidence can be\nmitigated. TAL has been extensively evaluated on benchmark datasets, and the\nresults demonstrate its superiority over existing failure detection methods.\nSpecifically, TAL achieves a more than 5% improvement on CIFAR100 in terms of\nthe Area Under the Risk-Coverage Curve (AURC) compared to the state-of-the-art.\nCode is available at https://github.com/liuyijungoon/TAL.\n","authors":["Yijun Liu","Jiequan Cui","Zhuotao Tian","Senqiao Yang","Qingdong He","Xiaoling Wang","Jingyong Su"],"pdf_url":"https://arxiv.org/pdf/2411.01981v1.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2310.08421v4","updated":"2024-11-04T11:06:25Z","published":"2023-10-12T15:42:17Z","title":"Visual Self-supervised Learning Scheme for Dense Prediction Tasks on\n  X-ray Images","summary":"  Recently, significant advancements in artificial intelligence have been\nattributed to the integration of self-supervised learning (SSL) scheme. While\nSSL has shown impressive achievements in natural language processing (NLP), its\nprogress in computer vision has comparatively lagged behind. However, the\nincorporation of contrastive learning into existing visual SSL models has led\nto considerable progress, often surpassing supervised counterparts.\nNonetheless, these improvements have been mostly limited to classification\ntasks. Moreover, few studies have evaluated visual SSL models in real-world\nscenarios, as most have focused on datasets with class-wise portrait images,\nnotably ImageNet. Here, we focus on dense prediction tasks using security\ninspection x-ray images to evaluate our proposed model, Segment Localization\n(SegLoc). Based upon the Instance Localization (InsLoc) model, SegLoc addresses\none of the key challenges of contrastive learning, i.e., false negative pairs\nof query embeddings. Our pre-training dataset is synthesized by cutting,\ntransforming, and pasting labeled segments from an existing labeled dataset\n(PIDray) as foregrounds onto instances from an unlabeled dataset (SIXray) as\nbackgrounds. Furthermore, we fully leverage the labeled data by incorporating\nthe concept, one queue per class, into the MoCo-v2 memory bank, thereby\navoiding false negative pairs. In our experiments, SegLoc outperformed random\ninitialization by 3% to 6% while underperformed supervised initialization, in\nterms of AR and AP metrics across different IoU values over 20 to 30\npre-training epochs.\n","authors":["Shervin Halat","Mohammad Rahmati","Ehsan Nazerfard"],"pdf_url":"https://arxiv.org/pdf/2310.08421v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01975v1","updated":"2024-11-04T10:51:47Z","published":"2024-11-04T10:51:47Z","title":"SPECTRUM: Semantic Processing and Emotion-informed video-Captioning\n  Through Retrieval and Understanding Modalities","summary":"  Capturing a video's meaning and critical concepts by analyzing the subtle\ndetails is a fundamental yet challenging task in video captioning. Identifying\nthe dominant emotional tone in a video significantly enhances the perception of\nits context. Despite a strong emphasis on video captioning, existing models\noften need to adequately address emotional themes, resulting in suboptimal\ncaptioning results. To address these limitations, this paper proposes a novel\nSemantic Processing and Emotion-informed video-Captioning Through Retrieval and\nUnderstanding Modalities (SPECTRUM) framework to empower the generation of\nemotionally and semantically credible captions. Leveraging our pioneering\nstructure, SPECTRUM discerns multimodal semantics and emotional themes using\nVisual Text Attribute Investigation (VTAI) and determines the orientation of\ndescriptive captions through a Holistic Concept-Oriented Theme (HCOT),\nexpressing emotionally-informed and field-acquainted references. They exploit\nvideo-to-text retrieval capabilities and the multifaceted nature of video\ncontent to estimate the emotional probabilities of candidate captions. Then,\nthe dominant theme of the video is determined by appropriately weighting\nembedded attribute vectors and applying coarse- and fine-grained emotional\nconcepts, which define the video's contextual alignment. Furthermore, using two\nloss functions, SPECTRUM is optimized to integrate emotional information and\nminimize prediction errors. Extensive experiments on the EmVidCap, MSVD, and\nMSRVTT video captioning datasets demonstrate that our model significantly\nsurpasses state-of-the-art methods. Quantitative and qualitative evaluations\nhighlight the model's ability to accurately capture and convey video emotions\nand multimodal attributes.\n","authors":["Ehsan Faghihi","Mohammedreza Zarenejad","Ali-Asghar Beheshti Shirazi"],"pdf_url":"https://arxiv.org/pdf/2411.01975v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.11914v3","updated":"2024-11-04T10:48:54Z","published":"2024-05-20T09:49:13Z","title":"PT43D: A Probabilistic Transformer for Generating 3D Shapes from Single\n  Highly-Ambiguous RGB Images","summary":"  Generating 3D shapes from single RGB images is essential in various\napplications such as robotics. Current approaches typically target images\ncontaining clear and complete visual descriptions of the object, without\nconsidering common realistic cases where observations of objects that are\nlargely occluded or truncated. We thus propose a transformer-based\nautoregressive model to generate the probabilistic distribution of 3D shapes\nconditioned on an RGB image containing potentially highly ambiguous\nobservations of the object. To handle realistic scenarios such as occlusion or\nfield-of-view truncation, we create simulated image-to-shape training pairs\nthat enable improved fine-tuning for real-world scenarios. We then adopt\ncross-attention to effectively identify the most relevant region of interest\nfrom the input image for shape generation. This enables inference of sampled\nshapes with reasonable diversity and strong alignment with the input image. We\ntrain and test our model on our synthetic data then fine-tune and test it on\nreal-world data. Experiments demonstrate that our model outperforms state of\nthe art in both scenarios.\n","authors":["Yiheng Xiong","Angela Dai"],"pdf_url":"https://arxiv.org/pdf/2405.11914v3.pdf","comment":"10 pages, 6 figures. Accepted to BMVC 2024"},{"id":"http://arxiv.org/abs/2411.01969v1","updated":"2024-11-04T10:44:46Z","published":"2024-11-04T10:44:46Z","title":"Active Gaze Behavior Boosts Self-Supervised Object Learning","summary":"  Due to significant variations in the projection of the same object from\ndifferent viewpoints, machine learning algorithms struggle to recognize the\nsame object across various perspectives. In contrast, toddlers quickly learn to\nrecognize objects from different viewpoints with almost no supervision. Recent\nworks argue that toddlers develop this ability by mapping close-in-time visual\ninputs to similar representations while interacting with objects. High acuity\nvision is only available in the central visual field, which may explain why\ntoddlers (much like adults) constantly move their gaze around during such\ninteractions. It is unclear whether/how much toddlers curate their visual\nexperience through these eye movements to support learning object\nrepresentations. In this work, we explore whether a bio inspired visual\nlearning model can harness toddlers' gaze behavior during a play session to\ndevelop view-invariant object recognition. Exploiting head-mounted eye tracking\nduring dyadic play, we simulate toddlers' central visual field experience by\ncropping image regions centered on the gaze location. This visual stream feeds\na time-based self-supervised learning algorithm. Our experiments demonstrate\nthat toddlers' gaze strategy supports the learning of invariant object\nrepresentations. Our analysis also reveals that the limited size of the central\nvisual field where acuity is high is crucial for this. We further find that\ntoddlers' visual experience elicits more robust representations compared to\nadults' mostly because toddlers look at objects they hold themselves for longer\nbouts. Overall, our work reveals how toddlers' gaze behavior supports\nself-supervised learning of view-invariant object recognition.\n","authors":["Zhengyang Yu","Arthur Aubret","Marcel C. Raabe","Jane Yang","Chen Yu","Jochen Triesch"],"pdf_url":"https://arxiv.org/pdf/2411.01969v1.pdf","comment":"16 pages, 11 figures"},{"id":"http://arxiv.org/abs/2411.01966v1","updated":"2024-11-04T10:42:21Z","published":"2024-11-04T10:42:21Z","title":"UnSegMedGAT: Unsupervised Medical Image Segmentation using Graph\n  Attention Networks Clustering","summary":"  The data-intensive nature of supervised classification drives the interest of\nthe researchers towards unsupervised approaches, especially for problems such\nas medical image segmentation, where labeled data is scarce. Building on the\nrecent advancements of Vision transformers (ViT) in computer vision, we propose\nan unsupervised segmentation framework using a pre-trained Dino-ViT. In the\nproposed method, we leverage the inherent graph structure within the image to\nrealize a significant performance gain for segmentation in medical images. For\nthis, we introduce a modularity-based loss function coupled with a Graph\nAttention Network (GAT) to effectively capture the inherent graph topology\nwithin the image. Our method achieves state-of-the-art performance, even\nsignificantly surpassing or matching that of existing (semi)supervised\ntechnique such as MedSAM which is a Segment Anything Model in medical images.\nWe demonstrate this using two challenging medical image datasets ISIC-2018 and\nCVC-ColonDB. This work underscores the potential of unsupervised approaches in\nadvancing medical image analysis in scenarios where labeled data is scarce. The\ngithub repository of the code is available on\n[https://github.com/mudit-adityaja/UnSegMedGAT].\n","authors":["A. Mudit Adityaja","Saurabh J. Shigwan","Nitin Kumar"],"pdf_url":"https://arxiv.org/pdf/2411.01966v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01962v1","updated":"2024-11-04T10:38:33Z","published":"2024-11-04T10:38:33Z","title":"Deep Learning for Leopard Individual Identification: An Adaptive Angular\n  Margin Approach","summary":"  Accurate identification of individual leopards across camera trap images is\ncritical for population monitoring and ecological studies. This paper\nintroduces a deep learning framework to distinguish between individual leopards\nbased on their unique spot patterns. This approach employs a novel adaptive\nangular margin method in the form of a modified CosFace architecture. In\naddition, I propose a preprocessing pipeline that combines RGB channels with an\nedge detection channel to underscore the critical features learned by the\nmodel.\n  This approach significantly outperforms the Triplet Network baseline,\nachieving a Dynamic Top-5 Average Precision of 0.8814 and a Top-5 Rank Match\nDetection of 0.9533, demonstrating its potential for open-set learning in\nwildlife identification. While not surpassing the performance of the SIFT-based\nHotspotter algorithm, this method represents a substantial advancement in\napplying deep learning to patterned wildlife identification.\n  This research contributes to the field of computer vision and provides a\nvaluable tool for biologists aiming to study and protect leopard populations.\nIt also serves as a stepping stone for applying the power of deep learning in\nCapture-Recapture studies for other patterned species.\n","authors":["David Colomer Matachana"],"pdf_url":"https://arxiv.org/pdf/2411.01962v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01955v1","updated":"2024-11-04T10:27:57Z","published":"2024-11-04T10:27:57Z","title":"Robust plug-and-play methods for highly accelerated non-Cartesian MRI\n  reconstruction","summary":"  Achieving high-quality Magnetic Resonance Imaging (MRI) reconstruction at\naccelerated acquisition rates remains challenging due to the inherent ill-posed\nnature of the inverse problem. Traditional Compressed Sensing (CS) methods,\nwhile robust across varying acquisition settings, struggle to maintain good\nreconstruction quality at high acceleration factors ($\\ge$ 8). Recent advances\nin deep learning have improved reconstruction quality, but purely data-driven\nmethods are prone to overfitting and hallucination effects, notably when the\nacquisition setting is varying. Plug-and-Play (PnP) approaches have been\nproposed to mitigate the pitfalls of both frameworks. In a nutshell, PnP\nalgorithms amount to replacing suboptimal handcrafted CS priors with powerful\ndenoising deep neural network (DNNs). However, in MRI reconstruction, existing\nPnP methods often yield suboptimal results due to instabilities in the proximal\ngradient descent (PGD) schemes and the lack of curated, noiseless datasets for\ntraining robust denoisers. In this work, we propose a fully unsupervised\npreprocessing pipeline to generate clean, noiseless complex MRI signals from\nmulticoil data, enabling training of a high-performance denoising DNN.\nFurthermore, we introduce an annealed Half-Quadratic Splitting (HQS) algorithm\nto address the instability issues, leading to significant improvements over\nexisting PnP algorithms. When combined with preconditioning techniques, our\napproach achieves state-of-the-art results, providing a robust and efficient\nsolution for high-quality MRI reconstruction.\n","authors":["Pierre-Antoine Comby","Benjamin Lapostolle","Matthieu Terris","Philippe Ciuciu"],"pdf_url":"https://arxiv.org/pdf/2411.01955v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.00543v2","updated":"2024-11-04T10:21:57Z","published":"2024-11-01T12:50:38Z","title":"3D Equivariant Pose Regression via Direct Wigner-D Harmonics Prediction","summary":"  Determining the 3D orientations of an object in an image, known as\nsingle-image pose estimation, is a crucial task in 3D vision applications.\nExisting methods typically learn 3D rotations parametrized in the spatial\ndomain using Euler angles or quaternions, but these representations often\nintroduce discontinuities and singularities. SO(3)-equivariant networks enable\nthe structured capture of pose patterns with data-efficient learning, but the\nparametrizations in spatial domain are incompatible with their architecture,\nparticularly spherical CNNs, which operate in the frequency domain to enhance\ncomputational efficiency. To overcome these issues, we propose a\nfrequency-domain approach that directly predicts Wigner-D coefficients for 3D\nrotation regression, aligning with the operations of spherical CNNs. Our\nSO(3)-equivariant pose harmonics predictor overcomes the limitations of spatial\nparameterizations, ensuring consistent pose estimation under arbitrary\nrotations. Trained with a frequency-domain regression loss, our method achieves\nstate-of-the-art results on benchmarks such as ModelNet10-SO(3) and PASCAL3D+,\nwith significant improvements in accuracy, robustness, and data efficiency.\n","authors":["Jongmin Lee","Minsu Cho"],"pdf_url":"https://arxiv.org/pdf/2411.00543v2.pdf","comment":"Accepted to NeurIPS 2024, Project webpage at\n  http://cvlab.postech.ac.kr/research/3D_EquiPose"},{"id":"http://arxiv.org/abs/2402.14695v2","updated":"2024-11-04T10:20:25Z","published":"2024-02-22T16:49:58Z","title":"QIS : Interactive Segmentation via Quasi-Conformal Mappings","summary":"  Image segmentation plays a crucial role in extracting important objects of\ninterest from images, enabling various applications. While existing methods\nhave shown success in segmenting clean images, they often struggle to produce\naccurate segmentation results when dealing with degraded images, such as those\ncontaining noise or occlusions. To address this challenge, interactive\nsegmentation has emerged as a promising approach, allowing users to provide\nmeaningful input to guide the segmentation process. However, an important\nproblem in interactive segmentation lies in determining how to incorporate\nminimal yet meaningful user guidance into the segmentation model. In this\npaper, we propose the quasi-conformal interactive segmentation (QIS) model,\nwhich incorporates user input in the form of positive and negative clicks.\nUsers mark a few pixels belonging to the object region as positive clicks,\nindicating that the segmentation model should include a region around these\nclicks. Conversely, negative clicks are provided on pixels belonging to the\nbackground, instructing the model to exclude the region near these clicks from\nthe segmentation mask. Additionally, the segmentation mask is obtained by\ndeforming a template mask with the same topology as the object of interest\nusing an orientation-preserving quasiconformal mapping. This approach helps to\navoid topological errors in the segmentation results. We provide a thorough\nanalysis of the proposed model, including theoretical support for the ability\nof QIS to include or exclude regions of interest or disinterest based on the\nuser's indication. To evaluate the performance of QIS, we conduct experiments\non synthesized images, medical images, natural images and noisy natural images.\nThe results demonstrate the efficacy of our proposed method.\n","authors":["Han Zhang","Daoping Zhang","Lok Ming Lui"],"pdf_url":"https://arxiv.org/pdf/2402.14695v2.pdf","comment":"34 pages, 14 figures"},{"id":"http://arxiv.org/abs/2411.01948v1","updated":"2024-11-04T10:17:40Z","published":"2024-11-04T10:17:40Z","title":"Learning Where to Edit Vision Transformers","summary":"  Model editing aims to data-efficiently correct predictive errors of large\npre-trained models while ensuring generalization to neighboring failures and\nlocality to minimize unintended effects on unrelated examples. While\nsignificant progress has been made in editing Transformer-based large language\nmodels, effective strategies for editing vision Transformers (ViTs) in computer\nvision remain largely untapped. In this paper, we take initial steps towards\ncorrecting predictive errors of ViTs, particularly those arising from\nsubpopulation shifts. Taking a locate-then-edit approach, we first address the\nwhere-to-edit challenge by meta-learning a hypernetwork on CutMix-augmented\ndata generated for editing reliability. This trained hypernetwork produces\ngeneralizable binary masks that identify a sparse subset of structured model\nparameters, responsive to real-world failure samples. Afterward, we solve the\nhow-to-edit problem by simply fine-tuning the identified parameters using a\nvariant of gradient descent to achieve successful edits. To validate our\nmethod, we construct an editing benchmark that introduces subpopulation shifts\ntowards natural underrepresented images and AI-generated images, thereby\nrevealing the limitations of pre-trained ViTs for object recognition. Our\napproach not only achieves superior performance on the proposed benchmark but\nalso allows for adjustable trade-offs between generalization and locality. Our\ncode is available at https://github.com/hustyyq/Where-to-Edit.\n","authors":["Yunqiao Yang","Long-Kai Huang","Shengzhuang Chen","Kede Ma","Ying Wei"],"pdf_url":"https://arxiv.org/pdf/2411.01948v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.05964v2","updated":"2024-11-04T10:14:22Z","published":"2024-08-12T07:33:11Z","title":"Target Detection of Safety Protective Gear Using the Improved YOLOv5","summary":"  In high-risk railway construction, personal protective equipment monitoring\nis critical but challenging due to small and frequently obstructed targets. We\npropose YOLO-EA, an innovative model that enhances safety measure detection by\nintegrating ECA into its backbone's convolutional layers, improving discernment\nof minuscule objects like hardhats. YOLO-EA further refines target recognition\nunder occlusion by replacing GIoU with EIoU loss. YOLO-EA's effectiveness was\nempirically substantiated using a dataset derived from real-world railway\nconstruction site surveillance footage. It outperforms YOLOv5, achieving 98.9%\nprecision and 94.7% recall, up 2.5% and 0.5% respectively, while maintaining\nreal-time performance at 70.774 fps. This highly efficient and precise YOLO-EA\nholds great promise for practical application in intricate construction\nscenarios, enforcing stringent safety compliance during complex railway\nconstruction projects.\n","authors":["Hao Liu","Xue Qin"],"pdf_url":"https://arxiv.org/pdf/2408.05964v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23085v2","updated":"2024-11-04T10:01:38Z","published":"2024-10-30T15:00:06Z","title":"S3PT: Scene Semantics and Structure Guided Clustering to Boost\n  Self-Supervised Pre-Training for Autonomous Driving","summary":"  Recent self-supervised clustering-based pre-training techniques like DINO and\nCribo have shown impressive results for downstream detection and segmentation\ntasks. However, real-world applications such as autonomous driving face\nchallenges with imbalanced object class and size distributions and complex\nscene geometries. In this paper, we propose S3PT a novel scene semantics and\nstructure guided clustering to provide more scene-consistent objectives for\nself-supervised training. Specifically, our contributions are threefold: First,\nwe incorporate semantic distribution consistent clustering to encourage better\nrepresentation of rare classes such as motorcycles or animals. Second, we\nintroduce object diversity consistent spatial clustering, to handle imbalanced\nand diverse object sizes, ranging from large background areas to small objects\nsuch as pedestrians and traffic signs. Third, we propose a depth-guided spatial\nclustering to regularize learning based on geometric information of the scene,\nthus further refining region separation on the feature level. Our learned\nrepresentations significantly improve performance in downstream semantic\nsegmentation and 3D object detection tasks on the nuScenes, nuImages, and\nCityscapes datasets and show promising domain translation properties.\n","authors":["Maciej K. Wozniak","Hariprasath Govindarajan","Marvin Klingner","Camille Maurice","B Ravi Kiran","Senthil Yogamani"],"pdf_url":"https://arxiv.org/pdf/2410.23085v2.pdf","comment":"Accepted for WACV 2025"},{"id":"http://arxiv.org/abs/2410.22392v2","updated":"2024-11-04T09:56:16Z","published":"2024-10-29T17:56:05Z","title":"EfficientNet with Hybrid Attention Mechanisms for Enhanced Breast\n  Histopathology Classification: A Comprehensive Approach","summary":"  Breast cancer histopathology image classification is crucial for early cancer\ndetection, offering the potential to reduce mortality rates through timely\ndiagnosis. This paper introduces a novel approach integrating Hybrid\nEfficientNet models with advanced attention mechanisms, including Convolutional\nBlock Attention Module (CBAM), Self-Attention, and Deformable Attention, to\nenhance feature extraction and focus on critical image regions. We evaluate the\nperformance of our models across multiple magnification scales using publicly\navailable histopathological datasets. Our method achieves significant\nimprovements, with accuracy reaching 98.42% at 400X magnification, surpassing\nseveral state-of-the-art models, including VGG and ResNet architectures. The\nresults are validated using metrics such as accuracy, F1-score, precision, and\nrecall, demonstrating the clinical potential of our model in improving\ndiagnostic accuracy. Furthermore, the proposed method shows increased\ncomputational efficiency, making it suitable for integration into real-time\ndiagnostic workflows.\n","authors":["Naren Sengodan"],"pdf_url":"https://arxiv.org/pdf/2410.22392v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01925v1","updated":"2024-11-04T09:43:33Z","published":"2024-11-04T09:43:33Z","title":"Exploiting Contextual Uncertainty of Visual Data for Efficient Training\n  of Deep Models","summary":"  Objects, in the real world, rarely occur in isolation and exhibit typical\narrangements governed by their independent utility, and their expected\ninteraction with humans and other objects in the context. For example, a chair\nis expected near a table, and a computer is expected on top. Humans use this\nspatial context and relative placement as an important cue for visual\nrecognition in case of ambiguities. Similar to human's, DNN's exploit\ncontextual information from data to learn representations. Our research focuses\non harnessing the contextual aspects of visual data to optimize data annotation\nand enhance the training of deep networks. Our contributions can be summarized\nas follows: (1) We introduce the notion of contextual diversity for active\nlearning CDAL and show its applicability in three different visual tasks\nsemantic segmentation, object detection and image classification, (2) We\npropose a data repair algorithm to curate contextually fair data to reduce\nmodel bias, enabling the model to detect objects out of their obvious context,\n(3) We propose Class-based annotation, where contextually relevant classes are\nselected that are complementary for model training under domain shift.\nUnderstanding the importance of well-curated data, we also emphasize the\nnecessity of involving humans in the loop to achieve accurate annotations and\nto develop novel interaction strategies that allow humans to serve as\nfact-checkers. In line with this we are working on developing image retrieval\nsystem for wildlife camera trap images and reliable warning system for poor\nquality rural roads. For large-scale annotation, we are employing a strategic\ncombination of human expertise and zero-shot models, while also integrating\nhuman input at various stages for continuous feedback.\n","authors":["Sharat Agarwal"],"pdf_url":"https://arxiv.org/pdf/2411.01925v1.pdf","comment":"ICVGIP, Young Researchers Symposium"},{"id":"http://arxiv.org/abs/2411.01919v1","updated":"2024-11-04T09:34:55Z","published":"2024-11-04T09:34:55Z","title":"Real-Time Polygonal Semantic Mapping for Humanoid Robot Stair Climbing","summary":"  We present a novel algorithm for real-time planar semantic mapping tailored\nfor humanoid robots navigating complex terrains such as staircases. Our method\nis adaptable to any odometry input and leverages GPU-accelerated processes for\nplanar extraction, enabling the rapid generation of globally consistent\nsemantic maps. We utilize an anisotropic diffusion filter on depth images to\neffectively minimize noise from gradient jumps while preserving essential edge\ndetails, enhancing normal vector images' accuracy and smoothness. Both the\nanisotropic diffusion and the RANSAC-based plane extraction processes are\noptimized for parallel processing on GPUs, significantly enhancing\ncomputational efficiency. Our approach achieves real-time performance,\nprocessing single frames at rates exceeding $30~Hz$, which facilitates detailed\nplane extraction and map management swiftly and efficiently. Extensive testing\nunderscores the algorithm's capabilities in real-time scenarios and\ndemonstrates its practical application in humanoid robot gait planning,\nsignificantly improving its ability to navigate dynamic environments.\n","authors":["Teng Bin","Jianming Yao","Tin Lun Lam","Tianwei Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.01919v1.pdf","comment":"Accepted by The 2024 IEEE-RAS International Conference on Humanoid\n  Robots. The code: https://github.com/BTFrontier/polygon_mapping"},{"id":"http://arxiv.org/abs/2406.14056v3","updated":"2024-11-04T09:31:06Z","published":"2024-06-20T07:24:43Z","title":"VGA: Vision GUI Assistant -- Minimizing Hallucinations through\n  Image-Centric Fine-Tuning","summary":"  Recent advances in Large Vision-Language Models (LVLMs) have significantly\nimprove performance in image comprehension tasks, such as formatted charts and\nrich-content images. Yet, Graphical User Interface (GUI) pose a greater\nchallenge due to their structured format and detailed textual information.\nExisting LVLMs often overly depend on internal knowledge and neglect image\ncontent, resulting in hallucinations and incorrect responses in GUI\ncomprehension. To address these issues, we introduce VGA, a fine-tuned model\ndesigned for comprehensive GUI understanding. Our model aims to enhance the\ninterpretation of visual data of GUI and reduce hallucinations. We first\nconstruct a Vision Question Answering (VQA) dataset of 63.8k high-quality\nexamples with our propose Referent Method, which ensures the model's responses\nare highly depend on visual content within the image. We then design a\ntwo-stage fine-tuning method called Foundation and Advanced Comprehension (FAC)\nto enhance both the model's ability to extract information from image content\nand alignment with human intent. Experiments show that our approach enhances\nthe model's ability to extract information from images and achieves\nstate-of-the-art results in GUI understanding tasks. Our dataset and\nfine-tuning script will be released soon.\n","authors":["Ziyang Meng","Yu Dai","Zezheng Gong","Shaoxiong Guo","Minglong Tang","Tongquan Wei"],"pdf_url":"https://arxiv.org/pdf/2406.14056v3.pdf","comment":"Accepted by EMNLP2024"},{"id":"http://arxiv.org/abs/2411.01916v1","updated":"2024-11-04T09:28:18Z","published":"2024-11-04T09:28:18Z","title":"Masked Autoencoders are Parameter-Efficient Federated Continual Learners","summary":"  Federated learning is a specific distributed learning paradigm in which a\ncentral server aggregates updates from multiple clients' local models, thereby\nenabling the server to learn without requiring clients to upload their private\ndata, maintaining data privacy. While existing federated learning methods are\nprimarily designed for static data, real-world applications often require\nclients to learn new categories over time. This challenge necessitates the\nintegration of continual learning techniques, resulting in federated continual\nlearning (FCL). Although advanced prompt-based continual learning methods\nleverage pre-trained transformers to mitigate catastrophic forgetting, they do\nnot adequately address the non-IID challenges in federated learning. To address\nboth catastrophic forgetting and non-IID issues, we propose to use masked\nautoencoders (MAEs) as parameter-efficient federated continual learners, called\npMAE. pMAE learns reconstructive prompt on the client side through image\nreconstruction using MAEs. On the server side, it reconstructs the uploaded\nrestore information to capture the data distribution across previous tasks and\ndifferent clients, using these reconstructed images to finetune discriminative\nprompt and classifier parameters designed for classification, thereby\nalleviating catastrophic forgetting and non-IID challenges on a global scale.\nExperimental results demonstrate that pMAE achieves performance comparable to\nexisting prompt-based methods and can enhance their effectiveness, particularly\nwhen using self-supervised pre-trained transformers as the backbone. Code is\navailable at: https://github.com/ycheoo/pMAE.\n","authors":["Yuchen He","Xiangfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2411.01916v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01904v1","updated":"2024-11-04T09:15:21Z","published":"2024-11-04T09:15:21Z","title":"FPPL: An Efficient and Non-IID Robust Federated Continual Learning\n  Framework","summary":"  Federated continual learning (FCL) aims to learn from sequential data stream\nin the decentralized federated learning setting, while simultaneously\nmitigating the catastrophic forgetting issue in classical continual learning.\nExisting FCL methods usually employ typical rehearsal mechanisms, which could\nresult in privacy violations or additional onerous storage and computational\nburdens. In this work, an efficient and non-IID robust federated continual\nlearning framework, called Federated Prototype-Augmented Prompt Learning\n(FPPL), is proposed. The FPPL can collaboratively learn lightweight prompts\naugmented by prototypes without rehearsal. On the client side, a fusion\nfunction is employed to fully leverage the knowledge contained in task-specific\nprompts for alleviating catastrophic forgetting. Additionally, global\nprototypes aggregated from the server are used to obtain unified representation\nthrough contrastive learning, mitigating the impact of non-IID-derived data\nheterogeneity. On the server side, locally uploaded prototypes are utilized to\nperform debiasing on the classifier, further alleviating the performance\ndegradation caused by both non-IID and catastrophic forgetting. Empirical\nevaluations demonstrate the effectiveness of FPPL, achieving notable\nperformance with an efficient design while remaining robust to diverse non-IID\ndegrees. Code is available at: https://github.com/ycheoo/FPPL.\n","authors":["Yuchen He","Chuyun Shen","Xiangfeng Wang","Bo Jin"],"pdf_url":"https://arxiv.org/pdf/2411.01904v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05645v3","updated":"2024-11-04T09:14:03Z","published":"2024-07-08T06:14:37Z","title":"OneDiff: A Generalist Model for Image Difference Captioning","summary":"  In computer vision, Image Difference Captioning (IDC) is crucial for\naccurately describing variations between closely related images. Traditional\nIDC methods often rely on specialist models, which restrict their applicability\nacross varied contexts. This paper introduces the OneDiff model, a novel\ngeneralist approach that utilizes a robust vision-language model architecture,\nintegrating a siamese image encoder with a Visual Delta Module. This innovative\nconfiguration allows for the precise detection and articulation of fine-grained\ndifferences between image pairs. OneDiff is trained through a dual-phase\nstrategy, encompassing Coupled Sample Training and multi-task learning across a\ndiverse array of data types, supported by our newly developed DiffCap Dataset.\nThis dataset merges real-world and synthetic data, enhancing the training\nprocess and bolstering the model's robustness. Extensive testing on diverse IDC\nbenchmarks, such as Spot-the-Diff, Image-Editing-Request, and Birds-to-Words,\nshows that OneDiff consistently outperforms existing state-of-the-art models in\naccuracy and adaptability, achieving improvements of up to 97% CIDEr points in\naverage. By setting a new benchmark in IDC, OneDiff paves the way for more\nversatile and effective applications in detecting and describing visual\ndifferences. The code, models, and data will be made publicly available.\n","authors":["Erdong Hu","Longteng Guo","Tongtian Yue","Zijia Zhao","Shuning Xue","Jing Liu"],"pdf_url":"https://arxiv.org/pdf/2407.05645v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15706v3","updated":"2024-11-04T09:06:40Z","published":"2024-03-23T03:56:31Z","title":"GACL: Exemplar-Free Generalized Analytic Continual Learning","summary":"  Class incremental learning (CIL) trains a network on sequential tasks with\nseparated categories in each task but suffers from catastrophic forgetting,\nwhere models quickly lose previously learned knowledge when acquiring new\ntasks. The generalized CIL (GCIL) aims to address the CIL problem in a more\nreal-world scenario, where incoming data have mixed data categories and unknown\nsample size distribution. Existing attempts for the GCIL either have poor\nperformance or invade data privacy by saving exemplars. In this paper, we\npropose a new exemplar-free GCIL technique named generalized analytic continual\nlearning (GACL). The GACL adopts analytic learning (a gradient-free training\ntechnique) and delivers an analytical (i.e., closed-form) solution to the GCIL\nscenario. This solution is derived via decomposing the incoming data into\nexposed and unexposed classes, thereby attaining a weight-invariant property, a\nrare yet valuable property supporting an equivalence between incremental\nlearning and its joint training. Such an equivalence is crucial in GCIL\nsettings as data distributions among different tasks no longer pose challenges\nto adopting our GACL. Theoretically, this equivalence property is validated\nthrough matrix analysis tools. Empirically, we conduct extensive experiments\nwhere, compared with existing GCIL methods, our GACL exhibits a consistently\nleading performance across various datasets and GCIL settings. Source code is\navailable at https://github.com/CHEN-YIZHU/GACL.\n","authors":["Huiping Zhuang","Yizhu Chen","Di Fang","Run He","Kai Tong","Hongxin Wei","Ziqian Zeng","Cen Chen"],"pdf_url":"https://arxiv.org/pdf/2403.15706v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01896v1","updated":"2024-11-04T09:03:43Z","published":"2024-11-04T09:03:43Z","title":"MBDRes-U-Net: Multi-Scale Lightweight Brain Tumor Segmentation Network","summary":"  Accurate segmentation of brain tumors plays a key role in the diagnosis and\ntreatment of brain tumor diseases. It serves as a critical technology for\nquantifying tumors and extracting their features. With the increasing\napplication of deep learning methods, the computational burden has become\nprogressively heavier. To achieve a lightweight model with good segmentation\nperformance, this study proposes the MBDRes-U-Net model using the\nthree-dimensional (3D) U-Net codec framework, which integrates multibranch\nresidual blocks and fused attention into the model. The computational burden of\nthe model is reduced by the branch strategy, which effectively uses the rich\nlocal features in multimodal images and enhances the segmentation performance\nof subtumor regions. Additionally, during encoding, an adaptive weighted\nexpansion convolution layer is introduced into the multi-branch residual block,\nwhich enriches the feature expression and improves the segmentation accuracy of\nthe model. Experiments on the Brain Tumor Segmentation (BraTS) Challenge 2018\nand 2019 datasets show that the architecture could maintain a high precision of\nbrain tumor segmentation while considerably reducing the calculation\noverhead.Our code is released at\nhttps://github.com/Huaibei-normal-university-cv-laboratory/mbdresunet\n","authors":["Longfeng Shen","Yanqi Hou","Jiacong Chen","Liangjin Diao","Yaxi Duan"],"pdf_url":"https://arxiv.org/pdf/2411.01896v1.pdf","comment":"Brain tumor segmentation, lightweight model, Brain Tumor Segmentation\n  (BraTS) Challenge, group convolution"},{"id":"http://arxiv.org/abs/2405.17815v2","updated":"2024-11-04T09:03:31Z","published":"2024-05-28T04:23:00Z","title":"Visual Anchors Are Strong Information Aggregators For Multimodal Large\n  Language Model","summary":"  In the realm of Multimodal Large Language Models (MLLMs), vision-language\nconnector plays a crucial role to link the pre-trained vision encoders with\nLarge Language Models (LLMs). Despite its importance, the vision-language\nconnector has been relatively less explored. In this study, we aim to propose a\nstrong vision-language connector that enables MLLMs to achieve high accuracy\nwhile maintain low computation cost. We first reveal the existence of the\nvisual anchors in Vision Transformer and propose a cost-effective search\nalgorithm to extract them. Building on these findings, we introduce the Anchor\nFormer (AcFormer), a novel vision-language connector designed to leverage the\nrich prior knowledge obtained from these visual anchors during pretraining,\nguiding the aggregation of information. Through extensive experimentation, we\ndemonstrate that the proposed method significantly reduces computational costs\nby nearly two-thirds compared with baseline, while simultaneously outperforming\nbaseline methods. This highlights the effectiveness and efficiency of AcFormer.\nCodes are available at https://github.com/liuhaogeng/Anchor-Former.\n","authors":["Haogeng Liu","Quanzeng You","Xiaotian Han","Yongfei Liu","Huaibo Huang","Ran He","Hongxia Yang"],"pdf_url":"https://arxiv.org/pdf/2405.17815v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05643v2","updated":"2024-11-04T08:58:14Z","published":"2024-10-08T02:46:30Z","title":"TRACE: Temporal Grounding Video LLM via Causal Event Modeling","summary":"  Video Temporal Grounding (VTG) is a crucial capability for video\nunderstanding models and plays a vital role in downstream tasks such as video\nbrowsing and editing. To effectively handle various tasks simultaneously and\nenable zero-shot prediction, there is a growing trend in employing video LLMs\nfor VTG tasks. However, current video LLM-based methods rely exclusively on\nnatural language generation, lacking the ability to model the clear structure\ninherent in videos, which restricts their effectiveness in tackling VTG tasks.\nTo address this issue, this paper first formally introduces causal event\nmodeling framework, which represents videos as sequences of events, and predict\nthe current event using previous events, video inputs, and textural\ninstructions. Each event consists of three components: timestamps, salient\nscores, and textual captions. We then propose a novel task-interleaved video\nLLM called TRACE to effectively implement the causal event modeling framework\nin practice. The TRACE processes visual frames, timestamps, salient scores, and\ntext as distinct tasks, employing various encoders and decoding heads for each.\nTask tokens are arranged in an interleaved sequence according to the causal\nevent modeling framework's formulation. Extensive experiments on various VTG\ntasks and datasets demonstrate the superior performance of TRACE compared to\nstate-of-the-art video LLMs. Our model and code are available at\n\\url{https://github.com/gyxxyg/TRACE}.\n","authors":["Yongxin Guo","Jingyu Liu","Mingda Li","Xiaoying Tang","Qingbin Liu","Xi Chen"],"pdf_url":"https://arxiv.org/pdf/2410.05643v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01893v1","updated":"2024-11-04T08:50:16Z","published":"2024-11-04T08:50:16Z","title":"A Global Depth-Range-Free Multi-View Stereo Transformer Network with\n  Pose Embedding","summary":"  In this paper, we propose a novel multi-view stereo (MVS) framework that gets\nrid of the depth range prior. Unlike recent prior-free MVS methods that work in\na pair-wise manner, our method simultaneously considers all the source images.\nSpecifically, we introduce a Multi-view Disparity Attention (MDA) module to\naggregate long-range context information within and across multi-view images.\nConsidering the asymmetry of the epipolar disparity flow, the key to our method\nlies in accurately modeling multi-view geometric constraints. We integrate pose\nembedding to encapsulate information such as multi-view camera poses, providing\nimplicit geometric constraints for multi-view disparity feature fusion\ndominated by attention. Additionally, we construct corresponding hidden states\nfor each source image due to significant differences in the observation quality\nof the same pixel in the reference frame across multiple source frames. We\nexplicitly estimate the quality of the current pixel corresponding to sampled\npoints on the epipolar line of the source image and dynamically update hidden\nstates through the uncertainty estimation module. Extensive results on the DTU\ndataset and Tanks&Temple benchmark demonstrate the effectiveness of our method.\nThe code is available at our project page:\nhttps://zju3dv.github.io/GD-PoseMVS/.\n","authors":["Yitong Dong","Yijin Li","Zhaoyang Huang","Weikang Bian","Jingbo Liu","Hujun Bao","Zhaopeng Cui","Hongsheng Li","Guofeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.01893v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.02554v2","updated":"2024-11-04T08:48:19Z","published":"2024-02-04T15:59:35Z","title":"DeSparsify: Adversarial Attack Against Token Sparsification Mechanisms\n  in Vision Transformers","summary":"  Vision transformers have contributed greatly to advancements in the computer\nvision domain, demonstrating state-of-the-art performance in diverse tasks\n(e.g., image classification, object detection). However, their high\ncomputational requirements grow quadratically with the number of tokens used.\nToken sparsification mechanisms have been proposed to address this issue. These\nmechanisms employ an input-dependent strategy, in which uninformative tokens\nare discarded from the computation pipeline, improving the model's efficiency.\nHowever, their dynamism and average-case assumption makes them vulnerable to a\nnew threat vector - carefully crafted adversarial examples capable of fooling\nthe sparsification mechanism, resulting in worst-case performance. In this\npaper, we present DeSparsify, an attack targeting the availability of vision\ntransformers that use token sparsification mechanisms. The attack aims to\nexhaust the operating system's resources, while maintaining its stealthiness.\nOur evaluation demonstrates the attack's effectiveness on three token\nsparsification mechanisms and examines the attack's transferability between\nthem and its effect on the GPU resources. To mitigate the impact of the attack,\nwe propose various countermeasures.\n","authors":["Oryan Yehezkel","Alon Zolfi","Amit Baras","Yuval Elovici","Asaf Shabtai"],"pdf_url":"https://arxiv.org/pdf/2402.02554v2.pdf","comment":"18 pages, 6 figures"},{"id":"http://arxiv.org/abs/2403.15751v2","updated":"2024-11-04T08:48:10Z","published":"2024-03-23T07:39:13Z","title":"F-OAL: Forward-only Online Analytic Learning with Fast Training and Low\n  Memory Footprint in Class Incremental Learning","summary":"  Online Class Incremental Learning (OCIL) aims to train models incrementally,\nwhere data arrive in mini-batches, and previous data are not accessible. A\nmajor challenge in OCIL is Catastrophic Forgetting, i.e., the loss of\npreviously learned knowledge. Among existing baselines, replay-based methods\nshow competitive results but requires extra memory for storing exemplars, while\nexemplar-free (i.e., data need not be stored for replay in production) methods\nare resource-friendly but often lack accuracy. In this paper, we propose an\nexemplar-free approach--Forward-only Online Analytic Learning (F-OAL). Unlike\ntraditional methods, F-OAL does not rely on back-propagation and is\nforward-only, significantly reducing memory usage and computational time.\nCooperating with a pre-trained frozen encoder with Feature Fusion, F-OAL only\nneeds to update a linear classifier by recursive least square. This approach\nsimultaneously achieves high accuracy and low resource consumption. Extensive\nexperiments on benchmark datasets demonstrate F-OAL's robust performance in\nOCIL scenarios. Code is available at https://github.com/liuyuchen-cz/F-OAL.\n","authors":["Huiping Zhuang","Yuchen Liu","Run He","Kai Tong","Ziqian Zeng","Cen Chen","Yi Wang","Lap-Pui Chau"],"pdf_url":"https://arxiv.org/pdf/2403.15751v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19458v3","updated":"2024-11-04T08:43:30Z","published":"2024-05-29T19:12:08Z","title":"MemControl: Mitigating Memorization in Diffusion Models via Automated\n  Parameter Selection","summary":"  Diffusion models excel in generating images that closely resemble their\ntraining data but are also susceptible to data memorization, raising privacy,\nethical, and legal concerns, particularly in sensitive domains such as medical\nimaging. We hypothesize that this memorization stems from the\noverparameterization of deep models and propose that regularizing model\ncapacity during fine-tuning can mitigate this issue. Firstly, we empirically\nshow that regulating the model capacity via Parameter-efficient fine-tuning\n(PEFT) mitigates memorization to some extent, however, it further requires the\nidentification of the exact parameter subsets to be fine-tuned for high-quality\ngeneration. To identify these subsets, we introduce a bi-level optimization\nframework, MemControl, that automates parameter selection using memorization\nand generation quality metrics as rewards during fine-tuning. The parameter\nsubsets discovered through MemControl achieve a superior tradeoff between\ngeneration quality and memorization. For the task of medical image generation,\nour approach outperforms existing state-of-the-art memorization mitigation\nstrategies by fine-tuning as few as 0.019% of model parameters. Moreover, we\ndemonstrate that the discovered parameter subsets are transferable to\nnon-medical domains. Our framework is scalable to large datasets, agnostic to\nreward functions, and can be integrated with existing approaches for further\nmemorization mitigation. To the best of our knowledge, this is the first study\nto empirically evaluate memorization in medical images and propose a targeted\nyet universal mitigation strategy. The code is available at\nhttps://github.com/Raman1121/Diffusion_Memorization_HPO\n","authors":["Raman Dutt","Ondrej Bohdal","Pedro Sanchez","Sotirios A. Tsaftaris","Timothy Hospedales"],"pdf_url":"https://arxiv.org/pdf/2405.19458v3.pdf","comment":"Accepted at WACV'25 (Applications Track)"},{"id":"http://arxiv.org/abs/2411.01889v1","updated":"2024-11-04T08:37:12Z","published":"2024-11-04T08:37:12Z","title":"LiDAttack: Robust Black-box Attack on LiDAR-based Object Detection","summary":"  Since DNN is vulnerable to carefully crafted adversarial examples,\nadversarial attack on LiDAR sensors have been extensively studied. We introduce\na robust black-box attack dubbed LiDAttack. It utilizes a genetic algorithm\nwith a simulated annealing strategy to strictly limit the location and number\nof perturbation points, achieving a stealthy and effective attack. And it\nsimulates scanning deviations, allowing it to adapt to dynamic changes in real\nworld scenario variations. Extensive experiments are conducted on 3 datasets\n(i.e., KITTI, nuScenes, and self-constructed data) with 3 dominant object\ndetection models (i.e., PointRCNN, PointPillar, and PV-RCNN++). The results\nreveal the efficiency of the LiDAttack when targeting a wide range of object\ndetection models, with an attack success rate (ASR) up to 90%.\n","authors":["Jinyin Chen","Danxin Liao","Sheng Xiang","Haibin Zheng"],"pdf_url":"https://arxiv.org/pdf/2411.01889v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01870v1","updated":"2024-11-04T07:57:44Z","published":"2024-11-04T07:57:44Z","title":"Mining and Transferring Feature-Geometry Coherence for Unsupervised\n  Point Cloud Registration","summary":"  Point cloud registration, a fundamental task in 3D vision, has achieved\nremarkable success with learning-based methods in outdoor environments.\nUnsupervised outdoor point cloud registration methods have recently emerged to\ncircumvent the need for costly pose annotations. However, they fail to\nestablish reliable optimization objectives for unsupervised training, either\nrelying on overly strong geometric assumptions, or suffering from poor-quality\npseudo-labels due to inadequate integration of low-level geometric and\nhigh-level contextual information. We have observed that in the feature space,\nlatent new inlier correspondences tend to cluster around respective positive\nanchors that summarize features of existing inliers. Motivated by this\nobservation, we propose a novel unsupervised registration method termed INTEGER\nto incorporate high-level contextual information for reliable pseudo-label\nmining. Specifically, we propose the Feature-Geometry Coherence Mining module\nto dynamically adapt the teacher for each mini-batch of data during training\nand discover reliable pseudo-labels by considering both high-level feature\nrepresentations and low-level geometric cues. Furthermore, we propose\nAnchor-Based Contrastive Learning to facilitate contrastive learning with\nanchors for a robust feature space. Lastly, we introduce a Mixed-Density\nStudent to learn density-invariant features, addressing challenges related to\ndensity variation and low overlap in the outdoor scenario. Extensive\nexperiments on KITTI and nuScenes datasets demonstrate that our INTEGER\nachieves competitive performance in terms of accuracy and generalizability.\n","authors":["Kezheng Xiong","Haoen Xiang","Qingshan Xu","Chenglu Wen","Siqi Shen","Jonathan Li","Cheng Wang"],"pdf_url":"https://arxiv.org/pdf/2411.01870v1.pdf","comment":"Accepted by NeurIPS2024"},{"id":"http://arxiv.org/abs/2406.05967v2","updated":"2024-11-04T07:55:31Z","published":"2024-06-10T01:59:00Z","title":"CVQA: Culturally-diverse Multilingual Visual Question Answering\n  Benchmark","summary":"  Visual Question Answering (VQA) is an important task in multimodal AI, and it\nis often used to test the ability of vision-language models to understand and\nreason on knowledge present in both visual and textual data. However, most of\nthe current VQA models use datasets that are primarily focused on English and a\nfew major world languages, with images that are typically Western-centric.\nWhile recent efforts have tried to increase the number of languages covered on\nVQA datasets, they still lack diversity in low-resource languages. More\nimportantly, although these datasets often extend their linguistic range via\ntranslation or some other approaches, they usually keep images the same,\nresulting in narrow cultural representation. To address these limitations, we\nconstruct CVQA, a new Culturally-diverse multilingual Visual Question Answering\nbenchmark, designed to cover a rich set of languages and cultures, where we\nengage native speakers and cultural experts in the data collection process. As\na result, CVQA includes culturally-driven images and questions from across 30\ncountries on four continents, covering 31 languages with 13 scripts, providing\na total of 10k questions. We then benchmark several Multimodal Large Language\nModels (MLLMs) on CVQA, and show that the dataset is challenging for the\ncurrent state-of-the-art models. This benchmark can serve as a probing\nevaluation suite for assessing the cultural capability and bias of multimodal\nmodels and hopefully encourage more research efforts toward increasing cultural\nawareness and linguistic diversity in this field.\n","authors":["David Romero","Chenyang Lyu","Haryo Akbarianto Wibowo","Teresa Lynn","Injy Hamed","Aditya Nanda Kishore","Aishik Mandal","Alina Dragonetti","Artem Abzaliev","Atnafu Lambebo Tonja","Bontu Fufa Balcha","Chenxi Whitehouse","Christian Salamea","Dan John Velasco","David Ifeoluwa Adelani","David Le Meur","Emilio Villa-Cueva","Fajri Koto","Fauzan Farooqui","Frederico Belcavello","Ganzorig Batnasan","Gisela Vallejo","Grainne Caulfield","Guido Ivetta","Haiyue Song","Henok Biadglign Ademtew","Hernán Maina","Holy Lovenia","Israel Abebe Azime","Jan Christian Blaise Cruz","Jay Gala","Jiahui Geng","Jesus-German Ortiz-Barajas","Jinheon Baek","Jocelyn Dunstan","Laura Alonso Alemany","Kumaranage Ravindu Yasas Nagasinghe","Luciana Benotti","Luis Fernando D'Haro","Marcelo Viridiano","Marcos Estecha-Garitagoitia","Maria Camila Buitrago Cabrera","Mario Rodríguez-Cantelar","Mélanie Jouitteau","Mihail Mihaylov","Mohamed Fazli Mohamed Imam","Muhammad Farid Adilazuarda","Munkhjargal Gochoo","Munkh-Erdene Otgonbold","Naome Etori","Olivier Niyomugisha","Paula Mónica Silva","Pranjal Chitale","Raj Dabre","Rendi Chevi","Ruochen Zhang","Ryandito Diandaru","Samuel Cahyawijaya","Santiago Góngora","Soyeong Jeong","Sukannya Purkayastha","Tatsuki Kuribayashi","Teresa Clifford","Thanmay Jayakumar","Tiago Timponi Torrent","Toqeer Ehsan","Vladimir Araujo","Yova Kementchedjhieva","Zara Burzo","Zheng Wei Lim","Zheng Xin Yong","Oana Ignat","Joan Nwatu","Rada Mihalcea","Thamar Solorio","Alham Fikri Aji"],"pdf_url":"https://arxiv.org/pdf/2406.05967v2.pdf","comment":"38th Conference on Neural Information Processing Systems (NeurIPS\n  2024) Track on Datasets and Benchmarks"},{"id":"http://arxiv.org/abs/2411.01859v1","updated":"2024-11-04T07:21:06Z","published":"2024-11-04T07:21:06Z","title":"A Novel Deep Learning Tractography Fiber Clustering Framework for\n  Functionally Consistent White Matter Parcellation Using Multimodal Diffusion\n  MRI and Functional MRI","summary":"  Tractography fiber clustering using diffusion MRI (dMRI) is a crucial\nstrategy for white matter (WM) parcellation. Current methods primarily use the\ngeometric information of fibers (i.e., the spatial trajectories) to group\nsimilar fibers into clusters, overlooking the important functional signals\npresent along the fiber tracts. There is increasing evidence that neural\nactivity in the WM can be measured using functional MRI (fMRI), offering\npotentially valuable multimodal information for fiber clustering. In this\npaper, we develop a novel deep learning fiber clustering framework, namely Deep\nMulti-view Fiber Clustering (DMVFC), that uses joint dMRI and fMRI data to\nenable functionally consistent WM parcellation. DMVFC can effectively integrate\nthe geometric characteristics of the WM fibers with the fMRI BOLD signals along\nthe fiber tracts. It includes two major components: 1) a multi-view pretraining\nmodule to compute embedding features from fiber geometric information and\nfunctional signals separately, and 2) a collaborative fine-tuning module to\nsimultaneously refine the two kinds of embeddings. In the experiments, we\ncompare DMVFC with two state-of-the-art fiber clustering methods and\ndemonstrate superior performance in achieving functionally meaningful and\nconsistent WM parcellation results.\n","authors":["Jin Wang","Bocheng Guo","Yijie Li","Junyi Wang","Yuqian Chen","Jarrett Rushmore","Nikos Makris","Yogesh Rathi","Lauren J O'Donnell","Fan Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.01859v1.pdf","comment":"5 pages, 3 figures"},{"id":"http://arxiv.org/abs/2406.03879v2","updated":"2024-11-04T07:16:54Z","published":"2024-06-06T09:14:32Z","title":"Decay Pruning Method: Smooth Pruning With a Self-Rectifying Procedure","summary":"  Current structured pruning methods often result in considerable accuracy\ndrops due to abrupt network changes and loss of information from pruned\nstructures. To address these issues, we introduce the Decay Pruning Method\n(DPM), a novel smooth pruning approach with a self-rectifying mechanism. DPM\nconsists of two key components: (i) Smooth Pruning: It converts conventional\nsingle-step pruning into multi-step smooth pruning, gradually reducing\nredundant structures to zero over N steps with ongoing optimization. (ii)\nSelf-Rectifying: This procedure further enhances the aforementioned process by\nrectifying sub-optimal pruning based on gradient information. Our approach\ndemonstrates strong generalizability and can be easily integrated with various\nexisting pruning methods. We validate the effectiveness of DPM by integrating\nit with three popular pruning methods: OTOv2, Depgraph, and Gate Decorator.\nExperimental results show consistent improvements in performance compared to\nthe original pruning methods, along with further reductions of FLOPs in most\nscenarios.\n","authors":["Minghao Yang","Linlin Gao","Pengyuan Li","Wenbo Li","Yihong Dong","Zhiying Cui"],"pdf_url":"https://arxiv.org/pdf/2406.03879v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01853v1","updated":"2024-11-04T07:07:31Z","published":"2024-11-04T07:07:31Z","title":"GVKF: Gaussian Voxel Kernel Functions for Highly Efficient Surface\n  Reconstruction in Open Scenes","summary":"  In this paper we present a novel method for efficient and effective 3D\nsurface reconstruction in open scenes. Existing Neural Radiance Fields (NeRF)\nbased works typically require extensive training and rendering time due to the\nadopted implicit representations. In contrast, 3D Gaussian splatting (3DGS)\nuses an explicit and discrete representation, hence the reconstructed surface\nis built by the huge number of Gaussian primitives, which leads to excessive\nmemory consumption and rough surface details in sparse Gaussian areas. To\naddress these issues, we propose Gaussian Voxel Kernel Functions (GVKF), which\nestablish a continuous scene representation based on discrete 3DGS through\nkernel regression. The GVKF integrates fast 3DGS rasterization and highly\neffective scene implicit representations, achieving high-fidelity open scene\nsurface reconstruction. Experiments on challenging scene datasets demonstrate\nthe efficiency and effectiveness of our proposed GVKF, featuring with high\nreconstruction quality, real-time rendering speed, significant savings in\nstorage and training memory consumption.\n","authors":["Gaochao Song","Chong Cheng","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2411.01853v1.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.01851v1","updated":"2024-11-04T07:05:47Z","published":"2024-11-04T07:05:47Z","title":"Silver medal Solution for Image Matching Challenge 2024","summary":"  Image Matching Challenge 2024 is a competition focused on building 3D maps\nfrom diverse image sets, requiring participants to solve fundamental computer\nvision challenges in image matching across varying angles, lighting, and\nseasonal changes. This project develops a Pipeline method that combines\nmultiple advanced techniques: using pre-trained EfficientNet-B7 for initial\nfeature extraction and cosine distance-based image pair filtering, employing\nboth KeyNetAffNetHardNet and SuperPoint for keypoint feature extraction,\nutilizing AdaLAM and SuperGlue for keypoint matching, and finally applying\nPycolmap for 3D spatial analysis. The methodology achieved an excellent score\nof 0.167 on the private leaderboard, with experimental results demonstrating\nthat the combination of KeyNetAffNetHardNet and SuperPoint provides significant\nadvantages in keypoint detection and matching, particularly when dealing with\nchallenging variations in surface texture and environmental conditions that\ntypically degrade traditional algorithm performance.\n","authors":["Yian Wang"],"pdf_url":"https://arxiv.org/pdf/2411.01851v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01846v1","updated":"2024-11-04T06:42:24Z","published":"2024-11-04T06:42:24Z","title":"KptLLM: Unveiling the Power of Large Language Model for Keypoint\n  Comprehension","summary":"  Recent advancements in Multimodal Large Language Models (MLLMs) have greatly\nimproved their abilities in image understanding. However, these models often\nstruggle with grasping pixel-level semantic details, e.g., the keypoints of an\nobject. To bridge this gap, we introduce the novel challenge of Semantic\nKeypoint Comprehension, which aims to comprehend keypoints across different\ntask scenarios, including keypoint semantic understanding, visual prompt-based\nkeypoint detection, and textual prompt-based keypoint detection. Moreover, we\nintroduce KptLLM, a unified multimodal model that utilizes an\nidentify-then-detect strategy to effectively address these challenges. KptLLM\nunderscores the initial discernment of semantics in keypoints, followed by the\nprecise determination of their positions through a chain-of-thought process.\nWith several carefully designed modules, KptLLM adeptly handles various\nmodality inputs, facilitating the interpretation of both semantic contents and\nkeypoint locations. Our extensive experiments demonstrate KptLLM's superiority\nin various keypoint detection benchmarks and its unique semantic capabilities\nin interpreting keypoints.\n","authors":["Jie Yang","Wang Zeng","Sheng Jin","Lumin Xu","Wentao Liu","Chen Qian","Ruimao Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.01846v1.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2406.20024v3","updated":"2024-11-04T06:08:30Z","published":"2024-06-28T16:13:55Z","title":"eMoE-Tracker: Environmental MoE-based Transformer for Robust\n  Event-guided Object Tracking","summary":"  The unique complementarity of frame-based and event cameras for high frame\nrate object tracking has recently inspired some research attempts to develop\nmulti-modal fusion approaches. However, these methods directly fuse both\nmodalities and thus ignore the environmental attributes, e.g., motion blur,\nillumination variance, occlusion, scale variation, etc. Meanwhile, insufficient\ninteraction between search and template features makes distinguishing target\nobjects and backgrounds difficult. As a result, performance degradation is\ninduced especially in challenging conditions. This paper proposes a novel and\neffective Transformer-based event-guided tracking framework, called\neMoE-Tracker, which achieves new SOTA performance under various conditions. Our\nkey idea is to disentangle the environment into several learnable attributes to\ndynamically learn the attribute-specific features and strengthen the target\ninformation by improving the interaction between the target template and search\nregions. To achieve the goal, we first propose an environmental Mix-of-Experts\n(eMoE) module that is built upon the environmental Attributes Disentanglement\nto learn attribute-specific features and environmental Attributes Assembling to\nassemble the attribute-specific features by the learnable attribute scores\ndynamically. The eMoE module is a subtle router that prompt-tunes the\ntransformer backbone more efficiently. We then introduce a contrastive relation\nmodeling (CRM) module to emphasize target information by leveraging a\ncontrastive learning strategy between the target template and search regions.\nExtensive experiments on diverse event-based benchmark datasets showcase the\nsuperior performance of our eMoE-Tracker compared to the prior arts.\n","authors":["Yucheng Chen","Lin Wang"],"pdf_url":"https://arxiv.org/pdf/2406.20024v3.pdf","comment":"RGB-event single object tracking"},{"id":"http://arxiv.org/abs/2411.01833v1","updated":"2024-11-04T06:07:43Z","published":"2024-11-04T06:07:43Z","title":"OwMatch: Conditional Self-Labeling with Consistency for Open-World\n  Semi-Supervised Learning","summary":"  Semi-supervised learning (SSL) offers a robust framework for harnessing the\npotential of unannotated data. Traditionally, SSL mandates that all classes\npossess labeled instances. However, the emergence of open-world SSL (OwSSL)\nintroduces a more practical challenge, wherein unlabeled data may encompass\nsamples from unseen classes. This scenario leads to misclassification of unseen\nclasses as known ones, consequently undermining classification accuracy. To\novercome this challenge, this study revisits two methodologies from\nself-supervised and semi-supervised learning, self-labeling and consistency,\ntailoring them to address the OwSSL problem. Specifically, we propose an\neffective framework called OwMatch, combining conditional self-labeling and\nopen-world hierarchical thresholding. Theoretically, we analyze the estimation\nof class distribution on unlabeled data through rigorous statistical analysis,\nthus demonstrating that OwMatch can ensure the unbiasedness of the self-label\nassignment estimator with reliability. Comprehensive empirical analyses\ndemonstrate that our method yields substantial performance enhancements across\nboth known and unknown classes in comparison to previous studies. Code is\navailable at https://github.com/niusj03/OwMatch.\n","authors":["Shengjie Niu","Lifan Lin","Jian Huang","Chao Wang"],"pdf_url":"https://arxiv.org/pdf/2411.01833v1.pdf","comment":"NeurIPS 2024 camera-ready (10 pages, 4 figures) with the appendices\n  (10 pages, 7 figures)"},{"id":"http://arxiv.org/abs/2310.02901v3","updated":"2024-11-04T06:06:02Z","published":"2023-10-04T15:39:57Z","title":"Efficient Vectorized Backpropagation Algorithms for Training Feedforward\n  Networks Composed of Quadratic Neurons","summary":"  Higher order artificial neurons whose outputs are computed by applying an\nactivation function to a higher order multinomial function of the inputs have\nbeen considered in the past, but did not gain acceptance due to the extra\nparameters and computational cost. However, higher order neurons have\nsignificantly greater learning capabilities since the decision boundaries of\nhigher order neurons can be complex surfaces instead of just hyperplanes. The\nboundary of a single quadratic neuron can be a general hyper-quadric surface\nallowing it to learn many nonlinearly separable datasets. Since quadratic forms\ncan be represented by symmetric matrices, only $\\frac{n(n+1)}{2}$ additional\nparameters are needed instead of $n^2$. A quadratic Logistic regression model\nis first presented. Solutions to the XOR problem with a single quadratic neuron\nare considered. The complete vectorized equations for both forward and backward\npropagation in feedforward networks composed of quadratic neurons are derived.\nA reduced parameter quadratic neural network model with just $ n $ additional\nparameters per neuron that provides a compromise between learning ability and\ncomputational cost is presented. Comparison on benchmark classification\ndatasets are used to demonstrate that a final layer of quadratic neurons\nenables networks to achieve higher accuracy with significantly fewer hidden\nlayer neurons. In particular this paper shows that any dataset composed of\n$\\mathcal{C}$ bounded clusters can be separated with only a single layer of\n$\\mathcal{C}$ quadratic neurons.\n","authors":["Mathew Mithra Noel","Venkataraman Muthiah-Nakarajan"],"pdf_url":"https://arxiv.org/pdf/2310.02901v3.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2403.16999v3","updated":"2024-11-04T05:50:56Z","published":"2024-03-25T17:59:23Z","title":"Visual CoT: Advancing Multi-Modal Language Models with a Comprehensive\n  Dataset and Benchmark for Chain-of-Thought Reasoning","summary":"  Multi-Modal Large Language Models (MLLMs) have demonstrated impressive\nperformance in various VQA tasks. However, they often lack interpretability and\nstruggle with complex visual inputs, especially when the resolution of the\ninput image is high or when the interested region that could provide key\ninformation for answering the question is small. To address these challenges,\nwe collect and introduce the large-scale Visual CoT dataset comprising 438k\nquestion-answer pairs, annotated with intermediate bounding boxes highlighting\nkey regions essential for answering the questions. Additionally, about 98k\npairs of them are annotated with detailed reasoning steps. Importantly, we\npropose a multi-turn processing pipeline that dynamically focuses on visual\ninputs and provides interpretable thoughts. We also introduce the related\nbenchmark to evaluate the MLLMs in scenarios requiring specific local region\nidentification. Extensive experiments demonstrate the effectiveness of our\nframework and shed light on better inference strategies. The Visual CoT\ndataset, benchmark, and pre-trained models are available on\nhttps://hao-shao.com/projects/viscot.html to support further research in this\narea.\n","authors":["Hao Shao","Shengju Qian","Han Xiao","Guanglu Song","Zhuofan Zong","Letian Wang","Yu Liu","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2403.16999v3.pdf","comment":"Project Page: https://hao-shao.com/projects/viscot.html"},{"id":"http://arxiv.org/abs/2410.24060v2","updated":"2024-11-04T05:44:08Z","published":"2024-10-31T15:57:04Z","title":"Understanding Generalizability of Diffusion Models Requires Rethinking\n  the Hidden Gaussian Structure","summary":"  In this work, we study the generalizability of diffusion models by looking\ninto the hidden properties of the learned score functions, which are\nessentially a series of deep denoisers trained on various noise levels. We\nobserve that as diffusion models transition from memorization to\ngeneralization, their corresponding nonlinear diffusion denoisers exhibit\nincreasing linearity. This discovery leads us to investigate the linear\ncounterparts of the nonlinear diffusion models, which are a series of linear\nmodels trained to match the function mappings of the nonlinear diffusion\ndenoisers. Surprisingly, these linear denoisers are approximately the optimal\ndenoisers for a multivariate Gaussian distribution characterized by the\nempirical mean and covariance of the training dataset. This finding implies\nthat diffusion models have the inductive bias towards capturing and utilizing\nthe Gaussian structure (covariance information) of the training dataset for\ndata generation. We empirically demonstrate that this inductive bias is a\nunique property of diffusion models in the generalization regime, which becomes\nincreasingly evident when the model's capacity is relatively small compared to\nthe training dataset size. In the case that the model is highly\noverparameterized, this inductive bias emerges during the initial training\nphases before the model fully memorizes its training data. Our study provides\ncrucial insights into understanding the notable strong generalization\nphenomenon recently observed in real-world diffusion models.\n","authors":["Xiang Li","Yixiang Dai","Qing Qu"],"pdf_url":"https://arxiv.org/pdf/2410.24060v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08590v2","updated":"2024-11-04T05:43:17Z","published":"2024-04-12T16:38:48Z","title":"Vision-Aware Text Features in Referring Image Segmentation: From Object\n  Understanding to Context Understanding","summary":"  Referring image segmentation is a challenging task that involves generating\npixel-wise segmentation masks based on natural language descriptions. The\ncomplexity of this task increases with the intricacy of the sentences provided.\nExisting methods have relied mostly on visual features to generate the\nsegmentation masks while treating text features as supporting components.\nHowever, this under-utilization of text understanding limits the model's\ncapability to fully comprehend the given expressions. In this work, we propose\na novel framework that specifically emphasizes object and context comprehension\ninspired by human cognitive processes through Vision-Aware Text Features.\nFirstly, we introduce a CLIP Prior module to localize the main object of\ninterest and embed the object heatmap into the query initialization process.\nSecondly, we propose a combination of two components: Contextual Multimodal\nDecoder and Meaning Consistency Constraint, to further enhance the coherent and\nconsistent interpretation of language cues with the contextual understanding\nobtained from the image. Our method achieves significant performance\nimprovements on three benchmark datasets RefCOCO, RefCOCO+ and G-Ref. Project\npage: \\url{https://vatex.hkustvgd.com/}.\n","authors":["Hai Nguyen-Truong","E-Ro Nguyen","Tuan-Anh Vu","Minh-Triet Tran","Binh-Son Hua","Sai-Kit Yeung"],"pdf_url":"https://arxiv.org/pdf/2404.08590v2.pdf","comment":"This paper is accepted in WACV 2025"},{"id":"http://arxiv.org/abs/2411.01822v1","updated":"2024-11-04T05:41:31Z","published":"2024-11-04T05:41:31Z","title":"Distribution alignment based transfer fusion frameworks on quantum\n  devices for seeking quantum advantages","summary":"  The scarcity of labelled data is specifically an urgent challenge in the\nfield of quantum machine learning (QML). Two transfer fusion frameworks are\nproposed in this paper to predict the labels of a target domain data by\naligning its distribution to a different but related labelled source domain on\nquantum devices. The frameworks fuses the quantum data from two different, but\nrelated domains through a quantum information infusion channel. The predicting\ntasks in the target domain can be achieved with quantum advantages by\npost-processing quantum measurement results. One framework, the quantum basic\nlinear algebra subroutines (QBLAS) based implementation, can theoretically\nachieve the procedure of transfer fusion with quadratic speedup on a universal\nquantum computer. In addition, the other framework, a hardware-scalable\narchitecture, is implemented on the noisy intermediate-scale quantum (NISQ)\ndevices through a variational hybrid quantum-classical procedure. Numerical\nexperiments on the synthetic and handwritten digits datasets demonstrate that\nthe variatioinal transfer fusion (TF) framework can reach state-of-the-art\n(SOTA) quantum DA method performance.\n","authors":["Xi He","Feiyu Du","Xiaohan Yu","Yang Zhao","Tao Lei"],"pdf_url":"https://arxiv.org/pdf/2411.01822v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01819v1","updated":"2024-11-04T05:39:01Z","published":"2024-11-04T05:39:01Z","title":"DiffuMask-Editor: A Novel Paradigm of Integration Between the\n  Segmentation Diffusion Model and Image Editing to Improve Segmentation\n  Ability","summary":"  Semantic segmentation models, like mask2former, often demand a substantial\namount of manually annotated data, which is time-consuming and inefficient to\nacquire. Leveraging state-of-the-art text-to-image models like Midjourney and\nStable Diffusion has emerged as an effective strategy for automatically\ngenerating synthetic data instead of human annotations. However, prior\napproaches have been constrained to synthesizing single-instance images due to\nthe instability inherent in generating multiple instances with Stable\nDiffusion. To expand the domains and diversity of synthetic datasets, this\npaper introduces a novel paradigm named DiffuMask-Editor, which combines the\nDiffusion Model for Segmentation with Image Editing. By integrating multiple\nobjects into images using Text2Image models, our method facilitates the\ncreation of more realistic datasets that closely resemble open-world settings\nwhile simultaneously generating accurate masks. Our approach significantly\nreduces the laborious effort associated with manual annotation while ensuring\nprecise mask generation. Experimental results demonstrate that synthetic data\ngenerated by DiffuMask-Editor enable segmentation methods to achieve superior\nperformance compared to real data. Particularly in zero-shot backgrounds,\nDiffuMask-Editor achieves new state-of-the-art results on Unseen classes of VOC\n2012. The code and models will be publicly available soon.\n","authors":["Bo Gao","Fangxu Xing","Daniel Tang"],"pdf_url":"https://arxiv.org/pdf/2411.01819v1.pdf","comment":"13 pages,4 figures"},{"id":"http://arxiv.org/abs/2411.01801v1","updated":"2024-11-04T05:00:49Z","published":"2024-11-04T05:00:49Z","title":"Bootstrapping Top-down Information for Self-modulating Slot Attention","summary":"  Object-centric learning (OCL) aims to learn representations of individual\nobjects within visual scenes without manual supervision, facilitating efficient\nand effective visual reasoning. Traditional OCL methods primarily employ\nbottom-up approaches that aggregate homogeneous visual features to represent\nobjects. However, in complex visual environments, these methods often fall\nshort due to the heterogeneous nature of visual features within an object. To\naddress this, we propose a novel OCL framework incorporating a top-down\npathway. This pathway first bootstraps the semantics of individual objects and\nthen modulates the model to prioritize features relevant to these semantics. By\ndynamically modulating the model based on its own output, our top-down pathway\nenhances the representational quality of objects. Our framework achieves\nstate-of-the-art performance across multiple synthetic and real-world\nobject-discovery benchmarks.\n","authors":["Dongwon Kim","Seoyeon Kim","Suha Kwak"],"pdf_url":"https://arxiv.org/pdf/2411.01801v1.pdf","comment":"Accepted to NeurIPS2 2024"},{"id":"http://arxiv.org/abs/2407.04172v2","updated":"2024-11-04T04:59:45Z","published":"2024-07-04T22:16:40Z","title":"ChartGemma: Visual Instruction-tuning for Chart Reasoning in the Wild","summary":"  Given the ubiquity of charts as a data analysis, visualization, and\ndecision-making tool across industries and sciences, there has been a growing\ninterest in developing pre-trained foundation models as well as general purpose\ninstruction-tuned models for chart understanding and reasoning. However,\nexisting methods suffer crucial drawbacks across two critical axes affecting\nthe performance of chart representation models: they are trained on data\ngenerated from underlying data tables of the charts, ignoring the visual trends\nand patterns in chart images, and use weakly aligned vision-language backbone\nmodels for domain-specific training, limiting their generalizability when\nencountering charts in the wild. We address these important drawbacks and\nintroduce ChartGemma, a novel chart understanding and reasoning model developed\nover PaliGemma. Rather than relying on underlying data tables, ChartGemma is\ntrained on instruction-tuning data generated directly from chart images, thus\ncapturing both high-level trends and low-level visual information from a\ndiverse set of charts. Our simple approach achieves state-of-the-art results\nacross $5$ benchmarks spanning chart summarization, question answering, and\nfact-checking, and our elaborate qualitative studies on real-world charts show\nthat ChartGemma generates more realistic and factually correct summaries\ncompared to its contemporaries. We release the code, model checkpoints,\ndataset, and demos at https://github.com/vis-nlp/ChartGemma.\n","authors":["Ahmed Masry","Megh Thakkar","Aayush Bajaj","Aaryaman Kartha","Enamul Hoque","Shafiq Joty"],"pdf_url":"https://arxiv.org/pdf/2407.04172v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01800v1","updated":"2024-11-04T04:58:20Z","published":"2024-11-04T04:58:20Z","title":"Expanding Sparse Tuning for Low Memory Usage","summary":"  Parameter-efficient fine-tuning (PEFT) is an effective method for adapting\npre-trained vision models to downstream tasks by tuning a small subset of\nparameters. Among PEFT methods, sparse tuning achieves superior performance by\nonly adjusting the weights most relevant to downstream tasks, rather than\ndensely tuning the whole weight matrix. However, this performance improvement\nhas been accompanied by increases in memory usage, which stems from two\nfactors, i.e., the storage of the whole weight matrix as learnable parameters\nin the optimizer and the additional storage of tunable weight indexes. In this\npaper, we propose a method named SNELL (Sparse tuning with kerNELized LoRA) for\nsparse tuning with low memory usage. To achieve low memory usage, SNELL\ndecomposes the tunable matrix for sparsification into two learnable low-rank\nmatrices, saving from the costly storage of the whole original matrix. A\ncompetition-based sparsification mechanism is further proposed to avoid the\nstorage of tunable weight indexes. To maintain the effectiveness of sparse\ntuning with low-rank matrices, we extend the low-rank decomposition by applying\nnonlinear kernel functions to the whole-matrix merging. Consequently, we gain\nan increase in the rank of the merged matrix, enhancing the ability of SNELL in\nadapting the pre-trained models to downstream tasks. Extensive experiments on\nmultiple downstream tasks show that SNELL achieves state-of-the-art performance\nwith low memory usage, endowing PEFT with sparse tuning to large-scale models.\nCodes are available at https://github.com/ssfgunner/SNELL.\n","authors":["Shufan Shen","Junshu Sun","Xiangyang Ji","Qingming Huang","Shuhui Wang"],"pdf_url":"https://arxiv.org/pdf/2411.01800v1.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.01797v1","updated":"2024-11-04T04:45:45Z","published":"2024-11-04T04:45:45Z","title":"AIWR: Aerial Image Water Resource Dataset for Segmentation Analysis","summary":"  Effective water resource management is crucial in agricultural regions like\nnortheastern Thailand, where limited water retention in sandy soils poses\nsignificant challenges. In response to this issue, the Aerial Image Water\nResource (AIWR) dataset was developed, comprising 800 aerial images focused on\nnatural and artificial water bodies in this region. The dataset was created\nusing Bing Maps and follows the standards of the Fundamental Geographic Data\nSet (FGDS). It includes ground truth annotations validated by experts in remote\nsensing, making it an invaluable resource for researchers in geoinformatics,\ncomputer vision, and artificial intelligence. The AIWR dataset presents\nconsiderable challenges, such as segmentation due to variations in the size,\ncolor, shape, and similarity of water bodies, which often resemble other land\nuse categories.\n","authors":["Sangdaow Noppitaka","Emmanuel Okafor","Olarik Surinta"],"pdf_url":"https://arxiv.org/pdf/2411.01797v1.pdf","comment":"12 pages, 8 figures"},{"id":"http://arxiv.org/abs/2411.01788v1","updated":"2024-11-04T04:21:41Z","published":"2024-11-04T04:21:41Z","title":"Non rigid geometric distortions correction -- Application to atmospheric\n  turbulence stabilization","summary":"  A novel approach is presented to recover an image degraded by atmospheric\nturbulence. Given a sequence of frames affected by turbulence, we construct a\nvariational model to characterize the static image. The optimization problem is\nsolved by Bregman Iteration and the operator splitting method. Our algorithm is\nsimple, efficient, and can be easily generalized for different scenarios.\n","authors":["Yu Mao","Jerome Gilles"],"pdf_url":"https://arxiv.org/pdf/2411.01788v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01781v1","updated":"2024-11-04T04:14:39Z","published":"2024-11-04T04:14:39Z","title":"MSTA3D: Multi-scale Twin-attention for 3D Instance Segmentation","summary":"  Recently, transformer-based techniques incorporating superpoints have become\nprevalent in 3D instance segmentation. However, they often encounter an\nover-segmentation problem, especially noticeable with large objects.\nAdditionally, unreliable mask predictions stemming from superpoint mask\nprediction further compound this issue. To address these challenges, we propose\na novel framework called MSTA3D. It leverages multi-scale feature\nrepresentation and introduces a twin-attention mechanism to effectively capture\nthem. Furthermore, MSTA3D integrates a box query with a box regularizer,\noffering a complementary spatial constraint alongside semantic queries.\nExperimental evaluations on ScanNetV2, ScanNet200 and S3DIS datasets\ndemonstrate that our approach surpasses state-of-the-art 3D instance\nsegmentation methods.\n","authors":["Duc Dang Trung Tran","Byeongkeun Kang","Yeejin Lee"],"pdf_url":"https://arxiv.org/pdf/2411.01781v1.pdf","comment":"14 pages, 9 figures, 7 tables, conference"},{"id":"http://arxiv.org/abs/2411.01777v1","updated":"2024-11-04T03:58:09Z","published":"2024-11-04T03:58:09Z","title":"Learning predictable and robust neural representations by straightening\n  image sequences","summary":"  Prediction is a fundamental capability of all living organisms, and has been\nproposed as an objective for learning sensory representations. Recent work\ndemonstrates that in primate visual systems, prediction is facilitated by\nneural representations that follow straighter temporal trajectories than their\ninitial photoreceptor encoding, which allows for prediction by linear\nextrapolation. Inspired by these experimental findings, we develop a\nself-supervised learning (SSL) objective that explicitly quantifies and\npromotes straightening. We demonstrate the power of this objective in training\ndeep feedforward neural networks on smoothly-rendered synthetic image sequences\nthat mimic commonly-occurring properties of natural videos. The learned model\ncontains neural embeddings that are predictive, but also factorize the\ngeometric, photometric, and semantic attributes of objects. The representations\nalso prove more robust to noise and adversarial attacks compared to previous\nSSL methods that optimize for invariance to random augmentations. Moreover,\nthese beneficial properties can be transferred to other training procedures by\nusing the straightening objective as a regularizer, suggesting a broader\nutility for straightening as a principle for robust unsupervised learning.\n","authors":["Xueyan Niu","Cristina Savin","Eero P. Simoncelli"],"pdf_url":"https://arxiv.org/pdf/2411.01777v1.pdf","comment":"Accepted at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.01769v1","updated":"2024-11-04T03:29:51Z","published":"2024-11-04T03:29:51Z","title":"ARN-LSTM: A Multi-Stream Attention-Based Model for Action Recognition\n  with Temporal Dynamics","summary":"  This paper presents ARN-LSTM, a novel multi-stream action recognition model\ndesigned to address the challenge of simultaneously capturing spatial motion\nand temporal dynamics in action sequences. Traditional methods often focus\nsolely on spatial or temporal features, limiting their ability to comprehend\ncomplex human activities fully. Our proposed model integrates joint, motion,\nand temporal information through a multi-stream fusion architecture.\nSpecifically, it comprises a joint stream for extracting skeleton features, a\ntemporal stream for capturing dynamic temporal features, and an ARN-LSTM block\nthat utilizes Time-Distributed Long Short-Term Memory (TD-LSTM) layers followed\nby an Attention Relation Network (ARN) to model temporal relations. The outputs\nfrom these streams are fused in a fully connected layer to provide the final\naction prediction. Evaluations on the NTU RGB+D 60 and NTU RGB+D 120 datasets\ndemonstrate the effectiveness of our model, achieving effective performance,\nparticularly in group activity recognition.\n","authors":["Chuanchuan Wang","Ahmad Sufril Azlan Mohmamed","Xiao Yang","Xiang Li"],"pdf_url":"https://arxiv.org/pdf/2411.01769v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14133v2","updated":"2024-11-04T02:59:25Z","published":"2023-03-24T16:38:58Z","title":"Survey on Adversarial Attack and Defense for Medical Image Analysis:\n  Methods and Challenges","summary":"  Deep learning techniques have achieved superior performance in computer-aided\nmedical image analysis, yet they are still vulnerable to imperceptible\nadversarial attacks, resulting in potential misdiagnosis in clinical practice.\nOppositely, recent years have also witnessed remarkable progress in defense\nagainst these tailored adversarial examples in deep medical diagnosis systems.\nIn this exposition, we present a comprehensive survey on recent advances in\nadversarial attacks and defenses for medical image analysis with a systematic\ntaxonomy in terms of the application scenario. We also provide a unified\nframework for different types of adversarial attack and defense methods in the\ncontext of medical image analysis. For a fair comparison, we establish a new\nbenchmark for adversarially robust medical diagnosis models obtained by\nadversarial training under various scenarios. To the best of our knowledge,\nthis is the first survey paper that provides a thorough evaluation of\nadversarially robust medical diagnosis models. By analyzing qualitative and\nquantitative results, we conclude this survey with a detailed discussion of\ncurrent challenges for adversarial attack and defense in medical image analysis\nsystems to shed light on future research directions. Code is available on\n\\href{https://github.com/tomvii/Adv_MIA}{\\color{red}{GitHub}}.\n","authors":["Junhao Dong","Junxi Chen","Xiaohua Xie","Jianhuang Lai","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2303.14133v2.pdf","comment":"Accepted by ACM Computing Surveys (CSUR) (DOI:\n  https://doi.org/10.1145/3702638)"},{"id":"http://arxiv.org/abs/2305.04161v3","updated":"2024-11-04T02:52:56Z","published":"2023-05-07T02:26:00Z","title":"Camera-Based HRV Prediction for Remote Learning Environments","summary":"  In recent years, due to the widespread use of internet videos, remote\nphotoplethysmography (rPPG) has gained more and more attention in the fields of\naffective computing. Restoring blood volume pulse (BVP) signals from facial\nvideos is a challenging task that involves a series of preprocessing, image\nalgorithms, and postprocessing to restore waveforms. Not only is the heart rate\nmetric utilized for affective computing, but the heart rate variability (HRV)\nmetric is even more significant. The challenge in obtaining HRV indices through\nrPPG lies in the necessity for algorithms to precisely predict the BVP peak\npositions. In this paper, we collected the Remote Learning Affect and\nPhysiology (RLAP) dataset, which includes over 32 hours of highly synchronized\nvideo and labels from 58 subjects. This is a public dataset whose BVP labels\nhave been meticulously designed to better suit the training of HRV models.\nUsing the RLAP dataset, we trained a new model called Seq-rPPG, it is a model\nbased on one-dimensional convolution, and experimental results reveal that this\nstructure is more suitable for handling HRV tasks, which outperformed all other\nbaselines in HRV performance and also demonstrated significant advantages in\ncomputational efficiency.\n","authors":["Kegang Wang","Yantao Wei","Jiankai Tang","Yuntao Wang","Mingwen Tong","Jie Gao","Yujian Ma","Zhongjin Zhao"],"pdf_url":"https://arxiv.org/pdf/2305.04161v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01759v1","updated":"2024-11-04T02:52:02Z","published":"2024-11-04T02:52:02Z","title":"Automatic Structured Pruning for Efficient Architecture in Federated\n  Learning","summary":"  In Federated Learning (FL), training is conducted on client devices,\ntypically with limited computational resources and storage capacity. To address\nthese constraints, we propose an automatic pruning scheme tailored for FL\nsystems. Our solution improves computation efficiency on client devices, while\nminimizing communication costs. One of the challenges of tuning pruning\nhyper-parameters in FL systems is the restricted access to local data. Thus, we\nintroduce an automatic pruning paradigm that dynamically determines pruning\nboundaries. Additionally, we utilized a structured pruning algorithm optimized\nfor mobile devices that lack hardware support for sparse computations.\nExperimental results demonstrate the effectiveness of our approach, achieving\naccuracy comparable to existing methods. Our method notably reduces the number\nof parameters by 89% and FLOPS by 90%, with minimal impact on the accuracy of\nthe FEMNIST and CelebFaces datasets. Furthermore, our pruning method decreases\ncommunication overhead by up to 5x and halves inference time when deployed on\nAndroid devices.\n","authors":["Thai Vu Nguyen","Long Bao Le","Anderson Avila"],"pdf_url":"https://arxiv.org/pdf/2411.01759v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01758v1","updated":"2024-11-04T02:50:52Z","published":"2024-11-04T02:50:52Z","title":"Disentangled PET Lesion Segmentation","summary":"  PET imaging is an invaluable tool in clinical settings as it captures the\nfunctional activity of both healthy anatomy and cancerous lesions. Developing\nautomatic lesion segmentation methods for PET images is crucial since manual\nlesion segmentation is laborious and prone to inter- and intra-observer\nvariability. We propose PET-Disentangler, a 3D disentanglement method that uses\na 3D UNet-like encoder-decoder architecture to disentangle disease and normal\nhealthy anatomical features with losses for segmentation, reconstruction, and\nhealthy component plausibility. A critic network is used to encourage the\nhealthy latent features to match the distribution of healthy samples and thus\nencourages these features to not contain any lesion-related features. Our\nquantitative results show that PET-Disentangler is less prone to incorrectly\ndeclaring healthy and high tracer uptake regions as cancerous lesions, since\nsuch uptake pattern would be assigned to the disentangled healthy component.\n","authors":["Tanya Gatsak","Kumar Abhishek","Hanene Ben Yedder","Saeid Asgari Taghanaki","Ghassan Hamarneh"],"pdf_url":"https://arxiv.org/pdf/2411.01758v1.pdf","comment":"4 pages, 2 figures, 1 table"},{"id":"http://arxiv.org/abs/2411.01756v1","updated":"2024-11-04T02:43:55Z","published":"2024-11-04T02:43:55Z","title":"ChatTracker: Enhancing Visual Tracking Performance via Chatting with\n  Multimodal Large Language Model","summary":"  Visual object tracking aims to locate a targeted object in a video sequence\nbased on an initial bounding box. Recently, Vision-Language~(VL) trackers have\nproposed to utilize additional natural language descriptions to enhance\nversatility in various applications. However, VL trackers are still inferior to\nState-of-The-Art (SoTA) visual trackers in terms of tracking performance. We\nfound that this inferiority primarily results from their heavy reliance on\nmanual textual annotations, which include the frequent provision of ambiguous\nlanguage descriptions. In this paper, we propose ChatTracker to leverage the\nwealth of world knowledge in the Multimodal Large Language Model (MLLM) to\ngenerate high-quality language descriptions and enhance tracking performance.\nTo this end, we propose a novel reflection-based prompt optimization module to\niteratively refine the ambiguous and inaccurate descriptions of the target with\ntracking feedback. To further utilize semantic information produced by MLLM, a\nsimple yet effective VL tracking framework is proposed and can be easily\nintegrated as a plug-and-play module to boost the performance of both VL and\nvisual trackers. Experimental results show that our proposed ChatTracker\nachieves a performance comparable to existing methods.\n","authors":["Yiming Sun","Fan Yu","Shaoxiang Chen","Yu Zhang","Junwei Huang","Chenhui Li","Yang Li","Changbo Wang"],"pdf_url":"https://arxiv.org/pdf/2411.01756v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09728v2","updated":"2024-11-04T02:34:59Z","published":"2024-06-14T05:33:01Z","title":"Neural Pose Representation Learning for Generating and Transferring\n  Non-Rigid Object Poses","summary":"  We propose a novel method for learning representations of poses for 3D\ndeformable objects, which specializes in 1) disentangling pose information from\nthe object's identity, 2) facilitating the learning of pose variations, and 3)\ntransferring pose information to other object identities. Based on these\nproperties, our method enables the generation of 3D deformable objects with\ndiversity in both identities and poses, using variations of a single object. It\ndoes not require explicit shape parameterization such as skeletons or joints,\npoint-level or shape-level correspondence supervision, or variations of the\ntarget object for pose transfer. To achieve pose disentanglement, compactness\nfor generative models, and transferability, we first design the pose extractor\nto represent the pose as a keypoint-based hybrid representation and the pose\napplier to learn an implicit deformation field. To better distill pose\ninformation from the object's geometry, we propose the implicit pose applier to\noutput an intrinsic mesh property, the face Jacobian. Once the extracted pose\ninformation is transferred to the target object, the pose applier is fine-tuned\nin a self-supervised manner to better describe the target object's shapes with\npose variations. The extracted poses are also used to train a cascaded\ndiffusion model to enable the generation of novel poses. Our experiments with\nthe DeformThings4D and Human datasets demonstrate state-of-the-art performance\nin pose transfer and the ability to generate diverse deformed shapes with\nvarious objects and poses.\n","authors":["Seungwoo Yoo","Juil Koo","Kyeongmin Yeo","Minhyuk Sung"],"pdf_url":"https://arxiv.org/pdf/2406.09728v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2406.19464v2","updated":"2024-11-04T02:21:30Z","published":"2024-06-27T18:06:38Z","title":"ManiWAV: Learning Robot Manipulation from In-the-Wild Audio-Visual Data","summary":"  Audio signals provide rich information for the robot interaction and object\nproperties through contact. This information can surprisingly ease the learning\nof contact-rich robot manipulation skills, especially when the visual\ninformation alone is ambiguous or incomplete. However, the usage of audio data\nin robot manipulation has been constrained to teleoperated demonstrations\ncollected by either attaching a microphone to the robot or object, which\nsignificantly limits its usage in robot learning pipelines. In this work, we\nintroduce ManiWAV: an 'ear-in-hand' data collection device to collect\nin-the-wild human demonstrations with synchronous audio and visual feedback,\nand a corresponding policy interface to learn robot manipulation policy\ndirectly from the demonstrations. We demonstrate the capabilities of our system\nthrough four contact-rich manipulation tasks that require either passively\nsensing the contact events and modes, or actively sensing the object surface\nmaterials and states. In addition, we show that our system can generalize to\nunseen in-the-wild environments by learning from diverse in-the-wild human\ndemonstrations.\n","authors":["Zeyi Liu","Cheng Chi","Eric Cousineau","Naveen Kuppuswamy","Benjamin Burchfiel","Shuran Song"],"pdf_url":"https://arxiv.org/pdf/2406.19464v2.pdf","comment":"Conference on Robot Learning (CoRL) 2024; Project website:\n  https://maniwav.github.io/"},{"id":"http://arxiv.org/abs/2411.01749v1","updated":"2024-11-04T02:20:22Z","published":"2024-11-04T02:20:22Z","title":"Multi-task Geometric Estimation of Depth and Surface Normal from\n  Monocular 360° Images","summary":"  Geometric estimation is required for scene understanding and analysis in\npanoramic 360{\\deg} images. Current methods usually predict a single feature,\nsuch as depth or surface normal. These methods can lack robustness, especially\nwhen dealing with intricate textures or complex object surfaces. We introduce a\nnovel multi-task learning (MTL) network that simultaneously estimates depth and\nsurface normals from 360{\\deg} images. Our first innovation is our MTL\narchitecture, which enhances predictions for both tasks by integrating\ngeometric information from depth and surface normal estimation, enabling a\ndeeper understanding of 3D scene structure. Another innovation is our fusion\nmodule, which bridges the two tasks, allowing the network to learn shared\nrepresentations that improve accuracy and robustness. Experimental results\ndemonstrate that our MTL architecture significantly outperforms\nstate-of-the-art methods in both depth and surface normal estimation, showing\nsuperior performance in complex and diverse scenes. Our model's effectiveness\nand generalizability, particularly in handling intricate surface textures,\nestablish it as a new benchmark in 360{\\deg} image geometric estimation. The\ncode and model are available at\n\\url{https://github.com/huangkun101230/360MTLGeometricEstimation}.\n","authors":["Kun Huang","Fang-Lue Zhang","Fangfang Zhang","Yu-Kun Lai","Paul Rosin","Neil A. Dodgson"],"pdf_url":"https://arxiv.org/pdf/2411.01749v1.pdf","comment":"18 pages, this paper is accepted by Computational Visual Media\n  Journal (CVMJ) but not pushlished yet"},{"id":"http://arxiv.org/abs/2411.01748v1","updated":"2024-11-04T02:13:41Z","published":"2024-11-04T02:13:41Z","title":"Rotation Perturbation Robustness in Point Cloud Analysis: A Perspective\n  of Manifold Distillation","summary":"  Point cloud is often regarded as a discrete sampling of Riemannian manifold\nand plays a pivotal role in the 3D image interpretation. Particularly, rotation\nperturbation, an unexpected small change in rotation caused by various factors\n(like equipment offset, system instability, measurement errors and so on), can\neasily lead to the inferior results in point cloud learning tasks. However,\nclassical point cloud learning methods are sensitive to rotation perturbation,\nand the existing networks with rotation robustness also have much room for\nimprovements in terms of performance and noise tolerance. Given these, this\npaper remodels the point cloud from the perspective of manifold as well as\ndesigns a manifold distillation method to achieve the robustness of rotation\nperturbation without any coordinate transformation. In brief, during the\ntraining phase, we introduce a teacher network to learn the rotation robustness\ninformation and transfer this information to the student network through online\ndistillation. In the inference phase, the student network directly utilizes the\noriginal 3D coordinate information to achieve the robustness of rotation\nperturbation. Experiments carried out on four different datasets verify the\neffectiveness of our method. Averagely, on the Modelnet40 and ScanobjectNN\nclassification datasets with random rotation perturbations, our classification\naccuracy has respectively improved by 4.92% and 4.41%, compared to popular\nrotation-robust networks; on the ShapeNet and S3DIS segmentation datasets,\ncompared to the rotation-robust networks, the improvements of mIoU are 7.36%\nand 4.82%, respectively. Besides, from the experimental results, the proposed\nalgorithm also shows excellent performance in resisting noise and outliers.\n","authors":["Xinyu Xu","Huazhen Liu","Feiming Wei","Huilin Xiong","Wenxian Yu","Tao Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.01748v1.pdf","comment":"13 pages, 8 figures, submitted to TCSVT"},{"id":"http://arxiv.org/abs/2411.01742v1","updated":"2024-11-04T01:51:50Z","published":"2024-11-04T01:51:50Z","title":"Learning from Convolution-based Unlearnable Datastes","summary":"  The construction of large datasets for deep learning has raised concerns\nregarding unauthorized use of online data, leading to increased interest in\nprotecting data from third-parties who want to use it for training. The\nConvolution-based Unlearnable DAtaset (CUDA) method aims to make data\nunlearnable by applying class-wise blurs to every image in the dataset so that\nneural networks learn relations between blur kernels and labels, as opposed to\ninformative features for classifying clean data. In this work, we evaluate\nwhether CUDA data remains unlearnable after image sharpening and frequency\nfiltering, finding that this combination of simple transforms improves the\nutility of CUDA data for training. In particular, we observe a substantial\nincrease in test accuracy over adversarial training for models trained with\nCUDA unlearnable data from CIFAR-10, CIFAR-100, and ImageNet-100. In training\nmodels to high accuracy using unlearnable data, we underscore the need for\nongoing refinement in data poisoning techniques to ensure data privacy. Our\nmethod opens new avenues for enhancing the robustness of unlearnable datasets\nby highlighting that simple methods such as sharpening and frequency filtering\nare capable of breaking convolution-based unlearnable datasets.\n","authors":["Dohyun Kim","Pedro Sandoval-Segura"],"pdf_url":"https://arxiv.org/pdf/2411.01742v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01739v1","updated":"2024-11-04T01:42:41Z","published":"2024-11-04T01:42:41Z","title":"Not Just Object, But State: Compositional Incremental Learning without\n  Forgetting","summary":"  Most incremental learners excessively prioritize coarse classes of objects\nwhile neglecting various kinds of states (e.g. color and material) attached to\nthe objects. As a result, they are limited in the ability to reason\nfine-grained compositionality of state-object pairs. To remedy this limitation,\nwe propose a novel task called Compositional Incremental Learning\n(composition-IL), enabling the model to recognize state-object compositions as\na whole in an incremental learning fashion. Since the lack of suitable\nbenchmarks, we re-organize two existing datasets and make them tailored for\ncomposition-IL. Then, we propose a prompt-based Composition Incremental Learner\n(CompILer), to overcome the ambiguous composition boundary problem which\nchallenges composition-IL largely. Specifically, we exploit multi-pool prompt\nlearning, which is regularized by inter-pool prompt discrepancy and intra-pool\nprompt diversity. Besides, we devise object-injected state prompting by using\nobject prompts to guide the selection of state prompts. Furthermore, we fuse\nthe selected prompts by a generalized-mean strategy, to eliminate irrelevant\ninformation learned in the prompts. Extensive experiments on two datasets\nexhibit state-of-the-art performance achieved by CompILer.\n","authors":["Yanyi Zhang","Binglin Qiu","Qi Jia","Yu Liu","Ran He"],"pdf_url":"https://arxiv.org/pdf/2411.01739v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01734v1","updated":"2024-11-04T01:32:09Z","published":"2024-11-04T01:32:09Z","title":"Next Best View For Point-Cloud Model Acquisition: Bayesian Approximation\n  and Uncertainty Analysis","summary":"  The Next Best View problem is a computer vision problem widely studied in\nrobotics. To solve it, several methodologies have been proposed over the years.\nSome, more recently, propose the use of deep learning models. Predictions\nobtained with the help of deep learning models naturally have some uncertainty\nassociated with them. Despite this, the standard models do not allow for their\nquantification. However, Bayesian estimation theory contributed to the\ndemonstration that dropout layers allow to estimate prediction uncertainty in\nneural networks.\n  This work adapts the point-net-based neural network for Next-Best-View\n(PC-NBV). It incorporates dropout layers into the model's architecture, thus\nallowing the computation of the uncertainty estimate associated with its\npredictions. The aim of the work is to improve the network's accuracy in\ncorrectly predicting the next best viewpoint, proposing a way to make the 3D\nreconstruction process more efficient.\n  Two uncertainty measurements capable of reflecting the prediction's error and\naccuracy, respectively, were obtained. These enabled the reduction of the\nmodel's error and the increase in its accuracy from 30\\% to 80\\% by identifying\nand disregarding predictions with high values of uncertainty. Another method\nthat directly uses these uncertainty metrics to improve the final prediction\nwas also proposed. However, it showed very residual improvements.\n","authors":["Madalena Caldeira","Plinio Moreno"],"pdf_url":"https://arxiv.org/pdf/2411.01734v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14370v4","updated":"2024-11-04T01:25:37Z","published":"2024-03-21T12:57:30Z","title":"SyncTweedies: A General Generative Framework Based on Synchronized\n  Diffusions","summary":"  We introduce a general framework for generating diverse visual content,\nincluding ambiguous images, panorama images, mesh textures, and Gaussian splat\ntextures, by synchronizing multiple diffusion processes. We present exhaustive\ninvestigation into all possible scenarios for synchronizing multiple diffusion\nprocesses through a canonical space and analyze their characteristics across\napplications. In doing so, we reveal a previously unexplored case: averaging\nthe outputs of Tweedie's formula while conducting denoising in multiple\ninstance spaces. This case also provides the best quality with the widest\napplicability to downstream tasks. We name this case SyncTweedies. In our\nexperiments generating visual content aforementioned, we demonstrate the\nsuperior quality of generation by SyncTweedies compared to other\nsynchronization methods, optimization-based and iterative-update-based methods.\n","authors":["Jaihoon Kim","Juil Koo","Kyeongmin Yeo","Minhyuk Sung"],"pdf_url":"https://arxiv.org/pdf/2403.14370v4.pdf","comment":"Project page: https://synctweedies.github.io/ (NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2406.13155v2","updated":"2024-11-04T00:55:06Z","published":"2024-06-19T02:09:44Z","title":"Convolutional Kolmogorov-Arnold Networks","summary":"  In this paper, we introduce Convolutional Kolmogorov-Arnold Networks\n(Convolutional KANs), an innovative alternative to the standard Convolutional\nNeural Networks (CNNs) that have revolutionized the field of computer vision.\nBy integrating the learneable non-linear activation functions presented in\nKolmogorov-Arnold Networks (KANs) into convolutions, we propose a new layer.\nThroughout the paper, we empirically validate the performance of Convolutional\nKANs against traditional architectures across Fashion-MNIST dataset, finding\nthat, in some cases, this new approach maintains a similar level of accuracy\nwhile using half the number of parameters. This experiments show that KAN\nConvolutions seem to learn more per kernel, which opens up a new horizon of\npossibilities in deep learning for computer vision.\n","authors":["Alexander Dylan Bodner","Antonio Santiago Tepsich","Jack Natan Spolski","Santiago Pourteau"],"pdf_url":"https://arxiv.org/pdf/2406.13155v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01725v1","updated":"2024-11-04T00:49:47Z","published":"2024-11-04T00:49:47Z","title":"A Probabilistic Formulation of LiDAR Mapping with Neural Radiance Fields","summary":"  In this paper we reexamine the process through which a Neural Radiance Field\n(NeRF) can be trained to produce novel LiDAR views of a scene. Unlike image\napplications where camera pixels integrate light over time, LiDAR pulses arrive\nat specific times. As such, multiple LiDAR returns are possible for any given\ndetector and the classification of these returns is inherently probabilistic.\nApplying a traditional NeRF training routine can result in the network learning\nphantom surfaces in free space between conflicting range measurements, similar\nto how floater aberrations may be produced by an image model. We show that by\nformulating loss as an integral of probability (rather than as an integral of\noptical density) the network can learn multiple peaks for a given ray, allowing\nthe sampling of first, nth, or strongest returns from a single output channel.\nCode is available at https://github.com/mcdermatt/PLINK\n","authors":["Matthew McDermott","Jason Rife"],"pdf_url":"https://arxiv.org/pdf/2411.01725v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.15748v2","updated":"2024-11-04T00:48:12Z","published":"2023-05-25T05:55:53Z","title":"ReactFace: Online Multiple Appropriate Facial Reaction Generation in\n  Dyadic Interactions","summary":"  In dyadic interaction, predicting the listener's facial reactions is\nchallenging as different reactions could be appropriate in response to the same\nspeaker's behaviour. Previous approaches predominantly treated this task as an\ninterpolation or fitting problem, emphasizing deterministic outcomes but\nignoring the diversity and uncertainty of human facial reactions. Furthermore,\nthese methods often failed to model short-range and long-range dependencies\nwithin the interaction context, leading to issues in the synchrony and\nappropriateness of the generated facial reactions. To address these\nlimitations, this paper reformulates the task as an extrapolation or prediction\nproblem, and proposes an novel framework (called ReactFace) to generate\nmultiple different but appropriate facial reactions from a speaker behaviour\nrather than merely replicating the corresponding listener facial behaviours.\nOur ReactFace generates multiple different but appropriate photo-realistic\nhuman facial reactions by: (i) learning an appropriate facial reaction\ndistribution representing multiple different but appropriate facial reactions;\nand (ii) synchronizing the generated facial reactions with the speaker verbal\nand non-verbal behaviours at each time stamp, resulting in realistic 2D facial\nreaction sequences. Experimental results demonstrate the effectiveness of our\napproach in generating multiple diverse, synchronized, and appropriate facial\nreactions from each speaker's behaviour. The quality of the generated facial\nreactions is intimately tied to the speaker's speech and facial expressions,\nachieved through our novel speaker-listener interaction modules. Our code is\nmade publicly available at \\url{https://github.com/lingjivoo/ReactFace}.\n","authors":["Cheng Luo","Siyang Song","Weicheng Xie","Micol Spitale","Zongyuan Ge","Linlin Shen","Hatice Gunes"],"pdf_url":"https://arxiv.org/pdf/2305.15748v2.pdf","comment":"Accepted to IEEE Transactions on Visualization and Computer Graphics\n  (TVCG), 18 pages, 10 figures"}]},"2024-11-03T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2411.01713v1","updated":"2024-11-03T23:36:53Z","published":"2024-11-03T23:36:53Z","title":"Rethinking Weight Decay for Robust Fine-Tuning of Foundation Models","summary":"  Modern optimizers such as AdamW, equipped with momentum and adaptive learning\nrate, are designed to escape local minima and explore the vast parameter space.\nThis exploration is beneficial for finding good loss basins when training from\nscratch. It is not necessarily ideal when resuming from a powerful foundation\nmodel because it can lead to large deviations from the pre-trained\ninitialization and, consequently, worse robustness and generalization. At the\nsame time, strong regularization on all parameters can lead to under-fitting.\nWe hypothesize that selectively regularizing the parameter space is the key to\nfitting and retraining the pre-trained knowledge. This paper proposes a new\nweight decay technique, Selective Projection Decay (SPD), that selectively\nimposes a strong penalty on certain layers while allowing others to change\nfreely. Intuitively, SPD expands and contracts the parameter search space for\nlayers with consistent and inconsistent loss reduction, respectively.\nExperimentally, when equipped with SPD, Adam consistently provides better\nin-distribution generalization and out-of-distribution robustness performance\non multiple popular vision and language benchmarks. Code available\nat~\\url{https://github.com/GT-RIPL/Selective-Projection-Decay.git}\n","authors":["Junjiao Tian","Chengyue Huang","Zsolt Kira"],"pdf_url":"https://arxiv.org/pdf/2411.01713v1.pdf","comment":"Accepted to Neurips 2024"},{"id":"http://arxiv.org/abs/2411.01710v1","updated":"2024-11-03T23:02:30Z","published":"2024-11-03T23:02:30Z","title":"SPES: Spectrogram Perturbation for Explainable Speech-to-Text Generation","summary":"  Spurred by the demand for interpretable models, research on eXplainable AI\nfor language technologies has experienced significant growth, with feature\nattribution methods emerging as a cornerstone of this progress. While prior\nwork in NLP explored such methods for classification tasks and textual\napplications, explainability intersecting generation and speech is lagging,\nwith existing techniques failing to account for the autoregressive nature of\nstate-of-the-art models and to provide fine-grained, phonetically meaningful\nexplanations. We address this gap by introducing Spectrogram Perturbation for\nExplainable Speech-to-text Generation (SPES), a feature attribution technique\napplicable to sequence generation tasks with autoregressive models. SPES\nprovides explanations for each predicted token based on both the input\nspectrogram and the previously generated tokens. Extensive evaluation on speech\nrecognition and translation demonstrates that SPES generates explanations that\nare faithful and plausible to humans.\n","authors":["Dennis Fucci","Marco Gaido","Beatrice Savoldi","Matteo Negri","Mauro Cettolo","Luisa Bentivogli"],"pdf_url":"https://arxiv.org/pdf/2411.01710v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.16337v3","updated":"2024-11-03T22:44:20Z","published":"2024-05-25T19:40:50Z","title":"Learning to Reason via Program Generation, Emulation, and Search","summary":"  Program synthesis with language models (LMs) has unlocked a large set of\nreasoning abilities; code-tuned LMs have proven adept at generating programs\nthat solve a wide variety of algorithmic symbolic manipulation tasks (e.g. word\nconcatenation). However, not all reasoning tasks are easily expressible as\ncode, e.g. tasks involving commonsense reasoning, moral decision-making, and\nsarcasm understanding. Our goal is to extend an LM's program synthesis skills\nto such tasks and evaluate the results via pseudo-programs, namely Python\nprograms where some leaf function calls are left undefined. To that end, we\npropose, Code Generation and Emulated EXecution (CoGEX). CoGEX works by (1)\ntraining LMs to generate pseudo-programs, (2) teaching them to emulate their\ngenerated program's execution, including those leaf functions, allowing the\nLM's knowledge to fill in the execution gaps; and (3) using them to search over\nmany programs to find an optimal one. To adapt the CoGEX model to a new task,\nwe introduce a method for performing program search to find a single program\nwhose pseudo-execution yields optimal performance when applied to all the\ninstances of a given dataset. We show that our approach yields large\nimprovements compared to standard in-context learning approaches on a battery\nof tasks, both algorithmic and soft reasoning. This result thus demonstrates\nthat code synthesis can be applied to a much broader class of problems than\npreviously considered. Our released dataset, fine-tuned models, and\nimplementation can be found at \\url{https://github.com/nweir127/CoGEX}.\n","authors":["Nathaniel Weir","Muhammad Khalifa","Linlu Qiu","Orion Weller","Peter Clark"],"pdf_url":"https://arxiv.org/pdf/2405.16337v3.pdf","comment":"NeurIPS 2024 camera ready"},{"id":"http://arxiv.org/abs/2406.17261v2","updated":"2024-11-03T22:38:08Z","published":"2024-06-25T04:01:32Z","title":"TRAWL: Tensor Reduced and Approximated Weights for Large Language Models","summary":"  Recent research has shown that pruning large-scale language models for\ninference is an effective approach to improving model efficiency, significantly\nreducing model weights with minimal impact on performance. Interestingly,\npruning can sometimes even enhance accuracy by removing noise that accumulates\nduring training, particularly through matrix decompositions. However, recent\nwork has primarily focused on single matrix decompositions or lower precision\ntechniques, which may fail to fully capture structural patterns. To address\nthese limitations, we introduce TRAWL (Tensor Reduced and Approximated Weights\nfor Large Language Models), a technique that applies tensor decomposition\nacross multiple weight matrices to effectively denoise LLMs by capturing global\nstructural patterns. Our experiments show that TRAWL improves model performance\nby up to 16% over baseline models on benchmark datasets, without requiring\nadditional data, training, or fine-tuning.\n","authors":["Yiran Luo","Het Patel","Yu Fu","Dawon Ahn","Jia Chen","Yue Dong","Evangelos E. Papalexakis"],"pdf_url":"https://arxiv.org/pdf/2406.17261v2.pdf","comment":"4 pages. Submitted to NAACL 2025 and under review"},{"id":"http://arxiv.org/abs/2411.01706v1","updated":"2024-11-03T22:31:02Z","published":"2024-11-03T22:31:02Z","title":"Investigating Large Language Models for Complex Word Identification in\n  Multilingual and Multidomain Setups","summary":"  Complex Word Identification (CWI) is an essential step in the lexical\nsimplification task and has recently become a task on its own. Some variations\nof this binary classification task have emerged, such as lexical complexity\nprediction (LCP) and complexity evaluation of multi-word expressions (MWE).\nLarge language models (LLMs) recently became popular in the Natural Language\nProcessing community because of their versatility and capability to solve\nunseen tasks in zero/few-shot settings. Our work investigates LLM usage,\nspecifically open-source models such as Llama 2, Llama 3, and Vicuna v1.5, and\nclosed-source, such as ChatGPT-3.5-turbo and GPT-4o, in the CWI, LCP, and MWE\nsettings. We evaluate zero-shot, few-shot, and fine-tuning settings and show\nthat LLMs struggle in certain conditions or achieve comparable results against\nexisting methods. In addition, we provide some views on meta-learning combined\nwith prompt learning. In the end, we conclude that the current state of LLMs\ncannot or barely outperform existing methods, which are usually much smaller.\n","authors":["Răzvan-Alexandru Smădu","David-Gabriel Ion","Dumitru-Clementin Cercel","Florin Pop","Mihaela-Claudia Cercel"],"pdf_url":"https://arxiv.org/pdf/2411.01706v1.pdf","comment":"37 pages, 16 figures, Accepted by EMNLP 2024"},{"id":"http://arxiv.org/abs/2411.01705v1","updated":"2024-11-03T22:27:40Z","published":"2024-11-03T22:27:40Z","title":"Data Extraction Attacks in Retrieval-Augmented Generation via Backdoors","summary":"  Despite significant advancements, large language models (LLMs) still struggle\nwith providing accurate answers when lacking domain-specific or up-to-date\nknowledge. Retrieval-Augmented Generation (RAG) addresses this limitation by\nincorporating external knowledge bases, but it also introduces new attack\nsurfaces. In this paper, we investigate data extraction attacks targeting the\nknowledge databases of RAG systems. We demonstrate that previous attacks on RAG\nlargely depend on the instruction-following capabilities of LLMs, and that\nsimple fine-tuning can reduce the success rate of such attacks to nearly zero.\nThis makes these attacks impractical since fine-tuning is a common practice\nwhen deploying LLMs in specific domains. To further reveal the vulnerability,\nwe propose to backdoor RAG, where a small portion of poisoned data is injected\nduring the fine-tuning phase to create a backdoor within the LLM. When this\ncompromised LLM is integrated into a RAG system, attackers can exploit specific\ntriggers in prompts to manipulate the LLM to leak documents from the retrieval\ndatabase. By carefully designing the poisoned data, we achieve both verbatim\nand paraphrased document extraction. We show that with only 3\\% poisoned data,\nour method achieves an average success rate of 79.7\\% in verbatim extraction on\nLlama2-7B, with a ROUGE-L score of 64.21, and a 68.6\\% average success rate in\nparaphrased extraction, with an average ROUGE score of 52.6 across four\ndatasets. These results underscore the privacy risks associated with the supply\nchain when deploying RAG systems.\n","authors":["Yuefeng Peng","Junda Wang","Hong Yu","Amir Houmansadr"],"pdf_url":"https://arxiv.org/pdf/2411.01705v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01703v1","updated":"2024-11-03T22:19:20Z","published":"2024-11-03T22:19:20Z","title":"UniGuard: Towards Universal Safety Guardrails for Jailbreak Attacks on\n  Multimodal Large Language Models","summary":"  Multimodal large language models (MLLMs) have revolutionized vision-language\nunderstanding but are vulnerable to multimodal jailbreak attacks, where\nadversaries meticulously craft inputs to elicit harmful or inappropriate\nresponses. We propose UniGuard, a novel multimodal safety guardrail that\njointly considers the unimodal and cross-modal harmful signals. UniGuard is\ntrained such that the likelihood of generating harmful responses in a toxic\ncorpus is minimized, and can be seamlessly applied to any input prompt during\ninference with minimal computational costs. Extensive experiments demonstrate\nthe generalizability of UniGuard across multiple modalities and attack\nstrategies. It demonstrates impressive generalizability across multiple\nstate-of-the-art MLLMs, including LLaVA, Gemini Pro, GPT-4, MiniGPT-4, and\nInstructBLIP, thereby broadening the scope of our solution.\n","authors":["Sejoon Oh","Yiqiao Jin","Megha Sharma","Donghyun Kim","Eric Ma","Gaurav Verma","Srijan Kumar"],"pdf_url":"https://arxiv.org/pdf/2411.01703v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2410.21647v3","updated":"2024-11-03T21:24:10Z","published":"2024-10-29T01:21:05Z","title":"Can Language Models Replace Programmers? REPOCOD Says 'Not Yet'","summary":"  Large language models (LLMs) have achieved high accuracy, i.e., more than 90%\npass@1, in solving Python coding problems in HumanEval and MBPP. Thus, a\nnatural question is, whether LLMs achieve comparable code completion\nperformance compared to human developers? Unfortunately, one cannot answer this\nquestion using existing manual crafted or simple (e.g., single-line) code\ngeneration benchmarks, since such tasks fail to represent real-world software\ndevelopment tasks. In addition, existing benchmarks often use poor code\ncorrectness metrics, providing misleading conclusions.\n  To address these challenges, we create REPOCOD, a code generation benchmark\nwith 980 problems collected from 11 popular real-world projects, with more than\n58% of them requiring file-level or repository-level context information. In\naddition, REPOCOD has the longest average canonical solution length (331.6\ntokens) and the highest average cyclomatic complexity (9.00) compared to\nexisting benchmarks. Each task in REPOCOD includes 313.5 developer-written test\ncases on average for better correctness evaluation. In our evaluations of ten\nLLMs, none of the models achieve more than 30% pass@1 on REPOCOD, indicating\nthe necessity of building stronger LLMs that can help developers in real-world\nsoftware development. REPOCOD is available at\nhttps://github.com/lt-asset/REPOCOD\n","authors":["Shanchao Liang","Yiran Hu","Nan Jiang","Lin Tan"],"pdf_url":"https://arxiv.org/pdf/2410.21647v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12861v2","updated":"2024-11-03T20:58:35Z","published":"2024-07-10T11:31:20Z","title":"CiteME: Can Language Models Accurately Cite Scientific Claims?","summary":"  Thousands of new scientific papers are published each month. Such information\noverload complicates researcher efforts to stay current with the\nstate-of-the-art as well as to verify and correctly attribute claims. We pose\nthe following research question: Given a text excerpt referencing a paper,\ncould an LM act as a research assistant to correctly identify the referenced\npaper? We advance efforts to answer this question by building a benchmark that\nevaluates the abilities of LMs in citation attribution. Our benchmark, CiteME,\nconsists of text excerpts from recent machine learning papers, each referencing\na single other paper. CiteME use reveals a large gap between frontier LMs and\nhuman performance, with LMs achieving only 4.2-18.5% accuracy and humans 69.7%.\nWe close this gap by introducing CiteAgent, an autonomous system built on the\nGPT-4o LM that can also search and read papers, which achieves an accuracy of\n35.3\\% on CiteME. Overall, CiteME serves as a challenging testbed for\nopen-ended claim attribution, driving the research community towards a future\nwhere any claim made by an LM can be automatically verified and discarded if\nfound to be incorrect.\n","authors":["Ori Press","Andreas Hochlehnert","Ameya Prabhu","Vishaal Udandarao","Ofir Press","Matthias Bethge"],"pdf_url":"https://arxiv.org/pdf/2407.12861v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13743v3","updated":"2024-11-03T20:22:32Z","published":"2024-06-19T18:00:07Z","title":"GenAI-Bench: Evaluating and Improving Compositional Text-to-Visual\n  Generation","summary":"  While text-to-visual models now produce photo-realistic images and videos,\nthey struggle with compositional text prompts involving attributes,\nrelationships, and higher-order reasoning such as logic and comparison. In this\nwork, we conduct an extensive human study on GenAI-Bench to evaluate the\nperformance of leading image and video generation models in various aspects of\ncompositional text-to-visual generation. We also compare automated evaluation\nmetrics against our collected human ratings and find that VQAScore -- a metric\nmeasuring the likelihood that a VQA model views an image as accurately\ndepicting the prompt -- significantly outperforms previous metrics such as\nCLIPScore. In addition, VQAScore can improve generation in a black-box manner\n(without finetuning) via simply ranking a few (3 to 9) candidate images.\nRanking by VQAScore is 2x to 3x more effective than other scoring methods like\nPickScore, HPSv2, and ImageReward at improving human alignment ratings for\nDALL-E 3 and Stable Diffusion, especially on compositional prompts that require\nadvanced visio-linguistic reasoning. We release a new GenAI-Rank benchmark with\nover 40,000 human ratings to evaluate scoring metrics on ranking images\ngenerated from the same prompt. Lastly, we discuss promising areas for\nimprovement in VQAScore, such as addressing fine-grained visual details. We\nwill release all human ratings (over 80,000) to facilitate scientific\nbenchmarking of both generative models and automated metrics.\n","authors":["Baiqi Li","Zhiqiu Lin","Deepak Pathak","Jiayao Li","Yixin Fei","Kewen Wu","Tiffany Ling","Xide Xia","Pengchuan Zhang","Graham Neubig","Deva Ramanan"],"pdf_url":"https://arxiv.org/pdf/2406.13743v3.pdf","comment":"We open-source our dataset, model, and code at:\n  https://linzhiqiu.github.io/papers/genai_bench ; Project page:\n  https://linzhiqiu.github.io/papers/genai_bench ; GenAI-Bench was first\n  introduced in arxiv:2404.01291. This article extends it with an additional\n  GenAI-Rank benchmark"},{"id":"http://arxiv.org/abs/2406.18776v2","updated":"2024-11-03T19:56:24Z","published":"2024-06-26T22:10:15Z","title":"Implicit Discourse Relation Classification For Nigerian Pidgin","summary":"  Despite attempts to make Large Language Models multi-lingual, many of the\nworld's languages are still severely under-resourced. This widens the\nperformance gap between NLP and AI applications aimed at well-financed, and\nthose aimed at less-resourced languages. In this paper, we focus on Nigerian\nPidgin (NP), which is spoken by nearly 100 million people, but has\ncomparatively very few NLP resources and corpora. We address the task of\nImplicit Discourse Relation Classification (IDRC) and systematically compare an\napproach translating NP data to English and then using a well-resourced IDRC\ntool and back-projecting the labels versus creating a synthetic discourse\ncorpus for NP, in which we translate PDTB and project PDTB labels, and then\ntrain an NP IDR classifier. The latter approach of learning a \"native\" NP\nclassifier outperforms our baseline by 13.27\\% and 33.98\\% in f$_{1}$ score for\n4-way and 11-way classification, respectively.\n","authors":["Muhammed Saeed","Peter Bourgonje","Vera Demberg"],"pdf_url":"https://arxiv.org/pdf/2406.18776v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01663v1","updated":"2024-11-03T19:18:57Z","published":"2024-11-03T19:18:57Z","title":"Unlocking the Theory Behind Scaling 1-Bit Neural Networks","summary":"  Recently, 1-bit Large Language Models (LLMs) have emerged, showcasing an\nimpressive combination of efficiency and performance that rivals traditional\nLLMs. Research by Wang et al. (2023); Ma et al. (2024) indicates that the\nperformance of these 1-bit LLMs progressively improves as the number of\nparameters increases, hinting at the potential existence of a Scaling Law for\n1-bit Neural Networks. In this paper, we present the first theoretical result\nthat rigorously establishes this scaling law for 1-bit models. We prove that,\ndespite the constraint of weights restricted to $\\{-1, +1\\}$, the dynamics of\nmodel training inevitably align with kernel behavior as the network width\ngrows. This theoretical breakthrough guarantees convergence of the 1-bit model\nto an arbitrarily small loss as width increases. Furthermore, we introduce the\nconcept of the generalization difference, defined as the gap between the\noutputs of 1-bit networks and their full-precision counterparts, and\ndemonstrate that this difference maintains a negligible level as network width\nscales. Building on the work of Kaplan et al. (2020), we conclude by examining\nhow the training loss scales as a power-law function of the model size, dataset\nsize, and computational resources utilized for training. Our findings\nunderscore the promising potential of scaling 1-bit neural networks, suggesting\nthat int1 could become the standard in future neural network precision.\n","authors":["Majid Daliri","Zhao Song","Chiwun Yang"],"pdf_url":"https://arxiv.org/pdf/2411.01663v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05266v3","updated":"2024-11-03T18:38:50Z","published":"2024-03-08T12:42:36Z","title":"ERBench: An Entity-Relationship based Automatically Verifiable\n  Hallucination Benchmark for Large Language Models","summary":"  Large language models (LLMs) have achieved unprecedented performances in\nvarious applications, yet evaluating them is still challenging. Existing\nbenchmarks are either manually constructed or are automatic, but lack the\nability to evaluate the thought process of LLMs with arbitrary complexity. We\ncontend that utilizing existing relational databases based on the\nentity-relationship (ER) model is a promising approach for constructing\nbenchmarks as they contain structured knowledge that can be used to question\nLLMs. Unlike knowledge graphs, which are also used to evaluate LLMs, relational\ndatabases have integrity constraints that can be used to better construct\ncomplex in-depth questions and verify answers: (1) functional dependencies can\nbe used to pinpoint critical keywords that an LLM must know to properly answer\na given question containing certain attribute values; and (2) foreign key\nconstraints can be used to join relations and construct multi-hop questions,\nwhich can be arbitrarily long and used to debug intermediate answers. We thus\npropose ERBench, which uses these integrity constraints to convert any database\ninto an LLM benchmark. ERBench supports continuous evaluation as databases\nchange, multimodal questions, and various prompt engineering techniques. In our\nexperiments, we construct LLM benchmarks using databases of multiple domains\nand make an extensive comparison of contemporary LLMs. We show how ERBench can\nproperly evaluate any LLM by not only checking for answer correctness, but also\neffectively verifying the rationales by looking for the right keywords.\n","authors":["Jio Oh","Soyeon Kim","Junseok Seo","Jindong Wang","Ruochen Xu","Xing Xie","Steven Euijong Whang"],"pdf_url":"https://arxiv.org/pdf/2403.05266v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01653v1","updated":"2024-11-03T18:37:35Z","published":"2024-11-03T18:37:35Z","title":"Diagnosing Medical Datasets with Training Dynamics","summary":"  This study explores the potential of using training dynamics as an automated\nalternative to human annotation for evaluating the quality of training data.\nThe framework used is Data Maps, which classifies data points into categories\nsuch as easy-to-learn, hard-to-learn, and ambiguous (Swayamdipta et al., 2020).\nSwayamdipta et al. (2020) highlight that difficult-to-learn examples often\ncontain errors, and ambiguous cases significantly impact model training. To\nconfirm the reliability of these findings, we replicated the experiments using\na challenging dataset, with a focus on medical question answering. In addition\nto text comprehension, this field requires the acquisition of detailed medical\nknowledge, which further complicates the task. A comprehensive evaluation was\nconducted to assess the feasibility and transferability of the Data Maps\nframework to the medical domain. The evaluation indicates that the framework is\nunsuitable for addressing datasets' unique challenges in answering medical\nquestions.\n","authors":["Laura Wenderoth"],"pdf_url":"https://arxiv.org/pdf/2411.01653v1.pdf","comment":"https://github.com/LauraWenderoth/training-dynamics"},{"id":"http://arxiv.org/abs/2405.18415v2","updated":"2024-11-03T18:23:45Z","published":"2024-05-28T17:57:06Z","title":"Why are Visually-Grounded Language Models Bad at Image Classification?","summary":"  Image classification is one of the most fundamental capabilities of machine\nvision intelligence. In this work, we revisit the image classification task\nusing visually-grounded language models (VLMs) such as GPT-4V and LLaVA. We\nfind that existing proprietary and public VLMs, despite often using CLIP as a\nvision encoder and having many more parameters, significantly underperform CLIP\non standard image classification benchmarks like ImageNet. To understand the\nreason, we explore several hypotheses concerning the inference algorithms,\ntraining objectives, and data processing in VLMs. Our analysis reveals that the\nprimary cause is data-related: critical information for image classification is\nencoded in the VLM's latent space but can only be effectively decoded with\nenough training data. Specifically, there is a strong correlation between the\nfrequency of class exposure during VLM training and instruction-tuning and the\nVLM's performance in those classes; when trained with sufficient data, VLMs can\nmatch the accuracy of state-of-the-art classification models. Based on these\nfindings, we enhance a VLM by integrating classification-focused datasets into\nits training, and demonstrate that the enhanced classification performance of\nthe VLM transfers to its general capabilities, resulting in an improvement of\n11.8% on the newly collected ImageWikiQA dataset.\n","authors":["Yuhui Zhang","Alyssa Unell","Xiaohan Wang","Dhruba Ghosh","Yuchang Su","Ludwig Schmidt","Serena Yeung-Levy"],"pdf_url":"https://arxiv.org/pdf/2405.18415v2.pdf","comment":"Published at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2406.09519v2","updated":"2024-11-03T17:48:48Z","published":"2024-06-13T18:12:01Z","title":"Talking Heads: Understanding Inter-layer Communication in Transformer\n  Language Models","summary":"  Although it is known that transformer language models (LMs) pass features\nfrom early layers to later layers, it is not well understood how this\ninformation is represented and routed by the model. We analyze a mechanism used\nin two LMs to selectively inhibit items in a context in one task, and find that\nit underlies a commonly used abstraction across many context-retrieval\nbehaviors. Specifically, we find that models write into low-rank subspaces of\nthe residual stream to represent features which are then read out by later\nlayers, forming low-rank communication channels (Elhage et al., 2021) between\nlayers. A particular 3D subspace in model activations in GPT-2 can be traversed\nto positionally index items in lists, and we show that this mechanism can\nexplain an otherwise arbitrary-seeming sensitivity of the model to the order of\nitems in the prompt. That is, the model has trouble copying the correct\ninformation from context when many items ``crowd\" this limited space. By\ndecomposing attention heads with the Singular Value Decomposition (SVD), we\nfind that previously described interactions between heads separated by one or\nmore layers can be predicted via analysis of their weight matrices alone. We\nshow that it is possible to manipulate the internal model representations as\nwell as edit model weights based on the mechanism we discover in order to\nsignificantly improve performance on our synthetic Laundry List task, which\nrequires recall from a list, often improving task accuracy by over 20%. Our\nanalysis reveals a surprisingly intricate interpretable structure learned from\nlanguage model pretraining, and helps us understand why sophisticated LMs\nsometimes fail in simple domains, facilitating future analysis of more complex\nbehaviors.\n","authors":["Jack Merullo","Carsten Eickhoff","Ellie Pavlick"],"pdf_url":"https://arxiv.org/pdf/2406.09519v2.pdf","comment":"Neurips 2024"},{"id":"http://arxiv.org/abs/2411.01643v1","updated":"2024-11-03T17:37:06Z","published":"2024-11-03T17:37:06Z","title":"EcoAct: Economic Agent Determines When to Register What Action","summary":"  Recent advancements have enabled Large Language Models (LLMs) to function as\nagents that can perform actions using external tools. This requires\nregistering, i.e., integrating tool information into the LLM context prior to\ntaking actions. Current methods indiscriminately incorporate all candidate\ntools into the agent's context and retain them across multiple reasoning steps.\nThis process remains opaque to LLM agents and is not integrated into their\nreasoning procedures, leading to inefficiencies due to increased context length\nfrom irrelevant tools. To address this, we introduce EcoAct, a tool using\nalgorithm that allows LLMs to selectively register tools as needed, optimizing\ncontext use. By integrating the tool registration process into the reasoning\nprocedure, EcoAct reduces computational costs by over 50% in multiple steps\nreasoning tasks while maintaining performance, as demonstrated through\nextensive experiments. Moreover, it can be plugged into any reasoning pipeline\nwith only minor modifications to the prompt, making it applicable to LLM agents\nnow and future.\n","authors":["Shaokun Zhang","Jieyu Zhang","Dujian Ding","Mirian Hipolito Garcia","Ankur Mallick","Daniel Madrigal","Menglin Xia","Victor Rühle","Qingyun Wu","Chi Wang"],"pdf_url":"https://arxiv.org/pdf/2411.01643v1.pdf","comment":"16 pages, 10 figures"},{"id":"http://arxiv.org/abs/2411.01636v1","updated":"2024-11-03T17:24:02Z","published":"2024-11-03T17:24:02Z","title":"Leveraging Microservices Architecture for Dynamic Pricing in the Travel\n  Industry: Algorithms, Scalability, and Impact on Revenue and Customer\n  Satisfaction","summary":"  This research investigates the implementation of a real-time,\nmicroservices-oriented dynamic pricing system for the travel sector. The system\nis designed to address factors such as demand, competitor pricing, and other\nexternal circumstances in real-time. Both controlled simulation and real-life\napplication showed a respectable gain of 22% in revenue generation and a 17%\nimprovement in pricing response time which concern the issues of scaling and\nflexibility of classical pricing mechanisms. Demand forecasting, competitor\npricing strategies, and event-based pricing were implemented as separate\nmicroservices to enhance their scalability and reduce resource consumption by\n30% during peak loads. Customers were also more content as depicted by a 15%\nincrease in satisfaction score post-implementation given the appreciation of\nmore appropriate pricing. This research enhances the existing literature with\npractical illustrations of the possible application of microservices technology\nin developing dynamic pricing solutions in a complex and data-driven context.\nThere exist however areas for improvement for instance inter-service latency\nand the need for extensive real-time data pipelines. The present research goes\non to suggest combining these with direct data capture from customer behavior\nat the same time as machine learning capacity developments in pricing\nalgorithms to assist in more accurate real time pricing. It is determined that\nthe use of microservices is a reasonable and efficient model for dynamic\npricing, allowing the tourism sector to employ evidence-based and customer\ncentric pricing techniques, which ensures that their profits are not\njeopardized because of the need for customers.\n","authors":["Biman Barua","M. Shamim Kaiser"],"pdf_url":"https://arxiv.org/pdf/2411.01636v1.pdf","comment":"19 pages, 18 figures"},{"id":"http://arxiv.org/abs/2407.11214v2","updated":"2024-11-03T17:14:37Z","published":"2024-07-15T19:57:15Z","title":"PutnamBench: Evaluating Neural Theorem-Provers on the Putnam\n  Mathematical Competition","summary":"  We present PutnamBench, a new multi-language benchmark for evaluating the\nability of neural theorem-provers to solve competition mathematics problems.\nPutnamBench consists of 1692 hand-constructed formalizations of 640 theorems\nsourced from the William Lowell Putnam Mathematical Competition, the premier\nundergraduate-level mathematics competition in North America. All the problems\nhave formalizations in Lean 4 and Isabelle; a substantial subset also has Coq\nformalizations. PutnamBench requires significant problem-solving ability and\nproficiency in a broad range of topics taught in undergraduate mathematics\ncourses. We use PutnamBench to evaluate several established neural and symbolic\ntheorem-provers. These approaches can only solve a handful of the PutnamBench\nproblems, establishing the benchmark as a difficult open challenge for research\non neural theorem-proving. PutnamBench is available at\nhttps://github.com/trishullab/PutnamBench.\n","authors":["George Tsoukalas","Jasper Lee","John Jennings","Jimmy Xin","Michelle Ding","Michael Jennings","Amitayush Thakur","Swarat Chaudhuri"],"pdf_url":"https://arxiv.org/pdf/2407.11214v2.pdf","comment":"Accepted at NeurIPS 2024 Datasets & Benchmarks Track"},{"id":"http://arxiv.org/abs/2406.06007v3","updated":"2024-11-03T16:54:14Z","published":"2024-06-10T04:07:09Z","title":"CARES: A Comprehensive Benchmark of Trustworthiness in Medical Vision\n  Language Models","summary":"  Artificial intelligence has significantly impacted medical applications,\nparticularly with the advent of Medical Large Vision Language Models\n(Med-LVLMs), sparking optimism for the future of automated and personalized\nhealthcare. However, the trustworthiness of Med-LVLMs remains unverified,\nposing significant risks for future model deployment. In this paper, we\nintroduce CARES and aim to comprehensively evaluate the Trustworthiness of\nMed-LVLMs across the medical domain. We assess the trustworthiness of Med-LVLMs\nacross five dimensions, including trustfulness, fairness, safety, privacy, and\nrobustness. CARES comprises about 41K question-answer pairs in both closed and\nopen-ended formats, covering 16 medical image modalities and 27 anatomical\nregions. Our analysis reveals that the models consistently exhibit concerns\nregarding trustworthiness, often displaying factual inaccuracies and failing to\nmaintain fairness across different demographic groups. Furthermore, they are\nvulnerable to attacks and demonstrate a lack of privacy awareness. We publicly\nrelease our benchmark and code in https://cares-ai.github.io/.\n","authors":["Peng Xia","Ze Chen","Juanxi Tian","Yangrui Gong","Ruibo Hou","Yue Xu","Zhenbang Wu","Zhiyuan Fan","Yiyang Zhou","Kangyu Zhu","Wenhao Zheng","Zhaoyang Wang","Xiao Wang","Xuchao Zhang","Chetan Bansal","Marc Niethammer","Junzhou Huang","Hongtu Zhu","Yun Li","Jimeng Sun","Zongyuan Ge","Gang Li","James Zou","Huaxiu Yao"],"pdf_url":"https://arxiv.org/pdf/2406.06007v3.pdf","comment":"NeurIPS 2024 Datasets and Benchmarks Track"},{"id":"http://arxiv.org/abs/2410.20746v2","updated":"2024-11-03T16:19:49Z","published":"2024-10-28T05:25:50Z","title":"ElectionSim: Massive Population Election Simulation Powered by Large\n  Language Model Driven Agents","summary":"  The massive population election simulation aims to model the preferences of\nspecific groups in particular election scenarios. It has garnered significant\nattention for its potential to forecast real-world social trends. Traditional\nagent-based modeling (ABM) methods are constrained by their ability to\nincorporate complex individual background information and provide interactive\nprediction results. In this paper, we introduce ElectionSim, an innovative\nelection simulation framework based on large language models, designed to\nsupport accurate voter simulations and customized distributions, together with\nan interactive platform to dialogue with simulated voters. We present a\nmillion-level voter pool sampled from social media platforms to support\naccurate individual simulation. We also introduce PPE, a poll-based\npresidential election benchmark to assess the performance of our framework\nunder the U.S. presidential election scenario. Through extensive experiments\nand analyses, we demonstrate the effectiveness and robustness of our framework\nin U.S. presidential election simulations.\n","authors":["Xinnong Zhang","Jiayu Lin","Libo Sun","Weihong Qi","Yihang Yang","Yue Chen","Hanjia Lyu","Xinyi Mou","Siming Chen","Jiebo Luo","Xuanjing Huang","Shiping Tang","Zhongyu Wei"],"pdf_url":"https://arxiv.org/pdf/2410.20746v2.pdf","comment":"42 pages, 14 figures"},{"id":"http://arxiv.org/abs/2411.01612v1","updated":"2024-11-03T15:39:20Z","published":"2024-11-03T15:39:20Z","title":"Ontology Population using LLMs","summary":"  Knowledge graphs (KGs) are increasingly utilized for data integration,\nrepresentation, and visualization. While KG population is critical, it is often\ncostly, especially when data must be extracted from unstructured text in\nnatural language, which presents challenges, such as ambiguity and complex\ninterpretations. Large Language Models (LLMs) offer promising capabilities for\nsuch tasks, excelling in natural language understanding and content generation.\nHowever, their tendency to ``hallucinate'' can produce inaccurate outputs.\nDespite these limitations, LLMs offer rapid and scalable processing of natural\nlanguage data, and with prompt engineering and fine-tuning, they can\napproximate human-level performance in extracting and structuring data for KGs.\nThis study investigates LLM effectiveness for the KG population, focusing on\nthe Enslaved.org Hub Ontology. In this paper, we report that compared to the\nground truth, LLM's can extract ~90% of triples, when provided a modular\nontology as guidance in the prompts.\n","authors":["Sanaz Saki Norouzi","Adrita Barua","Antrea Christou","Nikita Gautam","Andrew Eells","Pascal Hitzler","Cogan Shimizu"],"pdf_url":"https://arxiv.org/pdf/2411.01612v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01610v1","updated":"2024-11-03T15:31:44Z","published":"2024-11-03T15:31:44Z","title":"Explaining and Improving Contrastive Decoding by Extrapolating the\n  Probabilities of a Huge and Hypothetical LM","summary":"  Contrastive decoding (CD) (Li et al., 2023) improves the next-token\ndistribution of a large expert language model (LM) using a small amateur LM.\nAlthough CD is applied to various LMs and domains to enhance open-ended text\ngeneration, it is still unclear why CD often works well, when it could fail,\nand how we can make it better. To deepen our understanding of CD, we first\ntheoretically prove that CD could be viewed as linearly extrapolating the\nnext-token logits from a huge and hypothetical LM. We also highlight that the\nlinear extrapolation could make CD unable to output the most obvious answers\nthat have already been assigned high probabilities by the amateur LM.\n  To overcome CD's limitation, we propose a new unsupervised decoding method\ncalled $\\mathbf{A}$symptotic $\\mathbf{P}$robability $\\mathbf{D}$ecoding (APD).\nAPD explicitly extrapolates the probability curves from the LMs of different\nsizes to infer the asymptotic probabilities from an infinitely large LM without\ninducing more inference costs than CD. In FactualityPrompts, an open-ended text\ngeneration benchmark, sampling using APD significantly boosts factuality in\ncomparison to the CD sampling and its variants, and achieves state-of-the-art\nresults for Pythia 6.9B and OPT 6.7B. Furthermore, in five commonsense QA\ndatasets, APD is often significantly better than CD and achieves a similar\neffect of using a larger LLM. For example, the perplexity of APD on top of\nPythia 6.9B is even lower than the perplexity of Pythia 12B in CommonsenseQA\nand LAMBADA.\n","authors":["Haw-Shiuan Chang","Nanyun Peng","Mohit Bansal","Anil Ramakrishna","Tagyoung Chung"],"pdf_url":"https://arxiv.org/pdf/2411.01610v1.pdf","comment":"EMNLP 2024 Oral"},{"id":"http://arxiv.org/abs/2406.00799v5","updated":"2024-11-03T14:52:35Z","published":"2024-06-02T16:53:21Z","title":"Are you still on track!? Catching LLM Task Drift with Activations","summary":"  Large Language Models are commonly used in retrieval-augmented applications\nto execute user instructions based on data from external sources. For example,\nmodern search engines use LLMs to answer queries based on relevant search\nresults; email plugins summarize emails by processing their content through an\nLLM. However, the potentially untrusted provenance of these data sources can\nlead to prompt injection attacks, where the LLM is manipulated by natural\nlanguage instructions embedded in the external data, causing it to deviate from\nthe user's original instruction(s). We define this deviation as task drift.\nTask drift is a significant concern as it allows attackers to exfiltrate data\nor influence the LLM's output for other users. We study LLM activations as a\nsolution to detect task drift, showing that activation deltas - the difference\nin activations before and after processing external data - are strongly\ncorrelated with this phenomenon. Through two probing methods, we demonstrate\nthat a simple linear classifier can detect drift with near-perfect ROC AUC on\nan out-of-distribution test set. We evaluate these methods by making minimal\nassumptions about how user's tasks, system prompts, and attacks can be phrased.\nWe observe that this approach generalizes surprisingly well to unseen task\ndomains, such as prompt injections, jailbreaks, and malicious instructions,\nwithout being trained on any of these attacks. Interestingly, the fact that\nthis solution does not require any modifications to the LLM (e.g.,\nfine-tuning), as well as its compatibility with existing meta-prompting\nsolutions, makes it cost-efficient and easy to deploy. To encourage further\nresearch on activation-based task inspection, decoding, and interpretability,\nwe release our large-scale TaskTracker toolkit, featuring a dataset of over\n500K instances, representations from six SoTA language models, and inspection\ntools.\n","authors":["Sahar Abdelnabi","Aideen Fay","Giovanni Cherubin","Ahmed Salem","Mario Fritz","Andrew Paverd"],"pdf_url":"https://arxiv.org/pdf/2406.00799v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10671v3","updated":"2024-11-03T14:42:13Z","published":"2024-06-15T15:28:02Z","title":"Augmenting Biomedical Named Entity Recognition with General-domain\n  Resources","summary":"  Training a neural network-based biomedical named entity recognition (BioNER)\nmodel usually requires extensive and costly human annotations. While several\nstudies have employed multi-task learning with multiple BioNER datasets to\nreduce human effort, this approach does not consistently yield performance\nimprovements and may introduce label ambiguity in different biomedical corpora.\nWe aim to tackle those challenges through transfer learning from easily\naccessible resources with fewer concept overlaps with biomedical datasets. In\nthis paper, we proposed GERBERA, a simple-yet-effective method that utilized a\ngeneral-domain NER dataset for training. Specifically, we performed multi-task\nlearning to train a pre-trained biomedical language model with both the target\nBioNER dataset and the general-domain dataset. Subsequently, we fine-tuned the\nmodels specifically for the BioNER dataset. We systematically evaluated GERBERA\non five datasets of eight entity types, collectively consisting of 81,410\ninstances. Despite using fewer biomedical resources, our models demonstrated\nsuperior performance compared to baseline models trained with multiple\nadditional BioNER datasets. Specifically, our models consistently outperformed\nthe baselines in six out of eight entity types, achieving an average\nimprovement of 0.9% over the best baseline performance across eight biomedical\nentity types sourced from five different corpora. Our method was especially\neffective in amplifying performance on BioNER datasets characterized by limited\ndata, with a 4.7% improvement in F1 scores on the JNLPBA-RNA dataset.\n","authors":["Yu Yin","Hyunjae Kim","Xiao Xiao","Chih Hsuan Wei","Jaewoo Kang","Zhiyong Lu","Hua Xu","Meng Fang","Qingyu Chen"],"pdf_url":"https://arxiv.org/pdf/2406.10671v3.pdf","comment":"Published in JBI 2024. We make data, codes, and models publicly\n  available via https://github.com/qingyu-qc/bioner_gerbera"},{"id":"http://arxiv.org/abs/2411.01562v1","updated":"2024-11-03T13:23:18Z","published":"2024-11-03T13:23:18Z","title":"Are LLMs good pragmatic speakers?","summary":"  Large language models (LLMs) are trained on data assumed to include natural\nlanguage pragmatics, but do they actually behave like pragmatic speakers? We\nattempt to answer this question using the Rational Speech Act (RSA) framework,\nwhich models pragmatic reasoning in human communication. Using the paradigm of\na reference game constructed from the TUNA corpus, we score candidate\nreferential utterances in both a state-of-the-art LLM (Llama3-8B-Instruct) and\nin the RSA model, comparing and contrasting these scores. Given that RSA\nrequires defining alternative utterances and a truth-conditional meaning\nfunction, we explore such comparison for different choices of each of these\nrequirements. We find that while scores from the LLM have some positive\ncorrelation with those from RSA, there isn't sufficient evidence to claim that\nit behaves like a pragmatic speaker. This initial study paves way for further\ntargeted efforts exploring different models and settings, including\nhuman-subject evaluation, to see if LLMs truly can, or be made to, behave like\npragmatic speakers.\n","authors":["Mingyue Jian","Siddharth Narayanaswamy"],"pdf_url":"https://arxiv.org/pdf/2411.01562v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09645v2","updated":"2024-11-03T13:21:57Z","published":"2024-04-15T10:24:32Z","title":"Real-world Instance-specific Image Goal Navigation: Bridging Domain Gaps\n  via Contrastive Learning","summary":"  Improving instance-specific image goal navigation (InstanceImageNav), which\nlocates the identical object in a real-world environment from a query image, is\nessential for robotic systems to assist users in finding desired objects. The\nchallenge lies in the domain gap between low-quality images observed by the\nmoving robot, characterized by motion blur and low-resolution, and high-quality\nquery images provided by the user. Such domain gaps could significantly reduce\nthe task success rate but have not been the focus of previous work. To address\nthis, we propose a novel method called Few-shot Cross-quality Instance-aware\nAdaptation (CrossIA), which employs contrastive learning with an instance\nclassifier to align features between massive low- and few high-quality images.\nThis approach effectively reduces the domain gap by bringing the latent\nrepresentations of cross-quality images closer on an instance basis.\nAdditionally, the system integrates an object image collection with a\npre-trained deblurring model to enhance the observed image quality. Our method\nfine-tunes the SimSiam model, pre-trained on ImageNet, using CrossIA. We\nevaluated our method's effectiveness through an InstanceImageNav task with 20\ndifferent types of instances, where the robot identifies the same instance in a\nreal-world environment as a high-quality query image. Our experiments showed\nthat our method improves the task success rate by up to three times compared to\nthe baseline, a conventional approach based on SuperGlue. These findings\nhighlight the potential of leveraging contrastive learning and image\nenhancement techniques to bridge the domain gap and improve object localization\nin robotic applications. The project website is\nhttps://emergentsystemlabstudent.github.io/DomainBridgingNav/.\n","authors":["Taichi Sakaguchi","Akira Taniguchi","Yoshinobu Hagiwara","Lotfi El Hafi","Shoichi Hasegawa","Tadahiro Taniguchi"],"pdf_url":"https://arxiv.org/pdf/2404.09645v2.pdf","comment":"See website at\n  https://emergentsystemlabstudent.github.io/DomainBridgingNav/. Accepted to\n  IEEE IRC2024"},{"id":"http://arxiv.org/abs/2407.10657v3","updated":"2024-11-03T12:44:42Z","published":"2024-07-15T12:16:33Z","title":"An Empirical Study of Validating Synthetic Data for Formula Generation","summary":"  Large language models (LLMs) can be leveraged to help with writing formulas\nin spreadsheets, but resources on these formulas are scarce, impacting both the\nbase performance of pre-trained models and limiting the ability to fine-tune\nthem. Given a corpus of formulas, we can use a(nother) model to generate\nsynthetic natural language utterances for fine-tuning. However, it is important\nto validate whether the NL generated by the LLM is indeed accurate to be\nbeneficial for fine-tuning. In this paper, we provide empirical results on the\nimpact of validating these synthetic training examples with surrogate\nobjectives that evaluate the accuracy of the synthetic annotations. We\ndemonstrate that validation improves performance over raw data across four\nmodels (2 open and 2 closed weight). Interestingly, we show that although\nvalidation tends to prune more challenging examples, it increases the\ncomplexity of problems that models can solve after being fine-tuned on\nvalidated data.\n","authors":["Usneek Singh","José Cambronero","Sumit Gulwani","Aditya Kanade","Anirudh Khatry","Vu Le","Mukul Singh","Gust Verbruggen"],"pdf_url":"https://arxiv.org/pdf/2407.10657v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01539v1","updated":"2024-11-03T12:03:12Z","published":"2024-11-03T12:03:12Z","title":"LLMs and the Madness of Crowds","summary":"  We investigate the patterns of incorrect answers produced by large language\nmodels (LLMs) during evaluation. These errors exhibit highly non-intuitive\nbehaviors unique to each model. By analyzing these patterns, we measure the\nsimilarities between LLMs and construct a taxonomy that categorizes them based\non their error correlations. Our findings reveal that the incorrect responses\nare not randomly distributed but systematically correlated across models,\nproviding new insights into the underlying structures and relationships among\nLLMs.\n","authors":["William F. Bradley"],"pdf_url":"https://arxiv.org/pdf/2411.01539v1.pdf","comment":"11 pages, 6 figures"},{"id":"http://arxiv.org/abs/2411.01533v1","updated":"2024-11-03T11:39:50Z","published":"2024-11-03T11:39:50Z","title":"Enhancing LLM Evaluations: The Garbling Trick","summary":"  As large language models (LLMs) become increasingly powerful, traditional\nevaluation metrics tend to saturate, making it challenging to distinguish\nbetween models based on their performance. We propose a general method to\ntransform existing LLM evaluations into a series of progressively more\ndifficult tasks. These enhanced evaluations emphasize reasoning capabilities\nand can reveal relative performance differences that are not apparent in the\noriginal assessments.\n  To demonstrate the effectiveness of our approach, we create a new\nmultiple-choice test corpus, extend it into a family of evaluations, and assess\na collection of LLMs. Our results offer insights into the comparative reasoning\nabilities of these models, particularly highlighting distinctions between\nOpenAI's o1-preview and Google's gemini-pro-1.5-002.\n","authors":["William F. Bradley"],"pdf_url":"https://arxiv.org/pdf/2411.01533v1.pdf","comment":"13 pages, 3 figures"},{"id":"http://arxiv.org/abs/2411.01531v1","updated":"2024-11-03T11:25:39Z","published":"2024-11-03T11:25:39Z","title":"DAG: Dictionary-Augmented Generation for Disambiguation of Sentences in\n  Endangered Uralic Languages using ChatGPT","summary":"  We showcase that ChatGPT can be used to disambiguate lemmas in two endangered\nlanguages ChatGPT is not proficient in, namely Erzya and Skolt Sami. We augment\nour prompt by providing dictionary translations of the candidate lemmas to a\nmajority language - Finnish in our case. This dictionary augmented generation\napproach results in 50\\% accuracy for Skolt Sami and 41\\% accuracy for Erzya.\nOn a closer inspection, many of the error types were of the kind even an\nuntrained human annotator would make.\n","authors":["Mika Hämäläinen"],"pdf_url":"https://arxiv.org/pdf/2411.01531v1.pdf","comment":"IWCLUL 2024"},{"id":"http://arxiv.org/abs/2405.09719v3","updated":"2024-11-03T11:12:14Z","published":"2024-05-15T22:28:23Z","title":"Spectral Editing of Activations for Large Language Model Alignment","summary":"  Large language models (LLMs) often exhibit undesirable behaviours, such as\ngenerating untruthful or biased content. Editing their internal representations\nhas been shown to be effective in mitigating such behaviours on top of the\nexisting alignment methods. We propose a novel inference-time editing method,\nnamely spectral editing of activations (SEA), to project the input\nrepresentations into directions with maximal covariance with the positive\ndemonstrations (e.g., truthful) while minimising covariance with the negative\ndemonstrations (e.g., hallucinated). We also extend our method to non-linear\nediting using feature functions. We run extensive experiments on benchmarks\nconcerning truthfulness and bias with six open-source LLMs of different sizes\nand model families. The results demonstrate the superiority of SEA in\neffectiveness, generalisation to similar tasks, as well as computation and data\nefficiency. We also show that SEA editing only has a limited negative impact on\nother model capabilities.\n","authors":["Yifu Qiu","Zheng Zhao","Yftah Ziser","Anna Korhonen","Edoardo M. Ponti","Shay B. Cohen"],"pdf_url":"https://arxiv.org/pdf/2405.09719v3.pdf","comment":"24 pages, NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.01523v1","updated":"2024-11-03T11:03:52Z","published":"2024-11-03T11:03:52Z","title":"SinaTools: Open Source Toolkit for Arabic Natural Language Processing","summary":"  We introduce SinaTools, an open-source Python package for Arabic natural\nlanguage processing and understanding. SinaTools is a unified package allowing\npeople to integrate it into their system workflow, offering solutions for\nvarious tasks such as flat and nested Named Entity Recognition (NER),\nfully-flagged Word Sense Disambiguation (WSD), Semantic Relatedness, Synonymy\nExtractions and Evaluation, Lemmatization, Part-of-speech Tagging, Root\nTagging, and additional helper utilities such as corpus processing, text\nstripping methods, and diacritic-aware word matching. This paper presents\nSinaTools and its benchmarking results, demonstrating that SinaTools\noutperforms all similar tools on the aforementioned tasks, such as Flat NER\n(87.33%), Nested NER (89.42%), WSD (82.63%), Semantic Relatedness (0.49\nSpearman rank), Lemmatization (90.5%), POS tagging (97.5%), among others.\nSinaTools can be downloaded from (https://sina.birzeit.edu/sinatools).\n","authors":["Tymaa Hammouda","Mustafa Jarrar","Mohammed Khalilia"],"pdf_url":"https://arxiv.org/pdf/2411.01523v1.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2411.01511v1","updated":"2024-11-03T10:25:55Z","published":"2024-11-03T10:25:55Z","title":"Integration of Large Vision Language Models for Efficient Post-disaster\n  Damage Assessment and Reporting","summary":"  Traditional natural disaster response involves significant coordinated\nteamwork where speed and efficiency are key. Nonetheless, human limitations can\ndelay critical actions and inadvertently increase human and economic losses.\nAgentic Large Vision Language Models (LVLMs) offer a new avenue to address this\nchallenge, with the potential for substantial socio-economic impact,\nparticularly by improving resilience and resource access in underdeveloped\nregions. We introduce DisasTeller, the first multi-LVLM-powered framework\ndesigned to automate tasks in post-disaster management, including on-site\nassessment, emergency alerts, resource allocation, and recovery planning. By\ncoordinating four specialised LVLM agents with GPT-4 as the core model,\nDisasTeller autonomously implements disaster response activities, reducing\nhuman execution time and optimising resource distribution. Our evaluations\nthrough both LVLMs and humans demonstrate DisasTeller's effectiveness in\nstreamlining disaster response. This framework not only supports expert teams\nbut also simplifies access to disaster management processes for non-experts,\nbridging the gap between traditional response methods and LVLM-driven\nefficiency.\n","authors":["Zhaohui Chen","Elyas Asadi Shamsabadi","Sheng Jiang","Luming Shen","Daniel Dias-da-Costa"],"pdf_url":"https://arxiv.org/pdf/2411.01511v1.pdf","comment":"13 pages, 4 figures"},{"id":"http://arxiv.org/abs/2404.08763v4","updated":"2024-11-03T10:25:47Z","published":"2024-04-12T18:42:18Z","title":"CATS: Contextually-Aware Thresholding for Sparsity in Large Language\n  Models","summary":"  Large Language Models (LLMs) have dramatically advanced AI applications, yet\ntheir deployment remains challenging due to their immense inference costs.\nRecent studies ameliorate the computational costs of LLMs by increasing their\nactivation sparsity but suffer from significant performance degradation on\ndownstream tasks. In this work, we introduce a new framework for sparsifying\nthe activations of base LLMs and reducing inference costs, dubbed Contextually\nAware Thresholding for Sparsity (CATS). CATS is relatively simple, easy to\nimplement, and highly effective. At the heart of our framework is a new\nnon-linear activation function. We demonstrate that CATS can be applied to\nvarious base models, including Mistral-7B and Llama2-7B, and outperforms\nexisting sparsification techniques in downstream task performance. More\nprecisely, CATS-based models often achieve downstream task performance within\n1-2% of their base models without any fine-tuning and even at activation\nsparsity levels of 50%. Furthermore, CATS-based models converge faster and\ndisplay better task performance than competing techniques when fine-tuning is\napplied. Finally, we develop a custom GPU kernel for efficient implementation\nof CATS that translates the activation of sparsity of CATS to real wall-clock\ntime speedups. Our custom kernel implementation of CATS results in a ~15%\nimprovement in wall-clock inference latency of token generation on both\nLlama-7B and Mistral-7B.\n","authors":["Donghyun Lee","Je-Yong Lee","Genghan Zhang","Mo Tiwari","Azalia Mirhoseini"],"pdf_url":"https://arxiv.org/pdf/2404.08763v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11430v4","updated":"2024-11-03T09:42:35Z","published":"2024-06-17T11:35:16Z","title":"A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression","summary":"  The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability.\n","authors":["Alessio Devoto","Yu Zhao","Simone Scardapane","Pasquale Minervini"],"pdf_url":"https://arxiv.org/pdf/2406.11430v4.pdf","comment":"This is an extended version of a paper published in the proceedings\n  of the 2024 Conference on Empirical Methods in Natural Language Processing\n  (EMNLP 2024); this version was presented at the 4th NeurIPS Workshop on\n  Efficient Natural Language and Speech Processing (ENLSP-IV)"},{"id":"http://arxiv.org/abs/2410.23114v2","updated":"2024-11-03T09:35:12Z","published":"2024-10-30T15:25:06Z","title":"Unified Triplet-Level Hallucination Evaluation for Large Vision-Language\n  Models","summary":"  Despite the outstanding performance in vision-language reasoning, Large\nVision-Language Models (LVLMs) might generate hallucinated contents that do not\nexist in the given image. Most existing LVLM hallucination benchmarks are\nconstrained to evaluate the object-related hallucinations. However, the\npotential hallucination on the relations between two objects, i.e., relation\nhallucination, still lacks investigation. To remedy that, in this paper we\ndesign a unified framework to measure object and relation hallucination in\nLVLMs simultaneously. The core idea of our framework is to conduct\nhallucination evaluation on (object, relation, object) triplets extracted from\nLVLMs' responses, and thus, could be easily generalized to different\nvision-language tasks. Based on our framework, we further introduce Tri-HE, a\nnovel Triplet-level Hallucination Evaluation benchmark which can be used to\nstudy both object and relation hallucination at the same time. We conduct\ncomprehensive evaluations on Tri-HE and observe that the relation hallucination\nissue is even more serious than object hallucination among existing LVLMs,\nhighlighting a previously neglected problem towards reliable LVLMs. Moreover,\nbased on our findings, we design a simple yet effective training-free approach\nto mitigate hallucinations for LVLMs, with which, we exceed all open-sourced\ncounterparts on Tri-HE, achieving comparable performance with the powerful\nGPT-4V. Our dataset and code for the reproduction of our experiments are\navailable publicly at https://github.com/wujunjie1998/Tri-HE.\n","authors":["Junjie Wu","Tsz Ting Chung","Kai Chen","Dit-Yan Yeung"],"pdf_url":"https://arxiv.org/pdf/2410.23114v2.pdf","comment":"Project Page: https://kaichen1998.github.io/projects/tri-he/"},{"id":"http://arxiv.org/abs/2405.13911v2","updated":"2024-11-03T09:25:57Z","published":"2024-05-22T18:35:10Z","title":"TOPA: Extending Large Language Models for Video Understanding via\n  Text-Only Pre-Alignment","summary":"  Recent advancements in image understanding have benefited from the extensive\nuse of web image-text pairs. However, video understanding remains a challenge\ndespite the availability of substantial web video-text data. This difficulty\nprimarily arises from the inherent complexity of videos and the inefficient\nlanguage supervision in recent web-collected video-text datasets. In this\npaper, we introduce Text-Only Pre-Alignment (TOPA), a novel approach to extend\nlarge language models (LLMs) for video understanding, without the need for\npre-training on real video data. Specifically, we first employ an advanced LLM\nto automatically generate Textual Videos comprising continuous textual frames,\nalong with corresponding annotations to simulate real video-text data. Then,\nthese annotated textual videos are used to pre-align a language-only LLM with\nthe video modality. To bridge the gap between textual and real videos, we\nemploy the CLIP model as the feature extractor to align image and text\nmodalities. During text-only pre-alignment, the continuous textual frames,\nencoded as a sequence of CLIP text features, are analogous to continuous CLIP\nimage features, thus aligning the LLM with real video representation. Extensive\nexperiments, including zero-shot evaluation and finetuning on various video\nunderstanding tasks, demonstrate that TOPA is an effective and efficient\nframework for aligning video content with LLMs. In particular, without training\non any video data, the TOPA-Llama2-13B model achieves a Top-1 accuracy of 51.0%\non the challenging long-form video understanding benchmark, Egoschema. This\nperformance surpasses previous video-text pre-training approaches and proves\ncompetitive with recent GPT-3.5-based video agents.\n","authors":["Wei Li","Hehe Fan","Yongkang Wong","Mohan Kankanhalli","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2405.13911v2.pdf","comment":"NeurIPS 2024 (Spotlight)"},{"id":"http://arxiv.org/abs/2410.15365v2","updated":"2024-11-03T09:23:12Z","published":"2024-10-20T11:47:17Z","title":"BERTtime Stories: Investigating the Role of Synthetic Story Data in\n  Language pre-training","summary":"  We describe our contribution to the Strict and Strict-Small tracks of the 2nd\niteration of the BabyLM Challenge. The shared task is centered around efficient\npre-training given data constraints motivated by human development. In\nresponse, we study the effect of synthetic story data in language pre-training\nusing TinyStories: a recently introduced dataset of short stories. Initially,\nwe train GPT-Neo models on subsets of TinyStories, while varying the amount of\navailable data. We find that, even with access to less than 100M words, the\nmodels are able to generate high-quality, original completions to a given\nstory, and acquire substantial linguistic knowledge. To measure the effect of\nsynthetic story data, we train LTG-BERT encoder models on a combined dataset\nof: a subset of TinyStories, story completions generated by GPT-Neo, and a\nsubset of the BabyLM dataset. Our experimentation reveals that synthetic data\ncan occasionally offer modest gains, but overall have a negative influence on\nlinguistic understanding. Our work offers an initial study on synthesizing\nstory data in low resource settings and underscores their potential for\naugmentation in data-constrained language modeling. We publicly release our\nmodels and implementation on our GitHub.\n","authors":["Nikitas Theodoropoulos","Giorgos Filandrianos","Vassilis Lyberatos","Maria Lymperaiou","Giorgos Stamou"],"pdf_url":"https://arxiv.org/pdf/2410.15365v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01493v1","updated":"2024-11-03T09:18:28Z","published":"2024-11-03T09:18:28Z","title":"Sample-Efficient Alignment for LLMs","summary":"  We study methods for efficiently aligning large language models (LLMs) with\nhuman preferences given budgeted online feedback. We first formulate the LLM\nalignment problem in the frame of contextual dueling bandits. This formulation,\nsubsuming recent paradigms such as online RLHF and online DPO, inherently\nquests for sample-efficient algorithms that incorporate online active\nexploration. Leveraging insights from bandit theory, we introduce a unified\nalgorithm based on Thompson sampling and highlight its applications in two\ndistinct LLM alignment scenarios. The practical agent that efficiently\nimplements this algorithm, named SEA (Sample-Efficient Alignment), is\nempirically validated through extensive experiments across three model scales\n(1B, 2.8B, 6.9B) and three preference learning algorithms (DPO, IPO, SLiC). The\nresults demonstrate that SEA achieves highly sample-efficient alignment with\noracle's preferences, outperforming recent active exploration methods for LLMs.\nAdditionally, we release the implementation of SEA together with an efficient\ncodebase designed for online alignment of LLMs, aiming to accelerate future\nresearch in this field.\n","authors":["Zichen Liu","Changyu Chen","Chao Du","Wee Sun Lee","Min Lin"],"pdf_url":"https://arxiv.org/pdf/2411.01493v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05137v2","updated":"2024-11-03T09:09:21Z","published":"2024-09-08T15:42:48Z","title":"READoc: A Unified Benchmark for Realistic Document Structured Extraction","summary":"  Document Structured Extraction (DSE) aims to extract structured content from\nraw documents. Despite the emergence of numerous DSE systems, their unified\nevaluation remains inadequate, significantly hindering the field's advancement.\nThis problem is largely attributed to existing benchmark paradigms, which\nexhibit fragmented and localized characteristics. To address these limitations\nand offer a thorough evaluation of DSE systems, we introduce a novel benchmark\nnamed READoc, which defines DSE as a realistic task of converting unstructured\nPDFs into semantically rich Markdown. The READoc dataset is derived from 2,233\ndiverse and real-world documents from arXiv and GitHub. In addition, we develop\na DSE Evaluation S$^3$uite comprising Standardization, Segmentation and Scoring\nmodules, to conduct a unified evaluation of state-of-the-art DSE approaches. By\nevaluating a range of pipeline tools, expert visual models, and general VLMs,\nwe identify the gap between current work and the unified, realistic DSE\nobjective for the first time. We aspire that READoc will catalyze future\nresearch in DSE, fostering more comprehensive and practical solutions.\n","authors":["Zichao Li","Aizier Abulaiti","Yaojie Lu","Xuanang Chen","Jia Zheng","Hongyu Lin","Xianpei Han","Le Sun"],"pdf_url":"https://arxiv.org/pdf/2409.05137v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.20092v2","updated":"2024-11-03T09:04:54Z","published":"2024-05-30T14:31:33Z","title":"Divide-and-Conquer Meets Consensus: Unleashing the Power of Functions in\n  Code Generation","summary":"  Despite recent progress made by large language models in code generation,\nthey still struggle with programs that meet complex requirements. Recent work\nutilizes plan-and-solve decomposition to decrease the complexity and leverage\nself-tests to refine the generated program. Yet, planning deep-inside\nrequirements in advance can be challenging, and the tests need to be accurate\nto accomplish self-improvement. To this end, we propose FunCoder, a code\ngeneration framework incorporating the divide-and-conquer strategy with\nfunctional consensus. Specifically, FunCoder recursively branches off\nsub-functions as smaller goals during code generation, represented by a tree\nhierarchy. These sub-functions are then composited to attain more complex\nobjectives. Additionally, we designate functions via a consensus formed by\nidentifying similarities in program behavior, mitigating error propagation.\nFunCoder outperforms state-of-the-art methods by +9.8% on average in HumanEval,\nMBPP, xCodeEval and MATH with GPT-3.5 and GPT-4. Moreover, our method\ndemonstrates superiority on smaller models: With FunCoder, StableCode-3b\nsurpasses GPT-3.5 by +18.6% and achieves 97.7% of GPT-4's performance on\nHumanEval. Further analysis reveals that our proposed dynamic function\ndecomposition is capable of handling complex requirements, and the functional\nconsensus prevails over self-testing in correctness evaluation.\n","authors":["Jingchang Chen","Hongxuan Tang","Zheng Chu","Qianglong Chen","Zekun Wang","Ming Liu","Bing Qin"],"pdf_url":"https://arxiv.org/pdf/2405.20092v2.pdf","comment":"NeurIPS 2024 oral"},{"id":"http://arxiv.org/abs/2411.01485v1","updated":"2024-11-03T08:57:41Z","published":"2024-11-03T08:57:41Z","title":"Domain-specific Guided Summarization for Mental Health Posts","summary":"  In domain-specific contexts, particularly mental health, abstractive\nsummarization requires advanced techniques adept at handling specialized\ncontent to generate domain-relevant and faithful summaries. In response to\nthis, we introduce a guided summarizer equipped with a dual-encoder and an\nadapted decoder that utilizes novel domain-specific guidance signals, i.e.,\nmental health terminologies and contextually rich sentences from the source\ndocument, to enhance its capacity to align closely with the content and context\nof guidance, thereby generating a domain-relevant summary. Additionally, we\npresent a post-editing correction model to rectify errors in the generated\nsummary, thus enhancing its consistency with the original content in detail.\nEvaluation on the MentSum dataset reveals that our model outperforms existing\nbaseline models in terms of both ROUGE and FactCC scores. Although the\nexperiments are specifically designed for mental health posts, the methodology\nwe've developed offers broad applicability, highlighting its versatility and\neffectiveness in producing high-quality domain-specific summaries.\n","authors":["Lu Qian","Yuqi Wang","Zimu Wang","Haiyang Zhang","Wei Wang","Ting Yu","Anh Nguyen"],"pdf_url":"https://arxiv.org/pdf/2411.01485v1.pdf","comment":"Accepted at PACLIC 2024. Camera-ready version"},{"id":"http://arxiv.org/abs/2411.01483v1","updated":"2024-11-03T08:49:55Z","published":"2024-11-03T08:49:55Z","title":"Teaching Models to Improve on Tape","summary":"  Large Language Models (LLMs) often struggle when prompted to generate content\nunder specific constraints. However, in such cases it is often easy to check\nwhether these constraints are satisfied or violated. Recent works have shown\nthat LLMs can benefit from such ``corrective feedback''. Here we claim that\nthis skill of LLMs can be significantly enhanced via training. We introduce an\nRL framework for teaching models to use such rewards, by simulating interaction\nsessions, and rewarding the model according to its ability to satisfy the\nconstraints. We refer to our method as CORGI (Controlled Generation with RL for\nGuided Interaction), and evaluate it on a variety of controlled generation\ntasks using unlabeled training data. We find that CORGI consistently\noutperforms the baseline reinforcement learning method that does not\nincorporate conversational feedback. Furthermore, CORGI's interactive framework\nenables meta-learning, allowing the LLM to generalize better to guided\ninteraction in new tasks. Our results clearly show that conversational\noptimization, when combined with reinforcement learning, significantly improves\nthe effectiveness of LLMs in controlled generation contexts.\n","authors":["Liat Bezalel","Eyal Orgad","Amir Globerson"],"pdf_url":"https://arxiv.org/pdf/2411.01483v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01477v1","updated":"2024-11-03T08:30:29Z","published":"2024-11-03T08:30:29Z","title":"DPCL-Diff: The Temporal Knowledge Graph Reasoning based on Graph Node\n  Diffusion Model with Dual-Domain Periodic Contrastive Learning","summary":"  Temporal knowledge graph (TKG) reasoning that infers future missing facts is\nan essential and challenging task. Predicting future events typically relies on\nclosely related historical facts, yielding more accurate results for repetitive\nor periodic events. However, for future events with sparse historical\ninteractions, the effectiveness of this method, which focuses on leveraging\nhigh-frequency historical information, diminishes. Recently, the capabilities\nof diffusion models in image generation have opened new opportunities for TKG\nreasoning. Therefore, we propose a graph node diffusion model with dual-domain\nperiodic contrastive learning (DPCL-Diff). Graph node diffusion model (GNDiff)\nintroduces noise into sparsely related events to simulate new events,\ngenerating high-quality data that better conforms to the actual distribution.\nThis generative mechanism significantly enhances the model's ability to reason\nabout new events. Additionally, the dual-domain periodic contrastive learning\n(DPCL) maps periodic and non-periodic event entities to Poincar\\'e and\nEuclidean spaces, leveraging their characteristics to distinguish similar\nperiodic events effectively. Experimental results on four public datasets\ndemonstrate that DPCL-Diff significantly outperforms state-of-the-art TKG\nmodels in event prediction, demonstrating our approach's effectiveness. This\nstudy also investigates the combined effectiveness of GNDiff and DPCL in TKG\ntasks.\n","authors":["Yukun Cao","Lisheng Wang","Luobing Huang"],"pdf_url":"https://arxiv.org/pdf/2411.01477v1.pdf","comment":"11 pages, 2 figures"},{"id":"http://arxiv.org/abs/2411.01474v1","updated":"2024-11-03T08:15:43Z","published":"2024-11-03T08:15:43Z","title":"MoCE: Adaptive Mixture of Contextualization Experts for Byte-based\n  Neural Machine Translation","summary":"  Byte-based machine translation systems have shown significant potential in\nmassively multilingual settings. Unicode encoding, which maps each character to\nspecific byte(s), eliminates the emergence of unknown words, even in new\nlanguages, enabling broad language scalability. However, byte-level\ntokenization results in sequences that are hard to interpret due to limited\nsemantic information per byte. Local contextualization has proven effective in\nassigning initial semantics to tokens, improving sentence comprehension.\nNevertheless, variations in encoding rules across languages necessitate an\nadaptive approach for effective contextualization. To this end, we propose\nAdaptive MultiScale-Headed Attention (Ada-MSHA), adaptively selecting and\nmixing attention heads, which are treated as contextualization experts. This\nenhances the flexibility of contextualization scales and improves the potential\nto discover a better strategy than previous methods. Experiment results show\nthat our method outperforms existing methods without extensive manual\nadjustment of hyper-parameters and surpasses subword-based models with fewer\nparameters in Ted-59 dataset. Our code is available at\nhttps://github.com/ictnlp/MoCE.\n","authors":["Langlin Huang","Mengyu Bu","Yang Feng"],"pdf_url":"https://arxiv.org/pdf/2411.01474v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18634v2","updated":"2024-11-03T08:14:34Z","published":"2024-10-24T10:47:30Z","title":"Little Giants: Synthesizing High-Quality Embedding Data at Scale","summary":"  Synthetic data generation has become an increasingly popular way of training\nmodels without the need for large, manually labeled datasets. For tasks like\ntext embedding, synthetic data offers diverse and scalable training examples,\nsignificantly reducing the cost of human annotation. However, most current\napproaches rely heavily on proprietary models like GPT-4, which are expensive\nand inefficient for generating large-scale embedding data. In this paper, we\nintroduce SPEED, a framework that aligns open-source small models (8B) to\nefficiently generate large-scale synthetic embedding data. Through supervised\nfine-tuning, preference optimization, and self-improvement, SPEED enables small\nopen-source models to produce high-quality data. Remarkably, SPEED uses only\nless than 1/10 of the GPT API calls, outperforming the state-of-the-art\nembedding model E5_mistral when both are trained solely on their synthetic\ndata. Using this efficient generator, we conduct a comprehensive study on how\nvarious factors within the alignment pipeline impact data quality and reveal\nthe scaling law for synthetic embedding data.\n","authors":["Haonan Chen","Liang Wang","Nan Yang","Yutao Zhu","Ziliang Zhao","Furu Wei","Zhicheng Dou"],"pdf_url":"https://arxiv.org/pdf/2410.18634v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09824v3","updated":"2024-11-03T06:58:34Z","published":"2024-10-13T12:57:08Z","title":"Dynamic and Textual Graph Generation Via Large-Scale LLM-based Agent\n  Simulation","summary":"  Graph generation is a fundamental task that has been extensively studied in\nsocial, technological, and scientific analysis. For modeling the dynamic graph\nevolution process, traditional rule-based methods struggle to capture community\nstructures within graphs, while deep learning methods only focus on fitting\ntraining graphs. This limits existing graph generators to producing graphs that\nadhere to predefined rules or closely resemble training datasets, achieving\npoor performance in dynamic graph generation. Given that graphs are abstract\nrepresentations arising from pairwise interactions in human activities, a\nrealistic simulation of human-wise interaction could provide deeper insights\ninto the graph evolution mechanism. With the increasing recognition of large\nlanguage models (LLMs) in simulating human behavior, we introduce\nGraphAgent-Generator (GAG), a novel simulation-based framework for dynamic\ngraph generation. Without training or fine-tuning process of LLM, our framework\neffectively replicates seven macro-level structural characteristics in\nestablished network science theories while surpassing existing baselines in\ngraph expansion tasks by 31\\% on specific evaluation metrics. Through node\nclassification task, we validate GAG effectively preserves characteristics of\nreal-world network for node-wise textual features in generated text-rich graph.\nFurthermore, by incorporating parallel acceleration, GAG supports generating\ngraphs with up to nearly 100,000 nodes or 10 million edges through large-scale\nLLM-based agent simulation, with a minimum speed-up of 90.4\\%. The source code\nis available at https://anonymous.4open.science/r/GraphAgent-2206.\n","authors":["Jiarui Ji","Runlin Lei","Jialing Bi","Zhewei Wei","Yankai Lin","Xuchen Pan","Yaliang Li","Bolin Ding"],"pdf_url":"https://arxiv.org/pdf/2410.09824v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16165v2","updated":"2024-11-03T06:03:56Z","published":"2024-10-21T16:31:23Z","title":"From Tokens to Materials: Leveraging Language Models for Scientific\n  Discovery","summary":"  Exploring the predictive capabilities of language models in material science\nis an ongoing interest. This study investigates the application of language\nmodel embeddings to enhance material property prediction in materials science.\nBy evaluating various contextual embedding methods and pre-trained models,\nincluding Bidirectional Encoder Representations from Transformers (BERT) and\nGenerative Pre-trained Transformers (GPT), we demonstrate that domain-specific\nmodels, particularly MatBERT significantly outperform general-purpose models in\nextracting implicit knowledge from compound names and material properties. Our\nfindings reveal that information-dense embeddings from the third layer of\nMatBERT, combined with a context-averaging approach, offer the most effective\nmethod for capturing material-property relationships from the scientific\nliterature. We also identify a crucial \"tokenizer effect,\" highlighting the\nimportance of specialized text processing techniques that preserve complete\ncompound names while maintaining consistent token counts. These insights\nunderscore the value of domain-specific training and tokenization in materials\nscience applications and offer a promising pathway for accelerating the\ndiscovery and development of new materials through AI-driven approaches.\n","authors":["Yuwei Wan","Tong Xie","Nan Wu","Wenjie Zhang","Chunyu Kit","Bram Hoex"],"pdf_url":"https://arxiv.org/pdf/2410.16165v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17283v2","updated":"2024-11-03T05:25:26Z","published":"2024-06-25T05:18:12Z","title":"A Recursive Encoding for Cuneiform Signs","summary":"  One of the most significant problems in cuneiform pedagogy is the process of\nlooking up unknown signs, which often involves a tedious page-by-page search\nthrough a sign list. This paper proposes a new \"recursive encoding\" for signs,\nwhich represents the arrangement of strokes in a way a computer can process. A\nseries of new algorithms then offers students a new way to look up signs by any\ndistinctive component, as well as providing new ways to render signs and\ntablets electronically.\n","authors":["Daniel M. Stelzer"],"pdf_url":"https://arxiv.org/pdf/2406.17283v2.pdf","comment":"27 pages, 29 figures, 5 tables"},{"id":"http://arxiv.org/abs/2402.15733v3","updated":"2024-11-03T05:00:52Z","published":"2024-02-24T06:05:15Z","title":"ArEEG_Chars: Dataset for Envisioned Speech Recognition using EEG for\n  Arabic Characters","summary":"  Brain-computer interfaces is an important and hot research topic that\nrevolutionize how people interact with the world, especially for individuals\nwith neurological disorders. While extensive research has been done in EEG\nsignals of English letters and words, a major limitation remains: the lack of\npublicly available EEG datasets for many non-English languages, such as Arabic.\nAlthough Arabic is one of the most spoken languages worldwide, to the best of\nour knowledge, there is no publicly available dataset for EEG signals of Arabic\ncharacters until now. To address this gap, we introduce ArEEG_Chars, a novel\nEEG dataset for Arabic 31 characters collected from 30 participants (21 males\nand 9 females), these records were collected using Epoc X 14 channels device\nfor 10 seconds long for each char record. The number of recorded signals were\n930 EEG recordings. To make the EEG signals suitable for analyzing, each\nrecording has been split into multiple signals with a time duration of 250ms,\nrespectively. Therefore, a total of 39857 recordings of EEG signals have been\ncollected in this study. Moreover, ArEEG_Chars will be publicly available for\nresearchers. We do hope that this dataset will fill an important gap in the\nresearch of Arabic EEG benefiting Arabic-speaking individuals with\ndisabilities.\n","authors":["Hazem Darwish","Abdalrahman Al Malah","Khloud Al Jallad","Nada Ghneim"],"pdf_url":"https://arxiv.org/pdf/2402.15733v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.10176v2","updated":"2024-11-03T03:48:02Z","published":"2024-02-15T18:26:11Z","title":"OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset","summary":"  Recent work has shown the immense potential of synthetically generated\ndatasets for training large language models (LLMs), especially for acquiring\ntargeted skills. Current large-scale math instruction tuning datasets such as\nMetaMathQA (Yu et al., 2024) and MAmmoTH (Yue et al., 2024) are constructed\nusing outputs from closed-source LLMs with commercially restrictive licenses. A\nkey reason limiting the use of open-source LLMs in these data generation\npipelines has been the wide gap between the mathematical skills of the best\nclosed-source LLMs, such as GPT-4, and the best open-source LLMs. Building on\nthe recent progress in open-source LLMs, our proposed prompting novelty, and\nsome brute-force scaling, we construct OpenMathInstruct-1, a math instruction\ntuning dataset with 1.8M problem-solution pairs. The dataset is constructed by\nsynthesizing code-interpreter solutions for GSM8K and MATH, two popular math\nreasoning benchmarks, using the recently released and permissively licensed\nMixtral model. Our best model, OpenMath-CodeLlama-70B, trained on a subset of\nOpenMathInstruct-1, achieves a score of 84.6% on GSM8K and 50.7% on MATH, which\nis competitive with the best gpt-distilled models. We release our code, models,\nand the OpenMathInstruct-1 dataset under a commercially permissive license.\n","authors":["Shubham Toshniwal","Ivan Moshkov","Sean Narenthiran","Daria Gitman","Fei Jia","Igor Gitman"],"pdf_url":"https://arxiv.org/pdf/2402.10176v2.pdf","comment":"Camera-ready version for NeurIPS 2024"},{"id":"http://arxiv.org/abs/2401.09395v6","updated":"2024-11-03T02:52:58Z","published":"2024-01-17T18:13:07Z","title":"Evaluating LLMs' Mathematical and Coding Competency through\n  Ontology-guided Interventions","summary":"  Recent advancements in Large Language Models (LLMs) have showcased striking\nresults on existing logical reasoning benchmarks, with some models even\nsurpassing human performance. However, the true depth of their competencies and\nrobustness in reasoning tasks remains an open question. To this end, in this\npaper, we focus on two popular reasoning tasks: arithmetic reasoning and code\ngeneration. Particularly, we introduce (i) a general ontology of perturbations\nfor math and coding questions, (ii) a semi-automatic method to apply these\nperturbations, and (iii) two datasets, GSMORE and HUMANEVAL-CORE, respectively,\nof perturbed math and coding problems to probe LLM capabilities in numeric\nreasoning and coding tasks. Through comprehensive evaluations of both\nclosed-source and open-source LLMs, we show a significant performance drop\nacross all the models against the perturbed questions, suggesting that the\ncurrent LLMs lack robust problem solving skills and structured reasoning\nabilities in many areas, as defined by our ontology. We open-source the\ndatasets and source codes at: https://github.com/declare-lab/LLM-ReasoningTest.\n","authors":["Pengfei Hong","Navonil Majumder","Deepanway Ghosal","Somak Aditya","Rada Mihalcea","Soujanya Poria"],"pdf_url":"https://arxiv.org/pdf/2401.09395v6.pdf","comment":"With o1 and GPT-4o results. Reformatted the data and presented more\n  analysis"},{"id":"http://arxiv.org/abs/2411.01409v1","updated":"2024-11-03T02:38:43Z","published":"2024-11-03T02:38:43Z","title":"Classifier-guided Gradient Modulation for Enhanced Multimodal Learning","summary":"  Multimodal learning has developed very fast in recent years. However, during\nthe multimodal training process, the model tends to rely on only one modality\nbased on which it could learn faster, thus leading to inadequate use of other\nmodalities. Existing methods to balance the training process always have some\nlimitations on the loss functions, optimizers and the number of modalities and\nonly consider modulating the magnitude of the gradients while ignoring the\ndirections of the gradients. To solve these problems, in this paper, we present\na novel method to balance multimodal learning with Classifier-Guided Gradient\nModulation (CGGM), considering both the magnitude and directions of the\ngradients. We conduct extensive experiments on four multimodal datasets:\nUPMC-Food 101, CMU-MOSI, IEMOCAP and BraTS 2021, covering classification,\nregression and segmentation tasks. The results show that CGGM outperforms all\nthe baselines and other state-of-the-art methods consistently, demonstrating\nits effectiveness and versatility. Our code is available at\nhttps://github.com/zrguo/CGGM.\n","authors":["Zirun Guo","Tao Jin","Jingyuan Chen","Zhou Zhao"],"pdf_url":"https://arxiv.org/pdf/2411.01409v1.pdf","comment":"Accepted at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2403.16038v3","updated":"2024-11-03T01:53:41Z","published":"2024-03-24T06:49:07Z","title":"Monotonic Paraphrasing Improves Generalization of Language Model\n  Prompting","summary":"  Performance of large language models (LLMs) may vary with different prompts\nor instructions of even the same task. One commonly recognized factor for this\nphenomenon is the model's familiarity with the given prompt or instruction,\nwhich is typically estimated by its perplexity. However, finding the prompt\nwith the lowest perplexity is challenging, given the enormous space of possible\nprompting phrases. In this paper, we propose monotonic paraphrasing (MonoPara),\nan end-to-end decoding strategy that paraphrases given prompts or instructions\ninto their lower perplexity counterparts based on an ensemble of a paraphrase\nLM for prompt (or instruction) rewriting, and a target LM (i.e. the prompt or\ninstruction executor) that constrains the generation for lower perplexity. The\nensemble decoding process can efficiently paraphrase the original prompt\nwithout altering its semantic meaning, while monotonically decreasing the\nperplexity of each generation as calculated by the target LM. We explore in\ndetail both greedy and search-based decoding as two alternative decoding\nschemes of MonoPara. Notably, MonoPara does not require any training and can\nmonotonically lower the perplexity of the paraphrased prompt or instruction,\nleading to improved performance of zero-shot LM prompting as evaluated on a\nwide selection of tasks. In addition, MonoPara is also shown to effectively\nimprove LMs' generalization on perturbed and unseen task instructions.\n","authors":["Qin Liu","Fei Wang","Nan Xu","Tianyi Yan","Tao Meng","Muhao Chen"],"pdf_url":"https://arxiv.org/pdf/2403.16038v3.pdf","comment":"EMNLP 2024 Camera Ready"},{"id":"http://arxiv.org/abs/2409.11564v2","updated":"2024-11-03T01:51:57Z","published":"2024-09-17T21:28:51Z","title":"Preference Tuning with Human Feedback on Language, Speech, and Vision\n  Tasks: A Survey","summary":"  Preference tuning is a crucial process for aligning deep generative models\nwith human preferences. This survey offers a thorough overview of recent\nadvancements in preference tuning and the integration of human feedback. The\npaper is organized into three main sections: 1) introduction and preliminaries:\nan introduction to reinforcement learning frameworks, preference tuning tasks,\nmodels, and datasets across various modalities: language, speech, and vision,\nas well as different policy approaches, 2) in-depth exploration of each\npreference tuning approach: a detailed analysis of the methods used in\npreference tuning, and 3) applications, discussion, and future directions: an\nexploration of the applications of preference tuning in downstream tasks,\nincluding evaluation methods for different modalities, and an outlook on future\nresearch directions. Our objective is to present the latest methodologies in\npreference tuning and model alignment, enhancing the understanding of this\nfield for researchers and practitioners. We hope to encourage further\nengagement and innovation in this area.\n","authors":["Genta Indra Winata","Hanyang Zhao","Anirban Das","Wenpin Tang","David D. Yao","Shi-Xiong Zhang","Sambit Sahu"],"pdf_url":"https://arxiv.org/pdf/2409.11564v2.pdf","comment":"Survey paper"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2409.15590v2","updated":"2024-11-03T23:51:33Z","published":"2024-09-23T22:48:04Z","title":"MapEx: Indoor Structure Exploration with Probabilistic Information Gain\n  from Global Map Predictions","summary":"  Exploration is a critical challenge in robotics, centered on understanding\nunknown environments. In this work, we focus on robots exploring structured\nindoor environments which are often predictable and composed of repeating\npatterns. Most existing approaches, such as conventional frontier approaches,\nhave difficulty leveraging the predictability and explore with simple\nheuristics such as `closest first'. Recent works use deep learning techniques\nto predict unknown regions of the map, using these predictions for information\ngain calculation. However, these approaches are often sensitive to the\npredicted map quality or do not reason over sensor coverage. To overcome these\nissues, our key insight is to jointly reason over what the robot can observe\nand its uncertainty to calculate probabilistic information gain. We introduce\nMapEx, a new exploration framework that uses predicted maps to form\nprobabilistic sensor model for information gain estimation. MapEx generates\nmultiple predicted maps based on observed information, and takes into\nconsideration both the computed variances of predicted maps and estimated\nvisible area to estimate the information gain of a given viewpoint. Experiments\non the real-world KTH dataset showed on average 12.4% improvement than\nrepresentative map-prediction based exploration and 25.4% improvement than\nnearest frontier approach.\n","authors":["Cherie Ho","Seungchan Kim","Brady Moon","Aditya Parandekar","Narek Harutyunyan","Chen Wang","Katia Sycara","Graeme Best","Sebastian Scherer"],"pdf_url":"https://arxiv.org/pdf/2409.15590v2.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2411.01713v1","updated":"2024-11-03T23:36:53Z","published":"2024-11-03T23:36:53Z","title":"Rethinking Weight Decay for Robust Fine-Tuning of Foundation Models","summary":"  Modern optimizers such as AdamW, equipped with momentum and adaptive learning\nrate, are designed to escape local minima and explore the vast parameter space.\nThis exploration is beneficial for finding good loss basins when training from\nscratch. It is not necessarily ideal when resuming from a powerful foundation\nmodel because it can lead to large deviations from the pre-trained\ninitialization and, consequently, worse robustness and generalization. At the\nsame time, strong regularization on all parameters can lead to under-fitting.\nWe hypothesize that selectively regularizing the parameter space is the key to\nfitting and retraining the pre-trained knowledge. This paper proposes a new\nweight decay technique, Selective Projection Decay (SPD), that selectively\nimposes a strong penalty on certain layers while allowing others to change\nfreely. Intuitively, SPD expands and contracts the parameter search space for\nlayers with consistent and inconsistent loss reduction, respectively.\nExperimentally, when equipped with SPD, Adam consistently provides better\nin-distribution generalization and out-of-distribution robustness performance\non multiple popular vision and language benchmarks. Code available\nat~\\url{https://github.com/GT-RIPL/Selective-Projection-Decay.git}\n","authors":["Junjiao Tian","Chengyue Huang","Zsolt Kira"],"pdf_url":"https://arxiv.org/pdf/2411.01713v1.pdf","comment":"Accepted to Neurips 2024"},{"id":"http://arxiv.org/abs/2410.02401v4","updated":"2024-11-03T22:57:56Z","published":"2024-10-03T11:29:09Z","title":"SynCo: Synthetic Hard Negatives in Contrastive Learning for Better\n  Unsupervised Visual Representations","summary":"  Contrastive learning has become a dominant approach in self-supervised visual\nrepresentation learning. Hard negatives - samples closely resembling the anchor\n- are key to enhancing learned representations' discriminative power. However,\nefficiently leveraging hard negatives remains challenging. We introduce SynCo\n(Synthetic Negatives in Contrastive learning), a novel approach that improves\nmodel performance by generating synthetic hard negatives on the representation\nspace. Building on the MoCo framework, SynCo introduces six strategies for\ncreating diverse synthetic hard negatives on-the-fly with minimal computational\noverhead. SynCo achieves faster training and better representation learning,\nreaching 67.9% top-1 accuracy on ImageNet ILSVRC-2012 linear evaluation after\n200 pretraining epochs, surpassing MoCo's 67.5% using the same ResNet-50\nencoder. It also transfers more effectively to detection tasks: on PASCAL VOC,\nit outperforms both the supervised baseline and MoCo with 82.5% AP; on COCO, it\nsets new benchmarks with 40.9% AP for bounding box detection and 35.5% AP for\ninstance segmentation. Our synthetic hard negative generation approach\nsignificantly enhances visual representations learned through self-supervised\ncontrastive learning. Code is available at\nhttps://github.com/giakoumoglou/synco.\n","authors":["Nikolaos Giakoumoglou","Tania Stathaki"],"pdf_url":"https://arxiv.org/pdf/2410.02401v4.pdf","comment":"10 pages, 5 figures, 4 tables"},{"id":"http://arxiv.org/abs/2410.09474v2","updated":"2024-11-03T22:31:31Z","published":"2024-10-12T10:27:23Z","title":"Distilling Invariant Representations with Dual Augmentation","summary":"  Knowledge distillation (KD) has been widely used to transfer knowledge from\nlarge, accurate models (teachers) to smaller, efficient ones (students). Recent\nmethods have explored enforcing consistency by incorporating causal\ninterpretations to distill invariant representations. In this work, we extend\nthis line of research by introducing a dual augmentation strategy to promote\ninvariant feature learning in both teacher and student models. Our approach\nleverages different augmentations applied to both models during distillation,\npushing the student to capture robust, transferable features. This dual\naugmentation strategy complements invariant causal distillation by ensuring\nthat the learned representations remain stable across a wider range of data\nvariations and transformations. Extensive experiments on CIFAR-100 demonstrate\nthe effectiveness of this approach, achieving competitive results in\nsame-architecture KD.\n","authors":["Nikolaos Giakoumoglou","Tania Stathaki"],"pdf_url":"https://arxiv.org/pdf/2410.09474v2.pdf","comment":"This paper presents preliminary results from a project that we have\n  since discontinued, as our research focus has shifted to new directions"},{"id":"http://arxiv.org/abs/2403.14468v4","updated":"2024-11-03T21:16:54Z","published":"2024-03-21T15:15:00Z","title":"AnyV2V: A Tuning-Free Framework For Any Video-to-Video Editing Tasks","summary":"  In the dynamic field of digital content creation using generative models,\nstate-of-the-art video editing models still do not offer the level of quality\nand control that users desire. Previous works on video editing either extended\nfrom image-based generative models in a zero-shot manner or necessitated\nextensive fine-tuning, which can hinder the production of fluid video edits.\nFurthermore, these methods frequently rely on textual input as the editing\nguidance, leading to ambiguities and limiting the types of edits they can\nperform. Recognizing these challenges, we introduce AnyV2V, a novel tuning-free\nparadigm designed to simplify video editing into two primary steps: (1)\nemploying an off-the-shelf image editing model to modify the first frame, (2)\nutilizing an existing image-to-video generation model to generate the edited\nvideo through temporal feature injection. AnyV2V can leverage any existing\nimage editing tools to support an extensive array of video editing tasks,\nincluding prompt-based editing, reference-based style transfer, subject-driven\nediting, and identity manipulation, which were unattainable by previous\nmethods. AnyV2V can also support any video length. Our evaluation shows that\nAnyV2V achieved CLIP-scores comparable to other baseline methods. Furthermore,\nAnyV2V significantly outperformed these baselines in human evaluations,\ndemonstrating notable improvements in visual consistency with the source video\nwhile producing high-quality edits across all editing tasks.\n","authors":["Max Ku","Cong Wei","Weiming Ren","Harry Yang","Wenhu Chen"],"pdf_url":"https://arxiv.org/pdf/2403.14468v4.pdf","comment":"Published in Transactions on Machine Learning Research (TMLR 2024)\n  (11/2024)"},{"id":"http://arxiv.org/abs/2411.01683v1","updated":"2024-11-03T20:46:50Z","published":"2024-11-03T20:46:50Z","title":"ROAD-Waymo: Action Awareness at Scale for Autonomous Driving","summary":"  Autonomous Vehicle (AV) perception systems require more than simply seeing,\nvia e.g., object detection or scene segmentation. They need a holistic\nunderstanding of what is happening within the scene for safe interaction with\nother road users. Few datasets exist for the purpose of developing and training\nalgorithms to comprehend the actions of other road users. This paper presents\nROAD-Waymo, an extensive dataset for the development and benchmarking of\ntechniques for agent, action, location and event detection in road scenes,\nprovided as a layer upon the (US) Waymo Open dataset. Considerably larger and\nmore challenging than any existing dataset (and encompassing multiple cities),\nit comes with 198k annotated video frames, 54k agent tubes, 3.9M bounding boxes\nand a total of 12.4M labels. The integrity of the dataset has been confirmed\nand enhanced via a novel annotation pipeline designed for automatically\nidentifying violations of requirements specifically designed for this dataset.\nAs ROAD-Waymo is compatible with the original (UK) ROAD dataset, it provides\nthe opportunity to tackle domain adaptation between real-world road scenarios\nin different countries within a novel benchmark: ROAD++.\n","authors":["Salman Khan","Izzeddin Teeti","Reza Javanmard Alitappeh","Mihaela C. Stoian","Eleonora Giunchiglia","Gurkirt Singh","Andrew Bradley","Fabio Cuzzolin"],"pdf_url":"https://arxiv.org/pdf/2411.01683v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.00985v3","updated":"2024-11-03T20:34:35Z","published":"2024-06-03T04:43:56Z","title":"ParallelEdits: Efficient Multi-Aspect Text-Driven Image Editing with\n  Attention Grouping","summary":"  Text-driven image synthesis has made significant advancements with the\ndevelopment of diffusion models, transforming how visual content is generated\nfrom text prompts. Despite these advances, text-driven image editing, a key\narea in computer graphics, faces unique challenges. A major challenge is making\nsimultaneous edits across multiple objects or attributes. Applying these\nmethods sequentially for multi-attribute edits increases computational demands\nand efficiency losses. In this paper, we address these challenges with\nsignificant contributions. Our main contribution is the development of\nParallelEdits, a method that seamlessly manages simultaneous edits across\nmultiple attributes. In contrast to previous approaches, ParallelEdits not only\npreserves the quality of single attribute edits but also significantly improves\nthe performance of multitasking edits. This is achieved through innovative\nattention distribution mechanism and multi-branch design that operates across\nseveral processing heads. Additionally, we introduce the PIE-Bench++ dataset,\nan expansion of the original PIE-Bench dataset, to better support evaluating\nimage-editing tasks involving multiple objects and attributes simultaneously.\nThis dataset is a benchmark for evaluating text-driven image editing methods in\nmultifaceted scenarios.\n","authors":["Mingzhen Huang","Jialing Cai","Shan Jia","Vishnu Suresh Lokhande","Siwei Lyu"],"pdf_url":"https://arxiv.org/pdf/2406.00985v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13743v3","updated":"2024-11-03T20:22:32Z","published":"2024-06-19T18:00:07Z","title":"GenAI-Bench: Evaluating and Improving Compositional Text-to-Visual\n  Generation","summary":"  While text-to-visual models now produce photo-realistic images and videos,\nthey struggle with compositional text prompts involving attributes,\nrelationships, and higher-order reasoning such as logic and comparison. In this\nwork, we conduct an extensive human study on GenAI-Bench to evaluate the\nperformance of leading image and video generation models in various aspects of\ncompositional text-to-visual generation. We also compare automated evaluation\nmetrics against our collected human ratings and find that VQAScore -- a metric\nmeasuring the likelihood that a VQA model views an image as accurately\ndepicting the prompt -- significantly outperforms previous metrics such as\nCLIPScore. In addition, VQAScore can improve generation in a black-box manner\n(without finetuning) via simply ranking a few (3 to 9) candidate images.\nRanking by VQAScore is 2x to 3x more effective than other scoring methods like\nPickScore, HPSv2, and ImageReward at improving human alignment ratings for\nDALL-E 3 and Stable Diffusion, especially on compositional prompts that require\nadvanced visio-linguistic reasoning. We release a new GenAI-Rank benchmark with\nover 40,000 human ratings to evaluate scoring metrics on ranking images\ngenerated from the same prompt. Lastly, we discuss promising areas for\nimprovement in VQAScore, such as addressing fine-grained visual details. We\nwill release all human ratings (over 80,000) to facilitate scientific\nbenchmarking of both generative models and automated metrics.\n","authors":["Baiqi Li","Zhiqiu Lin","Deepak Pathak","Jiayao Li","Yixin Fei","Kewen Wu","Tiffany Ling","Xide Xia","Pengchuan Zhang","Graham Neubig","Deva Ramanan"],"pdf_url":"https://arxiv.org/pdf/2406.13743v3.pdf","comment":"We open-source our dataset, model, and code at:\n  https://linzhiqiu.github.io/papers/genai_bench ; Project page:\n  https://linzhiqiu.github.io/papers/genai_bench ; GenAI-Bench was first\n  introduced in arxiv:2404.01291. This article extends it with an additional\n  GenAI-Rank benchmark"},{"id":"http://arxiv.org/abs/2411.01669v1","updated":"2024-11-03T19:49:52Z","published":"2024-11-03T19:49:52Z","title":"MamT$^4$: Multi-view Attention Networks for Mammography Cancer\n  Classification","summary":"  In this study, we introduce a novel method, called MamT$^4$, which is used\nfor simultaneous analysis of four mammography images. A decision is made based\non one image of a breast, with attention also devoted to three additional\nimages: another view of the same breast and two images of the other breast.\nThis approach enables the algorithm to closely replicate the practice of a\nradiologist who reviews the entire set of mammograms for a patient.\nFurthermore, this paper emphasizes the preprocessing of images, specifically\nproposing a cropping model (U-Net based on ResNet-34) to help the method remove\nimage artifacts and focus on the breast region. To the best of our knowledge,\nthis study is the first to achieve a ROC-AUC of 84.0 $\\pm$ 1.7 and an F1 score\nof 56.0 $\\pm$ 1.3 on an independent test dataset of Vietnam digital mammography\n(VinDr-Mammo), which is preprocessed with the cropping model.\n","authors":["Alisher Ibragimov","Sofya Senotrusova","Arsenii Litvinov","Egor Ushakov","Evgeny Karpulevich","Yury Markin"],"pdf_url":"https://arxiv.org/pdf/2411.01669v1.pdf","comment":"The crop model is available here:\n  https://github.com/ispras/mammo_crop"},{"id":"http://arxiv.org/abs/2403.07536v2","updated":"2024-11-03T19:21:04Z","published":"2024-03-12T11:19:46Z","title":"LaB-GATr: geometric algebra transformers for large biomedical surface\n  and volume meshes","summary":"  Many anatomical structures can be described by surface or volume meshes.\nMachine learning is a promising tool to extract information from these 3D\nmodels. However, high-fidelity meshes often contain hundreds of thousands of\nvertices, which creates unique challenges in building deep neural network\narchitectures. Furthermore, patient-specific meshes may not be canonically\naligned which limits the generalisation of machine learning algorithms. We\npropose LaB-GATr, a transfomer neural network with geometric tokenisation that\ncan effectively learn with large-scale (bio-)medical surface and volume meshes\nthrough sequence compression and interpolation. Our method extends the recently\nproposed geometric algebra transformer (GATr) and thus respects all Euclidean\nsymmetries, i.e. rotation, translation and reflection, effectively mitigating\nthe problem of canonical alignment between patients. LaB-GATr achieves\nstate-of-the-art results on three tasks in cardiovascular hemodynamics\nmodelling and neurodevelopmental phenotype prediction, featuring meshes of up\nto 200,000 vertices. Our results demonstrate that LaB-GATr is a powerful\narchitecture for learning with high-fidelity meshes which has the potential to\nenable interesting downstream applications. Our implementation is publicly\navailable.\n","authors":["Julian Suk","Baris Imre","Jelmer M. Wolterink"],"pdf_url":"https://arxiv.org/pdf/2403.07536v2.pdf","comment":"First published in \"Medical Image Computing and Computer Assisted\n  Intervention\" (MICCAI), pp 185-195, 2024 by Springer Nature"},{"id":"http://arxiv.org/abs/2410.22489v2","updated":"2024-11-03T19:00:34Z","published":"2024-10-29T19:28:41Z","title":"Multimodality Helps Few-Shot 3D Point Cloud Semantic Segmentation","summary":"  Few-shot 3D point cloud segmentation (FS-PCS) aims at generalizing models to\nsegment novel categories with minimal annotated support samples. While existing\nFS-PCS methods have shown promise, they primarily focus on unimodal point cloud\ninputs, overlooking the potential benefits of leveraging multimodal\ninformation. In this paper, we address this gap by introducing a cost-free\nmultimodal FS-PCS setup, utilizing textual labels and the potentially available\n2D image modality. Under this easy-to-achieve setup, we present the MultiModal\nFew-Shot SegNet (MM-FSS), a model effectively harnessing complementary\ninformation from multiple modalities. MM-FSS employs a shared backbone with two\nheads to extract intermodal and unimodal visual features, and a pretrained text\nencoder to generate text embeddings. To fully exploit the multimodal\ninformation, we propose a Multimodal Correlation Fusion (MCF) module to\ngenerate multimodal correlations, and a Multimodal Semantic Fusion (MSF) module\nto refine the correlations using text-aware semantic guidance. Additionally, we\npropose a simple yet effective Test-time Adaptive Cross-modal Calibration\n(TACC) technique to mitigate training bias, further improving generalization.\nExperimental results on S3DIS and ScanNet datasets demonstrate significant\nperformance improvements achieved by our method. The efficacy of our approach\nindicates the benefits of leveraging commonly-ignored free modalities for\nFS-PCS, providing valuable insights for future research. The code is available\nat https://github.com/ZhaochongAn/Multimodality-3D-Few-Shot\n","authors":["Zhaochong An","Guolei Sun","Yun Liu","Runjia Li","Min Wu","Ming-Ming Cheng","Ender Konukoglu","Serge Belongie"],"pdf_url":"https://arxiv.org/pdf/2410.22489v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01656v1","updated":"2024-11-03T18:57:19Z","published":"2024-11-03T18:57:19Z","title":"Degradation-Aware Residual-Conditioned Optimal Transport for Unified\n  Image Restoration","summary":"  All-in-one image restoration has emerged as a practical and promising\nlow-level vision task for real-world applications. In this context, the key\nissue lies in how to deal with different types of degraded images\nsimultaneously. In this work, we present a Degradation-Aware\nResidual-Conditioned Optimal Transport (DA-RCOT) approach that models\n(all-in-one) image restoration as an optimal transport (OT) problem for\nunpaired and paired settings, introducing the transport residual as a\ndegradation-specific cue for both the transport cost and the transport map.\nSpecifically, we formalize image restoration with a residual-guided OT\nobjective by exploiting the degradation-specific patterns of the Fourier\nresidual in the transport cost. More crucially, we design the transport map for\nrestoration as a two-pass DA-RCOT map, in which the transport residual is\ncomputed in the first pass and then encoded as multi-scale residual embeddings\nto condition the second-pass restoration. This conditioning process injects\nintrinsic degradation knowledge (e.g., degradation type and level) and\nstructural information from the multi-scale residual embeddings into the OT\nmap, which thereby can dynamically adjust its behaviors for all-in-one\nrestoration. Extensive experiments across five degradations demonstrate the\nfavorable performance of DA-RCOT as compared to state-of-the-art methods, in\nterms of distortion measures, perceptual quality, and image structure\npreservation. Notably, DA-RCOT delivers superior adaptability to real-world\nscenarios even with multiple degradations and shows distinctive robustness to\nboth degradation levels and the number of degradations.\n","authors":["Xiaole Tang","Xiang Gu","Xiaoyi He","Xin Hu","Jian Sun"],"pdf_url":"https://arxiv.org/pdf/2411.01656v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01652v1","updated":"2024-11-03T18:30:37Z","published":"2024-11-03T18:30:37Z","title":"Optimizing Gastrointestinal Diagnostics: A CNN-Based Model for VCE Image\n  Classification","summary":"  In recent years, the diagnosis of gastrointestinal (GI) diseases has advanced\ngreatly with the advent of high-tech video capsule endoscopy (VCE) technology,\nwhich allows for non-invasive observation of the digestive system. The MisaHub\nCapsule Vision Challenge encourages the development of vendor-independent\nartificial intelligence models that can autonomously classify GI anomalies from\nVCE images. This paper presents CNN architecture designed specifically for\nmulticlass classification of ten gut pathologies, including angioectasia,\nbleeding, erosion, erythema, foreign bodies, lymphangiectasia, polyps, ulcers,\nand worms as well as their normal state.\n","authors":["Vaneeta Ahlawat","Rohit Sharma"," Urush"],"pdf_url":"https://arxiv.org/pdf/2411.01652v1.pdf","comment":"11 pages, 7 figuers"},{"id":"http://arxiv.org/abs/2405.18415v2","updated":"2024-11-03T18:23:45Z","published":"2024-05-28T17:57:06Z","title":"Why are Visually-Grounded Language Models Bad at Image Classification?","summary":"  Image classification is one of the most fundamental capabilities of machine\nvision intelligence. In this work, we revisit the image classification task\nusing visually-grounded language models (VLMs) such as GPT-4V and LLaVA. We\nfind that existing proprietary and public VLMs, despite often using CLIP as a\nvision encoder and having many more parameters, significantly underperform CLIP\non standard image classification benchmarks like ImageNet. To understand the\nreason, we explore several hypotheses concerning the inference algorithms,\ntraining objectives, and data processing in VLMs. Our analysis reveals that the\nprimary cause is data-related: critical information for image classification is\nencoded in the VLM's latent space but can only be effectively decoded with\nenough training data. Specifically, there is a strong correlation between the\nfrequency of class exposure during VLM training and instruction-tuning and the\nVLM's performance in those classes; when trained with sufficient data, VLMs can\nmatch the accuracy of state-of-the-art classification models. Based on these\nfindings, we enhance a VLM by integrating classification-focused datasets into\nits training, and demonstrate that the enhanced classification performance of\nthe VLM transfers to its general capabilities, resulting in an improvement of\n11.8% on the newly collected ImageWikiQA dataset.\n","authors":["Yuhui Zhang","Alyssa Unell","Xiaohan Wang","Dhruba Ghosh","Yuchang Su","Ludwig Schmidt","Serena Yeung-Levy"],"pdf_url":"https://arxiv.org/pdf/2405.18415v2.pdf","comment":"Published at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2401.15563v3","updated":"2024-11-03T18:04:52Z","published":"2024-01-28T04:07:59Z","title":"BrepGen: A B-rep Generative Diffusion Model with Structured Latent\n  Geometry","summary":"  This paper presents BrepGen, a diffusion-based generative approach that\ndirectly outputs a Boundary representation (B-rep) Computer-Aided Design (CAD)\nmodel. BrepGen represents a B-rep model as a novel structured latent geometry\nin a hierarchical tree. With the root node representing a whole CAD solid, each\nelement of a B-rep model (i.e., a face, an edge, or a vertex) progressively\nturns into a child-node from top to bottom. B-rep geometry information goes\ninto the nodes as the global bounding box of each primitive along with a latent\ncode describing the local geometric shape. The B-rep topology information is\nimplicitly represented by node duplication. When two faces share an edge, the\nedge curve will appear twice in the tree, and a T-junction vertex with three\nincident edges appears six times in the tree with identical node features.\nStarting from the root and progressing to the leaf, BrepGen employs\nTransformer-based diffusion models to sequentially denoise node features while\nduplicated nodes are detected and merged, recovering the B-Rep topology\ninformation. Extensive experiments show that BrepGen advances the task of CAD\nB-rep generation, surpassing existing methods on various benchmarks. Results on\nour newly collected furniture dataset further showcase its exceptional\ncapability in generating complicated geometry. While previous methods were\nlimited to generating simple prismatic shapes, BrepGen incorporates free-form\nand doubly-curved surfaces for the first time. Additional applications of\nBrepGen include CAD autocomplete and design interpolation. The code, pretrained\nmodels, and dataset are available at https://github.com/samxuxiang/BrepGen.\n","authors":["Xiang Xu","Joseph G. Lambourne","Pradeep Kumar Jayaraman","Zhengqing Wang","Karl D. D. Willis","Yasutaka Furukawa"],"pdf_url":"https://arxiv.org/pdf/2401.15563v3.pdf","comment":"Accepted to ACM SIGGRAPH 2024. Code at\n  https://github.com/samxuxiang/BrepGen"}]},"2024-11-02T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2406.10209v2","updated":"2024-11-02T23:19:18Z","published":"2024-06-14T17:44:22Z","title":"Be like a Goldfish, Don't Memorize! Mitigating Memorization in\n  Generative LLMs","summary":"  Large language models can memorize and repeat their training data, causing\nprivacy and copyright risks. To mitigate memorization, we introduce a subtle\nmodification to the next-token training objective that we call the goldfish\nloss. During training, randomly sampled subsets of tokens are excluded from the\nloss computation. These dropped tokens are not memorized by the model, which\nprevents verbatim reproduction of a complete chain of tokens from the training\nset. We run extensive experiments training billion-scale Llama-2 models, both\npre-trained and trained from scratch, and demonstrate significant reductions in\nextractable memorization with little to no impact on downstream benchmarks.\n","authors":["Abhimanyu Hans","Yuxin Wen","Neel Jain","John Kirchenbauer","Hamid Kazemi","Prajwal Singhania","Siddharth Singh","Gowthami Somepalli","Jonas Geiping","Abhinav Bhatele","Tom Goldstein"],"pdf_url":"https://arxiv.org/pdf/2406.10209v2.pdf","comment":"10 pages, 8 figures, and 1 table in the main body. Code available at\n  https://github.com/ahans30/goldfish-loss and checkpoints at\n  https://huggingface.co/collections/tomg-group-umd/goldfish-loss-mitigating-memorization-in-llms-66c175becb6aab07744f7272"},{"id":"http://arxiv.org/abs/2411.01369v1","updated":"2024-11-02T21:59:02Z","published":"2024-11-02T21:59:02Z","title":"Artificial Intelligence Driven Course Generation: A Case Study Using\n  ChatGPT","summary":"  This study explores Artificial Intelligence use, specifically ChatGPT, in\ncreating educational content. The study aims to elaborate on using ChatGPT to\ncreate course materials. The main objective is to assess the efficiency,\nquality, and impact of AI-driven course generation, and to create a Multimedia\nDatabases course as a case study. The study highlights the potential of AI to\nrevolutionize educational content creation, making it more accessible,\npersonalized, and efficient. The course content was generated in less than one\nday through iterative methods, using prompts for translation, content\nexpansion, practical examples, assignments, supplementary materials, and LaTeX\nformatting. Each part was verified immediately after generation to ensure\naccuracy. Post-generation analysis with Detectia and Turnitin showed similarity\nrates of 8.7% and 13%, indicating high originality. Experts and university\ncommittees reviewed and approved the course, with English university teachers\npraising its language quality. ChatGPT also created a well-structured and\ndiversified exam for the module. Key findings reveal significant time\nefficiency, comprehensive content coverage, and high flexibility. The study\nunderscores AI's transformative potential in education, addressing challenges\nrelated to data privacy, technology dependence, content accuracy, and\nalgorithmic biases. The conclusions emphasize the need for collaboration\nbetween educators, policymakers, and technology developers to harness AI's\nbenefits in education fully.\n","authors":["Djaber Rouabhia"],"pdf_url":"https://arxiv.org/pdf/2411.01369v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2403.11425v3","updated":"2024-11-02T21:12:03Z","published":"2024-03-18T02:42:01Z","title":"Narrative Feature or Structured Feature? A Study of Large Language\n  Models to Identify Cancer Patients at Risk of Heart Failure","summary":"  Cancer treatments are known to introduce cardiotoxicity, negatively impacting\noutcomes and survivorship. Identifying cancer patients at risk of heart failure\n(HF) is critical to improving cancer treatment outcomes and safety. This study\nexamined machine learning (ML) models to identify cancer patients at risk of HF\nusing electronic health records (EHRs), including traditional ML, Time-Aware\nlong short-term memory (T-LSTM), and large language models (LLMs) using novel\nnarrative features derived from the structured medical codes. We identified a\ncancer cohort of 12,806 patients from the University of Florida Health,\ndiagnosed with lung, breast, and colorectal cancers, among which 1,602\nindividuals developed HF after cancer. The LLM, GatorTron-3.9B, achieved the\nbest F1 scores, outperforming the traditional support vector machines by 39%,\nthe T-LSTM deep learning model by 7%, and a widely used transformer model,\nBERT, by 5.6%. The analysis shows that the proposed narrative features\nremarkably increased feature density and improved performance.\n","authors":["Ziyi Chen","Mengyuan Zhang","Mustafa Mohammed Ahmed","Yi Guo","Thomas J. George","Jiang Bian","Yonghui Wu"],"pdf_url":"https://arxiv.org/pdf/2403.11425v3.pdf","comment":"10 pages, 4 figures, 5 tables"},{"id":"http://arxiv.org/abs/2410.10855v2","updated":"2024-11-02T21:07:54Z","published":"2024-10-06T20:13:11Z","title":"CogDevelop2K: Reversed Cognitive Development in Multimodal Large\n  Language Models","summary":"  Are Multi-modal Large Language Models (MLLMs) stochastic parrots? Do they\ngenuinely understand? This paper aims to explore the core cognitive abilities\nthat human intelligence builds upon to perceive, comprehend, and reason in\nMLLMs. To this end, we propose CogDevelop2K, a comprehensive benchmark that\nspans 12 sub-concepts from primitive knowledge like object permanence and\nboundary to more complex abilities like intentionality understanding,\nstructured via the developmental trajectory of a human mind. We evaluate 46\nMLLMs on our benchmarks. Surprisingly, we observe a reversed cognitive\ndevelopmental trajectory compared to humans. Comprehensively, we further\nevaluate the influence of evaluation strategies and prompting techniques.\nWebsite with this $\\href{https://growing-ai-like-a-child.github.io/}{link}$.\n","authors":["Yijiang Li","Qingying Gao","Haoran Sun","Haiyun Lyu","Dezhi Luo","Hokin Deng"],"pdf_url":"https://arxiv.org/pdf/2410.10855v2.pdf","comment":"Website with this\n  $\\href{https://growing-ai-like-a-child.github.io/}{link}$"},{"id":"http://arxiv.org/abs/2411.01354v1","updated":"2024-11-02T20:05:31Z","published":"2024-11-02T20:05:31Z","title":"Online and Offline Evaluations of Collaborative Filtering and Content\n  Based Recommender Systems","summary":"  Recommender systems are widely used AI applications designed to help users\nefficiently discover relevant items. The effectiveness of such systems is tied\nto the satisfaction of both users and providers. However, user satisfaction is\ncomplex and cannot be easily framed mathematically using information retrieval\nand accuracy metrics. While many studies evaluate accuracy through offline\ntests, a growing number of researchers argue that online evaluation methods\nsuch as A/B testing are better suited for this purpose. We have employed a\nvariety of algorithms on different types of datasets divergent in size and\nsubject, producing recommendations in various platforms, including media\nstreaming services, digital publishing websites, e-commerce systems, and news\nbroadcasting networks. Notably, our target websites and datasets are in Persian\n(Farsi) language.\n  This study provides a comparative analysis of a large-scale recommender\nsystem that has been operating for the past year across about 70 websites in\nIran, processing roughly 300 requests per second collectively. The system\nemploys user-based and item-based recommendations using content-based,\ncollaborative filtering, trend-based methods, and hybrid approaches. Through\nboth offline and online evaluations, we aim to identify where these algorithms\nperform most efficiently and determine the best method for our specific needs,\nconsidering the dataset and system scale. Our methods of evaluation include\nmanual evaluation, offline tests including accuracy and ranking metrics like\nhit-rate@k and nDCG, and online tests consisting of click-through rate (CTR).\nAdditionally we analyzed and proposed methods to address cold-start and\npopularity bias.\n","authors":["Ali Elahi","Armin Zirak"],"pdf_url":"https://arxiv.org/pdf/2411.01354v1.pdf","comment":"9 pages, 9 figures"},{"id":"http://arxiv.org/abs/2411.01343v1","updated":"2024-11-02T19:14:19Z","published":"2024-11-02T19:14:19Z","title":"AMREx: AMR for Explainable Fact Verification","summary":"  With the advent of social media networks and the vast amount of information\ncirculating through them, automatic fact verification is an essential component\nto prevent the spread of misinformation. It is even more useful to have fact\nverification systems that provide explanations along with their classifications\nto ensure accurate predictions. To address both of these requirements, we\nimplement AMREx, an Abstract Meaning Representation (AMR)-based veracity\nprediction and explanation system for fact verification using a combination of\nSmatch, an AMR evaluation metric to measure meaning containment and textual\nsimilarity, and demonstrate its effectiveness in producing partially\nexplainable justifications using two community standard fact verification\ndatasets, FEVER and AVeriTeC. AMREx surpasses the AVeriTec baseline accuracy\nshowing the effectiveness of our approach for real-world claim verification. It\nfollows an interpretable pipeline and returns an explainable AMR node mapping\nto clarify the system's veracity predictions when applicable. We further\ndemonstrate that AMREx output can be used to prompt LLMs to generate\nnatural-language explanations using the AMR mappings as a guide to lessen the\nprobability of hallucinations.\n","authors":["Chathuri Jayaweera","Sangpil Youm","Bonnie Dorr"],"pdf_url":"https://arxiv.org/pdf/2411.01343v1.pdf","comment":"This study implements, evaluates, and analyzes an Abstract Meaning\n  Representation (AMR) based partially explainable system for fact\n  verification/ veracity classification. Accepted by EMNLP Workshop on Fact\n  Extraction and VERification (FEVER) 2024, 11 pages, 7 figures,"},{"id":"http://arxiv.org/abs/2405.17653v4","updated":"2024-11-02T19:13:06Z","published":"2024-05-27T20:53:22Z","title":"InversionView: A General-Purpose Method for Reading Information from\n  Neural Activations","summary":"  The inner workings of neural networks can be better understood if we can\nfully decipher the information encoded in neural activations. In this paper, we\nargue that this information is embodied by the subset of inputs that give rise\nto similar activations. We propose InversionView, which allows us to\npractically inspect this subset by sampling from a trained decoder model\nconditioned on activations. This helps uncover the information content of\nactivation vectors, and facilitates understanding of the algorithms implemented\nby transformer models. We present four case studies where we investigate models\nranging from small transformers to GPT-2. In these studies, we show that\nInversionView can reveal clear information contained in activations, including\nbasic information about tokens appearing in the context, as well as more\ncomplex information, such as the count of certain tokens, their relative\npositions, and abstract knowledge about the subject. We also provide causally\nverified circuits to confirm the decoded information.\n","authors":["Xinting Huang","Madhur Panwar","Navin Goyal","Michael Hahn"],"pdf_url":"https://arxiv.org/pdf/2405.17653v4.pdf","comment":"NeurIPS 2024; ICML 2024 Mechanistic Interpretability Workshop oral"},{"id":"http://arxiv.org/abs/2410.16531v3","updated":"2024-11-02T19:06:53Z","published":"2024-10-21T21:45:22Z","title":"Bayesian scaling laws for in-context learning","summary":"  In-context learning (ICL) is a powerful technique for getting language models\nto perform complex tasks with no training updates. Prior work has established\nstrong correlations between the number of in-context examples provided and the\naccuracy of the model's predictions. In this paper, we seek to explain this\ncorrelation by showing that ICL approximates a Bayesian learner. This\nperspective gives rise to a family of novel Bayesian scaling laws for ICL. In\nexperiments with \\mbox{GPT-2} models of different sizes, our scaling laws\nexceed or match existing scaling laws in accuracy while also offering\ninterpretable terms for task priors, learning efficiency, and per-example\nprobabilities. To illustrate the analytic power that such interpretable scaling\nlaws provide, we report on controlled synthetic dataset experiments designed to\ninform real-world studies of safety alignment. In our experimental protocol, we\nuse SFT to suppress an unwanted existing model capability and then use ICL to\ntry to bring that capability back (many-shot jailbreaking). We then experiment\non real-world instruction-tuned LLMs using capabilities benchmarks as well as a\nnew many-shot jailbreaking dataset. In all cases, Bayesian scaling laws\naccurately predict the conditions under which ICL will cause the suppressed\nbehavior to reemerge, which sheds light on the ineffectiveness of post-training\nat increasing LLM safety.\n","authors":["Aryaman Arora","Dan Jurafsky","Christopher Potts","Noah D. Goodman"],"pdf_url":"https://arxiv.org/pdf/2410.16531v3.pdf","comment":"10 pages main text, 26 pages total"},{"id":"http://arxiv.org/abs/2410.24049v2","updated":"2024-11-02T19:02:32Z","published":"2024-10-31T15:45:23Z","title":"Desert Camels and Oil Sheikhs: Arab-Centric Red Teaming of Frontier LLMs","summary":"  Large language models (LLMs) are widely used but raise ethical concerns due\nto embedded social biases. This study examines LLM biases against Arabs versus\nWesterners across eight domains, including women's rights, terrorism, and\nanti-Semitism and assesses model resistance to perpetuating these biases. To\nthis end, we create two datasets: one to evaluate LLM bias toward Arabs versus\nWesterners and another to test model safety against prompts that exaggerate\nnegative traits (\"jailbreaks\"). We evaluate six LLMs -- GPT-4, GPT-4o, LlaMA\n3.1 (8B & 405B), Mistral 7B, and Claude 3.5 Sonnet. We find 79% of cases\ndisplaying negative biases toward Arabs, with LlaMA 3.1-405B being the most\nbiased. Our jailbreak tests reveal GPT-4o as the most vulnerable, despite being\nan optimized version, followed by LlaMA 3.1-8B and Mistral 7B. All LLMs except\nClaude exhibit attack success rates above 87% in three categories. We also find\nClaude 3.5 Sonnet the safest, but it still displays biases in seven of eight\ncategories. Despite being an optimized version of GPT4, We find GPT-4o to be\nmore prone to biases and jailbreaks, suggesting optimization flaws. Our\nfindings underscore the pressing need for more robust bias mitigation\nstrategies and strengthened security measures in LLMs.\n","authors":["Muhammed Saeed","Elgizouli Mohamed","Mukhtar Mohamed","Shaina Raza","Shady Shehata","Muhammad Abdul-Mageed"],"pdf_url":"https://arxiv.org/pdf/2410.24049v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.20221v3","updated":"2024-11-02T18:46:54Z","published":"2024-10-26T16:27:34Z","title":"Generative linguistics contribution to artificial intelligence: Where\n  this contribution lies?","summary":"  This article aims to characterize Generative linguistics (GL) contribution to\nartificial intelligence (AI), alluding to the debate among linguists and AI\nscientists on whether linguistics belongs to humanities or science. In this\narticle, I will try not to be biased as a linguist, studying the phenomenon\nfrom an independent scientific perspective. The article walks the\nresearcher/reader through the scientific theorems and rationales involved in AI\nwhich belong from GL, specifically the Chomsky School. It, thus, provides good\nevidence from syntax, semantics, language faculty, Universal Grammar,\ncomputational system of human language, language acquisition, human brain,\nprogramming languages (e.g. Python), Large Language Models, and unbiased AI\nscientists that this contribution is huge, and that this contribution cannot be\ndenied. It concludes that however the huge GL contribution to AI, there are\nstill points of divergence including the nature and type of language input.\n","authors":["Mohammed Q. Shormani"],"pdf_url":"https://arxiv.org/pdf/2410.20221v3.pdf","comment":"28 pages, 3 figures"},{"id":"http://arxiv.org/abs/2404.00297v5","updated":"2024-11-02T18:03:03Z","published":"2024-03-30T09:20:43Z","title":"A hybrid transformer and attention based recurrent neural network for\n  robust and interpretable sentiment analysis of tweets","summary":"  Sentiment analysis is crucial for understanding public opinion and consumer\nbehavior. Existing models face challenges with linguistic diversity,\ngeneralizability, and explainability. We propose TRABSA, a hybrid framework\nintegrating transformer-based architectures, attention mechanisms, and BiLSTM\nnetworks to address this. Leveraging RoBERTa-trained on 124M tweets, we bridge\ngaps in sentiment analysis benchmarks, ensuring state-of-the-art accuracy.\nAugmenting datasets with tweets from 32 countries and US states, we compare six\nword-embedding techniques and three lexicon-based labeling techniques,\nselecting the best for optimal sentiment analysis. TRABSA outperforms\ntraditional ML and deep learning models with 94% accuracy and significant\nprecision, recall, and F1-score gains. Evaluation across diverse datasets\ndemonstrates consistent superiority and generalizability. SHAP and LIME\nanalyses enhance interpretability, improving confidence in predictions. Our\nstudy facilitates pandemic resource management, aiding resource planning,\npolicy formation, and vaccination tactics.\n","authors":["Md Abrar Jahin","Md Sakib Hossain Shovon","M. F. Mridha","Md Rashedul Islam","Yutaka Watanobe"],"pdf_url":"https://arxiv.org/pdf/2404.00297v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12624v4","updated":"2024-11-02T17:07:06Z","published":"2024-06-18T13:49:54Z","title":"Judging the Judges: Evaluating Alignment and Vulnerabilities in\n  LLMs-as-Judges","summary":"  Offering a promising solution to the scalability challenges associated with\nhuman evaluation, the LLM-as-a-judge paradigm is rapidly gaining traction as an\napproach to evaluating large language models (LLMs). However, there are still\nmany open questions about the strengths and weaknesses of this paradigm, and\nwhat potential biases it may hold. In this paper, we present a comprehensive\nstudy of the performance of various LLMs acting as judges, focusing on a clean\nscenario in which inter-human agreement is high. Investigating thirteen judge\nmodels of different model sizes and families, judging answers of nine different\n'examtaker models' - both base and instruction-tuned - we find that only the\nbest (and largest) models achieve reasonable alignment with humans. However,\nthey are still quite far behind inter-human agreement and their assigned scores\nmay still differ with up to 5 points from human-assigned scores. In terms of\ntheir ranking of the nine exam-taker models, instead, also smaller models and\neven the lexical metric contains may provide a reasonable signal. Through error\nanalysis and other studies, we identify vulnerabilities in judge models, such\nas their sensitivity to prompt complexity and length, and a tendency toward\nleniency. The fact that even the best judges differ from humans in this\ncomparatively simple setup suggest that caution may be wise when using judges\nin more complex setups. Lastly, our research rediscovers the importance of\nusing alignment metrics beyond simple percent alignment, showing that judges\nwith high percent agreement can still assign vastly different scores.\n","authors":["Aman Singh Thakur","Kartik Choudhary","Venkat Srinik Ramayapally","Sankaran Vaidyanathan","Dieuwke Hupkes"],"pdf_url":"https://arxiv.org/pdf/2406.12624v4.pdf","comment":null}]},"2024-11-05T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2411.03314v1","updated":"2024-11-05T18:59:51Z","published":"2024-11-05T18:59:51Z","title":"MME-Finance: A Multimodal Finance Benchmark for Expert-level\n  Understanding and Reasoning","summary":"  In recent years, multimodal benchmarks for general domains have guided the\nrapid development of multimodal models on general tasks. However, the financial\nfield has its peculiarities. It features unique graphical images (e.g.,\ncandlestick charts, technical indicator charts) and possesses a wealth of\nspecialized financial knowledge (e.g., futures, turnover rate). Therefore,\nbenchmarks from general fields often fail to measure the performance of\nmultimodal models in the financial domain, and thus cannot effectively guide\nthe rapid development of large financial models. To promote the development of\nlarge financial multimodal models, we propose MME-Finance, an bilingual\nopen-ended and practical usage-oriented Visual Question Answering (VQA)\nbenchmark. The characteristics of our benchmark are finance and expertise,\nwhich include constructing charts that reflect the actual usage needs of users\n(e.g., computer screenshots and mobile photography), creating questions\naccording to the preferences in financial domain inquiries, and annotating\nquestions by experts with 10+ years of experience in the financial industry.\nAdditionally, we have developed a custom-designed financial evaluation system\nin which visual information is first introduced in the multi-modal evaluation\nprocess. Extensive experimental evaluations of 19 mainstream MLLMs are\nconducted to test their perception, reasoning, and cognition capabilities. The\nresults indicate that models performing well on general benchmarks cannot do\nwell on MME-Finance; for instance, the top-performing open-source and\nclosed-source models obtain 65.69 (Qwen2VL-72B) and 63.18 (GPT-4o),\nrespectively. Their performance is particularly poor in categories most\nrelevant to finance, such as candlestick charts and technical indicator charts.\nIn addition, we propose a Chinese version, which helps compare performance of\nMLLMs under a Chinese context.\n","authors":["Ziliang Gan","Yu Lu","Dong Zhang","Haohan Li","Che Liu","Jian Liu","Ji Liu","Haipang Wu","Chaoyou Fu","Zenglin Xu","Rongjunchen Zhang","Yong Dai"],"pdf_url":"https://arxiv.org/pdf/2411.03314v1.pdf","comment":"Project Page: https://hithink-research.github.io/MME-Finance/"},{"id":"http://arxiv.org/abs/2411.03307v1","updated":"2024-11-05T18:01:12Z","published":"2024-11-05T18:01:12Z","title":"LLMs for Domain Generation Algorithm Detection","summary":"  This work analyzes the use of large language models (LLMs) for detecting\ndomain generation algorithms (DGAs). We perform a detailed evaluation of two\nimportant techniques: In-Context Learning (ICL) and Supervised Fine-Tuning\n(SFT), showing how they can improve detection. SFT increases performance by\nusing domain-specific data, whereas ICL helps the detection model to quickly\nadapt to new threats without requiring much retraining. We use Meta's Llama3 8B\nmodel, on a custom dataset with 68 malware families and normal domains,\ncovering several hard-to-detect schemes, including recent word-based DGAs.\nResults proved that LLM-based methods can achieve competitive results in DGA\ndetection. In particular, the SFT-based LLM DGA detector outperforms\nstate-of-the-art models using attention layers, achieving 94% accuracy with a\n4% false positive rate (FPR) and excelling at detecting word-based DGA domains.\n","authors":["Reynier Leyva La O","Carlos A. Catania","Tatiana Parlanti"],"pdf_url":"https://arxiv.org/pdf/2411.03307v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03300v1","updated":"2024-11-05T17:53:25Z","published":"2024-11-05T17:53:25Z","title":"VERITAS: A Unified Approach to Reliability Evaluation","summary":"  Large language models (LLMs) often fail to synthesize information from their\ncontext to generate an accurate response. This renders them unreliable in\nknowledge intensive settings where reliability of the output is key. A critical\ncomponent for reliable LLMs is the integration of a robust fact-checking system\nthat can detect hallucinations across various formats. While several\nopen-access fact-checking models are available, their functionality is often\nlimited to specific tasks, such as grounded question-answering or entailment\nverification, and they perform less effectively in conversational settings. On\nthe other hand, closed-access models like GPT-4 and Claude offer greater\nflexibility across different contexts, including grounded dialogue\nverification, but are hindered by high costs and latency. In this work, we\nintroduce VERITAS, a family of hallucination detection models designed to\noperate flexibly across diverse contexts while minimizing latency and costs.\nVERITAS achieves state-of-the-art results considering average performance on\nall major hallucination detection benchmarks, with $10\\%$ increase in average\nperformance when compared to similar-sized models and get close to the\nperformance of GPT4 turbo with LLM-as-a-judge setting.\n","authors":["Rajkumar Ramamurthy","Meghana Arakkal Rajeev","Oliver Molenschot","James Zou","Nazneen Rajani"],"pdf_url":"https://arxiv.org/pdf/2411.03300v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03284v1","updated":"2024-11-05T17:33:39Z","published":"2024-11-05T17:33:39Z","title":"SMoA: Improving Multi-agent Large Language Models with Sparse\n  Mixture-of-Agents","summary":"  While multi-agent systems have been shown to significantly enhance the\nperformance of Large Language Models (LLMs) across various tasks and\napplications, the dense interaction between scaling agents potentially hampers\ntheir efficiency and diversity. To address these challenges, we draw\ninspiration from the sparse mixture-of-agents (SMoE) and propose a sparse\nmixture-of-agents (SMoA) framework to improve the efficiency and diversity of\nmulti-agent LLMs. Unlike completely connected structures, SMoA introduces novel\nResponse Selection and Early Stopping mechanisms to sparsify information flows\namong individual LLM agents, striking a balance between performance and\nefficiency. Additionally, inspired by the expert diversity principle in SMoE\nframeworks for workload balance between experts, we assign distinct role\ndescriptions to each LLM agent, fostering diverse and divergent thinking.\nExtensive experiments on reasoning, alignment, and fairness benchmarks\ndemonstrate that SMoA achieves performance comparable to traditional\nmixture-of-agents approaches but with significantly lower computational costs.\nFurther analysis reveals that SMoA is more stable, has a greater capacity to\nscale, and offers considerable potential through hyper-parameter optimization.\nCode and data will be available at: https://github.com/David-Li0406/SMoA.\n","authors":["Dawei Li","Zhen Tan","Peijia Qian","Yifan Li","Kumar Satvik Chaudhary","Lijie Hu","Jiayi Shen"],"pdf_url":"https://arxiv.org/pdf/2411.03284v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2406.14550v2","updated":"2024-11-05T16:51:40Z","published":"2024-06-20T17:57:51Z","title":"GraphReader: Building Graph-based Agent to Enhance Long-Context\n  Abilities of Large Language Models","summary":"  Long-context capabilities are essential for large language models (LLMs) to\ntackle complex and long-input tasks. Despite numerous efforts made to optimize\nLLMs for long contexts, challenges persist in robustly processing long inputs.\nIn this paper, we introduce GraphReader, a graph-based agent system designed to\nhandle long texts by structuring them into a graph and employing an agent to\nexplore this graph autonomously. Upon receiving a question, the agent first\nundertakes a step-by-step analysis and devises a rational plan. It then invokes\na set of predefined functions to read node content and neighbors, facilitating\na coarse-to-fine exploration of the graph. Throughout the exploration, the\nagent continuously records new insights and reflects on current circumstances\nto optimize the process until it has gathered sufficient information to\ngenerate an answer. Experimental results on the LV-Eval dataset reveal that\nGraphReader, using a 4k context window, consistently outperforms GPT-4-128k\nacross context lengths from 16k to 256k by a large margin. Additionally, our\napproach demonstrates superior performance on four challenging single-hop and\nmulti-hop benchmarks.\n","authors":["Shilong Li","Yancheng He","Hangyu Guo","Xingyuan Bu","Ge Bai","Jie Liu","Jiaheng Liu","Xingwei Qu","Yangguang Li","Wanli Ouyang","Wenbo Su","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2406.14550v2.pdf","comment":"[EMNLP 2024] The first four authors contributed equally, 29 pages"},{"id":"http://arxiv.org/abs/2406.06484v4","updated":"2024-11-05T16:48:53Z","published":"2024-06-10T17:24:42Z","title":"Parallelizing Linear Transformers with the Delta Rule over Sequence\n  Length","summary":"  Transformers with linear attention (i.e., linear transformers) and\nstate-space models have recently been suggested as a viable linear-time\nalternative to transformers with softmax attention. However, these models still\nunderperform transformers especially on tasks that require in-context\nretrieval. While more expressive variants of linear transformers which replace\nthe additive update in linear transformers with the delta rule (DeltaNet) have\nbeen found to be more effective at associative recall, existing algorithms for\ntraining such models do not parallelize over sequence length and are thus\ninefficient to train on modern hardware. This work describes a\nhardware-efficient algorithm for training linear transformers with the delta\nrule, which exploits a memory-efficient representation for computing products\nof Householder matrices. This algorithm allows us to scale up DeltaNet to\nstandard language modeling settings. We train a 1.3B model for 100B tokens and\nfind that it outperforms recent linear-time baselines such as Mamba and GLA in\nterms of perplexity and zero-shot performance on downstream tasks. We also\nexperiment with two hybrid models which combine DeltaNet layers with (1)\nsliding-window attention layers every other layer or (2) two global attention\nlayers, and find that these hybrids outperform strong transformer baselines.\n","authors":["Songlin Yang","Bailin Wang","Yu Zhang","Yikang Shen","Yoon Kim"],"pdf_url":"https://arxiv.org/pdf/2406.06484v4.pdf","comment":"NeurIPS 2024 camera ready"},{"id":"http://arxiv.org/abs/2411.03250v1","updated":"2024-11-05T16:47:53Z","published":"2024-11-05T16:47:53Z","title":"DiffLM: Controllable Synthetic Data Generation via Diffusion Language\n  Models","summary":"  Recent advancements in large language models (LLMs) have significantly\nenhanced their knowledge and generative capabilities, leading to a surge of\ninterest in leveraging LLMs for high-quality data synthesis. However, synthetic\ndata generation via prompting LLMs remains challenging due to LLMs' limited\nunderstanding of target data distributions and the complexity of prompt\nengineering, especially for structured formatted data. To address these issues,\nwe introduce DiffLM, a controllable data synthesis framework based on\nvariational autoencoder (VAE), which further (1) leverages diffusion models to\nreserve more information of original distribution and format structure in the\nlearned latent distribution and (2) decouples the learning of target\ndistribution knowledge from the LLM's generative objectives via a plug-and-play\nlatent feature injection module. As we observed significant discrepancies\nbetween the VAE's latent representations and the real data distribution, the\nlatent diffusion module is introduced into our framework to learn a fully\nexpressive latent distribution. Evaluations on seven real-world datasets with\nstructured formatted data (i.e., Tabular, Code and Tool data) demonstrate that\nDiffLM generates high-quality data, with performance on downstream tasks\nsurpassing that of real data by 2-7 percent in certain cases. The data and code\nwill be publicly available upon completion of internal review.\n","authors":["Ying Zhou","Xinyao Wang","Yulei Niu","Yaojie Shen","Lexin Tang","Fan Chen","Ben He","Le Sun","Longyin Wen"],"pdf_url":"https://arxiv.org/pdf/2411.03250v1.pdf","comment":"17 pages, 8 figures"},{"id":"http://arxiv.org/abs/2406.11811v2","updated":"2024-11-05T16:47:43Z","published":"2024-06-17T17:52:54Z","title":"RepLiQA: A Question-Answering Dataset for Benchmarking LLMs on Unseen\n  Reference Content","summary":"  Large Language Models (LLMs) are trained on vast amounts of data, most of\nwhich is automatically scraped from the internet. This data includes\nencyclopedic documents that harbor a vast amount of general knowledge (e.g.,\nWikipedia) but also potentially overlap with benchmark datasets used for\nevaluating LLMs. Consequently, evaluating models on test splits that might have\nleaked into the training set is prone to misleading conclusions. To foster\nsound evaluation of language models, we introduce a new test dataset named\nRepLiQA, suited for question-answering and topic retrieval tasks. RepLiQA is a\ncollection of five splits of test sets, four of which have not been released to\nthe internet or exposed to LLM APIs prior to this publication. Each sample in\nRepLiQA comprises (1) a reference document crafted by a human annotator and\ndepicting an imaginary scenario (e.g., a news article) absent from the\ninternet; (2) a question about the document's topic; (3) a ground-truth answer\nderived directly from the information in the document; and (4) the paragraph\nextracted from the reference document containing the answer. As such, accurate\nanswers can only be generated if a model can find relevant content within the\nprovided document. We run a large-scale benchmark comprising several\nstate-of-the-art LLMs to uncover differences in performance across models of\nvarious types and sizes in a context-conditional language modeling setting.\nReleased splits of RepLiQA can be found here:\nhttps://huggingface.co/datasets/ServiceNow/repliqa.\n","authors":["Joao Monteiro","Pierre-Andre Noel","Etienne Marcotte","Sai Rajeswar","Valentina Zantedeschi","David Vazquez","Nicolas Chapados","Christopher Pal","Perouz Taslakian"],"pdf_url":"https://arxiv.org/pdf/2406.11811v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.14762v3","updated":"2024-11-05T16:40:21Z","published":"2024-02-22T18:21:59Z","title":"MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language\n  Models in Multi-Turn Dialogues","summary":"  The advent of Large Language Models (LLMs) has drastically enhanced dialogue\nsystems. However, comprehensively evaluating the dialogue abilities of LLMs\nremains a challenge. Previous benchmarks have primarily focused on single-turn\ndialogues or provided coarse-grained and incomplete assessments of multi-turn\ndialogues, overlooking the complexity and fine-grained nuances of real-life\ndialogues. To address this issue, we introduce MT-Bench-101, specifically\ndesigned to evaluate the fine-grained abilities of LLMs in multi-turn\ndialogues. By conducting a detailed analysis of real multi-turn dialogue data,\nwe construct a three-tier hierarchical ability taxonomy comprising 4208 turns\nacross 1388 multi-turn dialogues in 13 distinct tasks. We then evaluate 21\npopular LLMs based on MT-Bench-101, conducting comprehensive analyses from both\nability and task perspectives and observing differing trends in LLMs\nperformance across dialogue turns within various tasks. Further analysis\nindicates that neither utilizing common alignment techniques nor chat-specific\ndesigns has led to obvious enhancements in the multi-turn abilities of LLMs.\nExtensive case studies suggest that our designed tasks accurately assess the\ncorresponding multi-turn abilities. The data and code are available at\n\\url{https://github.com/mtbench101/mt-bench-101}.\n","authors":["Ge Bai","Jie Liu","Xingyuan Bu","Yancheng He","Jiaheng Liu","Zhanhui Zhou","Zhuoran Lin","Wenbo Su","Tiezheng Ge","Bo Zheng","Wanli Ouyang"],"pdf_url":"https://arxiv.org/pdf/2402.14762v3.pdf","comment":"[ACL 2024] The first three authors contribute equally, 34 pages, repo\n  at https://github.com/mtbench101/mt-bench-101"},{"id":"http://arxiv.org/abs/2311.17898v3","updated":"2024-11-05T16:31:24Z","published":"2023-11-29T18:51:46Z","title":"Contextual Knowledge Pursuit for Faithful Visual Synthesis","summary":"  Modern text-to-vision generative models often hallucinate when the prompt\ndescribing the scene to be generated is underspecified. In large language\nmodels (LLMs), a prevalent strategy to reduce hallucinations is to retrieve\nfactual knowledge from an external database. While such retrieval augmentation\nstrategies have great potential to enhance text-to-vision generators, existing\nstatic top-K retrieval methods explore the knowledge pool once, missing the\nbroader context necessary for high-quality generation. Furthermore, LLMs\ninternally possess rich world knowledge learned during large-scale training\n(parametric knowledge) that could mitigate the need for external data\nretrieval. This paper proposes Contextual Knowledge Pursuit (CKPT), a framework\nthat leverages the complementary strengths of external and parametric knowledge\nto help generators produce reliable visual content. Instead of the one-time\nretrieval of facts from an external database to improve a given prompt, CKPT\nuses (1) an LLM to decide whether to seek external knowledge or to self-elicit\ndescriptions from LLM parametric knowledge, (2) a knowledge pursuit process to\ncontextually seek and sequentially gather most relevant facts, (3) a knowledge\naggregator for prompt enhancement with the gathered fact context, and (4) a\nfiltered fine-tuning objective to improve visual synthesis with richer prompts.\nWe evaluate CKPT across multiple text-driven generative tasks (image, 3D\nrendering, and video) on datasets of rare objects and daily scenarios. Our\nresults show that CKPT is capable of generating faithful and semantically rich\ncontent across diverse visual domains, offering a promising data source for\nzero-shot synthesis and filtered fine-tuning of text-to-vision generative\nmodels.\n","authors":["Jinqi Luo","Kwan Ho Ryan Chan","Dimitris Dimos","René Vidal"],"pdf_url":"https://arxiv.org/pdf/2311.17898v3.pdf","comment":"Accepted in ECCV 2024 SDCV Workshop. GitHub repository at\n  https://github.com/peterljq/Contextual-Knowledge-Pursuit"},{"id":"http://arxiv.org/abs/2407.11019v2","updated":"2024-11-05T16:21:55Z","published":"2024-06-28T17:31:47Z","title":"Efficacy of Various Large Language Models in Generating Smart Contracts","summary":"  This study analyzes the application of code-generating Large Language Models\nin the creation of immutable Solidity smart contracts on the Ethereum\nBlockchain. Other works have previously analyzed Artificial Intelligence code\ngeneration abilities. This paper aims to expand this to a larger scope to\ninclude programs where security and efficiency are of utmost priority such as\nsmart contracts. The hypothesis leading into the study was that LLMs in general\nwould have difficulty in rigorously implementing security details in the code,\nwhich was shown through our results, but surprisingly generally succeeded in\nmany common types of contracts. We also discovered a novel way of generating\nsmart contracts through new prompting strategies.\n","authors":["Siddhartha Chatterjee","Bina Ramamurthy"],"pdf_url":"https://arxiv.org/pdf/2407.11019v2.pdf","comment":"18 pages, accepted for presentation at 8th annual Future of\n  Information and Communication Conference"},{"id":"http://arxiv.org/abs/2401.06477v4","updated":"2024-11-05T16:02:21Z","published":"2024-01-12T09:56:57Z","title":"Kun: Answer Polishment for Chinese Self-Alignment with Instruction\n  Back-Translation","summary":"  In this paper, we introduce Kun, a novel approach for creating high-quality\ninstruction-tuning datasets for large language models (LLMs) without relying on\nmanual annotations. Adapting a self-training algorithm based on instruction\nback-translation and answer polishment, Kun leverages unlabelled data from\ndiverse sources such as Wudao, Wanjuan, and SkyPile to generate a substantial\ndataset of over a million Chinese instructional data points. This approach\nsignificantly deviates from traditional methods by using a self-curation\nprocess to refine and select the most effective instruction-output pairs. Our\nexperiments with the 6B-parameter Yi model across various benchmarks\ndemonstrate Kun's robustness and scalability. Our method's core contributions\nlie in its algorithmic advancement, which enhances data retention and clarity,\nand its innovative data generation approach that substantially reduces the\nreliance on costly and time-consuming manual annotations. This methodology\npresents a scalable and efficient solution for improving the\ninstruction-following capabilities of LLMs, with significant implications for\ntheir application across diverse fields. The code and dataset can be found at\nhttps://github.com/Zheng0428/COIG-Kun\n","authors":["Tianyu Zheng","Shuyue Guo","Xingwei Qu","Jiawei Guo","Xinrun Du","Qi Jia","Chenghua Lin","Wenhao Huang","Jie Fu","Ge Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.06477v4.pdf","comment":"12 pages, 12 figures"},{"id":"http://arxiv.org/abs/2411.01483v2","updated":"2024-11-05T15:55:51Z","published":"2024-11-03T08:49:55Z","title":"Teaching Models to Improve on Tape","summary":"  Large Language Models (LLMs) often struggle when prompted to generate content\nunder specific constraints. However, in such cases it is often easy to check\nwhether these constraints are satisfied or violated. Recent works have shown\nthat LLMs can benefit from such ``corrective feedback''. Here we claim that\nthis skill of LLMs can be significantly enhanced via training. We introduce an\nRL framework for teaching models to use such rewards, by simulating interaction\nsessions, and rewarding the model according to its ability to satisfy the\nconstraints. We refer to our method as CORGI (Controlled Generation with RL for\nGuided Interaction), and evaluate it on a variety of controlled generation\ntasks using unlabeled training data. We find that CORGI consistently\noutperforms the baseline reinforcement learning method that does not\nincorporate conversational feedback. Furthermore, CORGI's interactive framework\nenables meta-learning, allowing the LLM to generalize better to guided\ninteraction in new tasks. Our results clearly show that conversational\noptimization, when combined with reinforcement learning, significantly improves\nthe effectiveness of LLMs in controlled generation contexts.\n","authors":["Liat Bezalel","Eyal Orgad","Amir Globerson"],"pdf_url":"https://arxiv.org/pdf/2411.01483v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04331v2","updated":"2024-11-05T15:43:18Z","published":"2024-06-06T17:59:10Z","title":"PaCE: Parsimonious Concept Engineering for Large Language Models","summary":"  Large Language Models (LLMs) are being used for a wide variety of tasks.\nWhile they are capable of generating human-like responses, they can also\nproduce undesirable output including potentially harmful information, racist or\nsexist language, and hallucinations. Alignment methods are designed to reduce\nsuch undesirable outputs via techniques such as fine-tuning, prompt\nengineering, and representation engineering. However, existing methods face\nseveral challenges: some require costly fine-tuning for every alignment task;\nsome do not adequately remove undesirable concepts, failing alignment; some\nremove benign concepts, lowering the linguistic capabilities of LLMs. To\naddress these issues, we propose Parsimonious Concept Engineering (PaCE), a\nnovel activation engineering framework for alignment. First, to sufficiently\nmodel the concepts, we construct a large-scale concept dictionary in the\nactivation space, in which each atom corresponds to a semantic concept. Given\nany alignment task, we instruct a concept partitioner to efficiently annotate\nthe concepts as benign or undesirable. Then, at inference time, we decompose\nthe LLM activations along the concept dictionary via sparse coding, to\naccurately represent the activations as linear combinations of benign and\nundesirable components. By removing the latter ones from the activations, we\nreorient the behavior of the LLM towards the alignment goal. We conduct\nexperiments on tasks such as response detoxification, faithfulness enhancement,\nand sentiment revising, and show that PaCE achieves state-of-the-art alignment\nperformance while maintaining linguistic capabilities.\n","authors":["Jinqi Luo","Tianjiao Ding","Kwan Ho Ryan Chan","Darshan Thaker","Aditya Chattopadhyay","Chris Callison-Burch","René Vidal"],"pdf_url":"https://arxiv.org/pdf/2406.04331v2.pdf","comment":"Accepted in NeurIPS 2024. GitHub repository at\n  https://github.com/peterljq/Parsimonious-Concept-Engineering"},{"id":"http://arxiv.org/abs/2411.01076v2","updated":"2024-11-05T15:03:45Z","published":"2024-11-01T23:14:30Z","title":"Privacy Risks of Speculative Decoding in Large Language Models","summary":"  Speculative decoding in large language models (LLMs) accelerates token\ngeneration by speculatively predicting multiple tokens cheaply and verifying\nthem in parallel, and has been widely deployed. In this paper, we provide the\nfirst study demonstrating the privacy risks of speculative decoding. We observe\nthat input-dependent patterns of correct and incorrect predictions can be\nleaked out to an adversary monitoring token generation times and packet sizes,\nleading to privacy breaches. By observing the pattern of correctly and\nincorrectly speculated tokens, we show that a malicious adversary can\nfingerprint queries and learn private user inputs with more than $90\\%$\naccuracy across three different speculative decoding techniques - REST (almost\n$100\\%$ accuracy), LADE (up to $92\\%$ accuracy), and BiLD (up to $95\\%$\naccuracy). We show that an adversary can also leak out confidential\nintellectual property used to design these techniques, such as data from\ndata-stores used for prediction (in REST) at a rate of more than $25$ tokens\nper second, or even hyper-parameters used for prediction (in LADE). We also\ndiscuss mitigation strategies, such as aggregating tokens across multiple\niterations and padding packets with additional bytes, to avoid such privacy or\nconfidentiality breaches.\n","authors":["Jiankun Wei","Abdulrahman Abdulrazzag","Tianchen Zhang","Adel Muursepp","Gururaj Saileshwar"],"pdf_url":"https://arxiv.org/pdf/2411.01076v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21333v2","updated":"2024-11-05T13:47:25Z","published":"2024-10-27T18:30:41Z","title":"Mind Your Step (by Step): Chain-of-Thought can Reduce Performance on\n  Tasks where Thinking Makes Humans Worse","summary":"  Chain-of-thought (CoT) prompting has become a widely used strategy for\nworking with large language and multimodal models. While CoT has been shown to\nimprove performance across many tasks, determining the settings in which it is\neffective remains an ongoing effort. In particular, it is still an open\nquestion in what settings CoT systematically reduces model performance. In this\npaper, we seek to identify the characteristics of tasks where CoT reduces\nperformance by drawing inspiration from cognitive psychology, looking at cases\nwhere (i) verbal thinking or deliberation hurts performance in humans, and (ii)\nthe constraints governing human performance generalize to language models.\nThree such cases are implicit statistical learning, visual recognition, and\nclassifying with patterns containing exceptions. In extensive experiments\nacross all three settings, we find that a diverse collection of\nstate-of-the-art models exhibit significant drop-offs in performance (e.g., up\nto 36.3% absolute accuracy for OpenAI o1-preview compared to GPT-4o) when using\ninference-time reasoning compared to zero-shot counterparts. We also identify\nthree tasks that satisfy condition (i) but not (ii), and find that while verbal\nthinking reduces human performance in these tasks, CoT retains or increases\nmodel performance. Overall, our results show that while there is not an exact\nparallel between the cognitive processes of models and those of humans,\nconsidering cases where thinking has negative consequences for human\nperformance can help us identify settings where it negatively impacts models.\nBy connecting the literature on human deliberation with evaluations of CoT, we\noffer a new tool that can be used in understanding the impact of prompt choices\nand inference-time reasoning.\n","authors":["Ryan Liu","Jiayi Geng","Addison J. Wu","Ilia Sucholutsky","Tania Lombrozo","Thomas L. Griffiths"],"pdf_url":"https://arxiv.org/pdf/2410.21333v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23074v2","updated":"2024-11-05T13:26:07Z","published":"2024-10-30T14:46:43Z","title":"Multi-Programming Language Sandbox for LLMs","summary":"  We introduce MPLSandbox, an out-of-the-box multi-programming language sandbox\ndesigned to provide unified and comprehensive feedback from compiler and\nanalysis tools for Large Language Models (LLMs). It can automatically identify\nthe programming language of the code, compiling and executing it within an\nisolated sub-sandbox to ensure safety and stability. In addition, MPLSandbox\nalso integrates both traditional and LLM-based code analysis tools, providing a\ncomprehensive analysis of generated code. MPLSandbox can be effortlessly\nintegrated into the training and deployment of LLMs to improve the quality and\ncorrectness of their generated code. It also helps researchers streamline their\nworkflows for various LLM-based code-related tasks, reducing the development\ncost. To validate the effectiveness of MPLSandbox, we integrate it into\ntraining and deployment approaches, and also employ it to optimize workflows\nfor a wide range of real-world code-related tasks. Our goal is to enhance\nresearcher productivity on LLM-based code-related tasks by simplifying and\nautomating workflows through delegation to MPLSandbox.\n","authors":["Shihan Dou","Jiazheng Zhang","Jianxiang Zang","Yunbo Tao","Weikang Zhou","Haoxiang Jia","Shichun Liu","Yuming Yang","Zhiheng Xi","Shenxi Wu","Shaoqing Zhang","Muling Wu","Changze Lv","Limao Xiong","Wenyu Zhan","Lin Zhang","Rongxiang Weng","Jingang Wang","Xunliang Cai","Yueming Wu","Ming Wen","Rui Zheng","Tao Ji","Yixin Cao","Tao Gui","Xipeng Qiu","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2410.23074v2.pdf","comment":"25 pages, 14 figures"},{"id":"http://arxiv.org/abs/2312.11517v4","updated":"2024-11-05T13:21:08Z","published":"2023-12-12T19:34:23Z","title":"A Natural Language Processing-Based Classification and Mode-Based\n  Ranking of Musculoskeletal Disorder Risk Factors","summary":"  This research delves into Musculoskeletal Disorder (MSD) risk factors, using\na blend of Natural Language Processing (NLP) and mode-based ranking. The aim is\nto refine understanding, classification, and prioritization for focused\nprevention and treatment. Eight NLP models are evaluated, combining pre-trained\ntransformers, cosine similarity, and distance metrics to categorize factors\ninto personal, biomechanical, workplace, psychological, and organizational\nclasses. BERT with cosine similarity achieves 28% accuracy; sentence\ntransformer with Euclidean, Bray-Curtis, and Minkowski distances scores 100%.\nWith 10-fold cross-validation, statistical tests ensure robust results. Survey\ndata and mode-based ranking determine severity hierarchy, aligning with the\nliterature. \"Working posture\" is the most severe, highlighting posture's role.\nSurvey insights emphasize \"Job insecurity,\" \"Effort reward imbalance,\" and\n\"Poor employee facility\" as significant contributors. Rankings offer actionable\ninsights for MSD prevention. The study suggests targeted interventions,\nworkplace improvements, and future research directions. This integrated NLP and\nranking approach enhances MSD comprehension and informs occupational health\nstrategies.\n","authors":["Md Abrar Jahin","Subrata Talapatra"],"pdf_url":"https://arxiv.org/pdf/2312.11517v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10229v2","updated":"2024-11-05T13:18:31Z","published":"2024-04-16T02:19:28Z","title":"Generative Text Steganography with Large Language Model","summary":"  Recent advances in large language models (LLMs) have blurred the boundary of\nhigh-quality text generation between humans and machines, which is favorable\nfor generative text steganography. While, current advanced steganographic\nmapping is not suitable for LLMs since most users are restricted to accessing\nonly the black-box API or user interface of the LLMs, thereby lacking access to\nthe training vocabulary and its sampling probabilities. In this paper, we\nexplore a black-box generative text steganographic method based on the user\ninterfaces of large language models, which is called LLM-Stega. The main goal\nof LLM-Stega is that the secure covert communication between Alice (sender) and\nBob (receiver) is conducted by using the user interfaces of LLMs. Specifically,\nWe first construct a keyword set and design a new encrypted steganographic\nmapping to embed secret messages. Furthermore, to guarantee accurate extraction\nof secret messages and rich semantics of generated stego texts, an optimization\nmechanism based on reject sampling is proposed. Comprehensive experiments\ndemonstrate that the proposed LLM-Stega outperforms current state-of-the-art\nmethods.\n","authors":["Jiaxuan Wu","Zhengxian Wu","Yiming Xue","Juan Wen","Wanli Peng"],"pdf_url":"https://arxiv.org/pdf/2404.10229v2.pdf","comment":"9 pages, 4 figures, accepted at ACM Multimedia 2024"},{"id":"http://arxiv.org/abs/2406.13618v2","updated":"2024-11-05T13:17:56Z","published":"2024-06-19T15:14:55Z","title":"In-Context Former: Lightning-fast Compressing Context for Large Language\n  Model","summary":"  With the rising popularity of Transformer-based large language models (LLMs),\nreducing their high inference costs has become a significant research focus.\nOne effective approach is to compress the long input contexts. Existing methods\ntypically leverage the self-attention mechanism of the LLM itself for context\ncompression. While these methods have achieved notable results, the compression\nprocess still involves quadratic time complexity, which limits their\napplicability. To mitigate this limitation, we propose the In-Context Former\n(IC-Former). Unlike previous methods, IC-Former does not depend on the target\nLLMs. Instead, it leverages the cross-attention mechanism and a small number of\nlearnable digest tokens to directly condense information from the contextual\nword embeddings. This approach significantly reduces inference time, which\nachieves linear growth in time complexity within the compression range.\nExperimental results indicate that our method requires only 1/32 of the\nfloating-point operations of the baseline during compression and improves\nprocessing speed by 68 to 112 times while achieving over 90% of the baseline\nperformance on evaluation metrics. Overall, our model effectively reduces\ncompression costs and makes real-time compression scenarios feasible.\n","authors":["Xiangfeng Wang","Zaiyi Chen","Zheyong Xie","Tong Xu","Yongyi He","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2406.13618v2.pdf","comment":"Accepted by EMNLP2024(Findings)"},{"id":"http://arxiv.org/abs/2411.03042v1","updated":"2024-11-05T12:26:25Z","published":"2024-11-05T12:26:25Z","title":"Predictor-Corrector Enhanced Transformers with Exponential Moving\n  Average Coefficient Learning","summary":"  Residual networks, as discrete approximations of Ordinary Differential\nEquations (ODEs), have inspired significant advancements in neural network\ndesign, including multistep methods, high-order methods, and multi-particle\ndynamical systems. The precision of the solution to ODEs significantly affects\nparameter optimization, thereby impacting model performance. In this work, we\npresent a series of advanced explorations of Transformer architecture design to\nminimize the error compared to the true ``solution.'' First, we introduce a\npredictor-corrector learning framework to minimize truncation errors, which\nconsists of a high-order predictor and a multistep corrector. Second, we\npropose an exponential moving average-based coefficient learning method to\nstrengthen our higher-order predictor. Extensive experiments on large-scale\nmachine translation, abstractive summarization, language modeling, and natural\nlanguage understanding benchmarks demonstrate the superiority of our approach.\nOn the WMT'14 English-German and English-French tasks, our model achieved BLEU\nscores of 30.95 and 44.27, respectively. Furthermore, on the OPUS multilingual\nmachine translation task, our model surpasses a robust 3.8B DeepNet by an\naverage of 2.9 SacreBLEU, using only 1/3 parameters. Notably, it also beats\nLLama models by 5.7 accuracy points on the LM Harness Evaluation.\n","authors":["Bei Li","Tong Zheng","Rui Wang","Jiahao Liu","Qingyan Guo","Junliang Guo","Xu Tan","Tong Xiao","Jingbo Zhu","Jingang Wang","Xunliang Cai"],"pdf_url":"https://arxiv.org/pdf/2411.03042v1.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.03039v1","updated":"2024-11-05T12:22:51Z","published":"2024-11-05T12:22:51Z","title":"Self-Compositional Data Augmentation for Scientific Keyphrase Generation","summary":"  State-of-the-art models for keyphrase generation require large amounts of\ntraining data to achieve good performance. However, obtaining keyphrase-labeled\ndocuments can be challenging and costly. To address this issue, we present a\nself-compositional data augmentation method. More specifically, we measure the\nrelatedness of training documents based on their shared keyphrases, and combine\nsimilar documents to generate synthetic samples. The advantage of our method\nlies in its ability to create additional training samples that keep domain\ncoherence, without relying on external data or resources. Our results on\nmultiple datasets spanning three different domains, demonstrate that our method\nconsistently improves keyphrase generation. A qualitative analysis of the\ngenerated keyphrases for the Computer Science domain confirms this improvement\ntowards their representativity property.\n","authors":["Mael Houbre","Florian Boudin","Beatrice Daille","Akiko Aizawa"],"pdf_url":"https://arxiv.org/pdf/2411.03039v1.pdf","comment":"Accepted to JCDL 2024 This version is not the final camera ready\n  version"},{"id":"http://arxiv.org/abs/2409.19839v2","updated":"2024-11-05T12:10:51Z","published":"2024-09-30T00:41:51Z","title":"ForecastBench: A Dynamic Benchmark of AI Forecasting Capabilities","summary":"  Forecasts of future events are essential inputs into informed\ndecision-making. Machine learning (ML) systems have the potential to deliver\nforecasts at scale, but there is no framework for evaluating the accuracy of ML\nsystems on a standardized set of forecasting questions. To address this gap, we\nintroduce ForecastBench: a dynamic benchmark that evaluates the accuracy of ML\nsystems on an automatically generated and regularly updated set of 1,000\nforecasting questions. To avoid any possibility of data leakage, ForecastBench\nis comprised solely of questions about future events that have no known answer\nat the time of submission. We quantify the capabilities of current ML systems\nby collecting forecasts from expert (human) forecasters, the general public,\nand LLMs on a random subset of questions from the benchmark ($N=200$). While\nLLMs have achieved super-human performance on many benchmarks, they perform\nless well here: expert forecasters outperform the top-performing LLM (p-value\n$=0.01$). We display system and human scores in a public leaderboard at\nwww.forecastbench.org.\n","authors":["Ezra Karger","Houtan Bastani","Chen Yueh-Han","Zachary Jacobs","Danny Halawi","Fred Zhang","Philip E. Tetlock"],"pdf_url":"https://arxiv.org/pdf/2409.19839v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18481v2","updated":"2024-11-05T11:40:07Z","published":"2024-10-24T07:10:18Z","title":"Dialog2Flow: Pre-training Soft-Contrastive Action-Driven Sentence\n  Embeddings for Automatic Dialog Flow Extraction","summary":"  Efficiently deriving structured workflows from unannotated dialogs remains an\nunderexplored and formidable challenge in computational linguistics. Automating\nthis process could significantly accelerate the manual design of workflows in\nnew domains and enable the grounding of large language models in\ndomain-specific flowcharts, enhancing transparency and controllability. In this\npaper, we introduce Dialog2Flow (D2F) embeddings, which differ from\nconventional sentence embeddings by mapping utterances to a latent space where\nthey are grouped according to their communicative and informative functions\n(i.e., the actions they represent). D2F allows for modeling dialogs as\ncontinuous trajectories in a latent space with distinct action-related regions.\nBy clustering D2F embeddings, the latent space is quantized, and dialogs can be\nconverted into sequences of region/action IDs, facilitating the extraction of\nthe underlying workflow. To pre-train D2F, we build a comprehensive dataset by\nunifying twenty task-oriented dialog datasets with normalized per-turn action\nannotations. We also introduce a novel soft contrastive loss that leverages the\nsemantic information of these actions to guide the representation learning\nprocess, showing superior performance compared to standard supervised\ncontrastive loss. Evaluation against various sentence embeddings, including\ndialog-specific ones, demonstrates that D2F yields superior qualitative and\nquantitative results across diverse domains.\n","authors":["Sergio Burdisso","Srikanth Madikeri","Petr Motlicek"],"pdf_url":"https://arxiv.org/pdf/2410.18481v2.pdf","comment":"Accepted to EMNLP 2024 main conference"},{"id":"http://arxiv.org/abs/2406.15570v2","updated":"2024-11-05T11:40:05Z","published":"2024-06-21T18:07:46Z","title":"DEM: Distribution Edited Model for Training with Mixed Data\n  Distributions","summary":"  Training with mixed data distributions is a common and important part of\ncreating multi-task and instruction-following models. The diversity of the data\ndistributions and cost of joint training makes the optimization procedure\nextremely challenging. Data mixing methods partially address this problem,\nalbeit having a sub-optimal performance across data sources and require\nmultiple expensive training runs. In this paper, we propose a simple and\nefficient alternative for better optimization of the data sources by combining\nmodels individually trained on each data source with the base model using basic\nelement-wise vector operations. The resulting model, namely Distribution Edited\nModel (DEM), is 11x cheaper than standard data mixing and outperforms strong\nbaselines on a variety of benchmarks, yielding upto 6.2% improvement on MMLU,\n11.5% on BBH, 16.1% on DROP, 6% on MathQA, and 9.3% on HELM with models of size\n3B to 13B. Notably, DEM does not require full re-training when modifying a\nsingle data-source, thus making it very flexible and scalable for training with\ndiverse data sources.\n","authors":["Dhananjay Ram","Aditya Rawal","Momchil Hardalov","Nikolaos Pappas","Sheng Zha"],"pdf_url":"https://arxiv.org/pdf/2406.15570v2.pdf","comment":"Accepted to EMNLP 2024 (Main Conference)"},{"id":"http://arxiv.org/abs/2410.08565v3","updated":"2024-11-05T11:29:59Z","published":"2024-10-11T06:44:31Z","title":"Ocean-omni: To Understand the World with Omni-modality","summary":"  The salient multimodal capabilities and interactive experience of GPT-4o\nhighlight its critical role in practical applications, yet it lacks a\nhigh-performing open-source counterpart. In this paper, we introduce\nOcean-omni, the first open-source 7B Multimodal Large Language Model (MLLM)\nadept at concurrently processing and analyzing modalities of image, video,\naudio, and text, while delivering an advanced multimodal interactive experience\nand strong performance. We propose an effective multimodal training schema\nstarting with 7B model and proceeding through two stages of multimodal\nalignment and multitask fine-tuning across audio, image, video, and text modal.\nThis approach equips the language model with the ability to handle visual and\naudio data effectively. Demonstrating strong performance across various\nomni-modal and multimodal benchmarks, we aim for this contribution to serve as\na competitive baseline for the open-source community in advancing multimodal\nunderstanding and real-time interaction.\n","authors":["Yadong Li","Haoze Sun","Mingan Lin","Tianpeng Li","Guosheng Dong","Tao Zhang","Bowen Ding","Wei Song","Zhenglin Cheng","Yuqi Huo","Song Chen","Xu Li","Da Pan","Shusen Zhang","Xin Wu","Zheng Liang","Jun Liu","Tao Zhang","Keer Lu","Yaqi Zhao","Yanjun Shen","Fan Yang","Kaicheng Yu","Tao Lin","Jianhua Xu","Zenan Zhou","Weipeng Chen"],"pdf_url":"https://arxiv.org/pdf/2410.08565v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03012v1","updated":"2024-11-05T11:25:12Z","published":"2024-11-05T11:25:12Z","title":"Leveraging Large Language Models in Code Question Answering: Baselines\n  and Issues","summary":"  Question answering over source code provides software engineers and project\nmanagers with helpful information about the implemented features of a software\nproduct. This paper presents a work devoted to using large language models for\nquestion answering over source code in Python. The proposed method for\nimplementing a source code question answering system involves fine-tuning a\nlarge language model on a unified dataset of questions and answers for Python\ncode. To achieve the highest quality answers, we tested various models trained\non datasets preprocessed in different ways: a dataset without grammar\ncorrection, a dataset with grammar correction, and a dataset augmented with the\ngenerated summaries. The model answers were also analyzed for errors manually.\nWe report BLEU-4, BERTScore F1, BLEURT, and Exact Match metric values, along\nwith the conclusions from the manual error analysis. The obtained experimental\nresults highlight the current problems of the research area, such as poor\nquality of the public genuine question-answering datasets. In addition, the\nfindings include the positive effect of the grammar correction of the training\ndata on the testing metric values. The addressed findings and issues could be\nimportant for other researchers who attempt to improve the quality of source\ncode question answering solutions. The training and evaluation code is publicly\navailable at https://github.com/IU-AES-AI4Code/CodeQuestionAnswering.\n","authors":["Georgy Andryushchenko","Vladimir Ivanov","Vladimir Makharev","Elizaveta Tukhtina","Aidar Valeev"],"pdf_url":"https://arxiv.org/pdf/2411.03012v1.pdf","comment":"15 pages, 3 figures, Accepted to NLP (CCIS) @ AIST'24"},{"id":"http://arxiv.org/abs/2408.07888v2","updated":"2024-11-05T11:07:19Z","published":"2024-08-15T02:22:48Z","title":"Evaluating Fine-Tuning Efficiency of Human-Inspired Learning Strategies\n  in Medical Question Answering","summary":"  Fine-tuning Large Language Models (LLMs) incurs considerable training costs,\ndriving the need for data-efficient training with optimised data ordering.\nHuman-inspired strategies offer a solution by organising data based on human\nlearning practices. This study evaluates the fine-tuning efficiency of five\nhuman-inspired strategies across four language models, three datasets, and both\nhuman- and LLM-labelled data in the context of medical question answering.\nThese strategies achieve the best accuracy gain of 1.81% and an average gain of\n1.02% across datasets, with interleaved strategies delivering the best average\nresults. However, the best strategy varies across model-dataset combinations,\nlimiting the generalisability of the effects of any single strategy.\nAdditionally, LLM-defined question difficulty outperforms human-defined labels\nin curriculum-based learning, showing the potential of model-generated data as\na cost-effective alternative for optimising fine-tuning.\n","authors":["Yushi Yang","Andrew M. Bean","Robert McCraith","Adam Mahdi"],"pdf_url":"https://arxiv.org/pdf/2408.07888v2.pdf","comment":"NeurIPS 2024 Workshop on Fine-Tuning in Modern Machine Learning:\n  Principles and Scalability (FITML)"},{"id":"http://arxiv.org/abs/2411.02989v1","updated":"2024-11-05T10:52:20Z","published":"2024-11-05T10:52:20Z","title":"Growing a Tail: Increasing Output Diversity in Large Language Models","summary":"  How diverse are the outputs of large language models when diversity is\ndesired? We examine the diversity of responses of various models to questions\nwith multiple possible answers, comparing them with human responses. Our\nfindings suggest that models' outputs are highly concentrated, reflecting a\nnarrow, mainstream 'worldview', in comparison to humans, whose responses\nexhibit a much longer-tail. We examine three ways to increase models' output\ndiversity: 1) increasing generation randomness via temperature sampling; 2)\nprompting models to answer from diverse perspectives; 3) aggregating outputs\nfrom several models. A combination of these measures significantly increases\nmodels' output diversity, reaching that of humans. We discuss implications of\nthese findings for AI policy that wishes to preserve cultural diversity, an\nessential building block of a democratic social fabric.\n","authors":["Michal Shur-Ofry","Bar Horowitz-Amsalem","Adir Rahamim","Yonatan Belinkov"],"pdf_url":"https://arxiv.org/pdf/2411.02989v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04691v4","updated":"2024-11-05T10:32:36Z","published":"2024-08-08T13:10:51Z","title":"Synthetic SQL Column Descriptions and Their Impact on Text-to-SQL\n  Performance","summary":"  Relational databases often suffer from uninformative descriptors of table\ncontents, such as ambiguous columns and hard-to-interpret values, impacting\nboth human users and text-to-SQL models. In this paper, we explore the use of\nlarge language models (LLMs) to automatically generate detailed natural\nlanguage descriptions for SQL database columns, aiming to improve text-to-SQL\nperformance and automate metadata creation. We create a dataset of gold column\ndescriptions based on the BIRD-Bench benchmark, manually refining its column\ndescriptions and creating a taxonomy for categorizing column difficulty. We\nthen evaluate several different LLMs in generating column descriptions across\nthe columns and different difficulties in the dataset, finding that models\nunsurprisingly struggle with columns that exhibit inherent ambiguity,\nhighlighting the need for manual expert input. We also find that incorporating\nsuch generated column descriptions consistently enhances text-to-SQL model\nperformance, particularly for larger models like GPT-4o, Qwen2 72B and Mixtral\n22Bx8. Notably, Qwen2-generated descriptions, containing by annotators deemed\nsuperfluous information, outperform manually curated gold descriptions,\nsuggesting that models benefit from more detailed metadata than humans expect.\nFuture work will investigate the specific features of these high-performing\ndescriptions and explore other types of metadata, such as numerical reasoning\nand synonyms, to further improve text-to-SQL systems. The dataset, annotations\nand code will all be made available.\n","authors":["Niklas Wretblad","Oskar Holmström","Erik Larsson","Axel Wiksäter","Oscar Söderlund","Hjalmar Öhman","Ture Pontén","Martin Forsberg","Martin Sörme","Fredrik Heintz"],"pdf_url":"https://arxiv.org/pdf/2408.04691v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12034v2","updated":"2024-11-05T10:24:42Z","published":"2024-06-30T22:18:49Z","title":"Understanding Transformers via N-gram Statistics","summary":"  Transformer based large-language models (LLMs) display extreme proficiency\nwith language yet a precise understanding of how they work remains elusive. One\nway of demystifying transformer predictions would be to describe how they\ndepend on their context in terms of simple template functions. This paper takes\na first step in this direction by considering families of functions (i.e.\nrules) formed out of simple N-gram based statistics of the training data. By\nstudying how well these rulesets approximate transformer predictions, we obtain\na variety of novel discoveries: a simple method to detect overfitting during\ntraining without using a holdout set, a quantitative measure of how\ntransformers progress from learning simple to more complex statistical rules\nover the course of training, a model-variance criterion governing when\ntransformer predictions tend to be described by N-gram rules, and insights into\nhow well transformers can be approximated by N-gram rulesets in the limit where\nthese rulesets become increasingly complex. In this latter direction, we find\nthat for 79% and 68% of LLM next-token distributions on TinyStories and\nWikipedia, respectively, their top-1 predictions agree with those provided by\nour N-gram rulesets.\n","authors":["Timothy Nguyen"],"pdf_url":"https://arxiv.org/pdf/2407.12034v2.pdf","comment":"NeurIPS 2024. Datasets and N-gram statistics open-sourced:\n  https://github.com/google-deepmind/transformer_ngrams"},{"id":"http://arxiv.org/abs/2411.02973v1","updated":"2024-11-05T10:18:53Z","published":"2024-11-05T10:18:53Z","title":"[Vision Paper] PRObot: Enhancing Patient-Reported Outcome Measures for\n  Diabetic Retinopathy using Chatbots and Generative AI","summary":"  We present an outline of the first large language model (LLM) based chatbot\napplication in the context of patient-reported outcome measures (PROMs) for\ndiabetic retinopathy. By utilizing the capabilities of current LLMs, we enable\npatients to provide feedback about their quality of life and treatment progress\nvia an interactive application. The proposed framework offers significant\nadvantages over the current approach, which encompasses only qualitative\ncollection of survey data or a static survey with limited answer options. Using\nthe PROBot LLM-PROM application, patients will be asked tailored questions\nabout their individual challenges, and can give more detailed feedback on the\nprogress of their treatment. Based on this input, we will use machine learning\nto infer conventional PROM scores, which can be used by clinicians to evaluate\nthe treatment status. The goal of the application is to improve adherence to\nthe healthcare system and treatments, and thus ultimately reduce cases of\nsubsequent vision impairment. The approach needs to be further validated using\na survey and a clinical study.\n","authors":["Maren Pielka","Tobias Schneider","Jan Terheyden","Rafet Sifa"],"pdf_url":"https://arxiv.org/pdf/2411.02973v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02948v1","updated":"2024-11-05T09:44:53Z","published":"2024-11-05T09:44:53Z","title":"Grounding Natural Language to SQL Translation with Data-Based\n  Self-Explanations","summary":"  Natural Language Interfaces for Databases empower non-technical users to\ninteract with data using natural language (NL). Advanced approaches, utilizing\neither neural sequence-to-sequence or more recent sophisticated large-scale\nlanguage models, typically implement NL to SQL (NL2SQL) translation in an\nend-to-end fashion. However, like humans, these end-to-end translation models\nmay not always generate the best SQL output on their first try. In this paper,\nwe propose CycleSQL, an iterative framework designed for end-to-end translation\nmodels to autonomously generate the best output through self-evaluation. The\nmain idea of CycleSQL is to introduce data-grounded NL explanations of query\nresults as self-provided feedback, and use the feedback to validate the\ncorrectness of the translation iteratively, hence improving the overall\ntranslation accuracy. Extensive experiments, including quantitative and\nqualitative evaluations, are conducted to study CycleSQL by applying it to\nseven existing translation models on five widely used benchmarks. The results\nshow that 1) the feedback loop introduced in CycleSQL can consistently improve\nthe performance of existing models, and in particular, by applying CycleSQL to\nRESDSQL, obtains a translation accuracy of 82.0% (+2.6%) on the validation set,\nand 81.6% (+3.2%) on the test set of Spider benchmark; 2) the generated NL\nexplanations can also provide insightful information for users, aiding in the\ncomprehension of translation results and consequently enhancing the\ninterpretability of NL2SQL translation.\n","authors":["Yuankai Fan","Tonghui Ren","Can Huang","Zhenying He","X. Sean Wang"],"pdf_url":"https://arxiv.org/pdf/2411.02948v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02943v1","updated":"2024-11-05T09:37:23Z","published":"2024-11-05T09:37:23Z","title":"Capturing research literature attitude towards Sustainable Development\n  Goals: an LLM-based topic modeling approach","summary":"  The world is facing a multitude of challenges that hinder the development of\nhuman civilization and the well-being of humanity on the planet. The\nSustainable Development Goals (SDGs) were formulated by the United Nations in\n2015 to address these global challenges by 2030. Natural language processing\ntechniques can help uncover discussions on SDGs within research literature. We\npropose a completely automated pipeline to 1) fetch content from the Scopus\ndatabase and prepare datasets dedicated to five groups of SDGs; 2) perform\ntopic modeling, a statistical technique used to identify topics in large\ncollections of textual data; and 3) enable topic exploration through\nkeywords-based search and topic frequency time series extraction. For topic\nmodeling, we leverage the stack of BERTopic scaled up to be applied on large\ncorpora of textual documents (we find hundreds of topics on hundreds of\nthousands of documents), introducing i) a novel LLM-based embeddings\ncomputation for representing scientific abstracts in the continuous space and\nii) a hyperparameter optimizer to efficiently find the best configuration for\nany new big datasets. We additionally produce the visualization of results on\ninteractive dashboards reporting topics' temporal evolution. Results are made\ninspectable and explorable, contributing to the interpretability of the topic\nmodeling process. Our proposed LLM-based topic modeling pipeline for big-text\ndatasets allows users to capture insights on the evolution of the attitude\ntoward SDGs within scientific abstracts in the 2006-2023 time span. All the\nresults are reproducible by using our system; the workflow can be generalized\nto be applied at any point in time to any big corpus of textual documents.\n","authors":["Francesco Invernici","Francesca Curati","Jelena Jakimov","Amirhossein Samavi","Anna Bernasconi"],"pdf_url":"https://arxiv.org/pdf/2411.02943v1.pdf","comment":"27 pages, 8 figures, 5 tables"},{"id":"http://arxiv.org/abs/2411.02939v1","updated":"2024-11-05T09:32:26Z","published":"2024-11-05T09:32:26Z","title":"A Post-Training Enhanced Optimization Approach for Small Language Models","summary":"  This paper delves into the continuous post-training optimization methods for\nsmall language models, and proposes a continuous post-training alignment data\nconstruction method for small language models. The core of this method is based\non the data guidance of large models, optimizing the diversity and accuracy of\nalignment data. In addition, to verify the effectiveness of the methods in this\npaper, we used Qwen2-0.5B-Instruct model as the baseline model for small\nlanguage models, using the alignment dataset constructed by our proposed\nmethod, we trained and compared several groups of experiments, including SFT\n(Supervised Fine Tuning) post-training experiment and KTO (Kahneman Tversky\noptimization) post-training experiment, as well as SFT-KTO two-stage\npost-training experiment and model weight fusion experiment. Finally, we\nevaluated and analyzed the performance of post-training models, and confirmed\nthat the continuous post-training optimization method proposed by us can\nsignificantly improve the performance of small language models.\n","authors":["Keke Zhai"],"pdf_url":"https://arxiv.org/pdf/2411.02939v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02937v1","updated":"2024-11-05T09:27:21Z","published":"2024-11-05T09:27:21Z","title":"Benchmarking Multimodal Retrieval Augmented Generation with Dynamic VQA\n  Dataset and Self-adaptive Planning Agent","summary":"  Multimodal Retrieval Augmented Generation (mRAG) plays an important role in\nmitigating the \"hallucination\" issue inherent in multimodal large language\nmodels (MLLMs). Although promising, existing heuristic mRAGs typically\npredefined fixed retrieval processes, which causes two issues: (1) Non-adaptive\nRetrieval Queries. (2) Overloaded Retrieval Queries. However, these flaws\ncannot be adequately reflected by current knowledge-seeking visual question\nanswering (VQA) datasets, since the most required knowledge can be readily\nobtained with a standard two-step retrieval. To bridge the dataset gap, we\nfirst construct Dyn-VQA dataset, consisting of three types of \"dynamic\"\nquestions, which require complex knowledge retrieval strategies variable in\nquery, tool, and time: (1) Questions with rapidly changing answers. (2)\nQuestions requiring multi-modal knowledge. (3) Multi-hop questions. Experiments\non Dyn-VQA reveal that existing heuristic mRAGs struggle to provide sufficient\nand precisely relevant knowledge for dynamic questions due to their rigid\nretrieval processes. Hence, we further propose the first self-adaptive planning\nagent for multimodal retrieval, OmniSearch. The underlying idea is to emulate\nthe human behavior in question solution which dynamically decomposes complex\nmultimodal questions into sub-question chains with retrieval action. Extensive\nexperiments prove the effectiveness of our OmniSearch, also provide direction\nfor advancing mRAG. The code and dataset will be open-sourced at\nhttps://github.com/Alibaba-NLP/OmniSearch.\n","authors":["Yangning Li","Yinghui Li","Xingyu Wang","Yong Jiang","Zhen Zhang","Xinran Zheng","Hui Wang","Hai-Tao Zheng","Philip S. Yu","Fei Huang","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2411.02937v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02930v1","updated":"2024-11-05T09:22:08Z","published":"2024-11-05T09:22:08Z","title":"Textual Aesthetics in Large Language Models","summary":"  Image aesthetics is a crucial metric in the field of image generation.\nHowever, textual aesthetics has not been sufficiently explored. With the\nwidespread application of large language models (LLMs), previous work has\nprimarily focused on the correctness of content and the helpfulness of\nresponses. Nonetheless, providing responses with textual aesthetics is also an\nimportant factor for LLMs, which can offer a cleaner layout and ensure greater\nconsistency and coherence in content. In this work, we introduce a pipeline for\naesthetics polishing and help construct a textual aesthetics dataset named\nTexAes. We propose a textual aesthetics-powered fine-tuning method based on\ndirect preference optimization, termed TAPO, which leverages textual aesthetics\nwithout compromising content correctness. Additionally, we develop two\nevaluation methods for textual aesthetics based on text and image analysis,\nrespectively. Our experiments demonstrate that using textual aesthetics data\nand employing the TAPO fine-tuning method not only improves aesthetic scores\nbut also enhances performance on general evaluation datasets such as\nAlpacalEval and Anera-hard.\n","authors":["Lingjie Jiang","Shaohan Huang","Xun Wu","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2411.02930v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13517v2","updated":"2024-11-05T09:08:28Z","published":"2024-10-17T13:06:02Z","title":"Bias in the Mirror: Are LLMs opinions robust to their own adversarial\n  attacks ?","summary":"  Large language models (LLMs) inherit biases from their training data and\nalignment processes, influencing their responses in subtle ways. While many\nstudies have examined these biases, little work has explored their robustness\nduring interactions. In this paper, we introduce a novel approach where two\ninstances of an LLM engage in self-debate, arguing opposing viewpoints to\npersuade a neutral version of the model. Through this, we evaluate how firmly\nbiases hold and whether models are susceptible to reinforcing misinformation or\nshifting to harmful viewpoints. Our experiments span multiple LLMs of varying\nsizes, origins, and languages, providing deeper insights into bias persistence\nand flexibility across linguistic and cultural contexts.\n","authors":["Virgile Rennard","Christos Xypolopoulos","Michalis Vazirgiannis"],"pdf_url":"https://arxiv.org/pdf/2410.13517v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.02839v3","updated":"2024-11-05T09:07:22Z","published":"2024-03-05T10:20:52Z","title":"An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned\n  Judge Model is not a General Substitute for GPT-4","summary":"  Recently, there has been a growing trend of utilizing Large Language Model\n(LLM) to evaluate the quality of other LLMs. Many studies have employed\nproprietary close-sourced models, especially GPT-4, as the evaluator.\nAlternatively, other works have fine-tuned judge models based on open-source\nLLMs as the evaluator. While the fine-tuned judge models are claimed to achieve\ncomparable evaluation capability with GPT-4, in this work, we conduct an\nempirical study of judge models. Our findings indicate that although the\nfine-tuned judge models achieve high performance on in-domain test sets, even\nsurpassing GPT-4, they underperform GPT-4 across several dimensions, including\ngeneralizability, fairness, aspect-specific evaluation, and scalability. We\nalso reveal that the fine-tuned judge model inherently operates as a\ntask-specific classifier, consequently imposing the limitations. Finally, we\nintroduce a integrated method, leveraging GPT-4 to compensate for the\nlimitations and improve the fine-tuned judges. Experiment results show our\nmethod achieves accuracy on par with GPT-4 with only 50% of the API expense.\n","authors":["Hui Huang","Yingqi Qu","Xingyuan Bu","Hongli Zhou","Jing Liu","Muyun Yang","Bing Xu","Tiejun Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.02839v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.11445v2","updated":"2024-11-05T08:46:01Z","published":"2024-09-17T03:39:45Z","title":"Jailbreaking Large Language Models with Symbolic Mathematics","summary":"  Recent advancements in AI safety have led to increased efforts in training\nand red-teaming large language models (LLMs) to mitigate unsafe content\ngeneration. However, these safety mechanisms may not be comprehensive, leaving\npotential vulnerabilities unexplored. This paper introduces MathPrompt, a novel\njailbreaking technique that exploits LLMs' advanced capabilities in symbolic\nmathematics to bypass their safety mechanisms. By encoding harmful natural\nlanguage prompts into mathematical problems, we demonstrate a critical\nvulnerability in current AI safety measures. Our experiments across 13\nstate-of-the-art LLMs reveal an average attack success rate of 73.6\\%,\nhighlighting the inability of existing safety training mechanisms to generalize\nto mathematically encoded inputs. Analysis of embedding vectors shows a\nsubstantial semantic shift between original and encoded prompts, helping\nexplain the attack's success. This work emphasizes the importance of a holistic\napproach to AI safety, calling for expanded red-teaming efforts to develop\nrobust safeguards across all potential input types and their associated risks.\n","authors":["Emet Bethany","Mazal Bethany","Juan Arturo Nolazco Flores","Sumit Kumar Jha","Peyman Najafirad"],"pdf_url":"https://arxiv.org/pdf/2409.11445v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02116v2","updated":"2024-11-05T08:35:14Z","published":"2024-11-04T14:29:28Z","title":"Advancements and limitations of LLMs in replicating human color-word\n  associations","summary":"  Color-word associations play a fundamental role in human cognition and design\napplications. Large Language Models (LLMs) have become widely available and\ndemonstrated intelligent behaviors in various benchmarks with natural\nconversation skills. However, their ability to replicate human color-word\nassociations remains understudied. We compared multiple generations of LLMs\n(from GPT-3 to GPT-4o) against human color-word associations using data\ncollected from over 10,000 Japanese participants, involving 17 colors and words\nfrom eight categories in Japanese. Our findings reveal a clear progression in\nLLM performance across generations, with GPT-4o achieving the highest accuracy\nin predicting the best voted word for each color and category. However, the\nhighest median performance was approximately 50% even for GPT-4o with visual\ninputs (chance level is 10%), and the performance levels varied significantly\nacross word categories and colors, indicating a failure to fully replicate\nhuman color-word associations. On the other hand, color discrimination ability\nestimated from our color-word association data showed that LLMs demonstrated\nhigh correlation with human color discrimination patterns, similarly to\nprevious studies. Our study highlights both the advancements in LLM\ncapabilities and their persistent limitations, suggesting differences in\nsemantic memory structures between humans and LLMs in representing color-word\nassociations.\n","authors":["Makoto Fukushima","Shusuke Eshita","Hiroshige Fukuhara"],"pdf_url":"https://arxiv.org/pdf/2411.02116v2.pdf","comment":"20 pages, 7 figures, 3 tables"},{"id":"http://arxiv.org/abs/2411.02902v1","updated":"2024-11-05T08:35:08Z","published":"2024-11-05T08:35:08Z","title":"Membership Inference Attacks against Large Vision-Language Models","summary":"  Large vision-language models (VLLMs) exhibit promising capabilities for\nprocessing multi-modal tasks across various application scenarios. However,\ntheir emergence also raises significant data security concerns, given the\npotential inclusion of sensitive information, such as private photos and\nmedical records, in their training datasets. Detecting inappropriately used\ndata in VLLMs remains a critical and unresolved issue, mainly due to the lack\nof standardized datasets and suitable methodologies. In this study, we\nintroduce the first membership inference attack (MIA) benchmark tailored for\nvarious VLLMs to facilitate training data detection. Then, we propose a novel\nMIA pipeline specifically designed for token-level image detection. Lastly, we\npresent a new metric called MaxR\\'enyi-K%, which is based on the confidence of\nthe model output and applies to both text and image data. We believe that our\nwork can deepen the understanding and methodology of MIAs in the context of\nVLLMs. Our code and datasets are available at\nhttps://github.com/LIONS-EPFL/VL-MIA.\n","authors":["Zhan Li","Yongtao Wu","Yihang Chen","Francesco Tonin","Elias Abad Rocamora","Volkan Cevher"],"pdf_url":"https://arxiv.org/pdf/2411.02902v1.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2405.14722v6","updated":"2024-11-05T08:22:00Z","published":"2024-05-23T15:51:24Z","title":"DAPE: Data-Adaptive Positional Encoding for Length Extrapolation","summary":"  Positional encoding plays a crucial role in transformers, significantly\nimpacting model performance and length generalization. Prior research has\nintroduced absolute positional encoding (APE) and relative positional encoding\n(RPE) to distinguish token positions in given sequences. However, both APE and\nRPE remain fixed after model training regardless of input data, limiting their\nadaptability and flexibility. Hence, we expect that the desired positional\nencoding should be data-adaptive and can be dynamically adjusted with the given\nattention. In this paper, we propose a Data-Adaptive Positional Encoding (DAPE)\nmethod, which dynamically and semantically adjusts based on input context and\nlearned fixed priors. Experimental validation on real-world datasets (Arxiv,\nBooks3, and CHE) demonstrates that DAPE enhances model performances in terms of\ntrained length and length generalization, where the improvements are\nstatistically significant. The model visualization suggests that our model can\nkeep both local and anti-local information. Finally, we successfully train the\nmodel on sequence length 128 and achieve better performance at evaluation\nsequence length 8192, compared with other static positional encoding methods,\nrevealing the benefit of the adaptive positional encoding method.\n","authors":["Chuanyang Zheng","Yihang Gao","Han Shi","Minbin Huang","Jingyao Li","Jing Xiong","Xiaozhe Ren","Michael Ng","Xin Jiang","Zhenguo Li","Yu Li"],"pdf_url":"https://arxiv.org/pdf/2405.14722v6.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.02887v1","updated":"2024-11-05T07:59:22Z","published":"2024-11-05T07:59:22Z","title":"The Translation of Circumlocution in Arabic Short Stories into English","summary":"  This study investigates the translation of circumlocution from Arabic to\nEnglish in a corpus of short stories by renowned Arabic authors. By analyzing\nthe source and target texts, the study aims to identify and categorize\ncircumlocution instances in Arabic and their corresponding renditions in\nEnglish. The study employs Nida's (1964) translation theory as a framework to\nassess the appropriateness of the translation strategies employed. It examines\nthe extent to which translators successfully rendered Arabic circumlocution\ninto English, identifying potential challenges and limitations in the\ntranslation process. The findings reveal significant similarities between\nArabic circumlocution categories and English metadiscourse categories,\nparticularly in terms of textual and interpersonal functions. However, the\nstudy also highlights instances where translators encountered difficulties in\naccurately conveying the nuances of circumlocution, often resorting to\nstrategies like addition, subtraction, and alteration.\n","authors":["Dalal Waadallah Shehab"],"pdf_url":"https://arxiv.org/pdf/2411.02887v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02886v1","updated":"2024-11-05T07:56:24Z","published":"2024-11-05T07:56:24Z","title":"TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection","summary":"  With the development of large language models (LLMs), the ability to handle\nlonger contexts has become a key capability for Web applications such as\ncross-document understanding and LLM-powered search systems. However, this\nprogress faces two major challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues hinder the\napplication of LLMs in long-context scenarios. In this paper, we propose\nDynamic Token-Level KV Cache Selection (TokenSelect), a model-agnostic,\ntraining-free method for efficient and accurate long-context inference.\nTokenSelect builds upon the observation of non-contiguous attention sparsity,\nusing Query-Key dot products to measure per-head KV Cache criticality at\ntoken-level. By per-head soft voting mechanism, TokenSelect selectively\ninvolves a small number of critical KV cache tokens in the attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesigned the Selection Cache based on observations of consecutive Query\nsimilarity and implemented efficient dot product kernel, significantly reducing\nthe overhead of token selection. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods.\n","authors":["Wei Wu","Zhuoshi Pan","Chao Wang","Liyi Chen","Yunchu Bai","Kun Fu","Zheng Wang","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2411.02886v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02864v1","updated":"2024-11-05T07:12:36Z","published":"2024-11-05T07:12:36Z","title":"Graph-DPEP: Decomposed Plug and Ensemble Play for Few-Shot Document\n  Relation Extraction with Graph-of-Thoughts Reasoning","summary":"  Large language models (LLMs) pre-trained on massive corpora have demonstrated\nimpressive few-shot learning capability on many NLP tasks. Recasting an NLP\ntask into a text-to-text generation task is a common practice so that\ngenerative LLMs can be prompted to resolve it. However, performing\ndocument-level relation extraction (DocRE) tasks with generative LLM models is\nstill challenging due to the structured output format of DocRE, which\ncomplicates the conversion to plain text. Limited information available in\nfew-shot samples and prompt instructions induce further difficulties and\nchallenges in relation extraction for mentioned entities in a document. In this\npaper, we represent the structured output as a graph-style triplet rather than\nnatural language expressions and leverage generative LLMs for the DocRE task.\nOur approach, the Graph-DPEP framework is grounded in the reasoning behind\ntriplet explanation thoughts presented in natural language. In this framework,\nwe first introduce a ``decomposed-plug\" method for performing the generation\nfrom LLMs over prompts with type-space decomposition to alleviate the burden of\ndistinguishing all relation types. Second, we employ a verifier for calibrating\nthe generation and identifying overlooked query entity pairs. Third, we develop\n\"ensemble-play\", reapplying generation on the entire type list by leveraging\nthe reasoning thoughts embedded in a sub-graph associated with the missing\nquery pair to address the missingness issue. Through extensive comparisons with\nexisting prompt techniques and alternative Language Models (LLMs), our\nframework demonstrates superior performance on publicly available benchmarks in\nexperiments.\n","authors":["Tao Zhang","Ning Yan","Masood Mortazavi","Hoang H. Nguyen","Zhongfen Deng","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2411.02864v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02851v1","updated":"2024-11-05T06:49:14Z","published":"2024-11-05T06:49:14Z","title":"Learning to Unify Audio, Visual and Text for Audio-Enhanced Multilingual\n  Visual Answer Localization","summary":"  The goal of Multilingual Visual Answer Localization (MVAL) is to locate a\nvideo segment that answers a given multilingual question. Existing methods\neither focus solely on visual modality or integrate visual and subtitle\nmodalities. However, these methods neglect the audio modality in videos,\nconsequently leading to incomplete input information and poor performance in\nthe MVAL task. In this paper, we propose a unified Audio-Visual-Textual Span\nLocalization (AVTSL) method that incorporates audio modality to augment both\nvisual and textual representations for the MVAL task. Specifically, we\nintegrate features from three modalities and develop three predictors, each\ntailored to the unique contributions of the fused modalities: an audio-visual\npredictor, a visual predictor, and a textual predictor. Each predictor\ngenerates predictions based on its respective modality. To maintain consistency\nacross the predicted results, we introduce an Audio-Visual-Textual Consistency\nmodule. This module utilizes a Dynamic Triangular Loss (DTL) function, allowing\neach modality's predictor to dynamically learn from the others. This\ncollaborative learning ensures that the model generates consistent and\ncomprehensive answers. Extensive experiments show that our proposed method\noutperforms several state-of-the-art (SOTA) methods, which demonstrates the\neffectiveness of the audio modality.\n","authors":["Zhibin Wen","Bin Li"],"pdf_url":"https://arxiv.org/pdf/2411.02851v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12629v4","updated":"2024-11-05T06:34:40Z","published":"2024-06-18T13:55:13Z","title":"SeTAR: Out-of-Distribution Detection with Selective Low-Rank\n  Approximation","summary":"  Out-of-distribution (OOD) detection is crucial for the safe deployment of\nneural networks. Existing CLIP-based approaches perform OOD detection by\ndevising novel scoring functions or sophisticated fine-tuning methods. In this\nwork, we propose SeTAR, a novel, training-free OOD detection method that\nleverages selective low-rank approximation of weight matrices in\nvision-language and vision-only models. SeTAR enhances OOD detection via\npost-hoc modification of the model's weight matrices using a simple greedy\nsearch algorithm. Based on SeTAR, we further propose SeTAR+FT, a fine-tuning\nextension optimizing model performance for OOD detection tasks. Extensive\nevaluations on ImageNet1K and Pascal-VOC benchmarks show SeTAR's superior\nperformance, reducing the relatively false positive rate by up to 18.95% and\n36.80% compared to zero-shot and fine-tuning baselines. Ablation studies\nfurther validate SeTAR's effectiveness, robustness, and generalizability across\ndifferent model backbones. Our work offers a scalable, efficient solution for\nOOD detection, setting a new state-of-the-art in this area.\n","authors":["Yixia Li","Boya Xiong","Guanhua Chen","Yun Chen"],"pdf_url":"https://arxiv.org/pdf/2406.12629v4.pdf","comment":"Accepted by NeurIPS 2024. Project page is live at\n  https://SeTAR-OOD.github.io. Code are available at\n  https://github.com/X1AOX1A/SeTAR"},{"id":"http://arxiv.org/abs/2402.05785v5","updated":"2024-11-05T06:32:38Z","published":"2024-02-08T16:23:29Z","title":"Limits of Transformer Language Models on Learning to Compose Algorithms","summary":"  We analyze the capabilities of Transformer language models in learning\ncompositional discrete tasks. To this end, we evaluate training LLaMA models\nand prompting GPT-4 and Gemini on four tasks demanding to learn a composition\nof several discrete sub-tasks. In particular, we measure how well these models\ncan reuse primitives observable in the sub-tasks to learn the composition task.\nOur results indicate that compositional learning in state-of-the-art\nTransformer language models is highly sample inefficient: LLaMA requires more\ndata samples than relearning all sub-tasks from scratch to learn the\ncompositional task; in-context prompting with few samples is unreliable and\nfails at executing the sub-tasks or correcting the errors in multi-round code\ngeneration. Further, by leveraging complexity theory, we support these findings\nwith a theoretical analysis focused on the sample inefficiency of gradient\ndescent in memorizing feedforward models. We open source our code at\nhttps://github.com/IBM/limitations-lm-algorithmic-compositional-learning.\n","authors":["Jonathan Thomm","Giacomo Camposampiero","Aleksandar Terzic","Michael Hersche","Bernhard Schölkopf","Abbas Rahimi"],"pdf_url":"https://arxiv.org/pdf/2402.05785v5.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.02832v1","updated":"2024-11-05T06:11:17Z","published":"2024-11-05T06:11:17Z","title":"PersianRAG: A Retrieval-Augmented Generation System for Persian Language","summary":"  Retrieval augmented generation (RAG) models, which integrate large-scale\npre-trained generative models with external retrieval mechanisms, have shown\nsignificant success in various natural language processing (NLP) tasks.\nHowever, applying RAG models in Persian language as a low-resource language,\nposes distinct challenges. These challenges primarily involve the\npreprocessing, embedding, retrieval, prompt construction, language modeling,\nand response evaluation of the system. In this paper, we address the challenges\ntowards implementing a real-world RAG system for Persian language called\nPersianRAG. We propose novel solutions to overcome these obstacles and evaluate\nour approach using several Persian benchmark datasets. Our experimental results\ndemonstrate the capability of the PersianRAG framework to enhance question\nanswering task in Persian.\n","authors":["Hossein Hosseini","Mohammad Siobhan Zare","Amir Hossein Mohammadi","Arefeh Kazemi","Zahra Zojaji","Mohammad Ali Nematbakhsh"],"pdf_url":"https://arxiv.org/pdf/2411.02832v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02830v1","updated":"2024-11-05T06:02:41Z","published":"2024-11-05T06:02:41Z","title":"Mixtures of In-Context Learners","summary":"  In-context learning (ICL) adapts LLMs by providing demonstrations without\nfine-tuning the model parameters; however, it does not differentiate between\ndemonstrations and quadratically increases the complexity of Transformer LLMs,\nexhausting the memory. As a solution, we propose Mixtures of In-Context\nLearners (MoICL), a novel approach to treat subsets of demonstrations as\nexperts and learn a weighting function to merge their output distributions\nbased on a training set. In our experiments, we show performance improvements\non 5 out of 7 classification datasets compared to a set of strong baselines (up\nto +13\\% compared to ICL and LENS). Moreover, we enhance the Pareto frontier of\nICL by reducing the inference time needed to achieve the same performance with\nfewer demonstrations. Finally, MoICL is more robust to out-of-domain (up to\n+11\\%), imbalanced (up to +49\\%), or noisy demonstrations (up to +38\\%) or can\nfilter these out from datasets. Overall, MoICL is a more expressive approach to\nlearning from demonstrations without exhausting the context window or memory.\n","authors":["Giwon Hong","Emile van Krieken","Edoardo Ponti","Nikolay Malkin","Pasquale Minervini"],"pdf_url":"https://arxiv.org/pdf/2411.02830v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.14419v2","updated":"2024-11-05T05:43:30Z","published":"2024-04-14T07:06:12Z","title":"Evaluation and Improvement of Fault Detection for Large Language Models","summary":"  Large language models (LLMs) have recently achieved significant success\nacross various application domains, garnering substantial attention from\ndifferent communities. Unfortunately, even for the best LLM, many\n\\textit{faults} still exist that LLM cannot properly predict. Such faults will\nharm the usability of LLMs in general and could introduce safety issues in\nreliability-critical systems such as autonomous driving systems. How to quickly\nreveal these faults in real-world datasets that LLM could face is important,\nbut challenging. The major reason is that the ground truth is necessary but the\ndata labeling process is heavy considering the time and human effort. To handle\nthis problem, in the conventional deep learning testing field, test selection\nmethods have been proposed for efficiently evaluating deep learning models by\nprioritizing faults. However, despite their importance, the usefulness of these\nmethods on LLMs is unclear, and lack of exploration. In this paper, we conduct\nthe first empirical study to investigate the effectiveness of existing fault\ndetection methods for LLMs. Experimental results on four different\ntasks~(including both code tasks and natural language processing tasks) and\nfour LLMs~(e.g., LLaMA3 and GPT4) demonstrated that simple methods such as\nMargin perform well on LLMs but there is still a big room for improvement.\nBased on the study, we further propose \\textbf{MuCS}, a prompt\n\\textbf{Mu}tation-based prediction \\textbf{C}onfidence \\textbf{S}moothing\nframework to boost the fault detection capability of existing methods.\nConcretely, multiple prompt mutation techniques have been proposed to help\ncollect more diverse outputs for confidence smoothing. The results show that\nour proposed framework significantly enhances existing methods with the\nimprovement of test relative coverage by up to 70.53\\%.\n","authors":["Qiang Hu","Jin Wen","Maxime Cordy","Yuheng Huang","Wei Ma","Xiaofei Xie","Lei Ma"],"pdf_url":"https://arxiv.org/pdf/2404.14419v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02820v1","updated":"2024-11-05T05:41:41Z","published":"2024-11-05T05:41:41Z","title":"DroidSpeak: Enhancing Cross-LLM Communication","summary":"  In multi-agent systems utilizing Large Language Models (LLMs), communication\nbetween agents traditionally relies on natural language. This communication\noften includes the full context of the query so far, which can introduce\nsignificant prefill-phase latency, especially with long contexts.\n  We introduce DroidSpeak, a novel framework to target this cross-LLM\ncommunication by leveraging the reuse of intermediate data, such as input\nembeddings (E-cache) and key-value caches (KV-cache). We efficiently bypass the\nneed to reprocess entire contexts for fine-tuned versions of the same\nfoundational model. This approach allows faster context integration while\nmaintaining the quality of task performance. Experimental evaluations\ndemonstrate DroidSpeak's ability to significantly accelerate inter-agent\ncommunication, achieving up to a 2.78x speedup in prefill latency with\nnegligible loss in accuracy. Our findings underscore the potential to create\nmore efficient and scalable multi-agent systems.\n","authors":["Yuhan Liu","Esha Choukse","Shan Lu","Junchen Jiang","Madan Musuvathi"],"pdf_url":"https://arxiv.org/pdf/2411.02820v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.10884v2","updated":"2024-11-05T05:13:13Z","published":"2024-02-16T18:42:08Z","title":"Multi-modal Preference Alignment Remedies Degradation of Visual\n  Instruction Tuning on Language Models","summary":"  Multi-modal large language models (MLLMs) are expected to support multi-turn\nqueries of interchanging image and text modalities in production. However, the\ncurrent MLLMs trained with visual-question-answering (VQA) datasets could\nsuffer from degradation, as VQA datasets lack the diversity and complexity of\nthe original text instruction datasets with which the underlying language model\nwas trained. To address this degradation, we first collect a lightweight,\n5k-sample VQA preference dataset where answers were annotated by Gemini for\nfive quality metrics in a granular fashion and investigate standard Supervised\nFine-tuning, rejection sampling, Direct Preference Optimization (DPO) and\nSteerLM algorithms. Our findings indicate that with DPO, we can surpass the\ninstruction-following capabilities of the language model, achieving a 6.73\nscore on MT-Bench, compared to Vicuna's 6.57 and LLaVA's 5.99. This enhancement\nin textual instruction-following capability correlates with boosted visual\ninstruction performance (+4.9\\% on MM-Vet, +6\\% on LLaVA-Bench), with minimal\nalignment tax on visual knowledge benchmarks compared to the previous RLHF\napproach. In conclusion, we propose a distillation-based multi-modal alignment\nmodel with fine-grained annotations on a small dataset that restores and boosts\nMLLM's language capability after visual instruction tuning.\n","authors":["Shengzhi Li","Rongyu Lin","Shichao Pei"],"pdf_url":"https://arxiv.org/pdf/2402.10884v2.pdf","comment":"Project code, model and data: https://github.com/findalexli/mllm-dpo"},{"id":"http://arxiv.org/abs/2410.11772v2","updated":"2024-11-05T05:13:00Z","published":"2024-10-15T16:53:26Z","title":"Layer-wise Importance Matters: Less Memory for Better Performance in\n  Parameter-efficient Fine-tuning of Large Language Models","summary":"  Parameter-Efficient Fine-Tuning (PEFT) methods have gained significant\npopularity for adapting pre-trained Large Language Models (LLMs) to downstream\ntasks, primarily due to their potential to significantly reduce memory and\ncomputational overheads. However, a common limitation in most PEFT approaches\nis their application of a uniform architectural design across all layers. This\nuniformity involves identical trainable modules and ignores the varying\nimportance of each layer, leading to sub-optimal fine-tuning results. To\novercome the above limitation and obtain better performance, we develop a novel\napproach, Importance-aware Sparse Tuning (IST), to fully utilize the inherent\nsparsity and select the most important subset of full layers with effective\nlayer-wise importance scoring. The proposed IST is a versatile and\nplug-and-play technique compatible with various PEFT methods that operate on a\nper-layer basis. By leveraging the estimated importance scores, IST dynamically\nupdates these selected layers in PEFT modules, leading to reduced memory\ndemands. We further provide theoretical proof of convergence and empirical\nevidence of superior performance to demonstrate the advantages of IST over\nuniform updating strategies. Extensive experiments on a range of LLMs, PEFTs,\nand downstream tasks substantiate the effectiveness of our proposed method,\nshowcasing IST's capacity to enhance existing layer-based PEFT methods. Our\ncode is available at https://github.com/Kaiseem/IST.\n","authors":["Kai Yao","Penglei Gao","Lichun Li","Yuan Zhao","Xiaofeng Wang","Wei Wang","Jianke Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.11772v2.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2405.17977v2","updated":"2024-11-05T04:37:22Z","published":"2024-05-28T09:06:18Z","title":"Aligning to Thousands of Preferences via System Message Generalization","summary":"  Although humans inherently have diverse values, current large language model\n(LLM) alignment methods often assume that aligning LLMs with the general\npublic's preferences is optimal. A major challenge in adopting a more\nindividualized approach to LLM alignment is its lack of scalability, as it\ninvolves repeatedly acquiring preference data and training new reward models\nand LLMs for each individual's preferences. To address these challenges, we\npropose a new paradigm where users specify what they value most within the\nsystem message, steering the LLM's generation behavior to better align with the\nuser's intentions. However, a naive application of such an approach is\nnon-trivial since LLMs are typically trained on a uniform system message (e.g.,\n\"You are a helpful assistant\") which limits their ability to generalize to\ndiverse, unseen system messages. To improve this generalization, we create the\nMultifaceted Collection, a preference dataset with 192k combinations of values\nbeyond generic helpfulness and harmlessness, spanning 65k user instructions.\nUsing this dataset, we train a 7B LLM called Janus and test it on 921 prompts\nfrom 5 benchmarks (AlpacaEval 2.0, FLASK, Koala, MT-Bench, and Self-Instruct)\nby adding various unseen system messages that reflect user preferences. Janus\nachieves tie+win rate of 75.2%, 72.4%, and 66.4% against Mistral 7B Instruct\nv0.2, GPT-3.5 Turbo, and GPT-4, respectively. Unexpectedly, on three benchmarks\nfocused on response helpfulness (AlpacaEval 2.0, MT-Bench, Arena Hard Auto\nv0.1), Janus also outperforms LLaMA 3 8B Instruct by a +4.0%, +0.1%, +3.0%\nmargin, underscoring that training with a vast array of system messages could\nalso enhance alignment to the general public's preference as well. Our code,\ndataset, benchmark, and models are available at\nhttps://github.com/kaistAI/Janus.\n","authors":["Seongyun Lee","Sue Hyun Park","Seungone Kim","Minjoon Seo"],"pdf_url":"https://arxiv.org/pdf/2405.17977v2.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.01030v2","updated":"2024-11-05T04:35:33Z","published":"2024-11-01T21:01:13Z","title":"Birdie: Advancing State Space Models with Reward-Driven Objectives and\n  Curricula","summary":"  Efficient state space models (SSMs), such as linear recurrent neural networks\nand linear attention variants, offer computational advantages over Transformers\nbut struggle with tasks requiring long-range in-context retrieval-like text\ncopying, associative recall, and question answering over long contexts.\nPrevious efforts to address these challenges have focused on architectural\nmodifications, often reintroducing computational inefficiencies. In this paper,\nwe propose a novel training procedure, Birdie, that significantly enhances the\nin-context retrieval capabilities of SSMs without altering their architecture.\nOur approach combines bidirectional input processing with dynamic mixtures of\nspecialized pre-training objectives, optimized via reinforcement learning. We\nintroduce a new bidirectional SSM architecture that seamlessly transitions from\nbidirectional context processing to causal generation. Experimental evaluations\ndemonstrate that Birdie markedly improves performance on retrieval-intensive\ntasks such as multi-number phone book lookup, long paragraph\nquestion-answering, and infilling. This narrows the performance gap with\nTransformers, while retaining computational efficiency. Our findings highlight\nthe importance of training procedures in leveraging the fixed-state capacity of\nSSMs, offering a new direction to advance their capabilities. All code and\npre-trained models are available at https://www.github.com/samblouir/birdie,\nwith support for JAX and PyTorch.\n","authors":["Sam Blouir","Jimmy T. H. Smith","Antonios Anastasopoulos","Amarda Shehu"],"pdf_url":"https://arxiv.org/pdf/2411.01030v2.pdf","comment":"Accepted to EMNLP 2024 (Main Conference)"},{"id":"http://arxiv.org/abs/2411.02265v2","updated":"2024-11-05T04:14:25Z","published":"2024-11-04T16:56:26Z","title":"Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated\n  Parameters by Tencent","summary":"  In this paper, we introduce Hunyuan-Large, which is currently the largest\nopen-source Transformer-based mixture of experts model, with a total of 389\nbillion parameters and 52 billion activation parameters, capable of handling up\nto 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior\nperformance across various benchmarks including language understanding and\ngeneration, logical reasoning, mathematical problem-solving, coding,\nlong-context, and aggregated tasks, where it outperforms LLama3.1-70B and\nexhibits comparable performance when compared to the significantly larger\nLLama3.1-405B model. Key practice of Hunyuan-Large include large-scale\nsynthetic data that is orders larger than in previous literature, a mixed\nexpert routing strategy, a key-value cache compression technique, and an\nexpert-specific learning rate strategy. Additionally, we also investigate the\nscaling laws and learning rate schedule of mixture of experts models, providing\nvaluable insights and guidances for future model development and optimization.\nThe code and checkpoints of Hunyuan-Large are released to facilitate future\ninnovations and applications.\n  Codes: https://github.com/Tencent/Hunyuan-Large\n  Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large\n","authors":["Xingwu Sun","Yanfeng Chen","Yiqing Huang","Ruobing Xie","Jiaqi Zhu","Kai Zhang","Shuaipeng Li","Zhen Yang","Jonny Han","Xiaobo Shu","Jiahao Bu","Zhongzhi Chen","Xuemeng Huang","Fengzong Lian","Saiyong Yang","Jianfeng Yan","Yuyuan Zeng","Xiaoqin Ren","Chao Yu","Lulu Wu","Yue Mao","Jun Xia","Tao Yang","Suncong Zheng","Kan Wu","Dian Jiao","Jinbao Xue","Xipeng Zhang","Decheng Wu","Kai Liu","Dengpeng Wu","Guanghui Xu","Shaohua Chen","Shuang Chen","Xiao Feng","Yigeng Hong","Junqiang Zheng","Chengcheng Xu","Zongwei Li","Xiong Kuang","Jianglu Hu","Yiqi Chen","Yuchi Deng","Guiyang Li","Ao Liu","Chenchen Zhang","Shihui Hu","Zilong Zhao","Zifan Wu","Yao Ding","Weichao Wang","Han Liu","Roberts Wang","Hao Fei","Peijie She","Ze Zhao","Xun Cao","Hai Wang","Fusheng Xiang","Mengyuan Huang","Zhiyuan Xiong","Bin Hu","Xuebin Hou","Lei Jiang","Jiajia Wu","Yaping Deng","Yi Shen","Qian Wang","Weijie Liu","Jie Liu","Meng Chen","Liang Dong","Weiwen Jia","Hu Chen","Feifei Liu","Rui Yuan","Huilin Xu","Zhenxiang Yan","Tengfei Cao","Zhichao Hu","Xinhua Feng","Dong Du","Tinghao She","Yangyu Tao","Feng Zhang","Jianchen Zhu","Chengzhong Xu","Xirui Li","Chong Zha","Wen Ouyang","Yinben Xia","Xiang Li","Zekun He","Rongpeng Chen","Jiawei Song","Ruibin Chen","Fan Jiang","Chongqing Zhao","Bo Wang","Hao Gong","Rong Gan","Winston Hu","Zhanhui Kang","Yong Yang","Yuhong Liu","Di Wang","Jie Jiang"],"pdf_url":"https://arxiv.org/pdf/2411.02265v2.pdf","comment":"17 pages, 4 Figures"},{"id":"http://arxiv.org/abs/2408.15543v2","updated":"2024-11-05T04:13:55Z","published":"2024-08-28T05:36:25Z","title":"An Investigation of Warning Erroneous Chat Translations in Cross-lingual\n  Communication","summary":"  Machine translation models are still inappropriate for translating chats,\ndespite the popularity of translation software and plug-in applications. The\ncomplexity of dialogues poses significant challenges and can hinder\ncrosslingual communication. Instead of pursuing a flawless translation system,\na more practical approach would be to issue warning messages about potential\nmistranslations to reduce confusion. However, it is still unclear how\nindividuals perceive these warning messages and whether they benefit the crowd.\nThis paper tackles to investigate this question and demonstrates the warning\nmessages' contribution to making chat translation systems effective.\n","authors":["Yunmeng Li","Jun Suzuki","Makoto Morishita","Kaori Abe","Kentaro Inui"],"pdf_url":"https://arxiv.org/pdf/2408.15543v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02795v1","updated":"2024-11-05T04:10:05Z","published":"2024-11-05T04:10:05Z","title":"The Evolution of RWKV: Advancements in Efficient Language Modeling","summary":"  This paper reviews the development of the Receptance Weighted Key Value\n(RWKV) architecture, emphasizing its advancements in efficient language\nmodeling. RWKV combines the training efficiency of Transformers with the\ninference efficiency of RNNs through a novel linear attention mechanism. We\nexamine its core innovations, adaptations across various domains, and\nperformance advantages over traditional models. The paper also discusses\nchallenges and future directions for RWKV as a versatile architecture in deep\nlearning.\n","authors":["Akul Datta"],"pdf_url":"https://arxiv.org/pdf/2411.02795v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02793v1","updated":"2024-11-05T04:04:41Z","published":"2024-11-05T04:04:41Z","title":"Toward Robust Incomplete Multimodal Sentiment Analysis via Hierarchical\n  Representation Learning","summary":"  Multimodal Sentiment Analysis (MSA) is an important research area that aims\nto understand and recognize human sentiment through multiple modalities. The\ncomplementary information provided by multimodal fusion promotes better\nsentiment analysis compared to utilizing only a single modality. Nevertheless,\nin real-world applications, many unavoidable factors may lead to situations of\nuncertain modality missing, thus hindering the effectiveness of multimodal\nmodeling and degrading the model's performance. To this end, we propose a\nHierarchical Representation Learning Framework (HRLF) for the MSA task under\nuncertain missing modalities. Specifically, we propose a fine-grained\nrepresentation factorization module that sufficiently extracts valuable\nsentiment information by factorizing modality into sentiment-relevant and\nmodality-specific representations through crossmodal translation and sentiment\nsemantic reconstruction. Moreover, a hierarchical mutual information\nmaximization mechanism is introduced to incrementally maximize the mutual\ninformation between multi-scale representations to align and reconstruct the\nhigh-level semantics in the representations. Ultimately, we propose a\nhierarchical adversarial learning mechanism that further aligns and adapts the\nlatent distribution of sentiment-relevant representations to produce robust\njoint multimodal representations. Comprehensive experiments on three datasets\ndemonstrate that HRLF significantly improves MSA performance under uncertain\nmodality missing cases.\n","authors":["Mingcheng Li","Dingkang Yang","Yang Liu","Shunli Wang","Jiawei Chen","Shuaibing Wang","Jinjie Wei","Yue Jiang","Qingyao Xu","Xiaolu Hou","Mingyang Sun","Ziyun Qian","Dongliang Kou","Lihua Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.02793v1.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.02791v1","updated":"2024-11-05T04:01:41Z","published":"2024-11-05T04:01:41Z","title":"Language Models and Cycle Consistency for Self-Reflective Machine\n  Translation","summary":"  This paper introduces a novel framework that leverages large language models\n(LLMs) for machine translation (MT). We start with one conjecture: an ideal\ntranslation should contain complete and accurate information for a strong\nenough LLM to recover the original sentence. We generate multiple translation\ncandidates from a source language A to a target language B, and subsequently\ntranslate these candidates back to the original language A. By evaluating the\ncycle consistency between the original and back-translated sentences using\nmetrics such as token-level precision and accuracy, we implicitly estimate the\ntranslation quality in language B, without knowing its ground-truth. This also\nhelps to evaluate the LLM translation capability, only with monolingual\ncorpora. For each source sentence, we identify the translation candidate with\noptimal cycle consistency with the original sentence as the final answer. Our\nexperiments demonstrate that larger LLMs, or the same LLM with more forward\npasses during inference, exhibit increased cycle consistency, aligning with the\nLLM model size scaling law and test-time computation scaling law. This work\nprovide methods for, 1) to implicitly evaluate translation quality of a\nsentence in the target language, 2), to evaluate capability of LLM for\nany-to-any-language translation, and 3), how to generate a better translation\nfor a specific LLM.\n","authors":["Jianqiao Wangni"],"pdf_url":"https://arxiv.org/pdf/2411.02791v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02790v1","updated":"2024-11-05T03:55:25Z","published":"2024-11-05T03:55:25Z","title":"Memory Augmented Cross-encoders for Controllable Personalized Search","summary":"  Personalized search represents a problem where retrieval models condition on\nhistorical user interaction data in order to improve retrieval results.\nHowever, personalization is commonly perceived as opaque and not amenable to\ncontrol by users. Further, personalization necessarily limits the space of\nitems that users are exposed to. Therefore, prior work notes a tension between\npersonalization and users' ability for discovering novel items. While discovery\nof novel items in personalization setups may be resolved through search result\ndiversification, these approaches do little to allow user control over\npersonalization. Therefore, in this paper, we introduce an approach for\ncontrollable personalized search. Our model, CtrlCE presents a novel\ncross-encoder model augmented with an editable memory constructed from users\nhistorical items. Our proposed memory augmentation allows cross-encoder models\nto condition on large amounts of historical user data and supports interaction\nfrom users permitting control over personalization. Further, controllable\npersonalization for search must account for queries which don't require\npersonalization, and in turn user control. For this, we introduce a calibrated\nmixing model which determines when personalization is necessary. This allows\nsystem designers using CtrlCE to only obtain user input for control when\nnecessary. In multiple datasets of personalized search, we show CtrlCE to\nresult in effective personalization as well as fulfill various key goals for\ncontrollable personalized search.\n","authors":["Sheshera Mysore","Garima Dhanania","Kishor Patil","Surya Kallumadi","Andrew McCallum","Hamed Zamani"],"pdf_url":"https://arxiv.org/pdf/2411.02790v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2311.09180v2","updated":"2024-11-05T03:34:10Z","published":"2023-11-15T18:19:58Z","title":"Pearl: Personalizing Large Language Model Writing Assistants with\n  Generation-Calibrated Retrievers","summary":"  Powerful large language models have facilitated the development of writing\nassistants that promise to significantly improve the quality and efficiency of\ncomposition and communication. However, a barrier to effective assistance is\nthe lack of personalization in LLM outputs to the author's communication style,\nspecialized knowledge, and values. In this paper, we address this challenge by\nproposing Pearl, a LLM writing assistant personalized with a retriever that is\ntrained to be generation-calibrated for personalization. Generation calibration\nensures that our retriever selects historic user authored documents to augment\nan LLM prompt such that they are likely to help an LLM generation better adhere\nto a users' preferences. We propose two key novelties for training such a\nretriever: (1) A training data selection method that identifies user requests\nlikely to benefit from personalization and documents that provide that benefit;\nand (2) A scale-calibrating KL-divergence objective that ensures that our\nretriever scores remain proportional to the downstream generation quality from\nusing the document for personalized generation. In a series of holistic\nevaluations, we demonstrate the effectiveness of Pearl in generating long-form\ntexts on multiple social media datasets. Finally, we demonstrate how a\ngeneration-calibrated retriever can double as a performance predictor --\ndetecting low quality retrieval, and improving potentially under-performing\noutputs via revision with LLMs.\n","authors":["Sheshera Mysore","Zhuoran Lu","Mengting Wan","Longqi Yang","Bahareh Sarrafzadeh","Steve Menezes","Tina Baghaee","Emmanuel Barajas Gonzalez","Jennifer Neville","Tara Safavi"],"pdf_url":"https://arxiv.org/pdf/2311.09180v2.pdf","comment":"Accepted to Workshop on Customizable NLP at EMNLP 2024"},{"id":"http://arxiv.org/abs/2405.02132v3","updated":"2024-11-05T03:29:41Z","published":"2024-05-03T14:35:58Z","title":"Unveiling the Potential of LLM-Based ASR on Chinese Open-Source Datasets","summary":"  Large Language Models (LLMs) have demonstrated unparalleled effectiveness in\nvarious NLP tasks, and integrating LLMs with automatic speech recognition (ASR)\nis becoming a mainstream paradigm. Building upon this momentum, our research\ndelves into an in-depth examination of this paradigm on a large open-source\nChinese dataset. Specifically, our research aims to evaluate the impact of\nvarious configurations of speech encoders, LLMs, and projector modules in the\ncontext of the speech foundation encoder-LLM ASR paradigm. Furthermore, we\nintroduce a three-stage training approach, expressly developed to enhance the\nmodel's ability to align auditory and textual information. The implementation\nof this approach, alongside the strategic integration of ASR components,\nenabled us to achieve the SOTA performance on the AISHELL-1, Test_Net, and\nTest_Meeting test sets. Our analysis presents an empirical foundation for\nfuture research in LLM-based ASR systems and offers insights into optimizing\nperformance using Chinese datasets. We will publicly release all scripts used\nfor data preparation, training, inference, and scoring, as well as pre-trained\nmodels and training logs to promote reproducible research.\n","authors":["Xuelong Geng","Tianyi Xu","Kun Wei","Bingshen Mu","Hongfei Xue","He Wang","Yangze Li","Pengcheng Guo","Yuhang Dai","Longhao Li","Mingchen Shao","Lei Xie"],"pdf_url":"https://arxiv.org/pdf/2405.02132v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01539v2","updated":"2024-11-05T03:20:10Z","published":"2024-11-03T12:03:12Z","title":"LLMs and the Madness of Crowds","summary":"  We investigate the patterns of incorrect answers produced by large language\nmodels (LLMs) during evaluation. These errors exhibit highly non-intuitive\nbehaviors unique to each model. By analyzing these patterns, we measure the\nsimilarities between LLMs and construct a taxonomy that categorizes them based\non their error correlations. Our findings reveal that the incorrect responses\nare not randomly distributed but systematically correlated across models,\nproviding new insights into the underlying structures and relationships among\nLLMs.\n","authors":["William F. Bradley"],"pdf_url":"https://arxiv.org/pdf/2411.01539v2.pdf","comment":"11 pages, 6 figures"},{"id":"http://arxiv.org/abs/2411.01533v2","updated":"2024-11-05T03:17:34Z","published":"2024-11-03T11:39:50Z","title":"Enhancing LLM Evaluations: The Garbling Trick","summary":"  As large language models (LLMs) become increasingly powerful, traditional\nevaluation metrics tend to saturate, making it challenging to distinguish\nbetween models based on their performance. We propose a general method to\ntransform existing LLM evaluations into a series of progressively more\ndifficult tasks. These enhanced evaluations emphasize reasoning capabilities\nand can reveal relative performance differences that are not apparent in the\noriginal assessments.\n  To demonstrate the effectiveness of our approach, we create a new\nmultiple-choice test corpus, extend it into a family of evaluations, and assess\na collection of LLMs. Our results offer insights into the comparative reasoning\nabilities of these models, particularly highlighting distinctions between\nOpenAI's o1-preview and Google's gemini-pro-1.5-002.\n","authors":["William F. Bradley"],"pdf_url":"https://arxiv.org/pdf/2411.01533v2.pdf","comment":"13 pages, 3 figures"},{"id":"http://arxiv.org/abs/2407.02855v2","updated":"2024-11-05T02:30:56Z","published":"2024-07-03T07:14:05Z","title":"Safe Unlearning: A Surprisingly Effective and Generalizable Solution to\n  Defend Against Jailbreak Attacks","summary":"  LLMs are known to be vulnerable to jailbreak attacks, even after safety\nalignment. An important observation is that, while different types of jailbreak\nattacks can generate significantly different queries, they mostly result in\nsimilar responses that are rooted in the same harmful knowledge (e.g., detailed\nsteps to make a bomb). Therefore, we conjecture that directly unlearn the\nharmful knowledge in the LLM can be a more effective way to defend against\njailbreak attacks than the mainstream supervised fine-tuning (SFT) approaches.\nOur extensive experiments demonstrate the surprising generalizability of our\nunlearning-based approach: using only 20 raw harmful questions without any\njailbreak prompt during training, our solution reduced the Attack Success Rate\n(ASR) in Vicuna-7B from 82.6% to 7.7% on out-of-distribution (OOD) harmful\nquestions wrapped with various complex jailbreak prompts . This significantly\noutperforms Llama2-7B-Chat, which is fine-tuned on about 0.1M safety alignment\nsamples but still has an ASR of 21.9% even under the help of an additional\nsafety system prompt. Further analysis reveals that the generalization ability\nof our solution may stem from the intrinsic relatedness among harmful responses\nacross harmful questions (e.g., response patterns, shared steps and actions in\nresponse, and similarity among their learned representations in the LLM). Our\ncode is available at \\url{https://github.com/thu-coai/SafeUnlearning}.\n","authors":["Zhexin Zhang","Junxiao Yang","Pei Ke","Shiyao Cui","Chujie Zheng","Hongning Wang","Minlie Huang"],"pdf_url":"https://arxiv.org/pdf/2407.02855v2.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2410.12816v2","updated":"2024-11-05T02:27:30Z","published":"2024-10-01T09:33:45Z","title":"Rethinking Misalignment in Vision-Language Model Adaptation from a\n  Causal Perspective","summary":"  Foundational Vision-Language models such as CLIP have exhibited impressive\ngeneralization in downstream tasks. However, CLIP suffers from a two-level\nmisalignment issue, i.e., task misalignment and data misalignment, when\nadapting to specific tasks. Soft prompt tuning has mitigated the task\nmisalignment, yet the data misalignment remains a challenge. To analyze the\nimpacts of the data misalignment, we revisit the pre-training and adaptation\nprocesses of CLIP and develop a structural causal model. We discover that while\nwe expect to capture task-relevant information for downstream tasks accurately,\nthe task-irrelevant knowledge impacts the prediction results and hampers the\nmodeling of the true relationships between the images and the predicted\nclasses. As task-irrelevant knowledge is unobservable, we leverage the\nfront-door adjustment and propose Causality-Guided Semantic Decoupling and\nClassification (CDC) to mitigate the interference of task-irrelevant knowledge.\nSpecifically, we decouple semantics contained in the data of downstream tasks\nand perform classification based on each semantic. Furthermore, we employ the\nDempster-Shafer evidence theory to evaluate the uncertainty of each prediction\ngenerated by diverse semantics. Experiments conducted in multiple different\nsettings have consistently demonstrated the effectiveness of CDC.\n","authors":["Yanan Zhang","Jiangmeng Li","Lixiang Liu","Wenwen Qiang"],"pdf_url":"https://arxiv.org/pdf/2410.12816v2.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2405.17871v2","updated":"2024-11-05T02:26:51Z","published":"2024-05-28T06:44:13Z","title":"Seeing the Image: Prioritizing Visual Correlation by Contrastive\n  Alignment","summary":"  Existing image-text modality alignment in Vision Language Models (VLMs)\ntreats each text token equally in an autoregressive manner. Despite being\nsimple and effective, this method results in sub-optimal cross-modal alignment\nby over-emphasizing the text tokens that are less correlated with or even\ncontradictory with the input images. In this paper, we advocate for assigning\ndistinct contributions for each text token based on its visual correlation.\nSpecifically, we present by contrasting image inputs, the difference in\nprediction logits on each text token provides strong guidance of visual\ncorrelation. We therefore introduce Contrastive ALignment (CAL), a simple yet\neffective re-weighting strategy that prioritizes training visually correlated\ntokens. Our experimental results demonstrate that CAL consistently improves\ndifferent types of VLMs across different resolutions and model sizes on various\nbenchmark datasets. Importantly, our method incurs minimal additional\ncomputational overhead, rendering it highly efficient compared to alternative\ndata scaling strategies. Codes are available at\nhttps://github.com/foundation-multimodal-models/CAL.\n","authors":["Xin Xiao","Bohong Wu","Jiacong Wang","Chunyuan Li","Xun Zhou","Haoyuan Guo"],"pdf_url":"https://arxiv.org/pdf/2405.17871v2.pdf","comment":"NeurlPS 2024, Camera ready"},{"id":"http://arxiv.org/abs/2408.16725v3","updated":"2024-11-05T02:24:18Z","published":"2024-08-29T17:18:53Z","title":"Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming","summary":"  Recent advances in language models have achieved significant progress.\nGPT-4o, as a new milestone, has enabled real-time conversations with humans,\ndemonstrating near-human natural fluency. Such human-computer interaction\nnecessitates models with the capability to perform reasoning directly with the\naudio modality and generate output in streaming. However, this remains beyond\nthe reach of current academic models, as they typically depend on extra TTS\nsystems for speech synthesis, resulting in undesirable latency. This paper\nintroduces the Mini-Omni, an audio-based end-to-end conversational model,\ncapable of real-time speech interaction. To achieve this capability, we propose\na text-instructed speech generation method, along with batch-parallel\nstrategies during inference to further boost the performance. Our method also\nhelps to retain the original model's language capabilities with minimal\ndegradation, enabling other works to establish real-time interaction\ncapabilities. We call this training method \"Any Model Can Talk\". We also\nintroduce the VoiceAssistant-400K dataset to fine-tune models optimized for\nspeech output. To our best knowledge, Mini-Omni is the first fully end-to-end,\nopen-source model for real-time speech interaction, offering valuable potential\nfor future research.\n","authors":["Zhifei Xie","Changqiao Wu"],"pdf_url":"https://arxiv.org/pdf/2408.16725v3.pdf","comment":"Technical report, work in progress. Demo and code:\n  https://github.com/gpt-omni/mini-omni"},{"id":"http://arxiv.org/abs/2403.09017v3","updated":"2024-11-05T02:19:26Z","published":"2024-03-14T00:45:24Z","title":"AraTrust: An Evaluation of Trustworthiness for LLMs in Arabic","summary":"  The swift progress and widespread acceptance of artificial intelligence (AI)\nsystems highlight a pressing requirement to comprehend both the capabilities\nand potential risks associated with AI. Given the linguistic complexity,\ncultural richness, and underrepresented status of Arabic in AI research, there\nis a pressing need to focus on Large Language Models (LLMs) performance and\nsafety for Arabic-related tasks. Despite some progress in their development,\nthere is a lack of comprehensive trustworthiness evaluation benchmarks, which\npresents a major challenge in accurately assessing and improving the safety of\nLLMs when prompted in Arabic. In this paper, we introduce AraTrust, the first\ncomprehensive trustworthiness benchmark for LLMs in Arabic. AraTrust comprises\n522 human-written multiple-choice questions addressing diverse dimensions\nrelated to truthfulness, ethics, safety, physical health, mental health,\nunfairness, illegal activities, privacy, and offensive language. We evaluated a\nset of LLMs against our benchmark to assess their trustworthiness. GPT-4 was\nthe most trustworthy LLM, while open-source models, particularly AceGPT 7B and\nJais 13B, struggled to achieve a score of 60% in our benchmark.\n","authors":["Emad A. Alghamdi","Reem I. Masoud","Deema Alnuhait","Afnan Y. Alomairi","Ahmed Ashraf","Mohamed Zaytoon"],"pdf_url":"https://arxiv.org/pdf/2403.09017v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16444v2","updated":"2024-11-05T02:13:59Z","published":"2024-02-26T09:43:02Z","title":"ShieldLM: Empowering LLMs as Aligned, Customizable and Explainable\n  Safety Detectors","summary":"  The safety of Large Language Models (LLMs) has gained increasing attention in\nrecent years, but there still lacks a comprehensive approach for detecting\nsafety issues within LLMs' responses in an aligned, customizable and\nexplainable manner. In this paper, we propose ShieldLM, an LLM-based safety\ndetector, which aligns with common safety standards, supports customizable\ndetection rules, and provides explanations for its decisions. To train\nShieldLM, we compile a large bilingual dataset comprising 14,387 query-response\npairs, annotating the safety of responses based on various safety standards.\nThrough extensive experiments, we demonstrate that ShieldLM surpasses strong\nbaselines across four test sets, showcasing remarkable customizability and\nexplainability. Besides performing well on standard detection datasets,\nShieldLM has also been shown to be effective as a safety evaluator for advanced\nLLMs. ShieldLM is released at \\url{https://github.com/thu-coai/ShieldLM} to\nsupport accurate and explainable safety detection under various safety\nstandards.\n","authors":["Zhexin Zhang","Yida Lu","Jingyuan Ma","Di Zhang","Rui Li","Pei Ke","Hao Sun","Lei Sha","Zhifang Sui","Hongning Wang","Minlie Huang"],"pdf_url":"https://arxiv.org/pdf/2402.16444v2.pdf","comment":"19 pages. Camera ready version of EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2411.02738v1","updated":"2024-11-05T02:12:22Z","published":"2024-11-05T02:12:22Z","title":"Novelty-focused R&D landscaping using transformer and local outlier\n  factor","summary":"  While numerous studies have explored the field of research and development\n(R&D) landscaping, the preponderance of these investigations has emphasized\npredictive analysis based on R&D outcomes, specifically patents, and academic\nliterature. However, the value of research proposals and novelty analysis has\nseldom been addressed. This study proposes a systematic approach to\nconstructing and navigating the R&D landscape that can be utilized to guide\norganizations to respond in a reproducible and timely manner to the challenges\npresented by increasing number of research proposals. At the heart of the\nproposed approach is the composite use of the transformer-based language model\nand the local outlier factor (LOF). The semantic meaning of the research\nproposals is captured with our further-trained transformers, thereby\nconstructing a comprehensive R&D landscape. Subsequently, the novelty of the\nnewly selected research proposals within the annual landscape is quantified on\na numerical scale utilizing the LOF by assessing the dissimilarity of each\nproposal to others preceding and within the same year. A case study examining\nresearch proposals in the energy and resource sector in South Korea is\npresented. The systematic process and quantitative outcomes are expected to be\nuseful decision-support tools, providing future insights regarding R&D planning\nand roadmapping.\n","authors":["Jaewoong Choi"],"pdf_url":"https://arxiv.org/pdf/2411.02738v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02730v1","updated":"2024-11-05T01:58:31Z","published":"2024-11-05T01:58:31Z","title":"A Natural Language Processing Approach to Support Biomedical Data\n  Harmonization: Leveraging Large Language Models","summary":"  Biomedical research requires large, diverse samples to produce unbiased\nresults. Automated methods for matching variables across datasets can\naccelerate this process. Research in this area has been limited, primarily\nfocusing on lexical matching and ontology based semantic matching. We aimed to\ndevelop new methods, leveraging large language models (LLM) and ensemble\nlearning, to automate variable matching. Methods: We utilized data from two\nGERAS cohort (European and Japan) studies to develop variable matching methods.\nWe first manually created a dataset by matching 352 EU variables with 1322\ncandidate JP variables, where matched variable pairs were positive and\nunmatched pairs were negative instances. Using this dataset, we developed and\nevaluated two types of natural language processing (NLP) methods, which matched\nvariables based on variable labels and definitions from data dictionaries: (1)\nLLM-based and (2) fuzzy matching. We then developed an ensemble-learning\nmethod, using the Random Forest model, to integrate individual NLP methods. RF\nwas trained and evaluated on 50 trials. Each trial had a random split (4:1) of\ntraining and test sets, with the model's hyperparameters optimized through\ncross-validation on the training set. For each EU variable, 1322 candidate JP\nvariables were ranked based on NLP-derived similarity scores or RF's\nprobability scores, denoting their likelihood to match the EU variable. Ranking\nperformance was measured by top-n hit ratio (HRn) and mean reciprocal rank\n(MRR). Results:E5 performed best among individual methods, achieving 0.90 HR-30\nand 0.70 MRR. RF performed better than E5 on all metrics over 50 trials (P less\nthan 0.001) and achieved an average HR 30 of 0.98 and MRR of 0.73. LLM-derived\nfeatures contributed most to RF's performance. One major cause of errors in\nautomatic variable matching was ambiguous variable definitions within data\ndictionaries.\n","authors":["Zexu Li","Suraj P. Prabhu","Zachary T. Popp","Shubhi S. Jain","Vijetha Balakundi","Ting Fang Alvin Ang","Rhoda Au","Jinying Chen"],"pdf_url":"https://arxiv.org/pdf/2411.02730v1.pdf","comment":"32 pages, 2 figures"},{"id":"http://arxiv.org/abs/2406.02900v2","updated":"2024-11-05T01:44:14Z","published":"2024-06-05T03:41:37Z","title":"Scaling Laws for Reward Model Overoptimization in Direct Alignment\n  Algorithms","summary":"  Reinforcement Learning from Human Feedback (RLHF) has been crucial to the\nrecent success of Large Language Models (LLMs), however, it is often a complex\nand brittle process. In the classical RLHF framework, a reward model is first\ntrained to represent human preferences, which is in turn used by an online\nreinforcement learning (RL) algorithm to optimize the LLM. A prominent issue\nwith such methods is reward over-optimization or reward hacking, where\nperformance as measured by the learned proxy reward model increases, but true\nquality plateaus or even deteriorates. Direct Alignment Algorithms (DDAs) like\nDirect Preference Optimization have emerged as alternatives to the classical\nRLHF pipeline by circumventing the reward modeling phase. However, although\nDAAs do not use a separate proxy reward model, they still commonly deteriorate\nfrom over-optimization. While the so-called reward hacking phenomenon is not\nwell-defined for DAAs, we still uncover similar trends: at higher KL budgets,\nDAA algorithms exhibit similar degradation patterns to their classic RLHF\ncounterparts. In particular, we find that DAA methods deteriorate not only\nacross a wide range of KL budgets but also often before even a single epoch of\nthe dataset is completed. Through extensive empirical experimentation, this\nwork formulates and formalizes the reward over-optimization or hacking problem\nfor DAAs and explores its consequences across objectives, training regimes, and\nmodel scales.\n","authors":["Rafael Rafailov","Yaswanth Chittepu","Ryan Park","Harshit Sikchi","Joey Hejna","Bradley Knox","Chelsea Finn","Scott Niekum"],"pdf_url":"https://arxiv.org/pdf/2406.02900v2.pdf","comment":"30 pages, 38th Conference on Neural Information Processing Systems\n  (NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2407.03545v2","updated":"2024-11-05T01:38:45Z","published":"2024-07-03T23:53:27Z","title":"On Evaluating Explanation Utility for Human-AI Decision Making in NLP","summary":"  Is explainability a false promise? This debate has emerged from the\ninsufficient evidence that explanations help people in situations they are\nintroduced for. More human-centered, application-grounded evaluations of\nexplanations are needed to settle this. Yet, with no established guidelines for\nsuch studies in NLP, researchers accustomed to standardized proxy evaluations\nmust discover appropriate measurements, tasks, datasets, and sensible models\nfor human-AI teams in their studies.\n  To aid with this, we first review existing metrics suitable for\napplication-grounded evaluation. We then establish criteria to select\nappropriate datasets, and using them, we find that only 4 out of over 50\ndatasets available for explainability research in NLP meet them. We then\ndemonstrate the importance of reassessing the state of the art to form and\nstudy human-AI teams: teaming people with models for certain tasks might only\nnow start to make sense, and for others, it remains unsound. Finally, we\npresent the exemplar studies of human-AI decision-making for one of the\nidentified tasks -- verifying the correctness of a legal claim given a\ncontract. Our results show that providing AI predictions, with or without\nexplanations, does not cause decision makers to speed up their work without\ncompromising performance. We argue for revisiting the setup of human-AI teams\nand improving automatic deferral of instances to AI, where explanations could\nplay a useful role.\n","authors":["Fateme Hashemi Chaleshtori","Atreya Ghosal","Alexander Gill","Purbid Bambroo","Ana Marasović"],"pdf_url":"https://arxiv.org/pdf/2407.03545v2.pdf","comment":"EMNLP Findings 2024; 10 pages main, 7 pages references, 32 pages\n  appendix"},{"id":"http://arxiv.org/abs/2411.02722v1","updated":"2024-11-05T01:37:16Z","published":"2024-11-05T01:37:16Z","title":"Multimodal Commonsense Knowledge Distillation for Visual Question\n  Answering","summary":"  Existing Multimodal Large Language Models (MLLMs) and Visual Language\nPretrained Models (VLPMs) have shown remarkable performances in the general\nVisual Question Answering (VQA). However, these models struggle with VQA\nquestions that require external commonsense knowledge due to the challenges in\ngenerating high-quality prompts and the high computational costs of\nfine-tuning. In this work, we propose a novel graph-based multimodal\ncommonsense knowledge distillation framework that constructs a unified\nrelational graph over commonsense knowledge, visual objects and questions\nthrough a Graph Convolutional Network (GCN) following a teacher-student\nenvironment. This proposed framework is flexible with any type of teacher and\nstudent models without further fine-tuning, and has achieved competitive\nperformances on the ScienceQA dataset.\n","authors":["Shuo Yang","Siwen Luo","Soyeon Caren Han"],"pdf_url":"https://arxiv.org/pdf/2411.02722v1.pdf","comment":"AAAI 2025 (Accepted, Oral)"},{"id":"http://arxiv.org/abs/2411.02714v1","updated":"2024-11-05T01:26:35Z","published":"2024-11-05T01:26:35Z","title":"Game Plot Design with an LLM-powered Assistant: An Empirical Study with\n  Game Designers","summary":"  We introduce GamePlot, an LLM-powered assistant that supports game designers\nin crafting immersive narratives for turn-based games, and allows them to test\nthese games through a collaborative game play and refine the plot throughout\nthe process. Our user study with 14 game designers shows high levels of both\nsatisfaction with the generated game plots and sense of ownership over the\nnarratives, but also reconfirms that LLM are limited in their ability to\ngenerate complex and truly innovative content. We also show that diverse user\npopulations have different expectations from AI assistants, and encourage\nresearchers to study how tailoring assistants to diverse user groups could\npotentially lead to increased job satisfaction and greater creativity and\ninnovation over time.\n","authors":["Seyed Hossein Alavi","Weijia Xu","Nebojsa Jojic","Daniel Kennett","Raymond T. Ng","Sudha Rao","Haiyan Zhang","Bill Dolan","Vered Shwartz"],"pdf_url":"https://arxiv.org/pdf/2411.02714v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24190v2","updated":"2024-11-05T01:14:48Z","published":"2024-10-31T17:51:00Z","title":"Hidden Persuaders: LLMs' Political Leaning and Their Influence on Voters","summary":"  How could LLMs influence our democracy? We investigate LLMs' political\nleanings and the potential influence of LLMs on voters by conducting multiple\nexperiments in a U.S. presidential election context. Through a voting\nsimulation, we first demonstrate 18 open- and closed-weight LLMs' political\npreference for a Democratic nominee over a Republican nominee. We show how this\nleaning towards the Democratic nominee becomes more pronounced in\ninstruction-tuned models compared to their base versions by analyzing their\nresponses to candidate-policy related questions. We further explore the\npotential impact of LLMs on voter choice by conducting an experiment with 935\nU.S. registered voters. During the experiments, participants interacted with\nLLMs (Claude-3, Llama-3, and GPT-4) over five exchanges. The experiment results\nshow a shift in voter choices towards the Democratic nominee following LLM\ninteraction, widening the voting margin from 0.7% to 4.6%, even though LLMs\nwere not asked to persuade users to support the Democratic nominee during the\ndiscourse. This effect is larger than many previous studies on the\npersuasiveness of political campaigns, which have shown minimal effects in\npresidential elections. Many users also expressed a desire for further\npolitical interaction with LLMs. Which aspects of LLM interactions drove these\nshifts in voter choice requires further study. Lastly, we explore how a safety\nmethod can make LLMs more politically neutral, while leaving some open\nquestions.\n","authors":["Yujin Potter","Shiyang Lai","Junsol Kim","James Evans","Dawn Song"],"pdf_url":"https://arxiv.org/pdf/2410.24190v2.pdf","comment":"EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2411.02708v1","updated":"2024-11-05T01:11:28Z","published":"2024-11-05T01:11:28Z","title":"Exploring Response Uncertainty in MLLMs: An Empirical Evaluation under\n  Misleading Scenarios","summary":"  Ensuring that Multimodal Large Language Models (MLLMs) maintain consistency\nin their responses is essential for developing trustworthy multimodal\nintelligence. However, existing benchmarks include many samples where all MLLMs\n\\textit{exhibit high response uncertainty when encountering misleading\ninformation}, requiring even 5-15 response attempts per sample to effectively\nassess uncertainty. Therefore, we propose a two-stage pipeline: first, we\ncollect MLLMs' responses without misleading information, and then gather\nmisleading ones via specific misleading instructions. By calculating the\nmisleading rate, and capturing both correct-to-incorrect and\nincorrect-to-correct shifts between the two sets of responses, we can\neffectively metric the model's response uncertainty. Eventually, we establish a\n\\textbf{\\underline{M}}ultimodal \\textbf{\\underline{U}}ncertainty\n\\textbf{\\underline{B}}enchmark (\\textbf{MUB}) that employs both explicit and\nimplicit misleading instructions to comprehensively assess the vulnerability of\nMLLMs across diverse domains. Our experiments reveal that all open-source and\nclose-source MLLMs are highly susceptible to misleading instructions, with an\naverage misleading rate exceeding 86\\%. To enhance the robustness of MLLMs, we\nfurther fine-tune all open-source MLLMs by incorporating explicit and implicit\nmisleading data, which demonstrates a significant reduction in misleading\nrates. Our code is available at:\n\\href{https://github.com/Yunkai696/MUB}{https://github.com/Yunkai696/MUB}\n","authors":["Yunkai Dang","Mengxi Gao","Yibo Yan","Xin Zou","Yanggan Gu","Aiwei Liu","Xuming Hu"],"pdf_url":"https://arxiv.org/pdf/2411.02708v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02704v1","updated":"2024-11-05T01:02:51Z","published":"2024-11-05T01:02:51Z","title":"RT-Affordance: Affordances are Versatile Intermediate Representations\n  for Robot Manipulation","summary":"  We explore how intermediate policy representations can facilitate\ngeneralization by providing guidance on how to perform manipulation tasks.\nExisting representations such as language, goal images, and trajectory sketches\nhave been shown to be helpful, but these representations either do not provide\nenough context or provide over-specified context that yields less robust\npolicies. We propose conditioning policies on affordances, which capture the\npose of the robot at key stages of the task. Affordances offer expressive yet\nlightweight abstractions, are easy for users to specify, and facilitate\nefficient learning by transferring knowledge from large internet datasets. Our\nmethod, RT-Affordance, is a hierarchical model that first proposes an\naffordance plan given the task language, and then conditions the policy on this\naffordance plan to perform manipulation. Our model can flexibly bridge\nheterogeneous sources of supervision including large web datasets and robot\ntrajectories. We additionally train our model on cheap-to-collect in-domain\naffordance images, allowing us to learn new tasks without collecting any\nadditional costly robot trajectories. We show on a diverse set of novel tasks\nhow RT-Affordance exceeds the performance of existing methods by over 50%, and\nwe empirically demonstrate that affordances are robust to novel settings.\nVideos available at https://snasiriany.me/rt-affordance\n","authors":["Soroush Nasiriany","Sean Kirmani","Tianli Ding","Laura Smith","Yuke Zhu","Danny Driess","Dorsa Sadigh","Ted Xiao"],"pdf_url":"https://arxiv.org/pdf/2411.02704v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02695v1","updated":"2024-11-05T00:46:25Z","published":"2024-11-05T00:46:25Z","title":"JEL: Applying End-to-End Neural Entity Linking in JPMorgan Chase","summary":"  Knowledge Graphs have emerged as a compelling abstraction for capturing key\nrelationship among the entities of interest to enterprises and for integrating\ndata from heterogeneous sources. JPMorgan Chase (JPMC) is leading this trend by\nleveraging knowledge graphs across the organization for multiple mission\ncritical applications such as risk assessment, fraud detection, investment\nadvice, etc. A core problem in leveraging a knowledge graph is to link mentions\n(e.g., company names) that are encountered in textual sources to entities in\nthe knowledge graph. Although several techniques exist for entity linking, they\nare tuned for entities that exist in Wikipedia, and fail to generalize for the\nentities that are of interest to an enterprise. In this paper, we propose a\nnovel end-to-end neural entity linking model (JEL) that uses minimal context\ninformation and a margin loss to generate entity embeddings, and a Wide & Deep\nLearning model to match character and semantic information respectively. We\nshow that JEL achieves the state-of-the-art performance to link mentions of\ncompany names in financial news with entities in our knowledge graph. We report\non our efforts to deploy this model in the company-wide system to generate\nalerts in response to financial news. The methodology used for JEL is directly\napplicable and usable by other enterprises who need entity linking solutions\nfor data that are unique to their respective situations.\n","authors":["Wanying Ding","Vinay K. Chaudhri","Naren Chittar","Krishna Konakanchi"],"pdf_url":"https://arxiv.org/pdf/2411.02695v1.pdf","comment":"8 pages, 4 figures, IAAI-21"},{"id":"http://arxiv.org/abs/2411.02688v1","updated":"2024-11-05T00:16:01Z","published":"2024-11-05T00:16:01Z","title":"On the loss of context-awareness in general instruction fine-tuning","summary":"  Pretrained Large Language Models (LLMs) require post-training methods such as\nsupervised fine-tuning (SFT) on instruction-response pairs to enable\ninstruction following. However, this process can potentially harm existing\ncapabilities learned during pretraining. In this paper, we investigate the loss\nof context awareness after SFT, defined as the capability to extract and\nunderstand information from the user-provided context and respond accordingly.\nWe are the first to identify and show that the loss of context-awareness\nappears on instruction-finetuned LLMs when the chat template is applied to the\ninput prompts. We identify the performance decline is partially caused by the\nbias embedded into the chat template to focus less on the user-provided\ncontext. Based on these observations, we propose two methods to mitigate the\nloss of context awareness in instruct models: post-hoc attention steering on\nuser prompts and conditional instruction fine-tuning with a context-dependency\nindicator. Empirical experiments on 4 context-dependent downstream tasks and 3\npretrained LLMs of different sizes show that our methods effectively mitigates\nthe loss of context awareness without compromising the general ability to\nfollow instructions. Our findings also strongly advocate the necessity to\ncarefully benchmark context awareness after instruction fine-tuning.\n","authors":["Yihan Wang","Andrew Bai","Nanyun Peng","Cho-Jui Hsieh"],"pdf_url":"https://arxiv.org/pdf/2411.02688v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.07832v5","updated":"2024-11-05T23:50:14Z","published":"2024-07-31T14:49:35Z","title":"LADDER: Language Driven Slice Discovery and Error Rectification","summary":"  Error slice discovery associates structured patterns with model errors.\nExisting methods discover error slices by clustering the error-prone samples\nwith similar patterns or assigning discrete attributes to each sample for\npost-hoc analysis. While these methods aim for interpretability and easier\nmitigation through reweighting or rebalancing, they may not capture the full\ncomplexity of error patterns due to incomplete or missing attributes. Contrary\nto the existing approach, this paper utilizes the reasoning capabilities of the\nLarge Language Model (LLM) to analyze complex error patterns and generate\ntestable hypotheses. This paper proposes LADDER: Language Driven slice\nDiscovery and Error Rectification. It first projects the model's representation\ninto a language-aligned feature space (eg CLIP) to preserve semantics in the\noriginal model feature space. This ensures the accurate retrieval of sentences\nthat highlight the model's errors. Next, the LLM utilizes the sentences and\ngenerates hypotheses to discover error slices. Finally, we mitigate the error\nby fine-tuning the classification head by creating a group-balanced dataset\nusing the hypotheses. Our entire method does not require any attribute\nannotation, either explicitly or through external tagging models. We validate\nour method with \\textbf{five} image classification datasets.\n","authors":["Shantanu Ghosh","Rayan Syed","Chenyu Wang","Clare B. Poynton","Shyam Visweswaran","Kayhan Batmanghelich"],"pdf_url":"https://arxiv.org/pdf/2408.07832v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12843v3","updated":"2024-11-05T23:15:46Z","published":"2024-07-04T15:10:51Z","title":"NutriBench: A Dataset for Evaluating Large Language Models in\n  Carbohydrate Estimation from Meal Descriptions","summary":"  Accurate nutrition estimation helps people make informed dietary choices and\nis essential in the prevention of serious health complications. We present\nNutriBench, the first publicly available natural language meal description\nnutrition benchmark. NutriBench consists of 11,857 meal descriptions generated\nfrom real-world global dietary intake data. The data is human-verified and\nannotated with macro-nutrient labels, including carbohydrates, proteins, fats,\nand calories. We conduct an extensive evaluation of NutriBench on the task of\ncarbohydrate estimation, testing twelve leading Large Language Models (LLMs),\nincluding GPT-4o, Llama3.1, Qwen2, Gemma2, and OpenBioLLM models, using\nstandard, Chain-of-Thought and Retrieval-Augmented Generation strategies.\nAdditionally, we present a study involving professional nutritionists, finding\nthat LLMs can provide more accurate and faster estimates. Finally, we perform a\nreal-world risk assessment by simulating the effect of carbohydrate predictions\non the blood glucose levels of individuals with diabetes. Our work highlights\nthe opportunities and challenges of using LLMs for nutrition estimation,\ndemonstrating their potential to aid professionals and laypersons and improve\nhealth outcomes. Our benchmark is publicly available at:\nhttps://mehak126.github.io/nutribench.html\n","authors":["Andong Hua","Mehak Preet Dhaliwal","Ryan Burke","Laya Pullela","Yao Qin"],"pdf_url":"https://arxiv.org/pdf/2407.12843v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03550v1","updated":"2024-11-05T23:09:37Z","published":"2024-11-05T23:09:37Z","title":"Learning to Write Rationally: How Information Is Distributed in\n  Non-Native Speakers' Essays","summary":"  People tend to distribute information evenly in language production for\nbetter and clearer communication. In this study, we compared essays written by\nsecond language learners with various native language (L1) backgrounds to\ninvestigate how they distribute information in their non-native language (L2)\nproduction. Analyses of surprisal and constancy of entropy rate indicated that\nwriters with higher L2 proficiency can reduce the expected uncertainty of\nlanguage production while still conveying informative content. However, the\nuniformity of information distribution showed less variability among different\ngroups of L2 speakers, suggesting that this feature may be universal in L2\nessay writing and less affected by L2 writers' variability in L1 background and\nL2 proficiency.\n","authors":["Zixin Tang","Janet G. van Hell"],"pdf_url":"https://arxiv.org/pdf/2411.03550v1.pdf","comment":"To appear in main of Conference on Empirical Methods in Natural\n  Language Processing; EMNLP 2024"},{"id":"http://arxiv.org/abs/2411.03542v1","updated":"2024-11-05T22:45:10Z","published":"2024-11-05T22:45:10Z","title":"Exploring the Benefits of Domain-Pretraining of Generative Large\n  Language Models for Chemistry","summary":"  A proliferation of Large Language Models (the GPT series, BLOOM, LLaMA, and\nmore) are driving forward novel development of multipurpose AI for a variety of\ntasks, particularly natural language processing (NLP) tasks. These models\ndemonstrate strong performance on a range of tasks; however, there has been\nevidence of brittleness when applied to more niche or narrow domains where\nhallucinations or fluent but incorrect responses reduce performance. Given the\ncomplex nature of scientific domains, it is prudent to investigate the\ntrade-offs of leveraging off-the-shelf versus more targeted foundation models\nfor scientific domains. In this work, we examine the benefits of in-domain\npre-training for a given scientific domain, chemistry, and compare these to\nopen-source, off-the-shelf models with zero-shot and few-shot prompting. Our\nresults show that not only do in-domain base models perform reasonably well on\nin-domain tasks in a zero-shot setting but that further adaptation using\ninstruction fine-tuning yields impressive performance on chemistry-specific\ntasks such as named entity recognition and molecular formula generation.\n","authors":["Anurag Acharya","Shivam Sharma","Robin Cosbey","Megha Subramanian","Scott Howland","Maria Glenski"],"pdf_url":"https://arxiv.org/pdf/2411.03542v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03538v1","updated":"2024-11-05T22:37:43Z","published":"2024-11-05T22:37:43Z","title":"Long Context RAG Performance of Large Language Models","summary":"  Retrieval Augmented Generation (RAG) has emerged as a crucial technique for\nenhancing the accuracy of Large Language Models (LLMs) by incorporating\nexternal information. With the advent of LLMs that support increasingly longer\ncontext lengths, there is a growing interest in understanding how these models\nperform in RAG scenarios. Can these new long context models improve RAG\nperformance? This paper presents a comprehensive study of the impact of\nincreased context length on RAG performance across 20 popular open source and\ncommercial LLMs. We ran RAG workflows while varying the total context length\nfrom 2,000 to 128,000 tokens (and 2 million tokens when possible) on three\ndomain-specific datasets, and report key insights on the benefits and\nlimitations of long context in RAG applications. Our findings reveal that while\nretrieving more documents can improve performance, only a handful of the most\nrecent state of the art LLMs can maintain consistent accuracy at long context\nabove 64k tokens. We also identify distinct failure modes in long context\nscenarios, suggesting areas for future research.\n","authors":["Quinn Leng","Jacob Portes","Sam Havens","Matei Zaharia","Michael Carbin"],"pdf_url":"https://arxiv.org/pdf/2411.03538v1.pdf","comment":"2024 NeurIPS workshop on Adaptive Foundation Models: Evolving AI for\n  Personalized and Efficient Learning"},{"id":"http://arxiv.org/abs/2406.16450v2","updated":"2024-11-05T22:34:29Z","published":"2024-06-24T08:43:21Z","title":"Building on Efficient Foundations: Effectively Training LLMs with\n  Structured Feedforward Layers","summary":"  State-of-the-art results in large language models (LLMs) often rely on scale,\nwhich becomes computationally expensive. This has sparked a research agenda to\nreduce these models' parameter counts and computational costs without\nsignificantly impacting their performance. Our study focuses on\ntransformer-based LLMs, specifically targeting the computationally intensive\nfeedforward networks (FFNs), which are less studied than attention blocks. We\nconsider three structured linear parameterizations of the FFN using efficient\nlow-rank and block-diagonal matrices. In contrast to many previous works that\nexamined these approximations, our study i) explores these structures from a\ntraining-from-scratch perspective, ii) scales up to 1.3B parameters, and iii)\nis conducted within recent Transformer-based LLMs rather than convolutional\narchitectures. We demonstrate that these structures can lead to actual\ncomputational gains in various scenarios, including online decoding when using\na pre-merge technique. Additionally, we propose a novel training regime, called\n\\textit{self-guided training}, aimed at improving the poor training dynamics\nthat these approximations exhibit when used from initialization. Interestingly,\nthe scaling performance of structured matrices is explored, revealing steeper\ncurves in scaling training FLOPs, along with a favorable scaling trend in the\novertraining regime. Specifically, we show that wide and structured networks\ncan utilize training FLOPs more efficiently, with fewer parameters and lower\nloss than dense models at their optimal trade-off. Our code is available at\n\\url{https://github.com/CLAIRE-Labo/StructuredFFN/tree/main}.\n","authors":["Xiuying Wei","Skander Moalla","Razvan Pascanu","Caglar Gulcehre"],"pdf_url":"https://arxiv.org/pdf/2406.16450v2.pdf","comment":"Accepted by NeurIPS2024"},{"id":"http://arxiv.org/abs/2411.03524v1","updated":"2024-11-05T22:01:27Z","published":"2024-11-05T22:01:27Z","title":"Mitigating Metric Bias in Minimum Bayes Risk Decoding","summary":"  While Minimum Bayes Risk (MBR) decoding using metrics such as COMET or\nMetricX has outperformed traditional decoding methods such as greedy or beam\nsearch, it introduces a challenge we refer to as metric bias. As MBR decoding\naims to produce translations that score highly according to a specific utility\nmetric, this very process makes it impossible to use the same metric for both\ndecoding and evaluation, as improvements might simply be due to reward hacking\nrather than reflecting real quality improvements. In this work we find that\ncompared to human ratings, neural metrics not only overestimate the quality of\nMBR decoding when the same metric is used as the utility metric, but they also\noverestimate the quality of MBR/QE decoding with other neural utility metrics\nas well. We also show that the metric bias issue can be mitigated by using an\nensemble of utility metrics during MBR decoding: human evaluations show that\nMBR decoding using an ensemble of utility metrics outperforms a single utility\nmetric.\n","authors":["Geza Kovacs","Daniel Deutsch","Markus Freitag"],"pdf_url":"https://arxiv.org/pdf/2411.03524v1.pdf","comment":"To appear at WMT2024"},{"id":"http://arxiv.org/abs/2411.03513v1","updated":"2024-11-05T21:19:49Z","published":"2024-11-05T21:19:49Z","title":"Change Is the Only Constant: Dynamic LLM Slicing based on Layer\n  Redundancy","summary":"  This paper introduces a novel model compression approach through dynamic\nlayer-specific pruning in Large Language Models (LLMs), enhancing the\ntraditional methodology established by SliceGPT. By transitioning from constant\nto dynamic slicing, our method leverages the newly proposed Layer Redundancy\n(LR) score, which assesses how much change each layer changes its input by\nmeasuring the cosine similarity of the input to the output of the layer. We use\nthis score to prune parts of individual layers based on redundancy in such a\nway that the average pruned percentage for all layers is a fixed value. We\nconducted extensive experiments using models like Llama3-8B and Mistral-7B on\nmultiple datasets, evaluating different slicing bases and percentages to\ndetermine optimal configurations that balance efficiency and performance. Our\nfindings show that our dynamic slicing approach not only maintains but, in many\ncases, enhances model performance compared to the baseline established by\nconstant slicing methods. For instance, in several settings, we see performance\nimprovements of up to 5% over the SliceGPT baseline. Additionally, a perplexity\ndecrease by as much as 7% was observed across multiple benchmarks, validating\nthe effectiveness of our method. The code, model weights, and datasets are\nopen-sourced at https://github.com/RazvanDu/DynamicSlicing.\n","authors":["Razvan-Gabriel Dumitru","Paul-Ioan Clotan","Vikas Yadav","Darius Peteleaza","Mihai Surdeanu"],"pdf_url":"https://arxiv.org/pdf/2411.03513v1.pdf","comment":"Accepted at EMNLP Findings 2024"},{"id":"http://arxiv.org/abs/2403.00143v2","updated":"2024-11-05T21:04:23Z","published":"2024-02-29T21:49:31Z","title":"Tree-Averaging Algorithms for Ensemble-Based Unsupervised Discontinuous\n  Constituency Parsing","summary":"  We address unsupervised discontinuous constituency parsing, where we observe\na high variance in the performance of the only previous model in the\nliterature. We propose to build an ensemble of different runs of the existing\ndiscontinuous parser by averaging the predicted trees, to stabilize and boost\nperformance. To begin with, we provide comprehensive computational complexity\nanalysis (in terms of P and NP-complete) for tree averaging under different\nsetups of binarity and continuity. We then develop an efficient exact algorithm\nto tackle the task, which runs in a reasonable time for all samples in our\nexperiments. Results on three datasets show our method outperforms all\nbaselines in all metrics; we also provide in-depth analyses of our approach.\n","authors":["Behzad Shayegh","Yuqiao Wen","Lili Mou"],"pdf_url":"https://arxiv.org/pdf/2403.00143v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03497v1","updated":"2024-11-05T20:20:15Z","published":"2024-11-05T20:20:15Z","title":"Uncertainty Quantification for Clinical Outcome Predictions with (Large)\n  Language Models","summary":"  To facilitate healthcare delivery, language models (LMs) have significant\npotential for clinical prediction tasks using electronic health records (EHRs).\nHowever, in these high-stakes applications, unreliable decisions can result in\nhigh costs due to compromised patient safety and ethical concerns, thus\nincreasing the need for good uncertainty modeling of automated clinical\npredictions. To address this, we consider the uncertainty quantification of LMs\nfor EHR tasks in white- and black-box settings. We first quantify uncertainty\nin white-box models, where we can access model parameters and output logits. We\nshow that an effective reduction of model uncertainty can be achieved by using\nthe proposed multi-tasking and ensemble methods in EHRs. Continuing with this\nidea, we extend our approach to black-box settings, including popular\nproprietary LMs such as GPT-4. We validate our framework using longitudinal\nclinical data from more than 6,000 patients in ten clinical prediction tasks.\nResults show that ensembling methods and multi-task prediction prompts reduce\nuncertainty across different scenarios. These findings increase the\ntransparency of the model in white-box and black-box settings, thus advancing\nreliable AI healthcare.\n","authors":["Zizhang Chen","Peizhao Li","Xiaomeng Dong","Pengyu Hong"],"pdf_url":"https://arxiv.org/pdf/2411.03497v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03495v1","updated":"2024-11-05T20:18:53Z","published":"2024-11-05T20:18:53Z","title":"Automatic Generation of Question Hints for Mathematics Problems using\n  Large Language Models in Educational Technology","summary":"  The automatic generation of hints by Large Language Models (LLMs) within\nIntelligent Tutoring Systems (ITSs) has shown potential to enhance student\nlearning. However, generating pedagogically sound hints that address student\nmisconceptions and adhere to specific educational objectives remains\nchallenging. This work explores using LLMs (GPT-4o and Llama-3-8B-instruct) as\nteachers to generate effective hints for students simulated through LLMs\n(GPT-3.5-turbo, Llama-3-8B-Instruct, or Mistral-7B-instruct-v0.3) tackling math\nexercises designed for human high-school students, and designed using cognitive\nscience principles. We present here the study of several dimensions: 1)\nidentifying error patterns made by simulated students on secondary-level math\nexercises; 2) developing various prompts for GPT-4o as a teacher and evaluating\ntheir effectiveness in generating hints that enable simulated students to\nself-correct; and 3) testing the best-performing prompts, based on their\nability to produce relevant hints and facilitate error correction, with\nLlama-3-8B-Instruct as the teacher, allowing for a performance comparison with\nGPT-4o. The results show that model errors increase with higher temperature\nsettings. Notably, when hints are generated by GPT-4o, the most effective\nprompts include prompts tailored to specific errors as well as prompts\nproviding general hints based on common mathematical errors. Interestingly,\nLlama-3-8B-Instruct as a teacher showed better overall performance than GPT-4o.\nAlso the problem-solving and response revision capabilities of the LLMs as\nstudents, particularly GPT-3.5-turbo, improved significantly after receiving\nhints, especially at lower temperature settings. However, models like\nMistral-7B-Instruct demonstrated a decline in performance as the temperature\nincreased.\n","authors":["Junior Cedric Tonga","Benjamin Clement","Pierre-Yves Oudeyer"],"pdf_url":"https://arxiv.org/pdf/2411.03495v1.pdf","comment":"Accepted at NeurIPS 2024 Workshop on Large Foundation Models for\n  Educational Assessment (FM-Assess)"},{"id":"http://arxiv.org/abs/2411.03493v1","updated":"2024-11-05T20:18:28Z","published":"2024-11-05T20:18:28Z","title":"LASER: Attention with Exponential Transformation","summary":"  Transformers have had tremendous impact for several sequence related tasks,\nlargely due to their ability to retrieve from any part of the sequence via\nsoftmax based dot-product attention. This mechanism plays a crucial role in\nTransformer's performance. We analyze the gradients backpropagated through the\nsoftmax operation in the attention mechanism and observe that these gradients\ncan often be small. This poor gradient signal backpropagation can lead to\ninefficient learning of parameters preceeding the attention operations. To this\nend, we introduce a new attention mechanism called LASER, which we analytically\nshow to admit a larger gradient signal. We show that LASER Attention can be\nimplemented by making small modifications to existing attention\nimplementations. We conduct experiments on autoregressive large language models\n(LLMs) with upto 2.2 billion parameters where we show upto 3.38% and an average\nof ~1% improvement over standard attention on downstream evaluations. Using\nLASER gives the following relative improvements in generalization performance\nacross a variety of tasks (vision, text and speech): 4.67% accuracy in Vision\nTransformer (ViT) on Imagenet, 2.25% error rate in Conformer on the Librispeech\nspeech-to-text and 0.93% fraction of incorrect predictions in BERT with 2.2\nbillion parameters.\n","authors":["Sai Surya Duvvuri","Inderjit S. Dhillon"],"pdf_url":"https://arxiv.org/pdf/2411.03493v1.pdf","comment":"15 pages, under review in ICLR 2025"},{"id":"http://arxiv.org/abs/2411.03486v1","updated":"2024-11-05T20:10:25Z","published":"2024-11-05T20:10:25Z","title":"LLM Generated Distribution-Based Prediction of US Electoral Results,\n  Part I","summary":"  This paper introduces distribution-based prediction, a novel approach to\nusing Large Language Models (LLMs) as predictive tools by interpreting output\ntoken probabilities as distributions representing the models' learned\nrepresentation of the world. This distribution-based nature offers an\nalternative perspective for analyzing algorithmic fidelity, complementing the\napproach used in silicon sampling. We demonstrate the use of distribution-based\nprediction in the context of recent United States presidential election,\nshowing that this method can be used to determine task specific bias, prompt\nnoise, and algorithmic fidelity. This approach has significant implications for\nassessing the reliability and increasing transparency of LLM-based predictions\nacross various domains.\n","authors":["Caleb Bradshaw","Caelen Miller","Sean Warnick"],"pdf_url":"https://arxiv.org/pdf/2411.03486v1.pdf","comment":"17 pages, 10 Figures, Pre-print"},{"id":"http://arxiv.org/abs/2411.03471v1","updated":"2024-11-05T19:52:58Z","published":"2024-11-05T19:52:58Z","title":"MetRex: A Benchmark for Verilog Code Metric Reasoning Using LLMs","summary":"  Large Language Models (LLMs) have been applied to various hardware design\ntasks, including Verilog code generation, EDA tool scripting, and RTL bug\nfixing. Despite this extensive exploration, LLMs are yet to be used for the\ntask of post-synthesis metric reasoning and estimation of HDL designs. In this\npaper, we assess the ability of LLMs to reason about post-synthesis metrics of\nVerilog designs. We introduce MetRex, a large-scale dataset comprising 25,868\nVerilog HDL designs and their corresponding post-synthesis metrics, namely\narea, delay, and static power. MetRex incorporates a Chain of Thought (CoT)\ntemplate to enhance LLMs' reasoning about these metrics. Extensive experiments\nshow that Supervised Fine-Tuning (SFT) boosts the LLM's reasoning capabilities\non average by 37.0\\%, 25.3\\%, and 25.7\\% on the area, delay, and static power,\nrespectively. While SFT improves performance on our benchmark, it remains far\nfrom achieving optimal results, especially on complex problems. Comparing to\nstate-of-the-art regression models, our approach delivers accurate\npost-synthesis predictions for 17.4\\% more designs (within a 5\\% error margin),\nin addition to offering a 1.7x speedup by eliminating the need for\npre-processing. This work lays the groundwork for advancing LLM-based Verilog\ncode metric reasoning.\n","authors":["Manar Abdelatty","Jingxiao Ma","Sherief Reda"],"pdf_url":"https://arxiv.org/pdf/2411.03471v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.20424v3","updated":"2024-11-05T19:46:38Z","published":"2024-10-27T12:44:25Z","title":"AutoKaggle: A Multi-Agent Framework for Autonomous Data Science\n  Competitions","summary":"  Data science tasks involving tabular data present complex challenges that\nrequire sophisticated problem-solving approaches. We propose AutoKaggle, a\npowerful and user-centric framework that assists data scientists in completing\ndaily data pipelines through a collaborative multi-agent system. AutoKaggle\nimplements an iterative development process that combines code execution,\ndebugging, and comprehensive unit testing to ensure code correctness and logic\nconsistency. The framework offers highly customizable workflows, allowing users\nto intervene at each phase, thus integrating automated intelligence with human\nexpertise. Our universal data science toolkit, comprising validated functions\nfor data cleaning, feature engineering, and modeling, forms the foundation of\nthis solution, enhancing productivity by streamlining common tasks. We selected\n8 Kaggle competitions to simulate data processing workflows in real-world\napplication scenarios. Evaluation results demonstrate that AutoKaggle achieves\na validation submission rate of 0.85 and a comprehensive score of 0.82 in\ntypical data science pipelines, fully proving its effectiveness and\npracticality in handling complex data science tasks.\n","authors":["Ziming Li","Qianbo Zang","David Ma","Jiawei Guo","Tuney Zheng","Minghao Liu","Xinyao Niu","Yue Wang","Jian Yang","Jiaheng Liu","Wanjun Zhong","Wangchunshu Zhou","Wenhao Huang","Ge Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.20424v3.pdf","comment":"44 pages, 10 figures"},{"id":"http://arxiv.org/abs/2411.03445v1","updated":"2024-11-05T19:00:34Z","published":"2024-11-05T19:00:34Z","title":"Solving Trojan Detection Competitions with Linear Weight Classification","summary":"  Neural networks can conceal malicious Trojan backdoors that allow a trigger\nto covertly change the model behavior. Detecting signs of these backdoors,\nparticularly without access to any triggered data, is the subject of ongoing\nresearch and open challenges. In one common formulation of the problem, we are\ngiven a set of clean and poisoned models and need to predict whether a given\ntest model is clean or poisoned. In this paper, we introduce a detector that\nworks remarkably well across many of the existing datasets and domains. It is\nobtained by training a binary classifier on a large number of models' weights\nafter performing a few different pre-processing steps including feature\nselection and standardization, reference model weights subtraction, and model\nalignment prior to detection. We evaluate this algorithm on a diverse set of\nTrojan detection benchmarks and domains and examine the cases where the\napproach is most and least effective.\n","authors":["Todd Huster","Peter Lin","Razvan Stefanescu","Emmanuel Ekwedike","Ritu Chadha"],"pdf_url":"https://arxiv.org/pdf/2411.03445v1.pdf","comment":"9 pages, 4 Figures"},{"id":"http://arxiv.org/abs/2411.03417v1","updated":"2024-11-05T18:58:00Z","published":"2024-11-05T18:58:00Z","title":"Usefulness of LLMs as an Author Checklist Assistant for Scientific\n  Papers: NeurIPS'24 Experiment","summary":"  Large language models (LLMs) represent a promising, but controversial, tool\nin aiding scientific peer review. This study evaluates the usefulness of LLMs\nin a conference setting as a tool for vetting paper submissions against\nsubmission standards. We conduct an experiment at the 2024 Neural Information\nProcessing Systems (NeurIPS) conference, where 234 papers were voluntarily\nsubmitted to an \"LLM-based Checklist Assistant.\" This assistant validates\nwhether papers adhere to the author checklist used by NeurIPS, which includes\nquestions to ensure compliance with research and manuscript preparation\nstandards. Evaluation of the assistant by NeurIPS paper authors suggests that\nthe LLM-based assistant was generally helpful in verifying checklist\ncompletion. In post-usage surveys, over 70% of authors found the assistant\nuseful, and 70% indicate that they would revise their papers or checklist\nresponses based on its feedback. While causal attribution to the assistant is\nnot definitive, qualitative evidence suggests that the LLM contributed to\nimproving some submissions. Survey responses and analysis of re-submissions\nindicate that authors made substantive revisions to their submissions in\nresponse to specific feedback from the LLM. The experiment also highlights\ncommon issues with LLMs: inaccuracy (20/52) and excessive strictness (14/52)\nwere the most frequent issues flagged by authors. We also conduct experiments\nto understand potential gaming of the system, which reveal that the assistant\ncould be manipulated to enhance scores through fabricated justifications,\nhighlighting potential vulnerabilities of automated review tools.\n","authors":["Alexander Goldberg","Ihsan Ullah","Thanh Gia Hieu Khuong","Benedictus Kent Rachmat","Zhen Xu","Isabelle Guyon","Nihar B. Shah"],"pdf_url":"https://arxiv.org/pdf/2411.03417v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.01720v2","updated":"2024-11-05T18:45:22Z","published":"2024-01-26T18:37:21Z","title":"Deep Learning Based Amharic Chatbot for FAQs in Universities","summary":"  University students often spend a considerable amount of time seeking answers\nto common questions from administrators or teachers. This can become tedious\nfor both parties, leading to a need for a solution. In response, this paper\nproposes a chatbot model that utilizes natural language processing and deep\nlearning techniques to answer frequently asked questions (FAQs) in the Amharic\nlanguage. Chatbots are computer programs that simulate human conversation\nthrough the use of artificial intelligence (AI), acting as a virtual assistant\nto handle questions and other tasks. The proposed chatbot program employs\ntokenization, normalization, stop word removal, and stemming to analyze and\ncategorize Amharic input sentences. Three machine learning model algorithms\nwere used to classify tokens and retrieve appropriate responses: Support Vector\nMachine (SVM), Multinomial Na\\\"ive Bayes, and deep neural networks implemented\nthrough TensorFlow, Keras, and NLTK. The deep learning model achieved the best\nresults with 91.55% accuracy and a validation loss of 0.3548 using an Adam\noptimizer and SoftMax activation function. The chatbot model was integrated\nwith Facebook Messenger and deployed on a Heroku server for 24-hour\naccessibility. The experimental results demonstrate that the chatbot framework\nachieved its objectives and effectively addressed challenges such as Amharic\nFidel variation, morphological variation, and lexical gaps. Future research\ncould explore the integration of Amharic WordNet to narrow the lexical gap and\nsupport more complex questions.\n","authors":["Goitom Ybrah Hailu","Hadush Hailu","Shishay Welay"],"pdf_url":"https://arxiv.org/pdf/2402.01720v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14319v2","updated":"2024-11-05T18:43:57Z","published":"2024-06-20T13:52:30Z","title":"LiveMind: Low-latency Large Language Models with Simultaneous Inference","summary":"  In this paper, we introduce LiveMind, a novel low-latency inference framework\nfor large language model (LLM) inference which enables LLMs to perform\ninferences with incomplete user input. By reallocating computational processes\nto the input phase, a substantial reduction in latency is achieved, thereby\nsignificantly enhancing the interactive experience for users of LLMs. The\nframework adeptly manages the visibility of the streaming input to the model,\nallowing it to infer from incomplete user input or await additional content.\nCompared with traditional inference methods on complete user input, our\napproach demonstrates an average reduction in response latency of 84.0% on the\nMMLU dataset and 71.6% on the MMLU-Pro dataset, while maintaining comparable\naccuracy. Additionally, our framework facilitates collaborative inference and\noutput across different models. By employing an large LLM for inference and a\nsmall LLM for output, we achieve an average 37% reduction in response latency,\nalongside a 4.30% improvement in accuracy on the MMLU-Pro dataset compared with\nthe baseline. The proposed LiveMind framework advances the field of human-AI\ninteraction by enabling more responsive and efficient communication between\nusers and AI systems.\n","authors":["Chuangtao Chen","Grace Li Zhang","Xunzhao Yin","Cheng Zhuo","Ulf Schlichtmann","Bing Li"],"pdf_url":"https://arxiv.org/pdf/2406.14319v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.00593v2","updated":"2024-11-05T18:35:39Z","published":"2024-11-01T13:53:14Z","title":"Adapting Language Models via Token Translation","summary":"  Modern large language models use a fixed tokenizer to effectively compress\ntext drawn from a source domain. However, applying the same tokenizer to a new\ntarget domain often leads to inferior compression, more costly inference, and\nreduced semantic alignment. To address this deficiency, we introduce Sparse\nSinkhorn Token Translation (S2T2). S2T2 trains a tailored tokenizer for the\ntarget domain and learns to translate between target and source tokens,\nenabling more effective reuse of the pre-trained next-source-token predictor.\nIn our experiments with finetuned English language models, S2T2 improves both\nthe perplexity and the compression of out-of-domain protein sequences,\noutperforming direct finetuning with either the source or target tokenizer. In\naddition, we find that token translations learned for smaller, less expensive\nmodels can be directly transferred to larger, more powerful models to reap the\nbenefits of S2T2 at lower cost.\n","authors":["Zhili Feng","Tanya Marwah","Nicolo Fusi","David Alvarez-Melis","Lester Mackey"],"pdf_url":"https://arxiv.org/pdf/2411.00593v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03397v1","updated":"2024-11-05T18:31:06Z","published":"2024-11-05T18:31:06Z","title":"SAUCE: Synchronous and Asynchronous User-Customizable Environment for\n  Multi-Agent LLM Interaction","summary":"  Many human interactions, such as political debates, are carried out in group\nsettings, where there are arbitrarily many participants, each with different\nviews and agendas. To explore such complex social settings, we present SAUCE: a\ncustomizable Python platform, allowing researchers to plug-and-play various\nLLMs participating in discussions on any topic chosen by the user. Our platform\ntakes care of instantiating the models, scheduling their responses, managing\nthe discussion history, and producing a comprehensive output log, all\ncustomizable through configuration files, requiring little to no coding skills.\nA novel feature of SAUCE is our asynchronous communication feature, where\nmodels decide when to speak in addition to what to say, thus modeling an\nimportant facet of human communication. We show SAUCE's attractiveness in two\ninitial experiments, and invite the community to use it in simulating various\ngroup simulations.\n","authors":["Shlomo Neuberger","Niv Eckhaus","Uri Berger","Amir Taubenfeld","Gabriel Stanovsky","Ariel Goldstein"],"pdf_url":"https://arxiv.org/pdf/2411.03397v1.pdf","comment":"https://github.com/Deep-Cognition-Lab/SAUCE"},{"id":"http://arxiv.org/abs/2411.03395v1","updated":"2024-11-05T18:30:13Z","published":"2024-11-05T18:30:13Z","title":"Exploring Large Language Models for Specialist-level Oncology Care","summary":"  Large language models (LLMs) have shown remarkable progress in encoding\nclinical knowledge and responding to complex medical queries with appropriate\nclinical reasoning. However, their applicability in subspecialist or complex\nmedical settings remains underexplored. In this work, we probe the performance\nof AMIE, a research conversational diagnostic AI system, in the subspecialist\ndomain of breast oncology care without specific fine-tuning to this challenging\ndomain. To perform this evaluation, we curated a set of 50 synthetic breast\ncancer vignettes representing a range of treatment-naive and\ntreatment-refractory cases and mirroring the key information available to a\nmultidisciplinary tumor board for decision-making (openly released with this\nwork). We developed a detailed clinical rubric for evaluating management plans,\nincluding axes such as the quality of case summarization, safety of the\nproposed care plan, and recommendations for chemotherapy, radiotherapy, surgery\nand hormonal therapy. To improve performance, we enhanced AMIE with the\ninference-time ability to perform web search retrieval to gather relevant and\nup-to-date clinical knowledge and refine its responses with a multi-stage\nself-critique pipeline. We compare response quality of AMIE with internal\nmedicine trainees, oncology fellows, and general oncology attendings under both\nautomated and specialist clinician evaluations. In our evaluations, AMIE\noutperformed trainees and fellows demonstrating the potential of the system in\nthis challenging and important domain. We further demonstrate through\nqualitative examples, how systems such as AMIE might facilitate conversational\ninteractions to assist clinicians in their decision making. However, AMIE's\nperformance was overall inferior to attending oncologists suggesting that\nfurther research is needed prior to consideration of prospective uses.\n","authors":["Anil Palepu","Vikram Dhillon","Polly Niravath","Wei-Hung Weng","Preethi Prasad","Khaled Saab","Ryutaro Tanno","Yong Cheng","Hanh Mai","Ethan Burns","Zainub Ajmal","Kavita Kulkarni","Philip Mansfield","Dale Webster","Joelle Barral","Juraj Gottweis","Mike Schaekermann","S. Sara Mahdavi","Vivek Natarajan","Alan Karthikesalingam","Tao Tu"],"pdf_url":"https://arxiv.org/pdf/2411.03395v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2411.03314v1","updated":"2024-11-05T18:59:51Z","published":"2024-11-05T18:59:51Z","title":"MME-Finance: A Multimodal Finance Benchmark for Expert-level\n  Understanding and Reasoning","summary":"  In recent years, multimodal benchmarks for general domains have guided the\nrapid development of multimodal models on general tasks. However, the financial\nfield has its peculiarities. It features unique graphical images (e.g.,\ncandlestick charts, technical indicator charts) and possesses a wealth of\nspecialized financial knowledge (e.g., futures, turnover rate). Therefore,\nbenchmarks from general fields often fail to measure the performance of\nmultimodal models in the financial domain, and thus cannot effectively guide\nthe rapid development of large financial models. To promote the development of\nlarge financial multimodal models, we propose MME-Finance, an bilingual\nopen-ended and practical usage-oriented Visual Question Answering (VQA)\nbenchmark. The characteristics of our benchmark are finance and expertise,\nwhich include constructing charts that reflect the actual usage needs of users\n(e.g., computer screenshots and mobile photography), creating questions\naccording to the preferences in financial domain inquiries, and annotating\nquestions by experts with 10+ years of experience in the financial industry.\nAdditionally, we have developed a custom-designed financial evaluation system\nin which visual information is first introduced in the multi-modal evaluation\nprocess. Extensive experimental evaluations of 19 mainstream MLLMs are\nconducted to test their perception, reasoning, and cognition capabilities. The\nresults indicate that models performing well on general benchmarks cannot do\nwell on MME-Finance; for instance, the top-performing open-source and\nclosed-source models obtain 65.69 (Qwen2VL-72B) and 63.18 (GPT-4o),\nrespectively. Their performance is particularly poor in categories most\nrelevant to finance, such as candlestick charts and technical indicator charts.\nIn addition, we propose a Chinese version, which helps compare performance of\nMLLMs under a Chinese context.\n","authors":["Ziliang Gan","Yu Lu","Dong Zhang","Haohan Li","Che Liu","Jian Liu","Ji Liu","Haipang Wu","Chaoyou Fu","Zenglin Xu","Rongjunchen Zhang","Yong Dai"],"pdf_url":"https://arxiv.org/pdf/2411.03314v1.pdf","comment":"Project Page: https://hithink-research.github.io/MME-Finance/"},{"id":"http://arxiv.org/abs/2411.03313v1","updated":"2024-11-05T18:58:15Z","published":"2024-11-05T18:58:15Z","title":"Classification Done Right for Vision-Language Pre-Training","summary":"  We introduce SuperClass, a super simple classification method for\nvision-language pre-training on image-text data. Unlike its contrastive\ncounterpart CLIP who contrast with a text encoder, SuperClass directly utilizes\ntokenized raw text as supervised classification labels, without the need for\nadditional text filtering or selection. Due to the absence of the text encoding\nas contrastive target, SuperClass does not require a text encoder and does not\nneed to maintain a large batch size as CLIP does. SuperClass demonstrated\nsuperior performance on various downstream tasks, including classic computer\nvision benchmarks and vision language downstream tasks. We further explored the\nscaling behavior of SuperClass on model size, training length, or data size,\nand reported encouraging results and comparisons to CLIP.\nhttps://github.com/x-cls/superclass\n","authors":["Huang Zilong","Ye Qinghao","Kang Bingyi","Feng Jiashi","Fan Haoqi"],"pdf_url":"https://arxiv.org/pdf/2411.03313v1.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.03312v1","updated":"2024-11-05T18:54:21Z","published":"2024-11-05T18:54:21Z","title":"Inference Optimal VLMs Need Only One Visual Token but Larger Models","summary":"  Vision Language Models (VLMs) have demonstrated strong capabilities across\nvarious visual understanding and reasoning tasks. However, their real-world\ndeployment is often constrained by high latency during inference due to\nsubstantial compute required to process the large number of input tokens\n(predominantly from the image) by the LLM. To reduce inference costs, one can\neither downsize the LLM or reduce the number of input image-tokens, the latter\nof which has been the focus of many recent works around token compression.\nHowever, it is unclear what the optimal trade-off is, as both the factors\ndirectly affect the VLM performance. We first characterize this optimal\ntrade-off between the number of visual tokens and LLM parameters by\nestablishing scaling laws that capture variations in performance with these two\nfactors. Our results reveal a surprising trend: for visual reasoning tasks, the\ninference-optimal behavior in VLMs, i.e., minimum downstream error at any given\nfixed inference compute, is achieved when using the largest LLM that fits\nwithin the inference budget while minimizing visual token count - often to a\nsingle token. While the token reduction literature has mainly focused on\nmaintaining base model performance by modestly reducing the token count (e.g.,\n$5-10\\times$), our results indicate that the compute-optimal inference regime\nrequires operating under even higher token compression ratios. Based on these\ninsights, we take some initial steps towards building approaches tailored for\nhigh token compression settings. Code is available at\nhttps://github.com/locuslab/llava-token-compression.\n","authors":["Kevin Y. Li","Sachin Goyal","Joao D. Semedo","J. Zico Kolter"],"pdf_url":"https://arxiv.org/pdf/2411.03312v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05438v2","updated":"2024-11-05T18:44:55Z","published":"2024-10-07T19:04:24Z","title":"DAAL: Density-Aware Adaptive Line Margin Loss for Multi-Modal Deep\n  Metric Learning","summary":"  Multi-modal deep metric learning is crucial for effectively capturing diverse\nrepresentations in tasks such as face verification, fine-grained object\nrecognition, and product search. Traditional approaches to metric learning,\nwhether based on distance or margin metrics, primarily emphasize class\nseparation, often overlooking the intra-class distribution essential for\nmulti-modal feature learning. In this context, we propose a novel loss function\ncalled Density-Aware Adaptive Margin Loss(DAAL), which preserves the density\ndistribution of embeddings while encouraging the formation of adaptive\nsub-clusters within each class. By employing an adaptive line strategy, DAAL\nnot only enhances intra-class variance but also ensures robust inter-class\nseparation, facilitating effective multi-modal representation. Comprehensive\nexperiments on benchmark fine-grained datasets demonstrate the superior\nperformance of DAAL, underscoring its potential in advancing retrieval\napplications and multi-modal deep metric learning.\n","authors":["Hadush Hailu Gebrerufael","Anil Kumar Tiwari","Gaurav Neupane","Goitom Ybrah Hailu"],"pdf_url":"https://arxiv.org/pdf/2410.05438v2.pdf","comment":"13 pages, 4 fugues, 2 tables"},{"id":"http://arxiv.org/abs/2404.00318v2","updated":"2024-11-05T17:51:36Z","published":"2024-03-30T10:54:59Z","title":"Cognitive Planning for Object Goal Navigation using Generative AI Models","summary":"  Recent advancements in Generative AI, particularly in Large Language Models\n(LLMs) and Large Vision-Language Models (LVLMs), offer new possibilities for\nintegrating cognitive planning into robotic systems. In this work, we present a\nnovel framework for solving the object goal navigation problem that generates\nefficient exploration strategies. Our approach enables a robot to navigate\nunfamiliar environments by leveraging LLMs and LVLMs to understand the semantic\nstructure of the scene. To address the challenge of representing complex\nenvironments without overwhelming the system, we propose a 3D modular scene\nrepresentation, enriched with semantic descriptions. This representation is\ndynamically pruned using an LLM-based mechanism, which filters irrelevant\ninformation and focuses on task-specific data. By combining these elements, our\nsystem generates high-level sub-goals that guide the exploration of the robot\ntoward the target object. We validate our approach in simulated environments,\ndemonstrating its ability to enhance object search efficiency while maintaining\nscalability in complex settings.\n","authors":["Arjun P S","Andrew Melnik","Gora Chand Nandi"],"pdf_url":"https://arxiv.org/pdf/2404.00318v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03286v1","updated":"2024-11-05T17:35:41Z","published":"2024-11-05T17:35:41Z","title":"DiT4Edit: Diffusion Transformer for Image Editing","summary":"  Despite recent advances in UNet-based image editing, methods for shape-aware\nobject editing in high-resolution images are still lacking. Compared to UNet,\nDiffusion Transformers (DiT) demonstrate superior capabilities to effectively\ncapture the long-range dependencies among patches, leading to higher-quality\nimage generation. In this paper, we propose DiT4Edit, the first Diffusion\nTransformer-based image editing framework. Specifically, DiT4Edit uses the\nDPM-Solver inversion algorithm to obtain the inverted latents, reducing the\nnumber of steps compared to the DDIM inversion algorithm commonly used in\nUNet-based frameworks. Additionally, we design unified attention control and\npatches merging, tailored for transformer computation streams. This integration\nallows our framework to generate higher-quality edited images faster. Our\ndesign leverages the advantages of DiT, enabling it to surpass UNet structures\nin image editing, especially in high-resolution and arbitrary-size images.\nExtensive experiments demonstrate the strong performance of DiT4Edit across\nvarious editing scenarios, highlighting the potential of Diffusion Transformers\nin supporting image editing.\n","authors":["Kunyu Feng","Yue Ma","Bingyuan Wang","Chenyang Qi","Haozhe Chen","Qifeng Chen","Zeyu Wang"],"pdf_url":"https://arxiv.org/pdf/2411.03286v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02188v2","updated":"2024-11-05T17:09:54Z","published":"2024-11-04T15:42:22Z","title":"Digi2Real: Bridging the Realism Gap in Synthetic Data Face Recognition\n  via Foundation Models","summary":"  The accuracy of face recognition systems has improved significantly in the\npast few years, thanks to the large amount of data collected and the\nadvancement in neural network architectures. However, these large-scale\ndatasets are often collected without explicit consent, raising ethical and\nprivacy concerns. To address this, there have been proposals to use synthetic\ndatasets for training face recognition models. Yet, such models still rely on\nreal data to train the generative models and generally exhibit inferior\nperformance compared to those trained on real datasets. One of these datasets,\nDigiFace, uses a graphics pipeline to generate different identities and\ndifferent intra-class variations without using real data in training the\nmodels. However, the performance of this approach is poor on face recognition\nbenchmarks, possibly due to the lack of realism in the images generated from\nthe graphics pipeline. In this work, we introduce a novel framework for realism\ntransfer aimed at enhancing the realism of synthetically generated face images.\nOur method leverages the large-scale face foundation model, and we adapt the\npipeline for realism enhancement. By integrating the controllable aspects of\nthe graphics pipeline with our realism enhancement technique, we generate a\nlarge amount of realistic variations-combining the advantages of both\napproaches. Our empirical evaluations demonstrate that models trained using our\nenhanced dataset significantly improve the performance of face recognition\nsystems over the baseline. The source code and datasets will be made available\npublicly.\n","authors":["Anjith George","Sebastien Marcel"],"pdf_url":"https://arxiv.org/pdf/2411.02188v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2411.03260v1","updated":"2024-11-05T16:59:06Z","published":"2024-11-05T16:59:06Z","title":"ShadowMamba: State-Space Model with Boundary-Region Selective Scan for\n  Shadow Removal","summary":"  Image shadow removal is a typical low-level vision problem, where the\npresence of shadows leads to abrupt changes in brightness in certain regions,\naffecting the accuracy of upstream tasks. Current shadow removal methods still\nface challenges such as residual boundary artifacts, and capturing feature\ninformation at shadow boundaries is crucial for removing shadows and\neliminating residual boundary artifacts. Recently, Mamba has achieved\nremarkable success in computer vision by globally modeling long-sequence\ninformation with linear complexity. However, when applied to image shadow\nremoval, the original Mamba scanning method overlooks the semantic continuity\nof shadow boundaries as well as the continuity of semantics within the same\nregion. Based on the unique characteristics of shadow images, this paper\nproposes a novel selective scanning method called boundary-region selective\nscanning. This method scans boundary regions, shadow regions, and non-shadow\nregions independently, bringing pixels of the same region type closer together\nin the long sequence, especially focusing on the local information at the\nboundaries, which is crucial for shadow removal. This method combines with\nglobal scanning and channel scanning to jointly accomplish the shadow removal.\nWe name our model ShadowMamba, the first Mamba-based model for shadow removal.\nExtensive experimental results show that our method outperforms current\nstate-of-the-art models across most metrics on multiple datasets. The code for\nShadowMamba is available at (Code will be released upon acceptance).\n","authors":["Xiujin Zhu","Chee-Onn Chow","Joon Huang Chuah"],"pdf_url":"https://arxiv.org/pdf/2411.03260v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17387v3","updated":"2024-11-05T16:52:39Z","published":"2024-03-26T05:12:18Z","title":"Decoupled Pseudo-labeling for Semi-Supervised Monocular 3D Object\n  Detection","summary":"  We delve into pseudo-labeling for semi-supervised monocular 3D object\ndetection (SSM3OD) and discover two primary issues: a misalignment between the\nprediction quality of 3D and 2D attributes and the tendency of depth\nsupervision derived from pseudo-labels to be noisy, leading to significant\noptimization conflicts with other reliable forms of supervision. We introduce a\nnovel decoupled pseudo-labeling (DPL) approach for SSM3OD. Our approach\nfeatures a Decoupled Pseudo-label Generation (DPG) module, designed to\nefficiently generate pseudo-labels by separately processing 2D and 3D\nattributes. This module incorporates a unique homography-based method for\nidentifying dependable pseudo-labels in BEV space, specifically for 3D\nattributes. Additionally, we present a DepthGradient Projection (DGP) module to\nmitigate optimization conflicts caused by noisy depth supervision of\npseudo-labels, effectively decoupling the depth gradient and removing\nconflicting gradients. This dual decoupling strategy-at both the pseudo-label\ngeneration and gradient levels-significantly improves the utilization of\npseudo-labels in SSM3OD. Our comprehensive experiments on the KITTI benchmark\ndemonstrate the superiority of our method over existing approaches.\n","authors":["Jiacheng Zhang","Jiaming Li","Xiangru Lin","Wei Zhang","Xiao Tan","Junyu Han","Errui Ding","Jingdong Wang","Guanbin Li"],"pdf_url":"https://arxiv.org/pdf/2403.17387v3.pdf","comment":"accepted to CVPR2024"},{"id":"http://arxiv.org/abs/2411.03239v1","updated":"2024-11-05T16:37:30Z","published":"2024-11-05T16:37:30Z","title":"Decoupling Fine Detail and Global Geometry for Compressed Depth Map\n  Super-Resolution","summary":"  Recovering high-quality depth maps from compressed sources has gained\nsignificant attention due to the limitations of consumer-grade depth cameras\nand the bandwidth restrictions during data transmission. However, current\nmethods still suffer from two challenges. First, bit-depth compression produces\na uniform depth representation in regions with subtle variations, hindering the\nrecovery of detailed information. Second, densely distributed random noise\nreduces the accuracy of estimating the global geometric structure of the scene.\nTo address these challenges, we propose a novel framework, termed\ngeometry-decoupled network (GDNet), for compressed depth map super-resolution\nthat decouples the high-quality depth map reconstruction process by handling\nglobal and detailed geometric features separately. To be specific, we propose\nthe fine geometry detail encoder (FGDE), which is designed to aggregate fine\ngeometry details in high-resolution low-level image features while\nsimultaneously enriching them with complementary information from\nlow-resolution context-level image features. In addition, we develop the global\ngeometry encoder (GGE) that aims at suppressing noise and extracting global\ngeometric information effectively via constructing compact feature\nrepresentation in a low-rank space. We conduct experiments on multiple\nbenchmark datasets, demonstrating that our GDNet significantly outperforms\ncurrent methods in terms of geometric consistency and detail recovery. In the\nECCV 2024 AIM Compressed Depth Upsampling Challenge, our solution won the 1st\nplace award. Our codes will be available.\n","authors":["Huan Zheng","Wencheng Han","Jianbing Shen"],"pdf_url":"https://arxiv.org/pdf/2411.03239v1.pdf","comment":"The 1st solution for the ECCV 2024 AIM Compressed Depth Upsampling\n  Challenge"},{"id":"http://arxiv.org/abs/2311.17898v3","updated":"2024-11-05T16:31:24Z","published":"2023-11-29T18:51:46Z","title":"Contextual Knowledge Pursuit for Faithful Visual Synthesis","summary":"  Modern text-to-vision generative models often hallucinate when the prompt\ndescribing the scene to be generated is underspecified. In large language\nmodels (LLMs), a prevalent strategy to reduce hallucinations is to retrieve\nfactual knowledge from an external database. While such retrieval augmentation\nstrategies have great potential to enhance text-to-vision generators, existing\nstatic top-K retrieval methods explore the knowledge pool once, missing the\nbroader context necessary for high-quality generation. Furthermore, LLMs\ninternally possess rich world knowledge learned during large-scale training\n(parametric knowledge) that could mitigate the need for external data\nretrieval. This paper proposes Contextual Knowledge Pursuit (CKPT), a framework\nthat leverages the complementary strengths of external and parametric knowledge\nto help generators produce reliable visual content. Instead of the one-time\nretrieval of facts from an external database to improve a given prompt, CKPT\nuses (1) an LLM to decide whether to seek external knowledge or to self-elicit\ndescriptions from LLM parametric knowledge, (2) a knowledge pursuit process to\ncontextually seek and sequentially gather most relevant facts, (3) a knowledge\naggregator for prompt enhancement with the gathered fact context, and (4) a\nfiltered fine-tuning objective to improve visual synthesis with richer prompts.\nWe evaluate CKPT across multiple text-driven generative tasks (image, 3D\nrendering, and video) on datasets of rare objects and daily scenarios. Our\nresults show that CKPT is capable of generating faithful and semantically rich\ncontent across diverse visual domains, offering a promising data source for\nzero-shot synthesis and filtered fine-tuning of text-to-vision generative\nmodels.\n","authors":["Jinqi Luo","Kwan Ho Ryan Chan","Dimitris Dimos","René Vidal"],"pdf_url":"https://arxiv.org/pdf/2311.17898v3.pdf","comment":"Accepted in ECCV 2024 SDCV Workshop. GitHub repository at\n  https://github.com/peterljq/Contextual-Knowledge-Pursuit"},{"id":"http://arxiv.org/abs/2409.18336v2","updated":"2024-11-05T16:30:30Z","published":"2024-09-26T23:18:25Z","title":"DeBaRA: Denoising-Based 3D Room Arrangement Generation","summary":"  Generating realistic and diverse layouts of furnished indoor 3D scenes\nunlocks multiple interactive applications impacting a wide range of industries.\nThe inherent complexity of object interactions, the limited amount of available\ndata and the requirement to fulfill spatial constraints all make generative\nmodeling for 3D scene synthesis and arrangement challenging. Current methods\naddress these challenges autoregressively or by using off-the-shelf diffusion\nobjectives by simultaneously predicting all attributes without 3D reasoning\nconsiderations. In this paper, we introduce DeBaRA, a score-based model\nspecifically tailored for precise, controllable and flexible arrangement\ngeneration in a bounded environment. We argue that the most critical component\nof a scene synthesis system is to accurately establish the size and position of\nvarious objects within a restricted area. Based on this insight, we propose a\nlightweight conditional score-based model designed with 3D spatial awareness at\nits core. We demonstrate that by focusing on spatial attributes of objects, a\nsingle trained DeBaRA model can be leveraged at test time to perform several\ndownstream applications such as scene synthesis, completion and re-arrangement.\nFurther, we introduce a novel Self Score Evaluation procedure so it can be\noptimally employed alongside external LLM models. We evaluate our approach\nthrough extensive experiments and demonstrate significant improvement upon\nstate-of-the-art approaches in a range of scenarios.\n","authors":["Léopold Maillard","Nicolas Sereyjol-Garros","Tom Durand","Maks Ovsjanikov"],"pdf_url":"https://arxiv.org/pdf/2409.18336v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2311.16484v2","updated":"2024-11-05T16:25:05Z","published":"2023-11-26T05:14:06Z","title":"Seeing Eye to AI: Comparing Human Gaze and Model Attention in Video\n  Memorability","summary":"  Understanding what makes a video memorable has important applications in\nadvertising or education technology. Towards this goal, we investigate\nspatio-temporal attention mechanisms underlying video memorability. Different\nfrom previous works that fuse multiple features, we adopt a simple\nCNN+Transformer architecture that enables analysis of spatio-temporal attention\nwhile matching state-of-the-art (SoTA) performance on video memorability\nprediction. We compare model attention against human gaze fixations collected\nthrough a small-scale eye-tracking study where humans perform the video memory\ntask. We uncover the following insights: (i) Quantitative saliency metrics show\nthat our model, trained only to predict a memorability score, exhibits similar\nspatial attention patterns to human gaze, especially for more memorable videos.\n(ii) The model assigns greater importance to initial frames in a video,\nmimicking human attention patterns. (iii) Panoptic segmentation reveals that\nboth (model and humans) assign a greater share of attention to things and less\nattention to stuff as compared to their occurrence probability.\n","authors":["Prajneya Kumar","Eshika Khandelwal","Makarand Tapaswi","Vishnu Sreekumar"],"pdf_url":"https://arxiv.org/pdf/2311.16484v2.pdf","comment":"Accepted to WACV 2025"},{"id":"http://arxiv.org/abs/2411.03228v1","updated":"2024-11-05T16:20:14Z","published":"2024-11-05T16:20:14Z","title":"Topograph: An efficient Graph-Based Framework for Strictly Topology\n  Preserving Image Segmentation","summary":"  Topological correctness plays a critical role in many image segmentation\ntasks, yet most networks are trained using pixel-wise loss functions, such as\nDice, neglecting topological accuracy. Existing topology-aware methods often\nlack robust topological guarantees, are limited to specific use cases, or\nimpose high computational costs. In this work, we propose a novel, graph-based\nframework for topologically accurate image segmentation that is both\ncomputationally efficient and generally applicable. Our method constructs a\ncomponent graph that fully encodes the topological information of both the\nprediction and ground truth, allowing us to efficiently identify topologically\ncritical regions and aggregate a loss based on local neighborhood information.\nFurthermore, we introduce a strict topological metric capturing the homotopy\nequivalence between the union and intersection of prediction-label pairs. We\nformally prove the topological guarantees of our approach and empirically\nvalidate its effectiveness on binary and multi-class datasets. Our loss\ndemonstrates state-of-the-art performance with up to fivefold faster loss\ncomputation compared to persistent homology methods.\n","authors":["Laurin Lux","Alexander H. Berger","Alexander Weers","Nico Stucki","Daniel Rueckert","Ulrich Bauer","Johannes C. Paetzold"],"pdf_url":"https://arxiv.org/pdf/2411.03228v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03226v1","updated":"2024-11-05T16:18:57Z","published":"2024-11-05T16:18:57Z","title":"Kernel Orthogonality does not necessarily imply a Decrease in Feature\n  Map Redundancy in CNNs: Convolutional Similarity Minimization","summary":"  Convolutional Neural Networks (CNNs) have been heavily used in Deep Learning\ndue to their success in various tasks. Nonetheless, it has been observed that\nCNNs suffer from redundancy in feature maps, leading to inefficient capacity\nutilization. Efforts to mitigate and solve this problem led to the emergence of\nmultiple methods, amongst which is kernel orthogonality through variant means.\nIn this work, we challenge the common belief that kernel orthogonality leads to\na decrease in feature map redundancy, which is, supposedly, the ultimate\nobjective behind kernel orthogonality. We prove, theoretically and empirically,\nthat kernel orthogonality has an unpredictable effect on feature map similarity\nand does not necessarily decrease it. Based on our theoretical result, we\npropose an effective method to reduce feature map similarity independently of\nthe input of the CNN. This is done by minimizing a novel loss function we call\nConvolutional Similarity. Empirical results show that minimizing the\nConvolutional Similarity increases the performance of classification models and\ncan accelerate their convergence. Furthermore, using our proposed method pushes\ntowards a more efficient use of the capacity of models, allowing the use of\nsignificantly smaller models to achieve the same levels of performance.\n","authors":["Zakariae Belmekki","Jun Li","Patrick Reuter","David Antonio Gómez Jáuregui","Karl Jenkins"],"pdf_url":"https://arxiv.org/pdf/2411.03226v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24010v2","updated":"2024-11-05T16:16:29Z","published":"2024-10-31T15:10:38Z","title":"Re-assembling the past: The RePAIR dataset and benchmark for real world\n  2D and 3D puzzle solving","summary":"  This paper proposes the RePAIR dataset that represents a challenging\nbenchmark to test modern computational and data driven methods for\npuzzle-solving and reassembly tasks. Our dataset has unique properties that are\nuncommon to current benchmarks for 2D and 3D puzzle solving. The fragments and\nfractures are realistic, caused by a collapse of a fresco during a World War II\nbombing at the Pompeii archaeological park. The fragments are also eroded and\nhave missing pieces with irregular shapes and different dimensions, challenging\nfurther the reassembly algorithms. The dataset is multi-modal providing high\nresolution images with characteristic pictorial elements, detailed 3D scans of\nthe fragments and meta-data annotated by the archaeologists. Ground truth has\nbeen generated through several years of unceasing fieldwork, including the\nexcavation and cleaning of each fragment, followed by manual puzzle solving by\narchaeologists of a subset of approx. 1000 pieces among the 16000 available.\nAfter digitizing all the fragments in 3D, a benchmark was prepared to challenge\ncurrent reassembly and puzzle-solving methods that often solve more simplistic\nsynthetic scenarios. The tested baselines show that there clearly exists a gap\nto fill in solving this computationally complex problem.\n","authors":["Theodore Tsesmelis","Luca Palmieri","Marina Khoroshiltseva","Adeela Islam","Gur Elkin","Ofir Itzhak Shahar","Gianluca Scarpellini","Stefano Fiorini","Yaniv Ohayon","Nadav Alali","Sinem Aslan","Pietro Morerio","Sebastiano Vascon","Elena Gravina","Maria Cristina Napolitano","Giuseppe Scarpati","Gabriel Zuchtriegel","Alexandra Spühler","Michel E. Fuchs","Stuart James","Ohad Ben-Shahar","Marcello Pelillo","Alessio Del Bue"],"pdf_url":"https://arxiv.org/pdf/2410.24010v2.pdf","comment":"NeurIPS 2024, Track Datasets and Benchmarks, 10 pages"},{"id":"http://arxiv.org/abs/2411.03225v1","updated":"2024-11-05T16:15:33Z","published":"2024-11-05T16:15:33Z","title":"Knowledge Graphs of Driving Scenes to Empower the Emerging Capabilities\n  of Neurosymbolic AI","summary":"  In the era of Generative AI, Neurosymbolic AI is emerging as a powerful\napproach for tasks spanning from perception to cognition. The use of\nNeurosymbolic AI has been shown to achieve enhanced capabilities, including\nimproved grounding, alignment, explainability, and reliability. However, due to\nits nascent stage, there is a lack of widely available real-world benchmark\ndatasets tailored to Neurosymbolic AI tasks. To address this gap and support\nthe evaluation of current and future methods, we introduce DSceneKG -- a suite\nof knowledge graphs of driving scenes built from real-world, high-quality\nscenes from multiple open autonomous driving datasets. In this article, we\ndetail the construction process of DSceneKG and highlight its application in\nseven different tasks. DSceneKG is publicly accessible at:\nhttps://github.com/ruwantw/DSceneKG\n","authors":["Ruwan Wickramarachchi","Cory Henson","Amit Sheth"],"pdf_url":"https://arxiv.org/pdf/2411.03225v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2411.03223v1","updated":"2024-11-05T16:12:12Z","published":"2024-11-05T16:12:12Z","title":"Beyond Grid Data: Exploring Graph Neural Networks for Earth Observation","summary":"  Earth Observation (EO) data analysis has been significantly revolutionized by\ndeep learning (DL), with applications typically limited to grid-like data\nstructures. Graph Neural Networks (GNNs) emerge as an important innovation,\npropelling DL into the non-Euclidean domain. Naturally, GNNs can effectively\ntackle the challenges posed by diverse modalities, multiple sensors, and the\nheterogeneous nature of EO data. To introduce GNNs in the related domains, our\nreview begins by offering fundamental knowledge on GNNs. Then, we summarize the\ngeneric problems in EO, to which GNNs can offer potential solutions. Following\nthis, we explore a broad spectrum of GNNs' applications to scientific problems\nin Earth systems, covering areas such as weather and climate analysis, disaster\nmanagement, air quality monitoring, agriculture, land cover classification,\nhydrological process modeling, and urban modeling. The rationale behind\nadopting GNNs in these fields is explained, alongside methodologies for\norganizing graphs and designing favorable architectures for various tasks.\nFurthermore, we highlight methodological challenges of implementing GNNs in\nthese domains and possible solutions that could guide future research. While\nacknowledging that GNNs are not a universal solution, we conclude the paper by\ncomparing them with other popular architectures like transformers and analyzing\ntheir potential synergies.\n","authors":["Shan Zhao","Zhaiyu Chen","Zhitong Xiong","Yilei Shi","Sudipan Saha","Xiao Xiang Zhu"],"pdf_url":"https://arxiv.org/pdf/2411.03223v1.pdf","comment":"Accepted for publication in Geoscience and Remote Sensing Magazine\n  (GRSM)"},{"id":"http://arxiv.org/abs/2403.09918v4","updated":"2024-11-05T15:37:00Z","published":"2024-03-14T23:31:41Z","title":"Attention-based Class-Conditioned Alignment for Multi-Source Domain\n  Adaptation of Object Detectors","summary":"  Domain adaptation methods for object detection (OD) strive to mitigate the\nimpact of distribution shifts by promoting feature alignment across source and\ntarget domains. Multi-source domain adaptation (MSDA) allows leveraging\nmultiple annotated source datasets and unlabeled target data to improve the\naccuracy and robustness of the detection model. Most state-of-the-art MSDA\nmethods for OD perform feature alignment in a class-agnostic manner. This is\nchallenging since the objects have unique modal information due to variations\nin object appearance across domains. A recent prototype-based approach proposed\na class-wise alignment, yet it suffers from error accumulation due to noisy\npseudo-labels that can negatively affect adaptation with imbalanced data. To\novercome these limitations, we propose an attention-based class-conditioned\nalignment method for MSDA that aligns instances of each object category across\ndomains. In particular, an attention module coupled with an adversarial domain\nclassifier allows learning domain-invariant and class-specific instance\nrepresentations. Experimental results on multiple benchmarking MSDA datasets\nindicate that our method outperforms the state-of-the-art methods and is robust\nto class imbalance using a conceptually simple class-conditioning method. Our\ncode is available at https://github.com/imatif17/ACIA.\n","authors":["Atif Belal","Akhil Meethal","Francisco Perdigon Romero","Marco Pedersoli","Eric Granger"],"pdf_url":"https://arxiv.org/pdf/2403.09918v4.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2309.14950"},{"id":"http://arxiv.org/abs/2411.03177v1","updated":"2024-11-05T15:22:26Z","published":"2024-11-05T15:22:26Z","title":"On Improved Conditioning Mechanisms and Pre-training Strategies for\n  Diffusion Models","summary":"  Large-scale training of latent diffusion models (LDMs) has enabled\nunprecedented quality in image generation. However, the key components of the\nbest performing LDM training recipes are oftentimes not available to the\nresearch community, preventing apple-to-apple comparisons and hindering the\nvalidation of progress in the field. In this work, we perform an in-depth study\nof LDM training recipes focusing on the performance of models and their\ntraining efficiency. To ensure apple-to-apple comparisons, we re-implement five\npreviously published models with their corresponding recipes. Through our\nstudy, we explore the effects of (i)~the mechanisms used to condition the\ngenerative model on semantic information (e.g., text prompt) and control\nmetadata (e.g., crop size, random flip flag, etc.) on the model performance,\nand (ii)~the transfer of the representations learned on smaller and\nlower-resolution datasets to larger ones on the training efficiency and model\nperformance. We then propose a novel conditioning mechanism that disentangles\nsemantic and control metadata conditionings and sets a new state-of-the-art in\nclass-conditional generation on the ImageNet-1k dataset -- with FID\nimprovements of 7% on 256 and 8% on 512 resolutions -- as well as text-to-image\ngeneration on the CC12M dataset -- with FID improvements of 8% on 256 and 23%\non 512 resolution.\n","authors":["Tariq Berrada Ifriqi","Pietro Astolfi","Melissa Hall","Reyhane Askari-Hemmat","Yohann Benchetrit","Marton Havasi","Matthew Muckley","Karteek Alahari","Adriana Romero-Soriano","Jakob Verbeek","Michal Drozdzal"],"pdf_url":"https://arxiv.org/pdf/2411.03177v1.pdf","comment":"Accepted as a conference paper (poster) for NeurIPS 2024"},{"id":"http://arxiv.org/abs/2309.05756v3","updated":"2024-11-05T15:18:15Z","published":"2023-09-11T18:35:14Z","title":"GlobalDoc: A Cross-Modal Vision-Language Framework for Real-World\n  Document Image Retrieval and Classification","summary":"  Visual document understanding (VDU) has rapidly advanced with the development\nof powerful multi-modal language models. However, these models typically\nrequire extensive document pre-training data to learn intermediate\nrepresentations and often suffer a significant performance drop in real-world\nonline industrial settings. A primary issue is their heavy reliance on OCR\nengines to extract local positional information within document pages, which\nlimits the models' ability to capture global information and hinders their\ngeneralizability, flexibility, and robustness. In this paper, we introduce\nGlobalDoc, a cross-modal transformer-based architecture pre-trained in a\nself-supervised manner using three novel pretext objective tasks. GlobalDoc\nimproves the learning of richer semantic concepts by unifying language and\nvisual representations, resulting in more transferable models. For proper\nevaluation, we also propose two novel document-level downstream VDU tasks,\nFew-Shot Document Image Classification (DIC) and Content-based Document Image\nRetrieval (DIR), designed to simulate industrial scenarios more closely.\nExtensive experimentation has been conducted to demonstrate GlobalDoc's\neffectiveness in practical settings.\n","authors":["Souhail Bakkali","Sanket Biswas","Zuheng Ming","Mickaël Coustaty","Marçal Rusiñol","Oriol Ramos Terrades","Josep Lladós"],"pdf_url":"https://arxiv.org/pdf/2309.05756v3.pdf","comment":"Accepted at WACV 2025"},{"id":"http://arxiv.org/abs/2411.03169v1","updated":"2024-11-05T15:18:02Z","published":"2024-11-05T15:18:02Z","title":"Pre-trained Visual Dynamics Representations for Efficient Policy\n  Learning","summary":"  Pre-training for Reinforcement Learning (RL) with purely video data is a\nvaluable yet challenging problem. Although in-the-wild videos are readily\navailable and inhere a vast amount of prior world knowledge, the absence of\naction annotations and the common domain gap with downstream tasks hinder\nutilizing videos for RL pre-training. To address the challenge of pre-training\nwith videos, we propose Pre-trained Visual Dynamics Representations (PVDR) to\nbridge the domain gap between videos and downstream tasks for efficient policy\nlearning. By adopting video prediction as a pre-training task, we use a\nTransformer-based Conditional Variational Autoencoder (CVAE) to learn visual\ndynamics representations. The pre-trained visual dynamics representations\ncapture the visual dynamics prior knowledge in the videos. This abstract prior\nknowledge can be readily adapted to downstream tasks and aligned with\nexecutable actions through online adaptation. We conduct experiments on a\nseries of robotics visual control tasks and verify that PVDR is an effective\nform for pre-training with videos to promote policy learning.\n","authors":["Hao Luo","Bohan Zhou","Zongqing Lu"],"pdf_url":"https://arxiv.org/pdf/2411.03169v1.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2410.02401v5","updated":"2024-11-05T14:38:30Z","published":"2024-10-03T11:29:09Z","title":"SynCo: Synthetic Hard Negatives in Contrastive Learning for Better\n  Unsupervised Visual Representations","summary":"  Contrastive learning has become a dominant approach in self-supervised visual\nrepresentation learning. Hard negatives - samples closely resembling the anchor\n- are key to enhancing learned representations' discriminative power. However,\nefficiently leveraging hard negatives remains challenging. We introduce SynCo\n(Synthetic Negatives in Contrastive learning), a novel approach that improves\nmodel performance by generating synthetic hard negatives on the representation\nspace. Building on the MoCo framework, SynCo introduces six strategies for\ncreating diverse synthetic hard negatives on-the-fly with minimal computational\noverhead. SynCo achieves faster training and better representation learning,\nreaching 67.9% top-1 accuracy on ImageNet ILSVRC-2012 linear evaluation after\n200 pretraining epochs, surpassing MoCo's 67.5% using the same ResNet-50\nencoder. It also transfers more effectively to detection tasks: on PASCAL VOC,\nit outperforms both the supervised baseline and MoCo with 82.5% AP; on COCO, it\nsets new benchmarks with 40.9% AP for bounding box detection and 35.5% AP for\ninstance segmentation. Our synthetic hard negative generation approach\nsignificantly enhances visual representations learned through self-supervised\ncontrastive learning. Code is available at\nhttps://github.com/giakoumoglou/synco.\n","authors":["Nikolaos Giakoumoglou","Tania Stathaki"],"pdf_url":"https://arxiv.org/pdf/2410.02401v5.pdf","comment":"10 pages, 5 figures, 4 tables"},{"id":"http://arxiv.org/abs/2411.02293v2","updated":"2024-11-05T14:33:41Z","published":"2024-11-04T17:21:42Z","title":"Tencent Hunyuan3D-1.0: A Unified Framework for Text-to-3D and\n  Image-to-3D Generation","summary":"  While 3D generative models have greatly improved artists' workflows, the\nexisting diffusion models for 3D generation suffer from slow generation and\npoor generalization. To address this issue, we propose a two-stage approach\nnamed Hunyuan3D-1.0 including a lite version and a standard version, that both\nsupport text- and image-conditioned generation. In the first stage, we employ a\nmulti-view diffusion model that efficiently generates multi-view RGB in\napproximately 4 seconds. These multi-view images capture rich details of the 3D\nasset from different viewpoints, relaxing the tasks from single-view to\nmulti-view reconstruction. In the second stage, we introduce a feed-forward\nreconstruction model that rapidly and faithfully reconstructs the 3D asset\ngiven the generated multi-view images in approximately 7 seconds. The\nreconstruction network learns to handle noises and in-consistency introduced by\nthe multi-view diffusion and leverages the available information from the\ncondition image to efficiently recover the 3D structure. Our framework involves\nthe text-to-image model, i.e., Hunyuan-DiT, making it a unified framework to\nsupport both text- and image-conditioned 3D generation. Our standard version\nhas 3x more parameters than our lite and other existing model. Our\nHunyuan3D-1.0 achieves an impressive balance between speed and quality,\nsignificantly reducing generation time while maintaining the quality and\ndiversity of the produced assets.\n","authors":["Xianghui Yang","Huiwen Shi","Bowen Zhang","Fan Yang","Jiacheng Wang","Hongxu Zhao","Xinhai Liu","Xinzhou Wang","Qingxiang Lin","Jiaao Yu","Lifu Wang","Zhuo Chen","Sicong Liu","Yuhong Liu","Yong Yang","Di Wang","Jie Jiang","Chunchao Guo"],"pdf_url":"https://arxiv.org/pdf/2411.02293v2.pdf","comment":"Technical Report; 3D Generation"},{"id":"http://arxiv.org/abs/2411.03129v1","updated":"2024-11-05T14:21:01Z","published":"2024-11-05T14:21:01Z","title":"MA^2: A Self-Supervised and Motion Augmenting Autoencoder for Gait-Based\n  Automatic Disease Detection","summary":"  Ground reaction force (GRF) is the force exerted by the ground on a body in\ncontact with it. GRF-based automatic disease detection (ADD) has become an\nemerging medical diagnosis method, which aims to learn and identify disease\npatterns corresponding to different gait pressures based on deep learning\nmethods. Although existing ADD methods can save doctors time in making\ndiagnoses, training deep models still struggles with the cost caused by the\nlabeling engineering for a large number of gait diagnostic data for subjects.\nOn the other hand, the accuracy of the deep model under the unified benchmark\nGRF dataset and the generalization ability on scalable gait datasets need to be\nfurther improved. To address these issues, we propose MA2, a GRF-based\nself-supervised and motion augmenting auto-encoder, which models the ADD task\nas an encoder-decoder paradigm. In the encoder, we introduce an embedding block\nincluding the 3-layer 1D convolution for extracting the token and a mask\ngenerator to randomly mask out the sequence of tokens to maximize the model's\npotential to capture high-level, discriminative, intrinsic representations.\nwhereafter, the decoder utilizes this information to reconstruct the pixel\nsequence of the origin input and calculate the reconstruction loss to optimize\nthe network. Moreover, the backbone of an auto-encoder is multi-head\nself-attention that can consider the global information of the token from the\ninput, not just the local neighborhood. This allows the model to capture\ngeneralized contextual information. Extensive experiments demonstrate MA2 has\nSOTA performance of 90.91% accuracy on 1% limited pathological GRF samples with\nlabels, and good generalization ability of 78.57% accuracy on scalable\nParkinson disease dataset.\n","authors":["Yiqun Liu","Ke Zhang","Yin Zhu"],"pdf_url":"https://arxiv.org/pdf/2411.03129v1.pdf","comment":"8 pages, 11 figures, article"},{"id":"http://arxiv.org/abs/2410.20595v3","updated":"2024-11-05T14:13:56Z","published":"2024-10-27T21:02:37Z","title":"A Framework for Real-Time Volcano-Seismic Event Recognition Based on\n  Multi-Station Seismograms and Semantic Segmentation Models","summary":"  In volcano monitoring, effective recognition of seismic events is essential\nfor understanding volcanic activity and raising timely warning alerts.\nTraditional methods rely on manual analysis, which can be subjective and\nlabor-intensive. Furthermore, current automatic approaches often tackle\ndetection and classification separately, mostly rely on single station\ninformation and generally require tailored preprocessing and representations to\nperform predictions. These limitations often hinder their application to\nreal-time monitoring and utilization across different volcano conditions. This\nstudy introduces a novel approach that utilizes Semantic Segmentation models to\nautomate seismic event recognition by applying a straight forward\ntransformation of multi-channel 1D signals into 2D representations, enabling\ntheir use as images. Our framework employs a data-driven, end-to-end design\nthat integrates multi-station seismic data with minimal preprocessing,\nperforming both detection and classification simultaneously for five seismic\nevent classes. We evaluated four state-of-the-art segmentation models (UNet,\nUNet++, DeepLabV3+ and SwinUNet) on approximately 25.000 seismic events\nrecorded at four different Chilean volcanoes: Nevados del Chill\\'an Volcanic\nComplex, Laguna del Maule, Villarrica and Puyehue-Cord\\'on Caulle. Among these\nmodels, the UNet architecture was identified as the most effective model,\nachieving mean F1 and Intersection over Union (IoU) scores of up to 0.91 and\n0.88, respectively, and demonstrating superior noise robustness and model\nflexibility to unseen volcano datasets.\n","authors":["Camilo Espinosa-Curilem","Millaray Curilem","Daniel Basualto"],"pdf_url":"https://arxiv.org/pdf/2410.20595v3.pdf","comment":"10 pages, 9 figures. This is a pre-print, it is currently under\n  review for publication"},{"id":"http://arxiv.org/abs/2312.05327v3","updated":"2024-11-05T14:12:40Z","published":"2023-12-08T19:24:05Z","title":"Better, Not Just More: Data-Centric Machine Learning for Earth\n  Observation","summary":"  Recent developments and research in modern machine learning have led to\nsubstantial improvements in the geospatial field. Although numerous deep\nlearning architectures and models have been proposed, the majority of them have\nbeen solely developed on benchmark datasets that lack strong real-world\nrelevance. Furthermore, the performance of many methods has already saturated\non these datasets. We argue that a shift from a model-centric view to a\ncomplementary data-centric perspective is necessary for further improvements in\naccuracy, generalization ability, and real impact on end-user applications.\nFurthermore, considering the entire machine learning cycle-from problem\ndefinition to model deployment with feedback-is crucial for enhancing machine\nlearning models that can be reliable in unforeseen situations. This work\npresents a definition as well as a precise categorization and overview of\nautomated data-centric learning approaches for geospatial data. It highlights\nthe complementary role of data-centric learning with respect to model-centric\nin the larger machine learning deployment cycle. We review papers across the\nentire geospatial field and categorize them into different groups. A set of\nrepresentative experiments shows concrete implementation examples. These\nexamples provide concrete steps to act on geospatial data with data-centric\nmachine learning approaches.\n","authors":["Ribana Roscher","Marc Rußwurm","Caroline Gevaert","Michael Kampffmeyer","Jefersson A. dos Santos","Maria Vakalopoulou","Ronny Hänsch","Stine Hansen","Keiller Nogueira","Jonathan Prexl","Devis Tuia"],"pdf_url":"https://arxiv.org/pdf/2312.05327v3.pdf","comment":"Accepted to Geoscience and Remote Sensing Magazine"},{"id":"http://arxiv.org/abs/2411.03114v1","updated":"2024-11-05T14:03:36Z","published":"2024-11-05T14:03:36Z","title":"Investigating the Applicability of a Snapshot Computed Tomography\n  Imaging Spectrometer for the Prediction of Brix and pH of Grapes","summary":"  In this paper, a recently developed snapshot hyperspectral imaging (HSI)\nsystem based on Computed Tomography Imaging Spectroscopy (CTIS) is utilized to\ndetermine Brix and pH values in Sheegene 20 table grapes through Partial Least\nSquares Regression (PLSR) modeling. The performance of the CTIS system is\ncompared with that of a state-of-the-art line scan HSI system by imaging 100\ngrapes across both platforms. Reference measurements of Brix and pH values are\nobtained directly using a refractometer and a pH meter, as these parameters are\nessential for assessing the quality of table and wine grapes. The findings\nindicate that the spectra captured by the CTIS camera correlate well with the\nreference measurements, despite the system's narrower spectral range. The CTIS\ncamera's advantages, including its lower cost, portability, and reduced\nsusceptibility to motion errors, highlight its potential for promising in-field\napplications in grape quality assessment.\n","authors":["Mads Svanborg Peters","Mads Juul Ahlebæk","Mads Toudal Frandsen","Bjarke Jørgensen","Christian Hald Jessen","Andreas Krogh Carlsen","Wei-Chih Huang","René Lynge Eriksen"],"pdf_url":"https://arxiv.org/pdf/2411.03114v1.pdf","comment":"15 pages, 10 figures"},{"id":"http://arxiv.org/abs/2401.17789v3","updated":"2024-11-05T14:00:12Z","published":"2024-01-31T12:32:17Z","title":"Robustly overfitting latents for flexible neural image compression","summary":"  Neural image compression has made a great deal of progress. State-of-the-art\nmodels are based on variational autoencoders and are outperforming classical\nmodels. Neural compression models learn to encode an image into a quantized\nlatent representation that can be efficiently sent to the decoder, which\ndecodes the quantized latent into a reconstructed image. While these models\nhave proven successful in practice, they lead to sub-optimal results due to\nimperfect optimization and limitations in the encoder and decoder capacity.\nRecent work shows how to use stochastic Gumbel annealing (SGA) to refine the\nlatents of pre-trained neural image compression models. We extend this idea by\nintroducing SGA+, which contains three different methods that build upon SGA.\nWe show how our method improves the overall compression performance in terms of\nthe R-D trade-off, compared to its predecessors. Additionally, we show how\nrefinement of the latents with our best-performing method improves the\ncompression performance on both the Tecnick and CLIC dataset. Our method is\ndeployed for a pre-trained hyperprior and for a more flexible model. Further,\nwe give a detailed analysis of our proposed methods and show that they are less\nsensitive to hyperparameter choices. Finally, we show how each method can be\nextended to three- instead of two-class rounding.\n","authors":["Yura Perugachi-Diaz","Arwin Gansekoele","Sandjai Bhulai"],"pdf_url":"https://arxiv.org/pdf/2401.17789v3.pdf","comment":"Accepted at Neural Information Processing Systems (NeurIPS) 2024"},{"id":"http://arxiv.org/abs/2411.03098v1","updated":"2024-11-05T13:44:25Z","published":"2024-11-05T13:44:25Z","title":"Local Lesion Generation is Effective for Capsule Endoscopy Image Data\n  Augmentation in a Limited Data Setting","summary":"  Limited medical imaging datasets challenge deep learning models by increasing\nrisks of overfitting and reduced generalization, particularly in Generative\nAdversarial Networks (GANs), where discriminators may overfit, leading to\ntraining divergence. This constraint also impairs classification models trained\non small datasets. Generative Data Augmentation (GDA) addresses this by\nexpanding training datasets with synthetic data, although it requires training\na generative model. We propose and evaluate two local lesion generation\napproaches to address the challenge of augmenting small medical image datasets.\nThe first approach employs the Poisson Image Editing algorithm, a classical\nimage processing technique, to create realistic image composites that\noutperform current state-of-the-art methods. The second approach introduces a\nnovel generative method, leveraging a fine-tuned Image Inpainting GAN to\nsynthesize realistic lesions within specified regions of real training images.\nA comprehensive comparison of the two proposed methods demonstrates that\neffective local lesion generation in a data-constrained setting allows for\nreaching new state-of-the-art results in capsule endoscopy lesion\nclassification. Combination of our techniques achieves a macro F1-score of\n33.07%, surpassing the previous best result by 7.84 percentage points (p.p.) on\nthe highly imbalanced Kvasir Capsule Dataset, a benchmark for capsule\nendoscopy. To the best of our knowledge, this work is the first to apply a\nfine-tuned Image Inpainting GAN for GDA in medical imaging, demonstrating that\nan image-conditional GAN can be adapted effectively to limited datasets to\ngenerate high-quality examples, facilitating effective data augmentation.\nAdditionally, we show that combining this GAN-based approach with classical\nimage processing techniques further enhances the results.\n","authors":["Adrian B. Chłopowiec","Adam R. Chłopowiec","Krzysztof Galus","Wojciech Cebula","Martin Tabakov"],"pdf_url":"https://arxiv.org/pdf/2411.03098v1.pdf","comment":"45 pages, 27 figures"},{"id":"http://arxiv.org/abs/2411.03086v1","updated":"2024-11-05T13:31:04Z","published":"2024-11-05T13:31:04Z","title":"HFGaussian: Learning Generalizable Gaussian Human with Integrated Human\n  Features","summary":"  Recent advancements in radiance field rendering show promising results in 3D\nscene representation, where Gaussian splatting-based techniques emerge as\nstate-of-the-art due to their quality and efficiency. Gaussian splatting is\nwidely used for various applications, including 3D human representation.\nHowever, previous 3D Gaussian splatting methods either use parametric body\nmodels as additional information or fail to provide any underlying structure,\nlike human biomechanical features, which are essential for different\napplications. In this paper, we present a novel approach called HFGaussian that\ncan estimate novel views and human features, such as the 3D skeleton, 3D key\npoints, and dense pose, from sparse input images in real time at 25 FPS. The\nproposed method leverages generalizable Gaussian splatting technique to\nrepresent the human subject and its associated features, enabling efficient and\ngeneralizable reconstruction. By incorporating a pose regression network and\nthe feature splatting technique with Gaussian splatting, HFGaussian\ndemonstrates improved capabilities over existing 3D human methods, showcasing\nthe potential of 3D human representations with integrated biomechanics. We\nthoroughly evaluate our HFGaussian method against the latest state-of-the-art\ntechniques in human Gaussian splatting and pose estimation, demonstrating its\nreal-time, state-of-the-art performance.\n","authors":["Arnab Dey","Cheng-You Lu","Andrew I. Comport","Srinath Sridhar","Chin-Teng Lin","Jean Martinet"],"pdf_url":"https://arxiv.org/pdf/2411.03086v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03082v1","updated":"2024-11-05T13:26:31Z","published":"2024-11-05T13:26:31Z","title":"Self-supervised cross-modality learning for uncertainty-aware object\n  detection and recognition in applications which lack pre-labelled training\n  data","summary":"  This paper shows how an uncertainty-aware, deep neural network can be trained\nto detect, recognise and localise objects in 2D RGB images, in applications\nlacking annotated train-ng datasets. We propose a self-supervising\nteacher-student pipeline, in which a relatively simple teacher classifier,\ntrained with only a few labelled 2D thumbnails, automatically processes a\nlarger body of unlabelled RGB-D data to teach a student network based on a\nmodified YOLOv3 architecture. Firstly, 3D object detection with back projection\nis used to automatically extract and teach 2D detection and localisation\ninformation to the student network. Secondly, a weakly supervised 2D thumbnail\nclassifier, with minimal training on a small number of hand-labelled images, is\nused to teach object category recognition. Thirdly, we use a Gaussian Process\nGP to encode and teach a robust uncertainty estimation functionality, so that\nthe student can output confidence scores with each categorization. The\nresulting student significantly outperforms the same YOLO architecture trained\ndirectly on the same amount of labelled data. Our GP-based approach yields\nrobust and meaningful uncertainty estimations for complex industrial object\nclassifications. The end-to-end network is also capable of real-time\nprocessing, needed for robotics applications. Our method can be applied to many\nimportant industrial tasks, where labelled datasets are typically unavailable.\nIn this paper, we demonstrate an example of detection, localisation, and object\ncategory recognition of nuclear mixed-waste materials in highly cluttered and\nunstructured scenes. This is critical for robotic sorting and handling of\nlegacy nuclear waste, which poses complex environmental remediation challenges\nin many nuclearised nations.\n","authors":["Irum Mehboob","Li Sun","Alireza Astegarpanah","Rustam Stolkin"],"pdf_url":"https://arxiv.org/pdf/2411.03082v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2410.22566v2","updated":"2024-11-05T13:21:26Z","published":"2024-10-29T22:15:03Z","title":"Deep Priors for Video Quality Prediction","summary":"  In this work, we designed a completely blind video quality assessment\nalgorithm using the deep video prior. This work mainly explores the utility of\ndeep video prior in estimating the visual quality of the video. In our work, we\nhave used a single distorted video and a reference video pair to learn the deep\nvideo prior. At inference time, the learned deep prior is used to restore the\noriginal videos from the distorted videos. The ability of learned deep video\nprior to restore the original video from the distorted video is measured to\nquantify distortion in the video. Our hypothesis is that the learned deep video\nprior fails in restoring the highly distorted videos. The restoring ability of\ndeep video prior is proportional to the distortion present in the video.\nTherefore, we propose to use the distance between the distorted video and the\nrestored video as the perceptual quality of the video. Our algorithm is trained\nusing a single video pair and it does not need any labelled data. We show that\nour proposed algorithm outperforms the existing unsupervised video quality\nassessment algorithms in terms of LCC and SROCC on a synthetically distorted\nvideo quality assessment dataset.\n","authors":["Siddharath Narayan Shakya","Parimala Kancharla"],"pdf_url":"https://arxiv.org/pdf/2410.22566v2.pdf","comment":"Indian Conference on Computer Vision, Graphics and Image Processing\n  (ICVGIP) 2024 conference tinny paper"},{"id":"http://arxiv.org/abs/2410.20535v3","updated":"2024-11-05T13:18:23Z","published":"2024-10-27T17:57:30Z","title":"Asynchronous Perception Machine For Efficient Test-Time-Training","summary":"  In this work, we propose Asynchronous Perception Machine (APM), a\ncomputationally-efficient architecture for test-time-training (TTT). APM can\nprocess patches of an image one at a time in any order asymmetrically and still\nencode semantic-awareness in the net. We demonstrate APM's ability to recognize\nout-of-distribution images without dataset-specific pre-training, augmentation\nor any-pretext task. APM offers competitive performance over existing TTT\napproaches. To perform TTT, APM just distills test sample's representation\nonce. APM possesses a unique property: it can learn using just this single\nrepresentation and starts predicting semantically-aware features.\n  APM demostrates potential applications beyond test-time-training: APM can\nscale up to a dataset of 2D images and yield semantic-clusterings in a single\nforward pass. APM also provides first empirical evidence towards validating\nGLOM's insight, i.e. input percept is a field. Therefore, APM helps us converge\ntowards an implementation which can do both interpolation and perception on a\nshared-connectionist hardware. Our code is publicly available at this link:\nhttps://rajatmodi62.github.io/apm_project_page/.\n","authors":["Rajat Modi","Yogesh Singh Rawat"],"pdf_url":"https://arxiv.org/pdf/2410.20535v3.pdf","comment":"Accepted to NeurIPS 2024 Main Track. APM is a step to getting\n  Geoffrey Hinton's GLOM working"},{"id":"http://arxiv.org/abs/2404.11614v3","updated":"2024-11-05T13:16:54Z","published":"2024-04-17T17:59:55Z","title":"Dynamic Typography: Bringing Text to Life via Video Diffusion Prior","summary":"  Text animation serves as an expressive medium, transforming static\ncommunication into dynamic experiences by infusing words with motion to evoke\nemotions, emphasize meanings, and construct compelling narratives. Crafting\nanimations that are semantically aware poses significant challenges, demanding\nexpertise in graphic design and animation. We present an automated text\nanimation scheme, termed \"Dynamic Typography\", which combines two challenging\ntasks. It deforms letters to convey semantic meaning and infuses them with\nvibrant movements based on user prompts. Our technique harnesses vector\ngraphics representations and an end-to-end optimization-based framework. This\nframework employs neural displacement fields to convert letters into base\nshapes and applies per-frame motion, encouraging coherence with the intended\ntextual concept. Shape preservation techniques and perceptual loss\nregularization are employed to maintain legibility and structural integrity\nthroughout the animation process. We demonstrate the generalizability of our\napproach across various text-to-video models and highlight the superiority of\nour end-to-end methodology over baseline methods, which might comprise separate\ntasks. Through quantitative and qualitative evaluations, we demonstrate the\neffectiveness of our framework in generating coherent text animations that\nfaithfully interpret user prompts while maintaining readability. Our code is\navailable at: https://animate-your-word.github.io/demo/.\n","authors":["Zichen Liu","Yihao Meng","Hao Ouyang","Yue Yu","Bolin Zhao","Daniel Cohen-Or","Huamin Qu"],"pdf_url":"https://arxiv.org/pdf/2404.11614v3.pdf","comment":"Our demo and code is available at:\n  https://animate-your-word.github.io/demo/"},{"id":"http://arxiv.org/abs/2410.02761v3","updated":"2024-11-05T13:14:23Z","published":"2024-10-03T17:59:34Z","title":"FakeShield: Explainable Image Forgery Detection and Localization via\n  Multi-modal Large Language Models","summary":"  The rapid development of generative AI is a double-edged sword, which not\nonly facilitates content creation but also makes image manipulation easier and\nmore difficult to detect. Although current image forgery detection and\nlocalization (IFDL) methods are generally effective, they tend to face two\nchallenges: \\textbf{1)} black-box nature with unknown detection principle,\n\\textbf{2)} limited generalization across diverse tampering methods (e.g.,\nPhotoshop, DeepFake, AIGC-Editing). To address these issues, we propose the\nexplainable IFDL task and design FakeShield, a multi-modal framework capable of\nevaluating image authenticity, generating tampered region masks, and providing\na judgment basis based on pixel-level and image-level tampering clues.\nAdditionally, we leverage GPT-4o to enhance existing IFDL datasets, creating\nthe Multi-Modal Tamper Description dataSet (MMTD-Set) for training FakeShield's\ntampering analysis capabilities. Meanwhile, we incorporate a Domain Tag-guided\nExplainable Forgery Detection Module (DTE-FDM) and a Multi-modal Forgery\nLocalization Module (MFLM) to address various types of tamper detection\ninterpretation and achieve forgery localization guided by detailed textual\ndescriptions. Extensive experiments demonstrate that FakeShield effectively\ndetects and localizes various tampering techniques, offering an explainable and\nsuperior solution compared to previous IFDL methods.\n","authors":["Zhipei Xu","Xuanyu Zhang","Runyi Li","Zecheng Tang","Qing Huang","Jian Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.02761v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03064v1","updated":"2024-11-05T12:54:01Z","published":"2024-11-05T12:54:01Z","title":"Exploiting the Segment Anything Model (SAM) for Lung Segmentation in\n  Chest X-ray Images","summary":"  Segment Anything Model (SAM), a new AI model from Meta AI released in April\n2023, is an ambitious tool designed to identify and separate individual objects\nwithin a given image through semantic interpretation. The advanced capabilities\nof SAM are the result of its training with millions of images and masks, and a\nfew days after its release, several researchers began testing the model on\nmedical images to evaluate its performance in this domain. With this\nperspective in focus -- i.e., optimizing work in the healthcare field -- this\nwork proposes the use of this new technology to evaluate and study chest X-ray\nimages. The approach adopted for this work, with the aim of improving the\nmodel's performance for lung segmentation, involved a transfer learning\nprocess, specifically the fine-tuning technique. After applying this\nadjustment, a substantial improvement was observed in the evaluation metrics\nused to assess SAM's performance compared to the masks provided by the\ndatasets. The results obtained by the model after the adjustments were\nsatisfactory and similar to cutting-edge neural networks, such as U-Net.\n","authors":["Gabriel Bellon de Carvalho","Jurandy Almeida"],"pdf_url":"https://arxiv.org/pdf/2411.03064v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03055v1","updated":"2024-11-05T12:42:42Z","published":"2024-11-05T12:42:42Z","title":"ATM: Improving Model Merging by Alternating Tuning and Merging","summary":"  Model merging has recently emerged as a cost-efficient paradigm for\nmulti-task learning. Among current approaches, task arithmetic stands out for\nits simplicity and effectiveness. In this paper, we motivate the effectiveness\nof task vectors by linking them to multi-task gradients. We show that in a\nsingle-epoch scenario, task vectors are mathematically equivalent to the\ngradients obtained via gradient descent in a multi-task setting, and still\napproximate these gradients in subsequent epochs. Furthermore, we show that\ntask vectors perform optimally when equality is maintained, and their\neffectiveness is largely driven by the first epoch's gradient. Building on this\ninsight, we propose viewing model merging as a single step in an iterative\nprocess that Alternates between Tuning and Merging (ATM). This method acts as a\nbridge between model merging and multi-task gradient descent, achieving\nstate-of-the-art results with the same data and computational requirements. We\nextensively evaluate ATM across diverse settings, achieving up to 20% higher\naccuracy in computer vision and NLP tasks, compared to the best\nbaselines.Finally, we provide both empirical and theoretical support for its\neffectiveness, demonstrating increased orthogonality between task vectors and\nproving that ATM minimizes an upper bound on the loss obtained by jointly\nfinetuning all tasks.\n","authors":["Luca Zhou","Daniele Solombrino","Donato Crisostomi","Maria Sofia Bucarelli","Fabrizio Silvestri","Emanuele Rodolà"],"pdf_url":"https://arxiv.org/pdf/2411.03055v1.pdf","comment":"Main paper: 10 Pages, 11 figures, 2 tables"},{"id":"http://arxiv.org/abs/2411.03053v1","updated":"2024-11-05T12:39:21Z","published":"2024-11-05T12:39:21Z","title":"Gradient-Guided Conditional Diffusion Models for Private Image\n  Reconstruction: Analyzing Adversarial Impacts of Differential Privacy and\n  Denoising","summary":"  We investigate the construction of gradient-guided conditional diffusion\nmodels for reconstructing private images, focusing on the adversarial interplay\nbetween differential privacy noise and the denoising capabilities of diffusion\nmodels. While current gradient-based reconstruction methods struggle with\nhigh-resolution images due to computational complexity and prior knowledge\nrequirements, we propose two novel methods that require minimal modifications\nto the diffusion model's generation process and eliminate the need for prior\nknowledge. Our approach leverages the strong image generation capabilities of\ndiffusion models to reconstruct private images starting from randomly generated\nnoise, even when a small amount of differentially private noise has been added\nto the gradients. We also conduct a comprehensive theoretical analysis of the\nimpact of differential privacy noise on the quality of reconstructed images,\nrevealing the relationship among noise magnitude, the architecture of attacked\nmodels, and the attacker's reconstruction capability. Additionally, extensive\nexperiments validate the effectiveness of our proposed methods and the accuracy\nof our theoretical findings, suggesting new directions for privacy risk\nauditing using conditional diffusion models.\n","authors":["Tao Huang","Jiayang Meng","Hong Chen","Guolong Zheng","Xu Yang","Xun Yi","Hua Wang"],"pdf_url":"https://arxiv.org/pdf/2411.03053v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03047v1","updated":"2024-11-05T12:30:07Z","published":"2024-11-05T12:30:07Z","title":"GarVerseLOD: High-Fidelity 3D Garment Reconstruction from a Single\n  In-the-Wild Image using a Dataset with Levels of Details","summary":"  Neural implicit functions have brought impressive advances to the\nstate-of-the-art of clothed human digitization from multiple or even single\nimages. However, despite the progress, current arts still have difficulty\ngeneralizing to unseen images with complex cloth deformation and body poses. In\nthis work, we present GarVerseLOD, a new dataset and framework that paves the\nway to achieving unprecedented robustness in high-fidelity 3D garment\nreconstruction from a single unconstrained image. Inspired by the recent\nsuccess of large generative models, we believe that one key to addressing the\ngeneralization challenge lies in the quantity and quality of 3D garment data.\nTowards this end, GarVerseLOD collects 6,000 high-quality cloth models with\nfine-grained geometry details manually created by professional artists. In\naddition to the scale of training data, we observe that having disentangled\ngranularities of geometry can play an important role in boosting the\ngeneralization capability and inference accuracy of the learned model. We hence\ncraft GarVerseLOD as a hierarchical dataset with levels of details (LOD),\nspanning from detail-free stylized shape to pose-blended garment with\npixel-aligned details. This allows us to make this highly under-constrained\nproblem tractable by factorizing the inference into easier tasks, each narrowed\ndown with smaller searching space. To ensure GarVerseLOD can generalize well to\nin-the-wild images, we propose a novel labeling paradigm based on conditional\ndiffusion models to generate extensive paired images for each garment model\nwith high photorealism. We evaluate our method on a massive amount of\nin-the-wild images. Experimental results demonstrate that GarVerseLOD can\ngenerate standalone garment pieces with significantly better quality than prior\napproaches. Project page: https://garverselod.github.io/\n","authors":["Zhongjin Luo","Haolin Liu","Chenghong Li","Wanghao Du","Zirong Jin","Wanhu Sun","Yinyu Nie","Weikai Chen","Xiaoguang Han"],"pdf_url":"https://arxiv.org/pdf/2411.03047v1.pdf","comment":"Project page: https://garverselod.github.io/"},{"id":"http://arxiv.org/abs/2411.03044v1","updated":"2024-11-05T12:27:24Z","published":"2024-11-05T12:27:24Z","title":"Evaluation of handwriting kinematics and pressure for differential\n  diagnosis of Parkinson's disease","summary":"  Objective: We present the PaHaW Parkinson's disease handwriting database,\nconsisting of handwriting samples from Parkinson's disease (PD) patients and\nhealthy controls. Our goal is to show that kinematic features and pressure\nfeatures in handwriting can be used for the differential diagnosis of PD.\nMethods and Material: The database contains records from 37 PD patients and 38\nhealthy controls performing eight different handwriting tasks. The tasks\ninclude drawing an Archimedean spiral, repetitively writing orthographically\nsimple syllables and words, and writing of a sentence. In addition to the\nconventional kinematic features related to the dynamics of handwriting, we\ninvestigated new pressure features based on the pressure exerted on the writing\nsurface. To discriminate between PD patients and healthy subjects, three\ndifferent classifiers were compared: K-nearest neighbors (K-NN), ensemble\nAdaBoost classifier, and support vector machines (SVM). Results: For predicting\nPD based on kinematic and pressure features of handwriting, the best performing\nmodel was SVM with classification accuracy of Pacc = 81.3% (sensitivity Psen =\n87.4% and specificity of Pspe = 80.9%). When evaluated separately, pressure\nfeatures proved to be relevant for PD diagnosis, yielding Pacc = 82.5% compared\nto Pacc = 75.4% using kinematic features. Conclusion: Experimental results\nshowed that an analysis of kinematic and pressure features during handwriting\ncan help assess subtle characteristics of handwriting and discriminate between\nPD patients and healthy controls.\n","authors":["Peter Drotár","Jiří Mekyska","Irena Rektorová","Lucia Masarová","Zdeněk Smékal","Marcos Faundez-Zanuy"],"pdf_url":"https://arxiv.org/pdf/2411.03044v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2410.00086v2","updated":"2024-11-05T12:25:32Z","published":"2024-09-30T17:56:27Z","title":"ACE: All-round Creator and Editor Following Instructions via Diffusion\n  Transformer","summary":"  Diffusion models have emerged as a powerful generative technology and have\nbeen found to be applicable in various scenarios. Most existing foundational\ndiffusion models are primarily designed for text-guided visual generation and\ndo not support multi-modal conditions, which are essential for many visual\nediting tasks. This limitation prevents these foundational diffusion models\nfrom serving as a unified model in the field of visual generation, like GPT-4\nin the natural language processing field. In this work, we propose ACE, an\nAll-round Creator and Editor, which achieves comparable performance compared to\nthose expert models in a wide range of visual generation tasks. To achieve this\ngoal, we first introduce a unified condition format termed Long-context\nCondition Unit (LCU), and propose a novel Transformer-based diffusion model\nthat uses LCU as input, aiming for joint training across various generation and\nediting tasks. Furthermore, we propose an efficient data collection approach to\naddress the issue of the absence of available training data. It involves\nacquiring pairwise images with synthesis-based or clustering-based pipelines\nand supplying these pairs with accurate textual instructions by leveraging a\nfine-tuned multi-modal large language model. To comprehensively evaluate the\nperformance of our model, we establish a benchmark of manually annotated pairs\ndata across a variety of visual generation tasks. The extensive experimental\nresults demonstrate the superiority of our model in visual generation fields.\nThanks to the all-in-one capabilities of our model, we can easily build a\nmulti-modal chat system that responds to any interactive request for image\ncreation using a single model to serve as the backend, avoiding the cumbersome\npipeline typically employed in visual agents. Code and models will be available\non the project page: https://ali-vilab.github.io/ace-page/.\n","authors":["Zhen Han","Zeyinzi Jiang","Yulin Pan","Jingfeng Zhang","Chaojie Mao","Chenwei Xie","Yu Liu","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.00086v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03041v1","updated":"2024-11-05T12:24:28Z","published":"2024-11-05T12:24:28Z","title":"Judge Like a Real Doctor: Dual Teacher Sample Consistency Framework for\n  Semi-supervised Medical Image Classification","summary":"  Semi-supervised learning (SSL) is a popular solution to alleviate the high\nannotation cost in medical image classification. As a main branch of SSL,\nconsistency regularization engages in imposing consensus between the\npredictions of a single sample from different views, termed as Absolute\nLocation consistency (AL-c). However, only AL-c may be insufficient. Just like\nwhen diagnosing a case in practice, besides the case itself, the doctor usually\nrefers to certain related trustworthy cases to make more reliable\ndecisions.Therefore, we argue that solely relying on AL-c may ignore the\nrelative differences across samples, which we interpret as relative locations,\nand only exploit limited information from one perspective. To address this\nissue, we propose a Sample Consistency Mean Teacher (SCMT) which not only\nincorporates AL c but also additionally enforces consistency between the\nsamples' relative similarities to its related samples, called Relative Location\nconsistency (RL c). AL c and RL c conduct consistency regularization from two\ndifferent perspectives, jointly extracting more diverse semantic information\nfor classification. On the other hand, due to the highly similar structures in\nmedical images, the sample distribution could be overly dense in feature space,\nmaking their relative locations susceptible to noise. To tackle this problem,\nwe further develop a Sample Scatter Mean Teacher (SSMT) by utilizing\ncontrastive learning to sparsify the sample distribution and obtain robust and\neffective relative locations. Extensive experiments on different datasets\ndemonstrate the superiority of our method.\n","authors":["Zhang Qixiang","Yang Yuxiang","Zu Chen","Zhang Jianjia","Wu Xi","Zhou Jiliu","Wang Yan"],"pdf_url":"https://arxiv.org/pdf/2411.03041v1.pdf","comment":"Accepted by IEEE Transactions on Emerging Topics in Computational\n  Intelligence"},{"id":"http://arxiv.org/abs/2405.19572v2","updated":"2024-11-05T12:21:24Z","published":"2024-05-29T23:38:12Z","title":"Blind Image Restoration via Fast Diffusion Inversion","summary":"  Image Restoration (IR) methods based on a pre-trained diffusion model have\ndemonstrated state-of-the-art performance. However, they have two fundamental\nlimitations: 1) they often assume that the degradation operator is completely\nknown and 2) they alter the diffusion sampling process, which may result in\nrestored images that do not lie onto the data manifold. To address these\nissues, we propose Blind Image Restoration via fast Diffusion inversion (BIRD)\na blind IR method that jointly optimizes for the degradation model parameters\nand the restored image. To ensure that the restored images lie onto the data\nmanifold, we propose a novel sampling technique on a pre-trained diffusion\nmodel. A key idea in our method is not to modify the reverse sampling, i.e, not\nto alter all the intermediate latents, once an initial noise is sampled. This\nis ultimately equivalent to casting the IR task as an optimization problem in\nthe space of the input noise. Moreover, to mitigate the computational cost\nassociated with inverting a fully unrolled diffusion model, we leverage the\ninherent capability of these models to skip ahead in the forward diffusion\nprocess using large time steps. We experimentally validate BIRD on several\nimage restoration tasks and show that it achieves state of the art performance\non all of them. Our code is available at\nhttps://github.com/hamadichihaoui/BIRD.\n","authors":["Hamadi Chihaoui","Abdelhak Lemkhenter","Paolo Favaro"],"pdf_url":"https://arxiv.org/pdf/2405.19572v2.pdf","comment":"Accepted to Neurips 2024"},{"id":"http://arxiv.org/abs/2411.03033v1","updated":"2024-11-05T12:10:02Z","published":"2024-11-05T12:10:02Z","title":"Rethinking Decoders for Transformer-based Semantic Segmentation:\n  Compression is All You Need","summary":"  State-of-the-art methods for Transformer-based semantic segmentation\ntypically adopt Transformer decoders that are used to extract additional\nembeddings from image embeddings via cross-attention, refine either or both\ntypes of embeddings via self-attention, and project image embeddings onto the\nadditional embeddings via dot-product. Despite their remarkable success, these\nempirical designs still lack theoretical justifications or interpretations,\nthus hindering potentially principled improvements. In this paper, we argue\nthat there are fundamental connections between semantic segmentation and\ncompression, especially between the Transformer decoders and Principal\nComponent Analysis (PCA). From such a perspective, we derive a white-box, fully\nattentional DEcoder for PrIncipled semantiC segemenTation (DEPICT), with the\ninterpretations as follows: 1) the self-attention operator refines image\nembeddings to construct an ideal principal subspace that aligns with the\nsupervision and retains most information; 2) the cross-attention operator seeks\nto find a low-rank approximation of the refined image embeddings, which is\nexpected to be a set of orthonormal bases of the principal subspace and\ncorresponds to the predefined classes; 3) the dot-product operation yields\ncompact representation for image embeddings as segmentation masks. Experiments\nconducted on dataset ADE20K find that DEPICT consistently outperforms its\nblack-box counterpart, Segmenter, and it is light weight and more robust.\n","authors":["Qishuai Wen","Chun-Guang Li"],"pdf_url":"https://arxiv.org/pdf/2411.03033v1.pdf","comment":"NeurIPS2024. Code:https://github.com/QishuaiWen/DEPICT/"},{"id":"http://arxiv.org/abs/2406.16658v3","updated":"2024-11-05T11:53:56Z","published":"2024-06-24T14:08:27Z","title":"Sampling Strategies in Bayesian Inversion: A Study of RTO and Langevin\n  Methods","summary":"  This paper studies two classes of sampling methods for the solution of\ninverse problems, namely Randomize-Then-Optimize (RTO), which is rooted in\nsensitivity analysis, and Langevin methods, which are rooted in the Bayesian\nframework. The two classes of methods correspond to different assumptions and\nyield samples from different target distributions. We highlight the main\nconceptual and theoretical differences between the two approaches and compare\nthem from a practical point of view by tackling two classical inverse problems\nin imaging: deblurring and inpainting. We show that the choice of the sampling\nmethod has a significant impact on the quality of the reconstruction and that\nthe RTO method is more robust to the choice of the parameters.\n","authors":["Remi Laumont","Yiqiu Dong","Martin Skovgaard Andersen"],"pdf_url":"https://arxiv.org/pdf/2406.16658v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03019v1","updated":"2024-11-05T11:42:26Z","published":"2024-11-05T11:42:26Z","title":"FEDLAD: Federated Evaluation of Deep Leakage Attacks and Defenses","summary":"  Federated Learning is a privacy preserving decentralized machine learning\nparadigm designed to collaboratively train models across multiple clients by\nexchanging gradients to the server and keeping private data local.\nNevertheless, recent research has revealed that the security of Federated\nLearning is compromised, as private ground truth data can be recovered through\na gradient inversion technique known as Deep Leakage. While these attacks are\ncrafted with a focus on applications in Federated Learning, they generally are\nnot evaluated in realistic scenarios. This paper introduces the FEDLAD\nFramework (Federated Evaluation of Deep Leakage Attacks and Defenses), a\ncomprehensive benchmark for evaluating Deep Leakage attacks and defenses within\na realistic Federated context. By implementing a unified benchmark that\nencompasses multiple state-of-the-art Deep Leakage techniques and various\ndefense strategies, our framework facilitates the evaluation and comparison of\nthe efficacy of these methods across different datasets and training states.\nThis work highlights a crucial trade-off between privacy and model accuracy in\nFederated Learning and aims to advance the understanding of security challenges\nin decentralized machine learning systems, stimulate future research, and\nenhance reproducibility in evaluating Deep Leakage attacks and defenses.\n","authors":["Isaac Baglin","Xiatian Zhu","Simon Hadfield"],"pdf_url":"https://arxiv.org/pdf/2411.03019v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2407.09510v4","updated":"2024-11-05T11:41:40Z","published":"2024-06-17T11:43:38Z","title":"3DGS.zip: A survey on 3D Gaussian Splatting Compression Methods","summary":"  3D Gaussian Splatting (3DGS) has emerged as a cutting-edge technique for\nreal-time radiance field rendering, offering state-of-the-art performance in\nterms of both quality and speed. 3DGS models a scene as a collection of\nthree-dimensional Gaussians, or splats, with additional attributes optimized to\nconform to the scene's geometric and visual properties. Despite its advantages\nin rendering speed and image fidelity, 3DGS is limited by its significant\nstorage and memory demands. These high demands make 3DGS impractical for mobile\ndevices or headsets, reducing its applicability in important areas of computer\ngraphics. To address these challenges and advance the practicality of 3DGS,\nthis survey provides a comprehensive and detailed examination of compression\nand compaction techniques developed to make 3DGS more efficient. We categorize\ncurrent approaches into compression techniques, which aim at achieving the\nhighest quality at minimal data size, and compaction techniques, which aim for\noptimal quality with the fewest Gaussians. We introduce the basic mathematical\nconcepts underlying the analyzed methods, as well as key implementation details\nand design choices. Our report thoroughly discusses similarities and\ndifferences among the methods, as well as their respective advantages and\ndisadvantages. We establish a consistent standard for comparing these methods\nbased on key performance metrics and datasets. Specifically, since these\nmethods have been developed in parallel and over a short period of time,\ncurrently, no comprehensive comparison exists. This survey, for the first time,\npresents a unified standard to evaluate 3DGS compression techniques. To\nfacilitate the continuous monitoring of emerging methodologies, we maintain a\ndedicated website that will be regularly updated with new techniques and\nrevisions of existing findings https://w-m.github.io/3dgs-compression-survey/ .\n","authors":["Milena T. Bagdasarian","Paul Knoll","Yi-Hsin Li","Florian Barthel","Anna Hilsmann","Peter Eisert","Wieland Morgenstern"],"pdf_url":"https://arxiv.org/pdf/2407.09510v4.pdf","comment":"3D Gaussian Splatting compression survey; 3DGS compression; new\n  approaches added"},{"id":"http://arxiv.org/abs/2408.05964v3","updated":"2024-11-05T11:30:40Z","published":"2024-08-12T07:33:11Z","title":"Target Detection of Safety Protective Gear Using the Improved YOLOv5","summary":"  In high-risk railway construction, personal protective equipment monitoring\nis critical but challenging due to small and frequently obstructed targets. We\npropose YOLO-EA, an innovative model that enhances safety measure detection by\nintegrating ECA into its backbone's convolutional layers, improving discernment\nof minuscule objects like hardhats. YOLO-EA further refines target recognition\nunder occlusion by replacing GIoU with EIoU loss. YOLO-EA's effectiveness was\nempirically substantiated using a dataset derived from real-world railway\nconstruction site surveillance footage. It outperforms YOLOv5, achieving 98.9%\nprecision and 94.7% recall, up 2.5% and 0.5% respectively, while maintaining\nreal-time performance at 70.774 fps. This highly efficient and precise YOLO-EA\nholds great promise for practical application in intricate construction\nscenarios, enforcing stringent safety compliance during complex railway\nconstruction projects.\n","authors":["Hao Liu","Xue Qin"],"pdf_url":"https://arxiv.org/pdf/2408.05964v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08565v3","updated":"2024-11-05T11:29:59Z","published":"2024-10-11T06:44:31Z","title":"Ocean-omni: To Understand the World with Omni-modality","summary":"  The salient multimodal capabilities and interactive experience of GPT-4o\nhighlight its critical role in practical applications, yet it lacks a\nhigh-performing open-source counterpart. In this paper, we introduce\nOcean-omni, the first open-source 7B Multimodal Large Language Model (MLLM)\nadept at concurrently processing and analyzing modalities of image, video,\naudio, and text, while delivering an advanced multimodal interactive experience\nand strong performance. We propose an effective multimodal training schema\nstarting with 7B model and proceeding through two stages of multimodal\nalignment and multitask fine-tuning across audio, image, video, and text modal.\nThis approach equips the language model with the ability to handle visual and\naudio data effectively. Demonstrating strong performance across various\nomni-modal and multimodal benchmarks, we aim for this contribution to serve as\na competitive baseline for the open-source community in advancing multimodal\nunderstanding and real-time interaction.\n","authors":["Yadong Li","Haoze Sun","Mingan Lin","Tianpeng Li","Guosheng Dong","Tao Zhang","Bowen Ding","Wei Song","Zhenglin Cheng","Yuqi Huo","Song Chen","Xu Li","Da Pan","Shusen Zhang","Xin Wu","Zheng Liang","Jun Liu","Tao Zhang","Keer Lu","Yaqi Zhao","Yanjun Shen","Fan Yang","Kaicheng Yu","Tao Lin","Jianhua Xu","Zenan Zhou","Weipeng Chen"],"pdf_url":"https://arxiv.org/pdf/2410.08565v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03013v1","updated":"2024-11-05T11:25:19Z","published":"2024-11-05T11:25:19Z","title":"CRT-Fusion: Camera, Radar, Temporal Fusion Using Motion Information for\n  3D Object Detection","summary":"  Accurate and robust 3D object detection is a critical component in autonomous\nvehicles and robotics. While recent radar-camera fusion methods have made\nsignificant progress by fusing information in the bird's-eye view (BEV)\nrepresentation, they often struggle to effectively capture the motion of\ndynamic objects, leading to limited performance in real-world scenarios. In\nthis paper, we introduce CRT-Fusion, a novel framework that integrates temporal\ninformation into radar-camera fusion to address this challenge. Our approach\ncomprises three key modules: Multi-View Fusion (MVF), Motion Feature Estimator\n(MFE), and Motion Guided Temporal Fusion (MGTF). The MVF module fuses radar and\nimage features within both the camera view and bird's-eye view, thereby\ngenerating a more precise unified BEV representation. The MFE module conducts\ntwo simultaneous tasks: estimation of pixel-wise velocity information and BEV\nsegmentation. Based on the velocity and the occupancy score map obtained from\nthe MFE module, the MGTF module aligns and fuses feature maps across multiple\ntimestamps in a recurrent manner. By considering the motion of dynamic objects,\nCRT-Fusion can produce robust BEV feature maps, thereby improving detection\naccuracy and robustness. Extensive evaluations on the challenging nuScenes\ndataset demonstrate that CRT-Fusion achieves state-of-the-art performance for\nradar-camera-based 3D object detection. Our approach outperforms the previous\nbest method in terms of NDS by +1.7%, while also surpassing the leading\napproach in mAP by +1.4%. These significant improvements in both metrics\nshowcase the effectiveness of our proposed fusion strategy in enhancing the\nreliability and accuracy of 3D object detection.\n","authors":["Jisong Kim","Minjae Seong","Jun Won Choi"],"pdf_url":"https://arxiv.org/pdf/2411.03013v1.pdf","comment":"Accepted at NeurIPS2024"},{"id":"http://arxiv.org/abs/2411.02999v1","updated":"2024-11-05T11:00:55Z","published":"2024-11-05T11:00:55Z","title":"Precise Drive with VLM: First Prize Solution for PRCV 2024 Drive LM\n  challenge","summary":"  This technical report outlines the methodologies we applied for the PRCV\nChallenge, focusing on cognition and decision-making in driving scenarios. We\nemployed InternVL-2.0, a pioneering open-source multi-modal model, and enhanced\nit by refining both the model input and training methodologies. For the input\ndata, we strategically concatenated and formatted the multi-view images. It is\nworth mentioning that we utilized the coordinates of the original images\nwithout transformation. In terms of model training, we initially pre-trained\nthe model on publicly available autonomous driving scenario datasets to bolster\nits alignment capabilities of the challenge tasks, followed by fine-tuning on\nthe DriveLM-nuscenes Dataset. During the fine-tuning phase, we innovatively\nmodified the loss function to enhance the model's precision in predicting\ncoordinate values. These approaches ensure that our model possesses advanced\ncognitive and decision-making capabilities in driving scenarios. Consequently,\nour model achieved a score of 0.6064, securing the first prize on the\ncompetition's final results.\n","authors":["Bin Huang","Siyu Wang","Yuanpeng Chen","Yidan Wu","Hui Song","Zifan Ding","Jing Leng","Chengpeng Liang","Peng Xue","Junliang Zhang","Tiankun Zhao"],"pdf_url":"https://arxiv.org/pdf/2411.02999v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02997v1","updated":"2024-11-05T10:58:37Z","published":"2024-11-05T10:58:37Z","title":"PV-faultNet: Optimized CNN Architecture to detect defects resulting\n  efficient PV production","summary":"  The global shift towards renewable energy has pushed PV cell manufacturing as\na pivotal point as they are the fundamental building block of green energy.\nHowever, the manufacturing process is complex enough to lose its purpose due to\nprobable defects experienced during the time impacting the overall efficiency.\nHowever, at the moment, manual inspection is being conducted to detect the\ndefects that can cause bias, leading to time and cost inefficiency. Even if\nautomated solutions have also been proposed, most of them are\nresource-intensive, proving ineffective in production environments. In that\ncontext, this study presents PV-faultNet, a lightweight Convolutional Neural\nNetwork (CNN) architecture optimized for efficient and real-time defect\ndetection in photovoltaic (PV) cells, designed to be deployable on\nresource-limited production devices. Addressing computational challenges in\nindustrial PV manufacturing environments, the model includes only 2.92 million\nparameters, significantly reducing processing demands without sacrificing\naccuracy. Comprehensive data augmentation techniques were implemented to tackle\ndata scarcity, thus enhancing model generalization and maintaining a balance\nbetween precision and recall. The proposed model achieved high performance with\n91\\% precision, 89\\% recall, and a 90\\% F1 score, demonstrating its\neffectiveness for scalable quality control in PV production.\n","authors":["Eiffat E Zaman","Rahima Khanam"],"pdf_url":"https://arxiv.org/pdf/2411.02997v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02992v1","updated":"2024-11-05T10:53:25Z","published":"2024-11-05T10:53:25Z","title":"Efficient and Effective Adaptation of Multimodal Foundation Models in\n  Sequential Recommendation","summary":"  Multimodal foundation models (MFMs) have revolutionized sequential\nrecommender systems through advanced representation learning. While\nParameter-efficient Fine-tuning (PEFT) is commonly used to adapt these models,\nstudies often prioritize parameter efficiency, neglecting GPU memory and\ntraining speed. To address this, we introduced the IISAN framework,\nsignificantly enhancing efficiency. However, IISAN was limited to symmetrical\nMFMs and identical text and image encoders, preventing the use of\nstate-of-the-art Large Language Models. To overcome this, we developed\nIISAN-Versa, a versatile plug-and-play architecture compatible with both\nsymmetrical and asymmetrical MFMs. IISAN-Versa employs a Decoupled PEFT\nstructure and utilizes both intra- and inter-modal adaptation. It effectively\nhandles asymmetry through a simple yet effective combination of group\nlayer-dropping and dimension transformation alignment. Our research\ndemonstrates that IISAN-Versa effectively adapts large text encoders, and we\nfurther identify a scaling effect where larger encoders generally perform\nbetter. IISAN-Versa also demonstrates strong versatility in our defined\nmultimodal scenarios, which include raw titles and captions generated from\nimages and videos. Additionally, IISAN-Versa achieved state-of-the-art\nperformance on the Microlens public benchmark. We will release our code and\ndatasets to support future research.\n","authors":["Junchen Fu","Xuri Ge","Xin Xin","Alexandros Karatzoglou","Ioannis Arapakis","Kaiwen Zheng","Yongxin Ni","Joemon M. Jose"],"pdf_url":"https://arxiv.org/pdf/2411.02992v1.pdf","comment":"The extension of IISAN in SIGIR2024"},{"id":"http://arxiv.org/abs/2308.02905v3","updated":"2024-11-05T10:51:30Z","published":"2023-08-05T15:54:06Z","title":"FASTER: A Font-Agnostic Scene Text Editing and Rendering Framework","summary":"  Scene Text Editing (STE) is a challenging research problem, that primarily\naims towards modifying existing texts in an image while preserving the\nbackground and the font style of the original text. Despite its utility in\nnumerous real-world applications, existing style-transfer-based approaches have\nshown sub-par editing performance due to (1) complex image backgrounds, (2)\ndiverse font attributes, and (3) varying word lengths within the text. To\naddress such limitations, in this paper, we propose a novel font-agnostic scene\ntext editing and rendering framework, named FASTER, for simultaneously\ngenerating text in arbitrary styles and locations while preserving a natural\nand realistic appearance and structure. A combined fusion of target mask\ngeneration and style transfer units, with a cascaded self-attention mechanism\nhas been proposed to focus on multi-level text region edits to handle varying\nword lengths. Extensive evaluation on a real-world database with further\nsubjective human evaluation study indicates the superiority of FASTER in both\nscene text editing and rendering tasks, in terms of model performance and\nefficiency. Our code will be released upon acceptance.\n","authors":["Alloy Das","Sanket Biswas","Prasun Roy","Subhankar Ghosh","Umapada Pal","Michael Blumenstein","Josep Lladós","Saumik Bhattacharya"],"pdf_url":"https://arxiv.org/pdf/2308.02905v3.pdf","comment":"Accepted in WACV 2025"},{"id":"http://arxiv.org/abs/2411.02979v1","updated":"2024-11-05T10:41:45Z","published":"2024-11-05T10:41:45Z","title":"CAD-NeRF: Learning NeRFs from Uncalibrated Few-view Images by CAD Model\n  Retrieval","summary":"  Reconstructing from multi-view images is a longstanding problem in 3D vision,\nwhere neural radiance fields (NeRFs) have shown great potential and get\nrealistic rendered images of novel views. Currently, most NeRF methods either\nrequire accurate camera poses or a large number of input images, or even both.\nReconstructing NeRF from few-view images without poses is challenging and\nhighly ill-posed. To address this problem, we propose CAD-NeRF, a method\nreconstructed from less than 10 images without any known poses. Specifically,\nwe build a mini library of several CAD models from ShapeNet and render them\nfrom many random views. Given sparse-view input images, we run a model and pose\nretrieval from the library, to get a model with similar shapes, serving as the\ndensity supervision and pose initializations. Here we propose a multi-view pose\nretrieval method to avoid pose conflicts among views, which is a new and unseen\nproblem in uncalibrated NeRF methods. Then, the geometry of the object is\ntrained by the CAD guidance. The deformation of the density field and camera\nposes are optimized jointly. Then texture and density are trained and\nfine-tuned as well. All training phases are in self-supervised manners.\nComprehensive evaluations of synthetic and real images show that CAD-NeRF\nsuccessfully learns accurate densities with a large deformation from retrieved\nCAD models, showing the generalization abilities.\n","authors":["Xin Wen","Xuening Zhu","Renjiao Yi","Zhifeng Wang","Chenyang Zhu","Kai Xu"],"pdf_url":"https://arxiv.org/pdf/2411.02979v1.pdf","comment":"The article has been accepted by Frontiers of Computer Science (FCS)"},{"id":"http://arxiv.org/abs/2305.11616v5","updated":"2024-11-05T10:41:42Z","published":"2023-05-19T11:47:51Z","title":"Diversifying Deep Ensembles: A Saliency Map Approach for Enhanced OOD\n  Detection, Calibration, and Accuracy","summary":"  Deep ensembles are capable of achieving state-of-the-art results in\nclassification and out-of-distribution (OOD) detection. However, their\neffectiveness is limited due to the homogeneity of learned patterns within\nensembles. To overcome this issue, our study introduces Saliency Diversified\nDeep Ensemble (SDDE), a novel approach that promotes diversity among ensemble\nmembers by leveraging saliency maps. Through incorporating saliency map\ndiversification, our method outperforms conventional ensemble techniques and\nimproves calibration in multiple classification and OOD detection tasks. In\nparticular, the proposed method achieves state-of-the-art OOD detection\nquality, calibration, and accuracy on multiple benchmarks, including\nCIFAR10/100 and large-scale ImageNet datasets.\n","authors":["Stanislav Dereka","Ivan Karpukhin","Maksim Zhdanov","Sergey Kolesnikov"],"pdf_url":"https://arxiv.org/pdf/2305.11616v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10464v2","updated":"2024-11-05T10:32:02Z","published":"2023-12-16T14:46:24Z","title":"Identity Curvature Laplace Approximation for Improved\n  Out-of-Distribution Detection","summary":"  Uncertainty estimation is crucial in safety-critical applications, where\nrobust out-of-distribution (OOD) detection is essential. Traditional Bayesian\nmethods, though effective, are often hindered by high computational demands. As\nan alternative, Laplace approximation offers a more practical and efficient\napproach to uncertainty estimation. In this paper, we introduce the Identity\nCurvature Laplace Approximation (ICLA), a novel method that challenges the\nconventional posterior covariance formulation by using identity curvature and\noptimizing prior precision. This innovative design significantly enhances OOD\ndetection performance on well-known datasets such as CIFAR-10, CIFAR-100, and\nImageNet, while maintaining calibration scores. We attribute this improvement\nto the alignment issues between typical feature embeddings and curvature as\nmeasured by the Fisher information matrix. Our findings are further supported\nby demonstrating that incorporating Fisher penalty or sharpness-aware\nminimization techniques can greatly enhance the uncertainty estimation\ncapabilities of standard Laplace approximation.\n","authors":["Maksim Zhdanov","Stanislav Dereka","Sergey Kolesnikov"],"pdf_url":"https://arxiv.org/pdf/2312.10464v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01739v2","updated":"2024-11-05T10:23:00Z","published":"2024-11-04T01:42:41Z","title":"Not Just Object, But State: Compositional Incremental Learning without\n  Forgetting","summary":"  Most incremental learners excessively prioritize coarse classes of objects\nwhile neglecting various kinds of states (e.g. color and material) attached to\nthe objects. As a result, they are limited in the ability to reason\nfine-grained compositionality of state-object pairs. To remedy this limitation,\nwe propose a novel task called Compositional Incremental Learning\n(composition-IL), enabling the model to recognize state-object compositions as\na whole in an incremental learning fashion. Since the lack of suitable\nbenchmarks, we re-organize two existing datasets and make them tailored for\ncomposition-IL. Then, we propose a prompt-based Composition Incremental Learner\n(CompILer), to overcome the ambiguous composition boundary problem which\nchallenges composition-IL largely. Specifically, we exploit multi-pool prompt\nlearning, which is regularized by inter-pool prompt discrepancy and intra-pool\nprompt diversity. Besides, we devise object-injected state prompting by using\nobject prompts to guide the selection of state prompts. Furthermore, we fuse\nthe selected prompts by a generalized-mean strategy, to eliminate irrelevant\ninformation learned in the prompts. Extensive experiments on two datasets\nexhibit state-of-the-art performance achieved by CompILer.\n","authors":["Yanyi Zhang","Binglin Qiu","Qi Jia","Yu Liu","Ran He"],"pdf_url":"https://arxiv.org/pdf/2411.01739v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.21130v2","updated":"2024-11-05T10:22:35Z","published":"2024-10-28T15:31:47Z","title":"Extrapolating Prospective Glaucoma Fundus Images through Diffusion Model\n  in Irregular Longitudinal Sequences","summary":"  The utilization of longitudinal datasets for glaucoma progression prediction\noffers a compelling approach to support early therapeutic interventions.\nPredominant methodologies in this domain have primarily focused on the direct\nprediction of glaucoma stage labels from longitudinal datasets. However, such\nmethods may not adequately encapsulate the nuanced developmental trajectory of\nthe disease. To enhance the diagnostic acumen of medical practitioners, we\npropose a novel diffusion-based model to predict prospective images by\nextrapolating from existing longitudinal fundus images of patients. The\nmethodology delineated in this study distinctively leverages sequences of\nimages as inputs. Subsequently, a time-aligned mask is employed to select a\nspecific year for image generation. During the training phase, the time-aligned\nmask resolves the issue of irregular temporal intervals in longitudinal image\nsequence sampling. Additionally, we utilize a strategy of randomly masking a\nframe in the sequence to establish the ground truth. This methodology aids the\nnetwork in continuously acquiring knowledge regarding the internal\nrelationships among the sequences throughout the learning phase. Moreover, the\nintroduction of textual labels is instrumental in categorizing images generated\nwithin the sequence. The empirical findings from the conducted experiments\nindicate that our proposed model not only effectively generates longitudinal\ndata but also significantly improves the precision of downstream classification\ntasks.\n","authors":["Zhihao Zhao","Junjie Yang","Shahrooz Faghihroohi","Yinzheng Zhao","Daniel Zapp","Kai Huang","Nassir Navab","M. Ali Nasseri"],"pdf_url":"https://arxiv.org/pdf/2410.21130v2.pdf","comment":"Accepted at BIBM 2024"},{"id":"http://arxiv.org/abs/2411.02974v1","updated":"2024-11-05T10:21:21Z","published":"2024-11-05T10:21:21Z","title":"Region-Guided Attack on the Segment Anything Model (SAM)","summary":"  The Segment Anything Model (SAM) is a cornerstone of image segmentation,\ndemonstrating exceptional performance across various applications, particularly\nin autonomous driving and medical imaging, where precise segmentation is\ncrucial. However, SAM is vulnerable to adversarial attacks that can\nsignificantly impair its functionality through minor input perturbations.\nTraditional techniques, such as FGSM and PGD, are often ineffective in\nsegmentation tasks due to their reliance on global perturbations that overlook\nspatial nuances. Recent methods like Attack-SAM-K and UAD have begun to address\nthese challenges, but they frequently depend on external cues and do not fully\nleverage the structural interdependencies within segmentation processes. This\nlimitation underscores the need for a novel adversarial strategy that exploits\nthe unique characteristics of segmentation tasks. In response, we introduce the\nRegion-Guided Attack (RGA), designed specifically for SAM. RGA utilizes a\nRegion-Guided Map (RGM) to manipulate segmented regions, enabling targeted\nperturbations that fragment large segments and expand smaller ones, resulting\nin erroneous outputs from SAM. Our experiments demonstrate that RGA achieves\nhigh success rates in both white-box and black-box scenarios, emphasizing the\nneed for robust defenses against such sophisticated attacks. RGA not only\nreveals SAM's vulnerabilities but also lays the groundwork for developing more\nresilient defenses against adversarial threats in image segmentation.\n","authors":["Xiaoliang Liu","Furao Shen","Jian Zhao"],"pdf_url":"https://arxiv.org/pdf/2411.02974v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02972v1","updated":"2024-11-05T10:16:14Z","published":"2024-11-05T10:16:14Z","title":"Exploring Seasonal Variability in the Context of Neural Radiance Fields\n  for 3D Reconstruction on Satellite Imagery","summary":"  In this work, the seasonal predictive capabilities of Neural Radiance Fields\n(NeRF) applied to satellite images are investigated. Focusing on the\nutilization of satellite data, the study explores how Sat-NeRF, a novel\napproach in computer vision, performs in predicting seasonal variations across\ndifferent months. Through comprehensive analysis and visualization, the study\nexamines the model's ability to capture and predict seasonal changes,\nhighlighting specific challenges and strengths. Results showcase the impact of\nthe sun direction on predictions, revealing nuanced details in seasonal\ntransitions, such as snow cover, color accuracy, and texture representation in\ndifferent landscapes. Given these results, we propose Planet-NeRF, an extension\nto Sat-NeRF capable of incorporating seasonal variability through a set of\nmonth embedding vectors. Comparative evaluations reveal that Planet-NeRF\noutperforms prior models in the case where seasonal changes are present. The\nextensive evaluation combined with the proposed method offers promising avenues\nfor future research in this domain.\n","authors":["Liv Kåreborn","Erica Ingerstad","Amanda Berg","Justus Karlsson","Leif Haglund"],"pdf_url":"https://arxiv.org/pdf/2411.02972v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02969v1","updated":"2024-11-05T10:13:23Z","published":"2024-11-05T10:13:23Z","title":"Multi-modal NeRF Self-Supervision for LiDAR Semantic Segmentation","summary":"  LiDAR Semantic Segmentation is a fundamental task in autonomous driving\nperception consisting of associating each LiDAR point to a semantic label.\nFully-supervised models have widely tackled this task, but they require labels\nfor each scan, which either limits their domain or requires impractical amounts\nof expensive annotations. Camera images, which are generally recorded alongside\nLiDAR pointclouds, can be processed by the widely available 2D foundation\nmodels, which are generic and dataset-agnostic. However, distilling knowledge\nfrom 2D data to improve LiDAR perception raises domain adaptation challenges.\nFor example, the classical perspective projection suffers from the parallax\neffect produced by the position shift between both sensors at their respective\ncapture times. We propose a Semi-Supervised Learning setup to leverage\nunlabeled LiDAR pointclouds alongside distilled knowledge from the camera\nimages. To self-supervise our model on the unlabeled scans, we add an auxiliary\nNeRF head and cast rays from the camera viewpoint over the unlabeled voxel\nfeatures. The NeRF head predicts densities and semantic logits at each sampled\nray location which are used for rendering pixel semantics. Concurrently, we\nquery the Segment-Anything (SAM) foundation model with the camera image to\ngenerate a set of unlabeled generic masks. We fuse the masks with the rendered\npixel semantics from LiDAR to produce pseudo-labels that supervise the pixel\npredictions. During inference, we drop the NeRF head and run our model with\nonly LiDAR. We show the effectiveness of our approach in three public LiDAR\nSemantic Segmentation benchmarks: nuScenes, SemanticKITTI and ScribbleKITTI.\n","authors":["Xavier Timoneda","Markus Herb","Fabian Duerr","Daniel Goehring","Fisher Yu"],"pdf_url":"https://arxiv.org/pdf/2411.02969v1.pdf","comment":"IEEE/RSJ International Conference on Intelligent Robots and Systems\n  (IROS) 2024"},{"id":"http://arxiv.org/abs/2406.15831v2","updated":"2024-11-05T09:56:56Z","published":"2024-06-22T12:24:49Z","title":"Shape2.5D: A Dataset of Texture-less Surfaces for Depth and Normals\n  Estimation","summary":"  Reconstructing texture-less surfaces poses unique challenges in computer\nvision, primarily due to the lack of specialized datasets that cater to the\nnuanced needs of depth and normals estimation in the absence of textural\ninformation. We introduce \"Shape2.5D,\" a novel, large-scale dataset designed to\naddress this gap. Comprising 1.17 million frames spanning over 39,772 3D models\nand 48 unique objects, our dataset provides depth and surface normal maps for\ntexture-less object reconstruction. The proposed dataset includes synthetic\nimages rendered with 3D modeling software to simulate various lighting\nconditions and viewing angles. It also includes a real-world subset comprising\n4,672 frames captured with a depth camera. Our comprehensive benchmarks\ndemonstrate the dataset's ability to support the development of algorithms that\nrobustly estimate depth and normals from RGB images and perform voxel\nreconstruction. Our open-source data generation pipeline allows the dataset to\nbe extended and adapted for future research. The dataset is publicly available\nat https://github.com/saifkhichi96/Shape25D.\n","authors":["Muhammad Saif Ullah Khan","Sankalp Sinha","Didier Stricker","Marcus Liwicki","Muhammad Zeshan Afzal"],"pdf_url":"https://arxiv.org/pdf/2406.15831v2.pdf","comment":"Accepted for publication in IEEE Access"},{"id":"http://arxiv.org/abs/2410.20084v2","updated":"2024-11-05T09:52:50Z","published":"2024-10-26T05:28:02Z","title":"UniVST: A Unified Framework for Training-free Localized Video Style\n  Transfer","summary":"  This paper presents UniVST, a unified framework for localized video style\ntransfer. It operates without the need for training, offering a distinct\nadvantage over existing methods that transfer style across entire videos. The\nendeavors of this paper comprise: (1) A point-matching mask propagation\nstrategy that leverages feature maps from the DDIM inversion. This streamlines\nthe model's architecture by obviating the need for tracking models. (2) An\nAdaIN-guided style transfer mechanism that operates at both the latent and\nattention levels. This balances content fidelity and style richness, mitigating\nthe loss of localized details commonly associated with direct video\nstylization. (3) A sliding window smoothing strategy that harnesses optical\nflow within the pixel representation and refines predicted noise to update the\nlatent space. This significantly enhances temporal consistency and diminishes\nartifacts in video outputs. Our proposed UniVST has been validated to be\nsuperior to existing methods in quantitative and qualitative metrics. It\nadeptly addresses the challenges of preserving the primary object's style while\nensuring temporal consistency and detail preservation.\n","authors":["Quanjian Song","Mingbao Lin","Wengyi Zhan","Shuicheng Yan","Liujuan Cao"],"pdf_url":"https://arxiv.org/pdf/2410.20084v2.pdf","comment":"10 pages not including reference"},{"id":"http://arxiv.org/abs/2411.02951v1","updated":"2024-11-05T09:51:59Z","published":"2024-11-05T09:51:59Z","title":"LDPM: Towards undersampled MRI reconstruction with MR-VAE and Latent\n  Diffusion Prior","summary":"  Diffusion model, as a powerful generative model, has found a wide range of\napplications including MRI reconstruction. However, most existing diffusion\nmodel-based MRI reconstruction methods operate directly in pixel space, which\nmakes their optimization and inference computationally expensive. Latent\ndiffusion models were introduced to address this problem in natural image\nprocessing, but directly applying them to MRI reconstruction still faces many\nchallenges, including the lack of control over the generated results, the\nadaptability of Variational AutoEncoder (VAE) to MRI, and the exploration of\napplicable data consistency in latent space. To address these challenges, a\nLatent Diffusion Prior based undersampled MRI reconstruction (LDPM) method is\nproposed. A sketcher module is utilized to provide appropriate control and\nbalance the quality and fidelity of the reconstructed MR images. A VAE adapted\nfor MRI tasks (MR-VAE) is explored, which can serve as the backbone for future\nMR-related tasks. Furthermore, a variation of the DDIM sampler, called the\nDual-Stage Sampler, is proposed to achieve high-fidelity reconstruction in the\nlatent space. The proposed method achieves competitive results on fastMRI\ndatasets, and the effectiveness of each module is demonstrated in ablation\nexperiments.\n","authors":["Xingjian Tang","Jingwei Guan","Linge Li","Youmei Zhang","Mengye Lyu","Li Yan"],"pdf_url":"https://arxiv.org/pdf/2411.02951v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.02485v2","updated":"2024-11-05T09:46:45Z","published":"2024-06-04T16:54:28Z","title":"Stable-Pose: Leveraging Transformers for Pose-Guided Text-to-Image\n  Generation","summary":"  Controllable text-to-image (T2I) diffusion models have shown impressive\nperformance in generating high-quality visual content through the incorporation\nof various conditions. Current methods, however, exhibit limited performance\nwhen guided by skeleton human poses, especially in complex pose conditions such\nas side or rear perspectives of human figures. To address this issue, we\npresent Stable-Pose, a novel adapter model that introduces a coarse-to-fine\nattention masking strategy into a vision Transformer (ViT) to gain accurate\npose guidance for T2I models. Stable-Pose is designed to adeptly handle pose\nconditions within pre-trained Stable Diffusion, providing a refined and\nefficient way of aligning pose representation during image synthesis. We\nleverage the query-key self-attention mechanism of ViTs to explore the\ninterconnections among different anatomical parts in human pose skeletons.\nMasked pose images are used to smoothly refine the attention maps based on\ntarget pose-related features in a hierarchical manner, transitioning from\ncoarse to fine levels. Additionally, our loss function is formulated to\nallocate increased emphasis to the pose region, thereby augmenting the model's\nprecision in capturing intricate pose details. We assessed the performance of\nStable-Pose across five public datasets under a wide range of indoor and\noutdoor human pose scenarios. Stable-Pose achieved an AP score of 57.1 in the\nLAION-Human dataset, marking around 13% improvement over the established\ntechnique ControlNet. The project link and code is available at\nhttps://github.com/ai-med/StablePose.\n","authors":["Jiajun Wang","Morteza Ghahremani","Yitong Li","Björn Ommer","Christian Wachinger"],"pdf_url":"https://arxiv.org/pdf/2406.02485v2.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.02327v2","updated":"2024-11-05T09:43:59Z","published":"2024-11-04T17:50:36Z","title":"PPLLaVA: Varied Video Sequence Understanding With Prompt Guidance","summary":"  The past year has witnessed the significant advancement of video-based large\nlanguage models. However, the challenge of developing a unified model for both\nshort and long video understanding remains unresolved. Most existing video LLMs\ncannot handle hour-long videos, while methods custom for long videos tend to be\nineffective for shorter videos and images. In this paper, we identify the key\nissue as the redundant content in videos. To address this, we propose a novel\npooling strategy that simultaneously achieves token compression and\ninstruction-aware visual feature aggregation. Our model is termed Prompt-guided\nPooling LLaVA, or PPLLaVA for short. Specifically, PPLLaVA consists of three\ncore components: the CLIP-based visual-prompt alignment that extracts visual\ninformation relevant to the user's instructions, the prompt-guided pooling that\ncompresses the visual sequence to arbitrary scales using convolution-style\npooling, and the clip context extension designed for lengthy prompt common in\nvisual dialogue. Moreover, our codebase also integrates the most advanced video\nDirect Preference Optimization (DPO) and visual interleave training. Extensive\nexperiments have validated the performance of our model. With superior\nthroughput and only 1024 visual context, PPLLaVA achieves better results on\nimage benchmarks as a video LLM, while achieving state-of-the-art performance\nacross various video benchmarks, excelling in tasks ranging from caption\ngeneration to multiple-choice questions, and handling video lengths from\nseconds to hours. Codes have been available at\nhttps://github.com/farewellthree/PPLLaVA.\n","authors":["Ruyang Liu","Haoran Tang","Haibo Liu","Yixiao Ge","Ying Shan","Chen Li","Jiankun Yang"],"pdf_url":"https://arxiv.org/pdf/2411.02327v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08773v4","updated":"2024-11-05T09:37:33Z","published":"2024-06-13T03:05:36Z","title":"DenoiseRep: Denoising Model for Representation Learning","summary":"  The denoising model has been proven a powerful generative model but has\nlittle exploration of discriminative tasks. Representation learning is\nimportant in discriminative tasks, which is defined as \"learning\nrepresentations (or features) of the data that make it easier to extract useful\ninformation when building classifiers or other predictors\". In this paper, we\npropose a novel Denoising Model for Representation Learning (DenoiseRep) to\nimprove feature discrimination with joint feature extraction and denoising.\nDenoiseRep views each embedding layer in a backbone as a denoising layer,\nprocessing the cascaded embedding layers as if we are recursively denoise\nfeatures step-by-step. This unifies the frameworks of feature extraction and\ndenoising, where the former progressively embeds features from low-level to\nhigh-level, and the latter recursively denoises features step-by-step. After\nthat, DenoiseRep fuses the parameters of feature extraction and denoising\nlayers, and theoretically demonstrates its equivalence before and after the\nfusion, thus making feature denoising computation-free. DenoiseRep is a\nlabel-free algorithm that incrementally improves features but also\ncomplementary to the label if available. Experimental results on various\ndiscriminative vision tasks, including re-identification (Market-1501,\nDukeMTMC-reID, MSMT17, CUHK-03, vehicleID), image classification (ImageNet,\nUB200, Oxford-Pet, Flowers), object detection (COCO), image segmentation\n(ADE20K) show stability and impressive improvements. We also validate its\neffectiveness on the CNN (ResNet) and Transformer (ViT, Swin, Vmamda)\narchitectures.\n","authors":["Zhengrui Xu","Guan'an Wang","Xiaowen Huang","Jitao Sang"],"pdf_url":"https://arxiv.org/pdf/2406.08773v4.pdf","comment":"Accepted by NeurIPS 2024 (Oral)"},{"id":"http://arxiv.org/abs/2411.02935v1","updated":"2024-11-05T09:24:59Z","published":"2024-11-05T09:24:59Z","title":"Mapping Africa Settlements: High Resolution Urban and Rural Map by Deep\n  Learning and Satellite Imagery","summary":"  Accurate Land Use and Land Cover (LULC) maps are essential for understanding\nthe drivers of sustainable development, in terms of its complex\ninterrelationships between human activities and natural resources. However,\nexisting LULC maps often lack precise urban and rural classifications,\nparticularly in diverse regions like Africa. This study presents a novel\nconstruction of a high-resolution rural-urban map using deep learning\ntechniques and satellite imagery. We developed a deep learning model based on\nthe DeepLabV3 architecture, which was trained on satellite imagery from\nLandsat-8 and the ESRI LULC dataset, augmented with human settlement data from\nthe GHS-SMOD. The model utilizes semantic segmentation to classify land into\ndetailed categories, including urban and rural areas, at a 10-meter resolution.\nOur findings demonstrate that incorporating LULC along with urban and rural\nclassifications significantly enhances the model's ability to accurately\ndistinguish between urban, rural, and non-human settlement areas. Therefore,\nour maps can support more informed decision-making for policymakers,\nresearchers, and stakeholders. We release a continent wide urban-rural map,\ncovering the period 2016 and 2022.\n","authors":["Mohammad Kakooei","James Bailie","Albin Söderberg","Albin Becevic","Adel Daoud"],"pdf_url":"https://arxiv.org/pdf/2411.02935v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12814v2","updated":"2024-11-05T09:09:12Z","published":"2024-10-01T08:52:46Z","title":"Leveraging generative models to characterize the failure conditions of\n  image classifiers","summary":"  We address in this work the question of identifying the failure conditions of\na given image classifier. To do so, we exploit the capacity of producing\ncontrollable distributions of high quality image data made available by recent\nGenerative Adversarial Networks (StyleGAN2): the failure conditions are\nexpressed as directions of strong performance degradation in the generative\nmodel latent space. This strategy of analysis is used to discover corner cases\nthat combine multiple sources of corruption, and to compare in more details the\nbehavior of different classifiers. The directions of degradation can also be\nrendered visually by generating data for better interpretability. Some\ndegradations such as image quality can affect all classes, whereas other ones\nsuch as shape are more class-specific. The approach is demonstrated on the\nMNIST dataset that has been completed by two sources of corruption: noise and\nblur, and shows a promising way to better understand and control the risks of\nexploiting Artificial Intelligence components for safety-critical applications.\n","authors":["Adrien LeCoz","Stéphane Herbin","Faouzi Adjed"],"pdf_url":"https://arxiv.org/pdf/2410.12814v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02920v1","updated":"2024-11-05T09:08:46Z","published":"2024-11-05T09:08:46Z","title":"Domain Expansion and Boundary Growth for Open-Set Single-Source Domain\n  Generalization","summary":"  Open-set single-source domain generalization aims to use a single-source\ndomain to learn a robust model that can be generalized to unknown target\ndomains with both domain shifts and label shifts. The scarcity of the source\ndomain and the unknown data distribution of the target domain pose a great\nchallenge for domain-invariant feature learning and unknown class recognition.\nIn this paper, we propose a novel learning approach based on domain expansion\nand boundary growth to expand the scarce source samples and enlarge the\nboundaries across the known classes that indirectly broaden the boundary\nbetween the known and unknown classes. Specifically, we achieve domain\nexpansion by employing both background suppression and style augmentation on\nthe source data to synthesize new samples. Then we force the model to distill\nconsistent knowledge from the synthesized samples so that the model can learn\ndomain-invariant information. Furthermore, we realize boundary growth across\nclasses by using edge maps as an additional modality of samples when training\nmulti-binary classifiers. In this way, it enlarges the boundary between the\ninliers and outliers, and consequently improves the unknown class recognition\nduring open-set generalization. Extensive experiments show that our approach\ncan achieve significant improvements and reach state-of-the-art performance on\nseveral cross-domain image classification datasets.\n","authors":["Pengkun Jiao","Na Zhao","Jingjing Chen","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2411.02920v1.pdf","comment":"TMM 2024"},{"id":"http://arxiv.org/abs/2406.06079v2","updated":"2024-11-05T09:07:21Z","published":"2024-06-10T07:52:29Z","title":"Latent Representation Matters: Human-like Sketches in One-shot Drawing\n  Tasks","summary":"  Humans can effortlessly draw new categories from a single exemplar, a feat\nthat has long posed a challenge for generative models. However, this gap has\nstarted to close with recent advances in diffusion models. This one-shot\ndrawing task requires powerful inductive biases that have not been\nsystematically investigated. Here, we study how different inductive biases\nshape the latent space of Latent Diffusion Models (LDMs). Along with standard\nLDM regularizers (KL and vector quantization), we explore supervised\nregularizations (including classification and prototype-based representation)\nand contrastive inductive biases (using SimCLR and redundancy reduction\nobjectives). We demonstrate that LDMs with redundancy reduction and\nprototype-based regularizations produce near-human-like drawings (regarding\nboth samples' recognizability and originality) -- better mimicking human\nperception (as evaluated psychophysically). Overall, our results suggest that\nthe gap between humans and machines in one-shot drawings is almost closed.\n","authors":["Victor Boutin","Rishav Mukherji","Aditya Agrawal","Sabine Muzellec","Thomas Fel","Thomas Serre","Rufin VanRullen"],"pdf_url":"https://arxiv.org/pdf/2406.06079v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13871v2","updated":"2024-11-05T09:07:14Z","published":"2024-10-02T12:14:31Z","title":"Explaining an image classifier with a generative model conditioned by\n  uncertainty","summary":"  We propose to condition a generative model by a given image classifier\nuncertainty in order to analyze and explain its behavior. Preliminary\nexperiments on synthetic data and a corrupted version of MNIST dataset\nillustrate the idea.\n","authors":["Adrien LeCoz","Stéphane Herbin","Faouzi Adjed"],"pdf_url":"https://arxiv.org/pdf/2410.13871v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02116v2","updated":"2024-11-05T08:35:14Z","published":"2024-11-04T14:29:28Z","title":"Advancements and limitations of LLMs in replicating human color-word\n  associations","summary":"  Color-word associations play a fundamental role in human cognition and design\napplications. Large Language Models (LLMs) have become widely available and\ndemonstrated intelligent behaviors in various benchmarks with natural\nconversation skills. However, their ability to replicate human color-word\nassociations remains understudied. We compared multiple generations of LLMs\n(from GPT-3 to GPT-4o) against human color-word associations using data\ncollected from over 10,000 Japanese participants, involving 17 colors and words\nfrom eight categories in Japanese. Our findings reveal a clear progression in\nLLM performance across generations, with GPT-4o achieving the highest accuracy\nin predicting the best voted word for each color and category. However, the\nhighest median performance was approximately 50% even for GPT-4o with visual\ninputs (chance level is 10%), and the performance levels varied significantly\nacross word categories and colors, indicating a failure to fully replicate\nhuman color-word associations. On the other hand, color discrimination ability\nestimated from our color-word association data showed that LLMs demonstrated\nhigh correlation with human color discrimination patterns, similarly to\nprevious studies. Our study highlights both the advancements in LLM\ncapabilities and their persistent limitations, suggesting differences in\nsemantic memory structures between humans and LLMs in representing color-word\nassociations.\n","authors":["Makoto Fukushima","Shusuke Eshita","Hiroshige Fukuhara"],"pdf_url":"https://arxiv.org/pdf/2411.02116v2.pdf","comment":"20 pages, 7 figures, 3 tables"},{"id":"http://arxiv.org/abs/2411.02902v1","updated":"2024-11-05T08:35:08Z","published":"2024-11-05T08:35:08Z","title":"Membership Inference Attacks against Large Vision-Language Models","summary":"  Large vision-language models (VLLMs) exhibit promising capabilities for\nprocessing multi-modal tasks across various application scenarios. However,\ntheir emergence also raises significant data security concerns, given the\npotential inclusion of sensitive information, such as private photos and\nmedical records, in their training datasets. Detecting inappropriately used\ndata in VLLMs remains a critical and unresolved issue, mainly due to the lack\nof standardized datasets and suitable methodologies. In this study, we\nintroduce the first membership inference attack (MIA) benchmark tailored for\nvarious VLLMs to facilitate training data detection. Then, we propose a novel\nMIA pipeline specifically designed for token-level image detection. Lastly, we\npresent a new metric called MaxR\\'enyi-K%, which is based on the confidence of\nthe model output and applies to both text and image data. We believe that our\nwork can deepen the understanding and methodology of MIAs in the context of\nVLLMs. Our code and datasets are available at\nhttps://github.com/LIONS-EPFL/VL-MIA.\n","authors":["Zhan Li","Yongtao Wu","Yihang Chen","Francesco Tonin","Elias Abad Rocamora","Volkan Cevher"],"pdf_url":"https://arxiv.org/pdf/2411.02902v1.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.02890v1","updated":"2024-11-05T08:04:43Z","published":"2024-11-05T08:04:43Z","title":"Fried deconvolution","summary":"  In this paper we present a new approach to deblur the effect of atmospheric\nturbulence in the case of long range imaging. Our method is based on an\nanalytical formulation, the Fried kernel, of the atmosphere modulation transfer\nfunction (MTF) and a framelet based deconvolution algorithm. An important\nparameter is the refractive index structure which requires specific\nmeasurements to be known. Then we propose a method which provides a good\nestimation of this parameter from the input blurred image. The final algorithms\nare very easy to implement and show very good results on both simulated blur\nand real images.\n","authors":["Jerome Gilles","Stanley Osher"],"pdf_url":"https://arxiv.org/pdf/2411.02890v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02889v1","updated":"2024-11-05T08:04:29Z","published":"2024-11-05T08:04:29Z","title":"Turbulence stabilization","summary":"  We recently developed a new approach to get a stabilized image from a\nsequence of frames acquired through atmospheric turbulence. The goal of this\nalgorihtm is to remove the geometric distortions due by the atmosphere\nmovements. This method is based on a variational formulation and is efficiently\nsolved by the use of Bregman iterations and the operator splitting method. In\nthis paper we propose to study the influence of the choice of the regularizing\nterm in the model. Then we proposed to experiment some of the most used\nregularization constraints available in the litterature.\n","authors":["Yu Mao","Jerome Gilles"],"pdf_url":"https://arxiv.org/pdf/2411.02889v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02888v1","updated":"2024-11-05T08:02:44Z","published":"2024-11-05T08:02:44Z","title":"A Symmetric Dynamic Learning Framework for Diffeomorphic Medical Image\n  Registration","summary":"  Diffeomorphic image registration is crucial for various medical imaging\napplications because it can preserve the topology of the transformation. This\nstudy introduces DCCNN-LSTM-Reg, a learning framework that evolves dynamically\nand learns a symmetrical registration path by satisfying a specified control\nincrement system. This framework aims to obtain symmetric diffeomorphic\ndeformations between moving and fixed images. To achieve this, we combine deep\nlearning networks with diffeomorphic mathematical mechanisms to create a\ncontinuous and dynamic registration architecture, which consists of multiple\nSymmetric Registration (SR) modules cascaded on five different scales.\nSpecifically, our method first uses two U-nets with shared parameters to\nextract multiscale feature pyramids from the images. We then develop an\nSR-module comprising a sequential CNN-LSTM architecture to progressively\ncorrect the forward and reverse multiscale deformation fields using control\nincrement learning and the homotopy continuation technique. Through extensive\nexperiments on three 3D registration tasks, we demonstrate that our method\noutperforms existing approaches in both quantitative and qualitative\nevaluations.\n","authors":["Jinqiu Deng","Ke Chen","Mingke Li","Daoping Zhang","Chong Chen","Alejandro F. Frangi","Jianping Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.02888v1.pdf","comment":"12 pages,7 figures"},{"id":"http://arxiv.org/abs/2411.02871v1","updated":"2024-11-05T07:26:24Z","published":"2024-11-05T07:26:24Z","title":"Enhancing Adversarial Robustness via Uncertainty-Aware Distributional\n  Adversarial Training","summary":"  Despite remarkable achievements in deep learning across various domains, its\ninherent vulnerability to adversarial examples still remains a critical concern\nfor practical deployment. Adversarial training has emerged as one of the most\neffective defensive techniques for improving model robustness against such\nmalicious inputs. However, existing adversarial training schemes often lead to\nlimited generalization ability against underlying adversaries with diversity\ndue to their overreliance on a point-by-point augmentation strategy by mapping\neach clean example to its adversarial counterpart during training. In addition,\nadversarial examples can induce significant disruptions in the statistical\ninformation w.r.t. the target model, thereby introducing substantial\nuncertainty and challenges to modeling the distribution of adversarial\nexamples. To circumvent these issues, in this paper, we propose a novel\nuncertainty-aware distributional adversarial training method, which enforces\nadversary modeling by leveraging both the statistical information of\nadversarial examples and its corresponding uncertainty estimation, with the\ngoal of augmenting the diversity of adversaries. Considering the potentially\nnegative impact induced by aligning adversaries to misclassified clean\nexamples, we also refine the alignment reference based on the statistical\nproximity to clean examples during adversarial training, thereby reframing\nadversarial training within a distribution-to-distribution matching framework\ninteracted between the clean and adversarial domains. Furthermore, we design an\nintrospective gradient alignment approach via matching input gradients between\nthese domains without introducing external models. Extensive experiments across\nfour benchmark datasets and various network architectures demonstrate that our\napproach achieves state-of-the-art adversarial robustness and maintains natural\nperformance.\n","authors":["Junhao Dong","Xinghua Qu","Z. Jane Wang","Yew-Soon Ong"],"pdf_url":"https://arxiv.org/pdf/2411.02871v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04802v3","updated":"2024-11-05T07:25:42Z","published":"2024-06-07T10:06:13Z","title":"Predictive Dynamic Fusion","summary":"  Multimodal fusion is crucial in joint decision-making systems for rendering\nholistic judgments. Since multimodal data changes in open environments, dynamic\nfusion has emerged and achieved remarkable progress in numerous applications.\nHowever, most existing dynamic multimodal fusion methods lack theoretical\nguarantees and easily fall into suboptimal problems, yielding unreliability and\ninstability. To address this issue, we propose a Predictive Dynamic Fusion\n(PDF) framework for multimodal learning. We proceed to reveal the multimodal\nfusion from a generalization perspective and theoretically derive the\npredictable Collaborative Belief (Co-Belief) with Mono- and Holo-Confidence,\nwhich provably reduces the upper bound of generalization error. Accordingly, we\nfurther propose a relative calibration strategy to calibrate the predicted\nCo-Belief for potential uncertainty. Extensive experiments on multiple\nbenchmarks confirm our superiority. Our code is available at\nhttps://github.com/Yinan-Xia/PDF.\n","authors":["Bing Cao","Yinan Xia","Yi Ding","Changqing Zhang","Qinghua Hu"],"pdf_url":"https://arxiv.org/pdf/2406.04802v3.pdf","comment":"Accepted by ICML 2024"},{"id":"http://arxiv.org/abs/2407.12735v3","updated":"2024-11-05T07:24:15Z","published":"2024-07-17T16:55:42Z","title":"EchoSight: Advancing Visual-Language Models with Wiki Knowledge","summary":"  Knowledge-based Visual Question Answering (KVQA) tasks require answering\nquestions about images using extensive background knowledge. Despite\nsignificant advancements, generative models often struggle with these tasks due\nto the limited integration of external knowledge. In this paper, we introduce\nEchoSight, a novel multimodal Retrieval-Augmented Generation (RAG) framework\nthat enables large language models (LLMs) to answer visual questions requiring\nfine-grained encyclopedic knowledge. To strive for high-performing retrieval,\nEchoSight first searches wiki articles by using visual-only information,\nsubsequently, these candidate articles are further reranked according to their\nrelevance to the combined text-image query. This approach significantly\nimproves the integration of multimodal knowledge, leading to enhanced retrieval\noutcomes and more accurate VQA responses. Our experimental results on the\nEncyclopedic VQA and InfoSeek datasets demonstrate that EchoSight establishes\nnew state-of-the-art results in knowledge-based VQA, achieving an accuracy of\n41.8% on Encyclopedic VQA and 31.3% on InfoSeek.\n","authors":["Yibin Yan","Weidi Xie"],"pdf_url":"https://arxiv.org/pdf/2407.12735v3.pdf","comment":"Technical Report; Project Page: https://go2heart.github.io/echosight"},{"id":"http://arxiv.org/abs/2411.02867v1","updated":"2024-11-05T07:16:32Z","published":"2024-11-05T07:16:32Z","title":"AtlasSeg: Atlas Prior Guided Dual-U-Net for Cortical Segmentation in\n  Fetal Brain MRI","summary":"  Accurate tissue segmentation in fetal brain MRI remains challenging due to\nthe dynamically changing anatomical anatomy and contrast during fetal\ndevelopment. To enhance segmentation accuracy throughout gestation, we\nintroduced AtlasSeg, a dual-U-shape convolution network incorporating\ngestational age (GA) specific information as guidance. By providing a publicly\navailable fetal brain atlas with segmentation label at the corresponding GA,\nAtlasSeg effectively extracted the contextual features of age-specific patterns\nin atlas branch and generated tissue segmentation in segmentation branch.\nMulti-scale attentive atlas feature fusions were constructed in all stages\nduring encoding and decoding, giving rise to a dual-U-shape network to assist\nfeature flow and information interactions between two branches. AtlasSeg\noutperformed six well-known segmentation networks in both our internal fetal\nbrain MRI dataset and the external FeTA dataset. Ablation experiments\ndemonstrate the efficiency of atlas guidance and the attention mechanism. The\nproposed AtlasSeg demonstrated superior segmentation performance against other\nconvolution networks with higher segmentation accuracy, and may facilitate\nfetal brain MRI analysis in large-scale fetal brain studies.\n","authors":["Haoan Xu","Tianshu Zheng","Xinyi Xu","Yao Shen","Jiwei Sun","Cong Sun","Guangbin Wang","Dan Wu"],"pdf_url":"https://arxiv.org/pdf/2411.02867v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02861v1","updated":"2024-11-05T07:09:27Z","published":"2024-11-05T07:09:27Z","title":"Centerness-based Instance-aware Knowledge Distillation with Task-wise\n  Mutual Lifting for Object Detection on Drone Imagery","summary":"  Developing accurate and efficient detectors for drone imagery is challenging\ndue to the inherent complexity of aerial scenes. While some existing methods\naim to achieve high accuracy by utilizing larger models, their computational\ncost is prohibitive for drones. Recently, Knowledge Distillation (KD) has shown\npromising potential for maintaining satisfactory accuracy while significantly\ncompressing models in general object detection. Considering the advantages of\nKD, this paper presents the first attempt to adapt it to object detection on\ndrone imagery and addresses two intrinsic issues: (1) low foreground-background\nratio and (2) small instances and complex backgrounds, which lead to inadequate\ntraining, resulting insufficient distillation. Therefore, we propose a\ntask-wise Lightweight Mutual Lifting (Light-ML) module with a Centerness-based\nInstance-aware Distillation (CID) strategy. The Light-ML module mutually\nharmonizes the classification and localization branches by channel shuffling\nand convolution, integrating teacher supervision across different tasks during\nback-propagation, thus facilitating training the student model. The CID\nstrategy extracts valuable regions surrounding instances through the centerness\nof proposals, enhancing distillation efficacy. Experiments on the VisDrone,\nUAVDT, and COCO benchmarks demonstrate that the proposed approach promotes the\naccuracies of existing state-of-the-art KD methods with comparable\ncomputational requirements. Codes will be available upon acceptance.\n","authors":["Bowei Du","Zhixuan Liao","Yanan Zhang","Zhi Cai","Jiaxin Chen","Di Huang"],"pdf_url":"https://arxiv.org/pdf/2411.02861v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02860v1","updated":"2024-11-05T07:09:14Z","published":"2024-11-05T07:09:14Z","title":"Continual Audio-Visual Sound Separation","summary":"  In this paper, we introduce a novel continual audio-visual sound separation\ntask, aiming to continuously separate sound sources for new classes while\npreserving performance on previously learned classes, with the aid of visual\nguidance. This problem is crucial for practical visually guided auditory\nperception as it can significantly enhance the adaptability and robustness of\naudio-visual sound separation models, making them more applicable for\nreal-world scenarios where encountering new sound sources is commonplace. The\ntask is inherently challenging as our models must not only effectively utilize\ninformation from both modalities in current tasks but also preserve their\ncross-modal association in old tasks to mitigate catastrophic forgetting during\naudio-visual continual learning. To address these challenges, we propose a\nnovel approach named ContAV-Sep (\\textbf{Cont}inual\n\\textbf{A}udio-\\textbf{V}isual Sound \\textbf{Sep}aration). ContAV-Sep presents\na novel Cross-modal Similarity Distillation Constraint (CrossSDC) to uphold the\ncross-modal semantic similarity through incremental tasks and retain previously\nacquired knowledge of semantic similarity in old models, mitigating the risk of\ncatastrophic forgetting. The CrossSDC can seamlessly integrate into the\ntraining process of different audio-visual sound separation frameworks.\nExperiments demonstrate that ContAV-Sep can effectively mitigate catastrophic\nforgetting and achieve significantly better performance compared to other\ncontinual learning baselines for audio-visual sound separation. Code is\navailable at: \\url{https://github.com/weiguoPian/ContAV-Sep_NeurIPS2024}.\n","authors":["Weiguo Pian","Yiyang Nan","Shijian Deng","Shentong Mo","Yunhui Guo","Yapeng Tian"],"pdf_url":"https://arxiv.org/pdf/2411.02860v1.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.02858v1","updated":"2024-11-05T07:02:25Z","published":"2024-11-05T07:02:25Z","title":"OLAF: A Plug-and-Play Framework for Enhanced Multi-object Multi-part\n  Scene Parsing","summary":"  Multi-object multi-part scene segmentation is a challenging task whose\ncomplexity scales exponentially with part granularity and number of scene\nobjects. To address the task, we propose a plug-and-play approach termed OLAF.\nFirst, we augment the input (RGB) with channels containing object-based\nstructural cues (fg/bg mask, boundary edge mask). We propose a weight\nadaptation technique which enables regular (RGB) pre-trained models to process\nthe augmented (5-channel) input in a stable manner during optimization. In\naddition, we introduce an encoder module termed LDF to provide low-level dense\nfeature guidance. This assists segmentation, particularly for smaller parts.\nOLAF enables significant mIoU gains of $\\mathbf{3.3}$ (Pascal-Parts-58),\n$\\mathbf{3.5}$ (Pascal-Parts-108) over the SOTA model. On the most challenging\nvariant (Pascal-Parts-201), the gain is $\\mathbf{4.0}$. Experimentally, we show\nthat OLAF's broad applicability enables gains across multiple architectures\n(CNN, U-Net, Transformer) and datasets. The code is available at\nolafseg.github.io\n","authors":["Pranav Gupta","Rishubh Singh","Pradeep Shenoy","Ravikiran Sarvadevabhatla"],"pdf_url":"https://arxiv.org/pdf/2411.02858v1.pdf","comment":"Accepted in The European Conference on Computer Vision (ECCV) 2024"},{"id":"http://arxiv.org/abs/2411.02855v1","updated":"2024-11-05T06:59:05Z","published":"2024-11-05T06:59:05Z","title":"Analyzing Poverty through Intra-Annual Time-Series: A Wavelet Transform\n  Approach","summary":"  Reducing global poverty is a key objective of the Sustainable Development\nGoals (SDGs). Achieving this requires high-frequency, granular data to capture\nneighborhood-level changes, particularly in data scarce regions such as low-\nand middle-income countries. To fill in the data gaps, recent computer vision\nmethods combining machine learning (ML) with earth observation (EO) data to\nimprove poverty estimation. However, while much progress have been made, they\noften omit intra-annual variations, which are crucial for estimating poverty in\nagriculturally dependent countries. We explored the impact of integrating\nintra-annual NDVI information with annual multi-spectral data on model\naccuracy. To evaluate our method, we created a simulated dataset using Landsat\nimagery and nighttime light data to evaluate EO-ML methods that use\nintra-annual EO data. Additionally, we evaluated our method against the\nDemographic and Health Survey (DHS) dataset across Africa. Our results indicate\nthat integrating specific NDVI-derived features with multi-spectral data\nprovides valuable insights for poverty analysis, emphasizing the importance of\nretaining intra-annual information.\n","authors":["Mohammad Kakooei","Klaudia Solska","Adel Daoud"],"pdf_url":"https://arxiv.org/pdf/2411.02855v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01781v2","updated":"2024-11-05T06:55:19Z","published":"2024-11-04T04:14:39Z","title":"MSTA3D: Multi-scale Twin-attention for 3D Instance Segmentation","summary":"  Recently, transformer-based techniques incorporating superpoints have become\nprevalent in 3D instance segmentation. However, they often encounter an\nover-segmentation problem, especially noticeable with large objects.\nAdditionally, unreliable mask predictions stemming from superpoint mask\nprediction further compound this issue. To address these challenges, we propose\na novel framework called MSTA3D. It leverages multi-scale feature\nrepresentation and introduces a twin-attention mechanism to effectively capture\nthem. Furthermore, MSTA3D integrates a box query with a box regularizer,\noffering a complementary spatial constraint alongside semantic queries.\nExperimental evaluations on ScanNetV2, ScanNet200 and S3DIS datasets\ndemonstrate that our approach surpasses state-of-the-art 3D instance\nsegmentation methods.\n","authors":["Duc Dang Trung Tran","Byeongkeun Kang","Yeejin Lee"],"pdf_url":"https://arxiv.org/pdf/2411.01781v2.pdf","comment":"14 pages, 9 figures, 7 tables, conference"},{"id":"http://arxiv.org/abs/2409.17612v2","updated":"2024-11-05T06:47:37Z","published":"2024-09-26T08:03:19Z","title":"Diversity-Driven Synthesis: Enhancing Dataset Distillation through\n  Directed Weight Adjustment","summary":"  The sharp increase in data-related expenses has motivated research into\ncondensing datasets while retaining the most informative features. Dataset\ndistillation has thus recently come to the fore. This paradigm generates\nsynthetic datasets that are representative enough to replace the original\ndataset in training a neural network. To avoid redundancy in these synthetic\ndatasets, it is crucial that each element contains unique features and remains\ndiverse from others during the synthesis stage. In this paper, we provide a\nthorough theoretical and empirical analysis of diversity within synthesized\ndatasets. We argue that enhancing diversity can improve the parallelizable yet\nisolated synthesizing approach. Specifically, we introduce a novel method that\nemploys dynamic and directed weight adjustment techniques to modulate the\nsynthesis process, thereby maximizing the representativeness and diversity of\neach synthetic instance. Our method ensures that each batch of synthetic data\nmirrors the characteristics of a large, varying subset of the original dataset.\nExtensive experiments across multiple datasets, including CIFAR, Tiny-ImageNet,\nand ImageNet-1K, demonstrate the superior performance of our method,\nhighlighting its effectiveness in producing diverse and representative\nsynthetic datasets with minimal computational expense. Our code is available at\nhttps://github.com/AngusDujw/Diversity-Driven-Synthesis.https://github.com/AngusDujw/Diversity-Driven-Synthesis.\n","authors":["Jiawei Du","Xin Zhang","Juncheng Hu","Wenxin Huang","Joey Tianyi Zhou"],"pdf_url":"https://arxiv.org/pdf/2409.17612v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23775v3","updated":"2024-11-05T06:41:27Z","published":"2024-10-31T09:45:00Z","title":"In-Context LoRA for Diffusion Transformers","summary":"  Recent research arXiv:2410.15027 has explored the use of diffusion\ntransformers (DiTs) for task-agnostic image generation by simply concatenating\nattention tokens across images. However, despite substantial computational\nresources, the fidelity of the generated images remains suboptimal. In this\nstudy, we reevaluate and streamline this framework by hypothesizing that\ntext-to-image DiTs inherently possess in-context generation capabilities,\nrequiring only minimal tuning to activate them. Through diverse task\nexperiments, we qualitatively demonstrate that existing text-to-image DiTs can\neffectively perform in-context generation without any tuning. Building on this\ninsight, we propose a remarkably simple pipeline to leverage the in-context\nabilities of DiTs: (1) concatenate images instead of tokens, (2) perform joint\ncaptioning of multiple images, and (3) apply task-specific LoRA tuning using\nsmall datasets (e.g., 20~100 samples) instead of full-parameter tuning with\nlarge datasets. We name our models In-Context LoRA (IC-LoRA). This approach\nrequires no modifications to the original DiT models, only changes to the\ntraining data. Remarkably, our pipeline generates high-fidelity image sets that\nbetter adhere to prompts. While task-specific in terms of tuning data, our\nframework remains task-agnostic in architecture and pipeline, offering a\npowerful tool for the community and providing valuable insights for further\nresearch on product-level task-agnostic generation systems. We release our\ncode, data, and models at https://github.com/ali-vilab/In-Context-LoRA\n","authors":["Lianghua Huang","Wei Wang","Zhi-Fan Wu","Yupeng Shi","Huanzhang Dou","Chen Liang","Yutong Feng","Yu Liu","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.23775v3.pdf","comment":"Tech report. Project page:\n  https://ali-vilab.github.io/In-Context-LoRA-Page/"},{"id":"http://arxiv.org/abs/2407.09562v3","updated":"2024-11-05T06:35:52Z","published":"2024-07-03T10:21:07Z","title":"Edge AI-Enabled Chicken Health Detection Based on Enhanced FCOS-Lite and\n  Knowledge Distillation","summary":"  The utilization of AIoT technology has become a crucial trend in modern\npoultry management, offering the potential to optimize farming operations and\nreduce human workloads. This paper presents a real-time and compact edge-AI\nenabled detector designed to identify chickens and their healthy statuses using\nframes captured by a lightweight and intelligent camera equipped with an\nedge-AI enabled CMOS sensor. To ensure efficient deployment of the proposed\ncompact detector within the memory-constrained edge-AI enabled CMOS sensor, we\nemploy a FCOS-Lite detector leveraging MobileNet as the backbone. To mitigate\nthe issue of reduced accuracy in compact edge-AI detectors without incurring\nadditional inference costs, we propose a gradient weighting loss function as\nclassification loss and introduce CIOU loss function as localization loss.\nAdditionally, we propose a knowledge distillation scheme to transfer valuable\ninformation from a large teacher detector to the proposed FCOS-Lite detector,\nthereby enhancing its performance while preserving a compact model size.\nExperimental results demonstrate the proposed edge-AI enabled detector achieves\ncommendable performance metrics, including a mean average precision (mAP) of\n95.1$\\%$ and an F1-score of 94.2$\\%$, etc. Notably, the proposed detector can\nbe efficiently deployed and operates at a speed exceeding 20 FPS on the edge-AI\nenabled CMOS sensor, achieved through int8 quantization. That meets practical\ndemands for automated poultry health monitoring using lightweight intelligent\ncameras with low power consumption and minimal bandwidth costs.\n","authors":["Qiang Tong","Jinrui Wang","Wenshuang Yang","Songtao Wu","Wenqi Zhang","Chen Sun","Kuanhong Xu"],"pdf_url":"https://arxiv.org/pdf/2407.09562v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12629v4","updated":"2024-11-05T06:34:40Z","published":"2024-06-18T13:55:13Z","title":"SeTAR: Out-of-Distribution Detection with Selective Low-Rank\n  Approximation","summary":"  Out-of-distribution (OOD) detection is crucial for the safe deployment of\nneural networks. Existing CLIP-based approaches perform OOD detection by\ndevising novel scoring functions or sophisticated fine-tuning methods. In this\nwork, we propose SeTAR, a novel, training-free OOD detection method that\nleverages selective low-rank approximation of weight matrices in\nvision-language and vision-only models. SeTAR enhances OOD detection via\npost-hoc modification of the model's weight matrices using a simple greedy\nsearch algorithm. Based on SeTAR, we further propose SeTAR+FT, a fine-tuning\nextension optimizing model performance for OOD detection tasks. Extensive\nevaluations on ImageNet1K and Pascal-VOC benchmarks show SeTAR's superior\nperformance, reducing the relatively false positive rate by up to 18.95% and\n36.80% compared to zero-shot and fine-tuning baselines. Ablation studies\nfurther validate SeTAR's effectiveness, robustness, and generalizability across\ndifferent model backbones. Our work offers a scalable, efficient solution for\nOOD detection, setting a new state-of-the-art in this area.\n","authors":["Yixia Li","Boya Xiong","Guanhua Chen","Yun Chen"],"pdf_url":"https://arxiv.org/pdf/2406.12629v4.pdf","comment":"Accepted by NeurIPS 2024. Project page is live at\n  https://SeTAR-OOD.github.io. Code are available at\n  https://github.com/X1AOX1A/SeTAR"},{"id":"http://arxiv.org/abs/2411.02844v1","updated":"2024-11-05T06:34:19Z","published":"2024-11-05T06:34:19Z","title":"Correlation of Object Detection Performance with Visual Saliency and\n  Depth Estimation","summary":"  As object detection techniques continue to evolve, understanding their\nrelationships with complementary visual tasks becomes crucial for optimising\nmodel architectures and computational resources. This paper investigates the\ncorrelations between object detection accuracy and two fundamental visual\ntasks: depth prediction and visual saliency prediction. Through comprehensive\nexperiments using state-of-the-art models (DeepGaze IIE, Depth Anything,\nDPT-Large, and Itti's model) on COCO and Pascal VOC datasets, we find that\nvisual saliency shows consistently stronger correlations with object detection\naccuracy (mA$\\rho$ up to 0.459 on Pascal VOC) compared to depth prediction\n(mA$\\rho$ up to 0.283). Our analysis reveals significant variations in these\ncorrelations across object categories, with larger objects showing correlation\nvalues up to three times higher than smaller objects. These findings suggest\nincorporating visual saliency features into object detection architectures\ncould be more beneficial than depth information, particularly for specific\nobject categories. The observed category-specific variations also provide\ninsights for targeted feature engineering and dataset design improvements,\npotentially leading to more efficient and accurate object detection systems.\n","authors":["Matthias Bartolo","Dylan Seychell"],"pdf_url":"https://arxiv.org/pdf/2411.02844v1.pdf","comment":"Code Available at:\n  https://github.com/mbar0075/Object-Detection-Correlation-Saliency-vs-Depth"},{"id":"http://arxiv.org/abs/2411.02843v1","updated":"2024-11-05T06:31:48Z","published":"2024-11-05T06:31:48Z","title":"Advances in Photoacoustic Imaging Reconstruction and Quantitative\n  Analysis for Biomedical Applications","summary":"  Photoacoustic imaging (PAI) represents an innovative biomedical imaging\nmodality that harnesses the advantages of optical resolution and acoustic\npenetration depth while ensuring enhanced safety. Despite its promising\npotential across a diverse array of preclinical and clinical applications, the\nclinical implementation of PAI faces significant challenges, including the\ntrade-off between penetration depth and spatial resolution, as well as the\ndemand for faster imaging speeds. This paper explores the fundamental\nprinciples underlying PAI, with a particular emphasis on three primary\nimplementations: photoacoustic computed tomography (PACT), photoacoustic\nmicroscopy (PAM), and photoacoustic endoscopy (PAE). We undertake a critical\nassessment of their respective strengths and practical limitations.\nFurthermore, recent developments in utilizing conventional or deep learning\n(DL) methodologies for image reconstruction and artefact mitigation across\nPACT, PAM, and PAE are outlined, demonstrating considerable potential to\nenhance image quality and accelerate imaging processes. Furthermore, this paper\nexamines the recent developments in quantitative analysis within PAI, including\nthe quantification of haemoglobin concentration, oxygen saturation, and other\nphysiological parameters within tissues. Finally, our discussion encompasses\ncurrent trends and future directions in PAI research while emphasizing the\ntransformative impact of deep learning on advancing PAI.\n","authors":["Lei Wang","Weiming Zeng","Kai Long","Rongfeng Lan","Li Liu","Wai Ting Siok","Nizhuan Wang"],"pdf_url":"https://arxiv.org/pdf/2411.02843v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02840v1","updated":"2024-11-05T06:23:44Z","published":"2024-11-05T06:23:44Z","title":"Test-Time Dynamic Image Fusion","summary":"  The inherent challenge of image fusion lies in capturing the correlation of\nmulti-source images and comprehensively integrating effective information from\ndifferent sources. Most existing techniques fail to perform dynamic image\nfusion while notably lacking theoretical guarantees, leading to potential\ndeployment risks in this field. Is it possible to conduct dynamic image fusion\nwith a clear theoretical justification? In this paper, we give our solution\nfrom a generalization perspective. We proceed to reveal the generalized form of\nimage fusion and derive a new test-time dynamic image fusion paradigm. It\nprovably reduces the upper bound of generalization error. Specifically, we\ndecompose the fused image into multiple components corresponding to its source\ndata. The decomposed components represent the effective information from the\nsource data, thus the gap between them reflects the Relative Dominability (RD)\nof the uni-source data in constructing the fusion image. Theoretically, we\nprove that the key to reducing generalization error hinges on the negative\ncorrelation between the RD-based fusion weight and the uni-source\nreconstruction loss. Intuitively, RD dynamically highlights the dominant\nregions of each source and can be naturally converted to the corresponding\nfusion weight, achieving robust results. Extensive experiments and discussions\nwith in-depth analysis on multiple benchmarks confirm our findings and\nsuperiority. Our code is available at https://github.com/Yinan-Xia/TTD.\n","authors":["Bing Cao","Yinan Xia","Yi Ding","Changqing Zhang","Qinghua Hu"],"pdf_url":"https://arxiv.org/pdf/2411.02840v1.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2404.11031v3","updated":"2024-11-05T06:21:42Z","published":"2024-04-17T03:13:58Z","title":"TaCOS: Task-Specific Camera Optimization with Simulation","summary":"  The performance of perception tasks is heavily influenced by imaging systems.\nHowever, designing cameras with high task performance is costly, requiring\nextensive camera knowledge and experimentation with physical hardware.\nAdditionally, cameras and perception tasks are mostly designed in isolation,\nwhereas recent methods that jointly design cameras and tasks have shown\nimproved performance. Therefore, we present a novel end-to-end optimization\napproach that co-designs cameras with specific vision tasks. This method\ncombines derivative-free and gradient-based optimizers to support both\ncontinuous and discrete camera parameters within manufacturing constraints. We\nleverage recent computer graphics techniques and physical camera\ncharacteristics to simulate the cameras in virtual environments, making the\ndesign process cost-effective. We validate our simulations against physical\ncameras and provide a procedurally generated virtual environment. Our\nexperiments demonstrate that our method designs cameras that outperform common\noff-the-shelf options, and more efficiently compared to the state-of-the-art\napproach, requiring only 2 minutes to design a camera on an example experiment\ncompared with 67 minutes for the competing method. Designed to support the\ndevelopment of cameras under manufacturing constraints, multiple cameras, and\nunconventional cameras, we believe this approach can advance the fully\nautomated design of cameras.\n","authors":["Chengyang Yan","Donald G. Dansereau"],"pdf_url":"https://arxiv.org/pdf/2404.11031v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.16014v3","updated":"2024-11-05T06:13:07Z","published":"2023-12-26T11:49:23Z","title":"Passive Non-Line-of-Sight Imaging with Light Transport Modulation","summary":"  Passive non-line-of-sight (NLOS) imaging has witnessed rapid development in\nrecent years, due to its ability to image objects that are out of sight. The\nlight transport condition plays an important role in this task since changing\nthe conditions will lead to different imaging models. Existing learning-based\nNLOS methods usually train independent models for different light transport\nconditions, which is computationally inefficient and impairs the practicality\nof the models. In this work, we propose NLOS-LTM, a novel passive NLOS imaging\nmethod that effectively handles multiple light transport conditions with a\nsingle network. We achieve this by inferring a latent light transport\nrepresentation from the projection image and using this representation to\nmodulate the network that reconstructs the hidden image from the projection\nimage. We train a light transport encoder together with a vector quantizer to\nobtain the light transport representation. To further regulate this\nrepresentation, we jointly learn both the reconstruction network and the\nreprojection network during training. A set of light transport modulation\nblocks is used to modulate the two jointly trained networks in a multi-scale\nway. Extensive experiments on a large-scale passive NLOS dataset demonstrate\nthe superiority of the proposed method. The code is available at\nhttps://github.com/JerryOctopus/NLOS-LTM.\n","authors":["Jiarui Zhang","Ruixu Geng","Xiaolong Du","Yan Chen","Houqiang Li","Yang Hu"],"pdf_url":"https://arxiv.org/pdf/2312.16014v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02833v1","updated":"2024-11-05T06:13:01Z","published":"2024-11-05T06:13:01Z","title":"Lost in Context: The Influence of Context on Feature Attribution Methods\n  for Object Recognition","summary":"  Contextual information plays a critical role in object recognition models\nwithin computer vision, where changes in context can significantly affect\naccuracy, underscoring models' dependence on contextual cues. This study\ninvestigates how context manipulation influences both model accuracy and\nfeature attribution, providing insights into the reliance of object recognition\nmodels on contextual information as understood through the lens of feature\nattribution methods.\n  We employ a range of feature attribution techniques to decipher the reliance\nof deep neural networks on context in object recognition tasks. Using the\nImageNet-9 and our curated ImageNet-CS datasets, we conduct experiments to\nevaluate the impact of contextual variations, analyzed through feature\nattribution methods. Our findings reveal several key insights: (a) Correctly\nclassified images predominantly emphasize object volume attribution over\ncontext volume attribution. (b) The dependence on context remains relatively\nstable across different context modifications, irrespective of classification\naccuracy. (c) Context change exerts a more pronounced effect on model\nperformance than Context perturbations. (d) Surprisingly, context attribution\nin `no-information' scenarios is non-trivial. Our research moves beyond\ntraditional methods by assessing the implications of broad-level modifications\non object recognition, either in the object or its context.\n","authors":["Sayanta Adhikari","Rishav Kumar","Konda Reddy Mopuri","Rajalakshmi Pachamuthu"],"pdf_url":"https://arxiv.org/pdf/2411.02833v1.pdf","comment":"Published in ICVGIP 2024"},{"id":"http://arxiv.org/abs/2411.02319v2","updated":"2024-11-05T06:08:43Z","published":"2024-11-04T17:45:44Z","title":"GenXD: Generating Any 3D and 4D Scenes","summary":"  Recent developments in 2D visual generation have been remarkably successful.\nHowever, 3D and 4D generation remain challenging in real-world applications due\nto the lack of large-scale 4D data and effective model design. In this paper,\nwe propose to jointly investigate general 3D and 4D generation by leveraging\ncamera and object movements commonly observed in daily life. Due to the lack of\nreal-world 4D data in the community, we first propose a data curation pipeline\nto obtain camera poses and object motion strength from videos. Based on this\npipeline, we introduce a large-scale real-world 4D scene dataset: CamVid-30K.\nBy leveraging all the 3D and 4D data, we develop our framework, GenXD, which\nallows us to produce any 3D or 4D scene. We propose multiview-temporal modules,\nwhich disentangle camera and object movements, to seamlessly learn from both 3D\nand 4D data. Additionally, GenXD employs masked latent conditions to support a\nvariety of conditioning views. GenXD can generate videos that follow the camera\ntrajectory as well as consistent 3D views that can be lifted into 3D\nrepresentations. We perform extensive evaluations across various real-world and\nsynthetic datasets, demonstrating GenXD's effectiveness and versatility\ncompared to previous methods in 3D and 4D generation.\n","authors":["Yuyang Zhao","Chung-Ching Lin","Kevin Lin","Zhiwen Yan","Linjie Li","Zhengyuan Yang","Jianfeng Wang","Gim Hee Lee","Lijuan Wang"],"pdf_url":"https://arxiv.org/pdf/2411.02319v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19582v2","updated":"2024-11-05T05:39:33Z","published":"2024-09-29T07:03:05Z","title":"Self-supervised Auxiliary Learning for Texture and Model-based Hybrid\n  Robust and Fair Featuring in Face Analysis","summary":"  In this work, we explore Self-supervised Learning (SSL) as an auxiliary task\nto blend the texture-based local descriptors into feature modelling for\nefficient face analysis. Combining a primary task and a self-supervised\nauxiliary task is beneficial for robust representation. Therefore, we used the\nSSL task of mask auto-encoder (MAE) as an auxiliary task to reconstruct texture\nfeatures such as local patterns along with the primary task for robust and\nunbiased face analysis. We experimented with our hypothesis on three major\nparadigms of face analysis: face attribute and face-based emotion analysis, and\ndeepfake detection. Our experiment results exhibit that better feature\nrepresentation can be gleaned from our proposed model for fair and bias-less\nface analysis.\n","authors":["Shukesh Reddy","Nishit Poddar","Srijan Das","Abhijit Das"],"pdf_url":"https://arxiv.org/pdf/2409.19582v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03561v1","updated":"2024-11-05T23:53:19Z","published":"2024-11-05T23:53:19Z","title":"Estimating Ego-Body Pose from Doubly Sparse Egocentric Video Data","summary":"  We study the problem of estimating the body movements of a camera wearer from\negocentric videos. Current methods for ego-body pose estimation rely on\ntemporally dense sensor data, such as IMU measurements from spatially sparse\nbody parts like the head and hands. However, we propose that even temporally\nsparse observations, such as hand poses captured intermittently from egocentric\nvideos during natural or periodic hand movements, can effectively constrain\noverall body motion. Naively applying diffusion models to generate full-body\npose from head pose and sparse hand pose leads to suboptimal results. To\novercome this, we develop a two-stage approach that decomposes the problem into\ntemporal completion and spatial completion. First, our method employs masked\nautoencoders to impute hand trajectories by leveraging the spatiotemporal\ncorrelations between the head pose sequence and intermittent hand poses,\nproviding uncertainty estimates. Subsequently, we employ conditional diffusion\nmodels to generate plausible full-body motions based on these temporally dense\ntrajectories of the head and hands, guided by the uncertainty estimates from\nthe imputation. The effectiveness of our method was rigorously tested and\nvalidated through comprehensive experiments conducted on various HMD setup with\nAMASS and Ego-Exo4D datasets.\n","authors":["Seunggeun Chi","Pin-Hao Huang","Enna Sachdeva","Hengbo Ma","Karthik Ramani","Kwonjoon Lee"],"pdf_url":"https://arxiv.org/pdf/2411.03561v1.pdf","comment":"Accepted at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2408.07832v5","updated":"2024-11-05T23:50:14Z","published":"2024-07-31T14:49:35Z","title":"LADDER: Language Driven Slice Discovery and Error Rectification","summary":"  Error slice discovery associates structured patterns with model errors.\nExisting methods discover error slices by clustering the error-prone samples\nwith similar patterns or assigning discrete attributes to each sample for\npost-hoc analysis. While these methods aim for interpretability and easier\nmitigation through reweighting or rebalancing, they may not capture the full\ncomplexity of error patterns due to incomplete or missing attributes. Contrary\nto the existing approach, this paper utilizes the reasoning capabilities of the\nLarge Language Model (LLM) to analyze complex error patterns and generate\ntestable hypotheses. This paper proposes LADDER: Language Driven slice\nDiscovery and Error Rectification. It first projects the model's representation\ninto a language-aligned feature space (eg CLIP) to preserve semantics in the\noriginal model feature space. This ensures the accurate retrieval of sentences\nthat highlight the model's errors. Next, the LLM utilizes the sentences and\ngenerates hypotheses to discover error slices. Finally, we mitigate the error\nby fine-tuning the classification head by creating a group-balanced dataset\nusing the hypotheses. Our entire method does not require any attribute\nannotation, either explicitly or through external tagging models. We validate\nour method with \\textbf{five} image classification datasets.\n","authors":["Shantanu Ghosh","Rayan Syed","Chenyu Wang","Clare B. Poynton","Shyam Visweswaran","Kayhan Batmanghelich"],"pdf_url":"https://arxiv.org/pdf/2408.07832v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03555v1","updated":"2024-11-05T23:28:57Z","published":"2024-11-05T23:28:57Z","title":"Object and Contact Point Tracking in Demonstrations Using 3D Gaussian\n  Splatting","summary":"  This paper introduces a method to enhance Interactive Imitation Learning\n(IIL) by extracting touch interaction points and tracking object movement from\nvideo demonstrations. The approach extends current IIL systems by providing\nrobots with detailed knowledge of both where and how to interact with objects,\nparticularly complex articulated ones like doors and drawers. By leveraging\ncutting-edge techniques such as 3D Gaussian Splatting and FoundationPose for\ntracking, this method allows robots to better understand and manipulate objects\nin dynamic environments. The research lays the foundation for more effective\ntask learning and execution in autonomous robotic systems.\n","authors":["Michael Büttner","Jonathan Francis","Helge Rhodin","Andrew Melnik"],"pdf_url":"https://arxiv.org/pdf/2411.03555v1.pdf","comment":"CoRL 2024, Workshop on Lifelong Learning for Home Robots, Munich,\n  Germany"},{"id":"http://arxiv.org/abs/2411.03554v1","updated":"2024-11-05T23:26:10Z","published":"2024-11-05T23:26:10Z","title":"Benchmarking Vision Language Model Unlearning via Fictitious Facial\n  Identity Dataset","summary":"  Machine unlearning has emerged as an effective strategy for forgetting\nspecific information in the training data. However, with the increasing\nintegration of visual data, privacy concerns in Vision Language Models (VLMs)\nremain underexplored. To address this, we introduce Facial Identity Unlearning\nBenchmark (FIUBench), a novel VLM unlearning benchmark designed to robustly\nevaluate the effectiveness of unlearning algorithms under the Right to be\nForgotten setting. Specifically, we formulate the VLM unlearning task via\nconstructing the Fictitious Facial Identity VQA dataset and apply a two-stage\nevaluation pipeline that is designed to precisely control the sources of\ninformation and their exposure levels. In terms of evaluation, since VLM\nsupports various forms of ways to ask questions with the same semantic meaning,\nwe also provide robust evaluation metrics including membership inference\nattacks and carefully designed adversarial privacy attacks to evaluate the\nperformance of algorithms. Through the evaluation of four baseline VLM\nunlearning algorithms within FIUBench, we find that all methods remain limited\nin their unlearning performance, with significant trade-offs between model\nutility and forget quality. Furthermore, our findings also highlight the\nimportance of privacy attacks for robust evaluations. We hope FIUBench will\ndrive progress in developing more effective VLM unlearning algorithms.\n","authors":["Yingzi Ma","Jiongxiao Wang","Fei Wang","Siyuan Ma","Jiazhao Li","Xiujun Li","Furong Huang","Lichao Sun","Bo Li","Yejin Choi","Muhao Chen","Chaowei Xiao"],"pdf_url":"https://arxiv.org/pdf/2411.03554v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03551v1","updated":"2024-11-05T23:11:26Z","published":"2024-11-05T23:11:26Z","title":"Enhancing Weakly Supervised Semantic Segmentation for Fibrosis via\n  Controllable Image Generation","summary":"  Fibrotic Lung Disease (FLD) is a severe condition marked by lung stiffening\nand scarring, leading to respiratory decline. High-resolution computed\ntomography (HRCT) is critical for diagnosing and monitoring FLD; however,\nfibrosis appears as irregular, diffuse patterns with unclear boundaries,\nleading to high inter-observer variability and time-intensive manual\nannotation. To tackle this challenge, we propose DiffSeg, a novel weakly\nsupervised semantic segmentation (WSSS) method that uses image-level\nannotations to generate pixel-level fibrosis segmentation, reducing the need\nfor fine-grained manual labeling. Additionally, our DiffSeg incorporates a\ndiffusion-based generative model to synthesize HRCT images with different\nlevels of fibrosis from healthy slices, enabling the generation of the\nfibrosis-injected slices and their paired fibrosis location. Experiments\nindicate that our method significantly improves the accuracy of pseudo masks\ngenerated by existing WSSS methods, greatly reducing the complexity of manual\nlabeling and enhancing the consistency of the generated masks.\n","authors":["Zhiling Yue","Yingying Fang","Liutao Yang","Nikhil Baid","Simon Walsh","Guang Yang"],"pdf_url":"https://arxiv.org/pdf/2411.03551v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03531v1","updated":"2024-11-05T22:14:35Z","published":"2024-11-05T22:14:35Z","title":"Personalized Video Summarization by Multimodal Video Understanding","summary":"  Video summarization techniques have been proven to improve the overall user\nexperience when it comes to accessing and comprehending video content. If the\nuser's preference is known, video summarization can identify significant\ninformation or relevant content from an input video, aiding them in obtaining\nthe necessary information or determining their interest in watching the\noriginal video. Adapting video summarization to various types of video and user\npreferences requires significant training data and expensive human labeling. To\nfacilitate such research, we proposed a new benchmark for video summarization\nthat captures various user preferences. Also, we present a pipeline called\nVideo Summarization with Language (VSL) for user-preferred video summarization\nthat is based on pre-trained visual language models (VLMs) to avoid the need to\ntrain a video summarization system on a large training dataset. The pipeline\ntakes both video and closed captioning as input and performs semantic analysis\nat the scene level by converting video frames into text. Subsequently, the\nuser's genre preference was used as the basis for selecting the pertinent\ntextual scenes. The experimental results demonstrate that our proposed pipeline\noutperforms current state-of-the-art unsupervised video summarization models.\nWe show that our method is more adaptable across different datasets compared to\nsupervised query-based video summarization models. In the end, the runtime\nanalysis demonstrates that our pipeline is more suitable for practical use when\nscaling up the number of user preferences and videos.\n","authors":["Brian Chen","Xiangyuan Zhao","Yingnan Zhu"],"pdf_url":"https://arxiv.org/pdf/2411.03531v1.pdf","comment":"In Proceedings of CIKM 2024 Applied Research Track"},{"id":"http://arxiv.org/abs/2312.05269v3","updated":"2024-11-05T22:08:14Z","published":"2023-12-07T19:19:25Z","title":"LifelongMemory: Leveraging LLMs for Answering Queries in Long-form\n  Egocentric Videos","summary":"  In this paper we introduce LifelongMemory, a new framework for accessing\nlong-form egocentric videographic memory through natural language question\nanswering and retrieval. LifelongMemory generates concise video activity\ndescriptions of the camera wearer and leverages the zero-shot capabilities of\npretrained large language models to perform reasoning over long-form video\ncontext. Furthermore, LifelongMemory uses a confidence and explanation module\nto produce confident, high-quality, and interpretable answers. Our approach\nachieves state-of-the-art performance on the EgoSchema benchmark for question\nanswering and is highly competitive on the natural language query (NLQ)\nchallenge of Ego4D. Code is available at\nhttps://github.com/agentic-learning-ai-lab/lifelong-memory.\n","authors":["Ying Wang","Yanlai Yang","Mengye Ren"],"pdf_url":"https://arxiv.org/pdf/2312.05269v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.08798v2","updated":"2024-11-05T21:10:54Z","published":"2023-09-15T22:45:02Z","title":"D3: Data Diversity Design for Systematic Generalization in Visual\n  Question Answering","summary":"  Systematic generalization is a crucial aspect of intelligence, which refers\nto the ability to generalize to novel tasks by combining known subtasks and\nconcepts. One critical factor that has been shown to influence systematic\ngeneralization is the diversity of training data. However, diversity can be\ndefined in various ways, as data have many factors of variation. A more\ngranular understanding of how different aspects of data diversity affect\nsystematic generalization is lacking. We present new evidence in the problem of\nVisual Question Answering (VQA) that reveals that the diversity of simple tasks\n(i.e. tasks formed by a few subtasks and concepts) plays a key role in\nachieving systematic generalization. This implies that it may not be essential\nto gather a large and varied number of complex tasks, which could be costly to\nobtain. We demonstrate that this result is independent of the similarity\nbetween the training and testing data and applies to well-known families of\nneural network architectures for VQA (i.e. monolithic architectures and neural\nmodule networks). Additionally, we observe that neural module networks leverage\nall forms of data diversity we evaluated, while monolithic architectures\nrequire more extensive amounts of data to do so. These findings provide a first\nstep towards understanding the interactions between data diversity design,\nneural network architectures, and systematic generalization capabilities.\n","authors":["Amir Rahimi","Vanessa D'Amario","Moyuru Yamada","Kentaro Takemoto","Tomotake Sasaki","Xavier Boix"],"pdf_url":"https://arxiv.org/pdf/2309.08798v2.pdf","comment":"TMLR (https://openreview.net/forum?id=ZAin13msOp)"},{"id":"http://arxiv.org/abs/2411.03511v1","updated":"2024-11-05T21:08:19Z","published":"2024-11-05T21:08:19Z","title":"Beyond Complete Shapes: A quantitative Evaluation of 3D Shape Matching\n  Algorithms","summary":"  Finding correspondences between 3D shapes is an important and long-standing\nproblem in computer vision, graphics and beyond. While approaches based on\nmachine learning dominate modern 3D shape matching, almost all existing\n(learning-based) methods require that at least one of the involved shapes is\ncomplete. In contrast, the most challenging and arguably most practically\nrelevant setting of matching partially observed shapes, is currently\nunderexplored. One important factor is that existing datasets contain only a\nsmall number of shapes (typically below 100), which are unable to serve\ndata-hungry machine learning approaches, particularly in the unsupervised\nregime. In addition, the type of partiality present in existing datasets is\noften artificial and far from realistic. To address these limitations and to\nencourage research on these relevant settings, we provide a generic and\nflexible framework for the procedural generation of challenging partial shape\nmatching scenarios. Our framework allows for a virtually infinite generation of\npartial shape matching instances from a finite set of shapes with complete\ngeometry. Further, we manually create cross-dataset correspondences between\nseven existing (complete geometry) shape matching datasets, leading to a total\nof 2543 shapes. Based on this, we propose several challenging partial benchmark\nsettings, for which we evaluate respective state-of-the-art methods as\nbaselines.\n","authors":["Viktoria Ehm","Nafie El Amrani","Yizheng Xie","Lennart Bastian","Maolin Gao","Weikang Wang","Lu Sang","Dongliang Cao","Zorah Lähner","Daniel Cremers","Florian Bernard"],"pdf_url":"https://arxiv.org/pdf/2411.03511v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04090v2","updated":"2024-11-05T20:51:06Z","published":"2024-06-06T14:01:28Z","title":"Interpretable Lightweight Transformer via Unrolling of Learned Graph\n  Smoothness Priors","summary":"  We build interpretable and lightweight transformer-like neural networks by\nunrolling iterative optimization algorithms that minimize graph smoothness\npriors -- the quadratic graph Laplacian regularizer (GLR) and the $\\ell_1$-norm\ngraph total variation (GTV) -- subject to an interpolation constraint. The\ncrucial insight is that a normalized signal-dependent graph learning module\namounts to a variant of the basic self-attention mechanism in conventional\ntransformers. Unlike \"black-box\" transformers that require learning of large\nkey, query and value matrices to compute scaled dot products as affinities and\nsubsequent output embeddings, resulting in huge parameter sets, our unrolled\nnetworks employ shallow CNNs to learn low-dimensional features per node to\nestablish pairwise Mahalanobis distances and construct sparse similarity\ngraphs. At each layer, given a learned graph, the target interpolated signal is\nsimply a low-pass filtered output derived from the minimization of an assumed\ngraph smoothness prior, leading to a dramatic reduction in parameter count.\nExperiments for two image interpolation applications verify the restoration\nperformance, parameter efficiency and robustness to covariate shift of our\ngraph-based unrolled networks compared to conventional transformers.\n","authors":["Tam Thuc Do","Parham Eftekhar","Seyed Alireza Hosseini","Gene Cheung","Philip Chou"],"pdf_url":"https://arxiv.org/pdf/2406.04090v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03505v1","updated":"2024-11-05T20:42:23Z","published":"2024-11-05T20:42:23Z","title":"SynthSet: Generative Diffusion Model for Semantic Segmentation in\n  Precision Agriculture","summary":"  This paper introduces a methodology for generating synthetic annotated data\nto address data scarcity in semantic segmentation tasks within the precision\nagriculture domain. Utilizing Denoising Diffusion Probabilistic Models (DDPMs)\nand Generative Adversarial Networks (GANs), we propose a dual diffusion model\narchitecture for synthesizing realistic annotated agricultural data, without\nany human intervention. We employ super-resolution to enhance the phenotypic\ncharacteristics of the synthesized images and their coherence with the\ncorresponding generated masks. We showcase the utility of the proposed method\nfor wheat head segmentation. The high quality of synthesized data underscores\nthe effectiveness of the proposed methodology in generating image-mask pairs.\nFurthermore, models trained on our generated data exhibit promising performance\nwhen tested on an external, diverse dataset of real wheat fields. The results\nshow the efficacy of the proposed methodology for addressing data scarcity for\nsemantic segmentation tasks. Moreover, the proposed approach can be readily\nadapted for various segmentation tasks in precision agriculture and beyond.\n","authors":["Andrew Heschl","Mauricio Murillo","Keyhan Najafian","Farhad Maleki"],"pdf_url":"https://arxiv.org/pdf/2411.03505v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03491v1","updated":"2024-11-05T20:16:15Z","published":"2024-11-05T20:16:15Z","title":"An Application-Agnostic Automatic Target Recognition System Using Vision\n  Language Models","summary":"  We present a novel Automatic Target Recognition (ATR) system using\nopen-vocabulary object detection and classification models. A primary advantage\nof this approach is that target classes can be defined just before runtime by a\nnon-technical end user, using either a few natural language text descriptions\nof the target, or a few image exemplars, or both. Nuances in the desired\ntargets can be expressed in natural language, which is useful for unique\ntargets with little or no training data. We also implemented a novel\ncombination of several techniques to improve performance, such as leveraging\nthe additional information in the sequence of overlapping frames to perform\ntubelet identification (i.e., sequential bounding box matching), bounding box\nre-scoring, and tubelet linking. Additionally, we developed a technique to\nvisualize the aggregate output of many overlapping frames as a mosaic of the\narea scanned during the aerial surveillance or reconnaissance, and a kernel\ndensity estimate (or heatmap) of the detected targets. We initially applied\nthis ATR system to the use case of detecting and clearing unexploded ordinance\non airfield runways and we are currently extending our research to other\nreal-world applications.\n","authors":["Anthony Palladino","Dana Gajewski","Abigail Aronica","Patryk Deptula","Alexander Hamme","Seiyoung C. Lee","Jeff Muri","Todd Nelling","Michael A. Riley","Brian Wong","Margaret Duff"],"pdf_url":"https://arxiv.org/pdf/2411.03491v1.pdf","comment":"Accepted to the Thirty-Seventh Annual Conference on Innovative\n  Applications of Artificial Intelligence (IAAI-25)"},{"id":"http://arxiv.org/abs/2411.03480v1","updated":"2024-11-05T20:06:50Z","published":"2024-11-05T20:06:50Z","title":"Rainfall regression from C-band Synthetic Aperture Radar using\n  Multi-Task Generative Adversarial Networks","summary":"  This paper introduces a data-driven approach to estimate precipitation rates\nfrom Synthetic Aperture Radar (SAR) at a spatial resolution of 200 meters per\npixel. It addresses previous challenges related to the collocation of SAR and\nweather radar data, specifically the misalignment in collocations and the\nscarcity of rainfall examples under strong wind. To tackle these challenges,\nthe paper proposes a multi-objective formulation, introducing patch-level\ncomponents and an adversarial component. It exploits the full NEXRAD archive to\nlook for potential co-locations with Sentinel-1 data. With additional\nenhancements to the training procedure and the incorporation of additional\ninputs, the resulting model demonstrates improved accuracy in rainfall\nestimates and the ability to extend its performance to scenarios up to 15 m/s.\n","authors":["Aurélien Colin","Romain Husson"],"pdf_url":"https://arxiv.org/pdf/2411.03480v1.pdf","comment":"36 pages, 13 figures"},{"id":"http://arxiv.org/abs/2411.03475v1","updated":"2024-11-05T19:59:40Z","published":"2024-11-05T19:59:40Z","title":"Self Supervised Networks for Learning Latent Space Representations of\n  Human Body Scans and Motions","summary":"  This paper introduces self-supervised neural network models to tackle several\nfundamental problems in the field of 3D human body analysis and processing.\nFirst, we propose VariShaPE (Varifold Shape Parameter Estimator), a novel\narchitecture for the retrieval of latent space representations of body shapes\nand poses. This network offers a fast and robust method to estimate the\nembedding of arbitrary unregistered meshes into the latent space. Second, we\ncomplement the estimation of latent codes with MoGeN (Motion Geometry Network)\na framework that learns the geometry on the latent space itself. This is\nachieved by lifting the body pose parameter space into a higher dimensional\nEuclidean space in which body motion mini-sequences from a training set of 4D\ndata can be approximated by simple linear interpolation. Using the SMPL latent\nspace representation we illustrate how the combination of these network models,\nonce trained, can be used to perform a variety of tasks with very limited\ncomputational cost. This includes operations such as motion interpolation,\nextrapolation and transfer as well as random shape and pose generation.\n","authors":["Emmanuel Hartman","Nicolas Charon","Martin Bauer"],"pdf_url":"https://arxiv.org/pdf/2411.03475v1.pdf","comment":"23 pages, 11 figures, 6 tables"},{"id":"http://arxiv.org/abs/2401.08426v4","updated":"2024-11-05T19:57:19Z","published":"2024-01-16T15:11:29Z","title":"GD doesn't make the cut: Three ways that non-differentiability affects\n  neural network training","summary":"  This paper investigates the distinctions between gradient methods applied to\nnon-differentiable functions (NGDMs) and classical gradient descents (GDs)\ndesigned for differentiable functions. First, we demonstrate significant\ndifferences in the convergence properties of NGDMs compared to GDs, challenging\nthe applicability of the extensive neural network convergence literature based\non $L-smoothness$ to non-smooth neural networks. Next, we demonstrate the\nparadoxical nature of NGDM solutions for $L_{1}$-regularized problems, showing\nthat increasing the regularization penalty leads to an increase in the $L_{1}$\nnorm of optimal solutions in NGDMs. Consequently, we show that widely adopted\n$L_{1}$ penalization-based techniques for network pruning do not yield expected\nresults. Additionally, we dispel the common belief that optimization algorithms\nlike Adam and RMSProp perform similarly in non-differentiable contexts.\nFinally, we explore the Edge of Stability phenomenon, indicating its\ninapplicability even to Lipschitz continuous convex differentiable functions,\nleaving its relevance to non-convex non-differentiable neural networks\ninconclusive. Our analysis exposes misguided interpretations of NGDMs in widely\nreferenced papers and texts due to an overreliance on strong smoothness\nassumptions, emphasizing the necessity for a nuanced understanding of\nfoundational assumptions in the analysis of these systems.\n","authors":["Siddharth Krishna Kumar"],"pdf_url":"https://arxiv.org/pdf/2401.08426v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.07079v2","updated":"2024-11-05T19:44:03Z","published":"2024-08-07T14:04:50Z","title":"Anatomical Foundation Models for Brain MRIs","summary":"  Deep Learning (DL) in neuroimaging has become increasingly relevant for\ndetecting neurological conditions and neurodegenerative disorders. One of the\nmost predominant biomarkers in neuroimaging is represented by brain age, which\nhas been shown to be a good indicator for different conditions, such as\nAlzheimer's Disease. Using brain age for pretraining DL models in transfer\nlearning settings has also recently shown promising results, especially when\ndealing with data scarcity of different conditions. On the other hand,\nanatomical information of brain MRIs (e.g. cortical thickness) can provide\nimportant information for learning good representations that can be transferred\nto many downstream tasks. In this work, we propose AnatCL, an anatomical\nfoundation model for brain MRIs that i.) leverages anatomical information with\na weakly contrastive learning approach and ii.) achieves state-of-the-art\nperformances in many different downstream tasks. To validate our approach we\nconsider 12 different downstream tasks for diagnosis classification, and\nprediction of 10 different clinical assessment scores. Pretrained models can be\nfound at https://github.com/EIDOSLAB/AnatCL.\n","authors":["Carlo Alberto Barbano","Matteo Brunello","Benoit Dufumier","Marco Grangetto"],"pdf_url":"https://arxiv.org/pdf/2408.07079v2.pdf","comment":"12 pages; added source url"},{"id":"http://arxiv.org/abs/2411.03464v1","updated":"2024-11-05T19:35:10Z","published":"2024-11-05T19:35:10Z","title":"TopoTxR: A topology-guided deep convolutional network for breast\n  parenchyma learning on DCE-MRIs","summary":"  Characterization of breast parenchyma in dynamic contrast-enhanced magnetic\nresonance imaging (DCE-MRI) is a challenging task owing to the complexity of\nunderlying tissue structures. Existing quantitative approaches, like radiomics\nand deep learning models, lack explicit quantification of intricate and subtle\nparenchymal structures, including fibroglandular tissue. To address this, we\npropose a novel topological approach that explicitly extracts multi-scale\ntopological structures to better approximate breast parenchymal structures, and\nthen incorporates these structures into a deep-learning-based prediction model\nvia an attention mechanism. Our topology-informed deep learning model,\n\\emph{TopoTxR}, leverages topology to provide enhanced insights into tissues\ncritical for disease pathophysiology and treatment response. We empirically\nvalidate \\emph{TopoTxR} using the VICTRE phantom breast dataset, showing that\nthe topological structures extracted by our model effectively approximate the\nbreast parenchymal structures. We further demonstrate \\emph{TopoTxR}'s efficacy\nin predicting response to neoadjuvant chemotherapy. Our qualitative and\nquantitative analyses suggest differential topological behavior of breast\ntissue in treatment-na\\\"ive imaging, in patients who respond favorably to\ntherapy as achieving pathological complete response (pCR) versus those who do\nnot. In a comparative analysis with several baselines on the publicly available\nI-SPY 1 dataset (N=161, including 47 patients with pCR and 114 without) and the\nRutgers proprietary dataset (N=120, with 69 patients achieving pCR and 51 not),\n\\emph{TopoTxR} demonstrates a notable improvement, achieving a 2.6\\% increase\nin accuracy and a 4.6\\% enhancement in AUC compared to the state-of-the-art\nmethod.\n","authors":["Fan Wang","Zhilin Zou","Nicole Sakla","Luke Partyka","Nil Rawal","Gagandeep Singh","Wei Zhao","Haibin Ling","Chuan Huang","Prateek Prasanna","Chao Chen"],"pdf_url":"https://arxiv.org/pdf/2411.03464v1.pdf","comment":"22 pages, 8 figures, 8 tables, accepted by Medical Image Analysis (\n  https://www.sciencedirect.com/science/article/abs/pii/S1361841524002986 )"},{"id":"http://arxiv.org/abs/2411.03456v1","updated":"2024-11-05T19:17:38Z","published":"2024-11-05T19:17:38Z","title":"BOston Neonatal Brain Injury Data for Hypoxic Ischemic Encephalopathy\n  (BONBID-HIE): II. 2-year Neurocognitive Outcome and NICU Outcome","summary":"  Hypoxic Ischemic Encephalopathy (HIE) affects approximately 1-5/1000 newborns\nglobally and leads to adverse neurocognitive outcomes in 30% to 50% of cases by\ntwo years of age. Despite therapeutic advances with Therapeutic Hypothermia\n(TH), prognosis remains challenging, highlighting the need for improved\nbiomarkers. This paper introduces the second release of the Boston Neonatal\nBrain Injury Dataset for Hypoxic-Ischemic Encephalopathy (BONBID-HIE), an\nopen-source, comprehensive MRI and clinical dataset featuring 237 patients,\nincluding NICU outcomes and 2-year neurocognitive outcomes from Massachusetts\nGeneral Hospital and Boston Children's Hospital.\n","authors":["Rina Bao","Yangming Ou"],"pdf_url":"https://arxiv.org/pdf/2411.03456v1.pdf","comment":"Data description for BONBID-HIE 2024 Challenge on MICCAI 2024"},{"id":"http://arxiv.org/abs/2411.02229v2","updated":"2024-11-05T19:06:16Z","published":"2024-11-04T16:21:00Z","title":"FewViewGS: Gaussian Splatting with Few View Matching and Multi-stage\n  Training","summary":"  The field of novel view synthesis from images has seen rapid advancements\nwith the introduction of Neural Radiance Fields (NeRF) and more recently with\n3D Gaussian Splatting. Gaussian Splatting became widely adopted due to its\nefficiency and ability to render novel views accurately. While Gaussian\nSplatting performs well when a sufficient amount of training images are\navailable, its unstructured explicit representation tends to overfit in\nscenarios with sparse input images, resulting in poor rendering performance. To\naddress this, we present a 3D Gaussian-based novel view synthesis method using\nsparse input images that can accurately render the scene from the viewpoints\nnot covered by the training images. We propose a multi-stage training scheme\nwith matching-based consistency constraints imposed on the novel views without\nrelying on pre-trained depth estimation or diffusion models. This is achieved\nby using the matches of the available training images to supervise the\ngeneration of the novel views sampled between the training frames with color,\ngeometry, and semantic losses. In addition, we introduce a locality preserving\nregularization for 3D Gaussians which removes rendering artifacts by preserving\nthe local color structure of the scene. Evaluation on synthetic and real-world\ndatasets demonstrates competitive or superior performance of our method in\nfew-shot novel view synthesis compared to existing state-of-the-art methods.\n","authors":["Ruihong Yin","Vladimir Yugay","Yue Li","Sezer Karaoglu","Theo Gevers"],"pdf_url":"https://arxiv.org/pdf/2411.02229v2.pdf","comment":"Accepted by NeurIPS2024"},{"id":"http://arxiv.org/abs/2411.03445v1","updated":"2024-11-05T19:00:34Z","published":"2024-11-05T19:00:34Z","title":"Solving Trojan Detection Competitions with Linear Weight Classification","summary":"  Neural networks can conceal malicious Trojan backdoors that allow a trigger\nto covertly change the model behavior. Detecting signs of these backdoors,\nparticularly without access to any triggered data, is the subject of ongoing\nresearch and open challenges. In one common formulation of the problem, we are\ngiven a set of clean and poisoned models and need to predict whether a given\ntest model is clean or poisoned. In this paper, we introduce a detector that\nworks remarkably well across many of the existing datasets and domains. It is\nobtained by training a binary classifier on a large number of models' weights\nafter performing a few different pre-processing steps including feature\nselection and standardization, reference model weights subtraction, and model\nalignment prior to detection. We evaluate this algorithm on a diverse set of\nTrojan detection benchmarks and domains and examine the cases where the\napproach is most and least effective.\n","authors":["Todd Huster","Peter Lin","Razvan Stefanescu","Emmanuel Ekwedike","Ritu Chadha"],"pdf_url":"https://arxiv.org/pdf/2411.03445v1.pdf","comment":"9 pages, 4 Figures"},{"id":"http://arxiv.org/abs/2411.03405v1","updated":"2024-11-05T18:39:25Z","published":"2024-11-05T18:39:25Z","title":"Fine-Grained Spatial and Verbal Losses for 3D Visual Grounding","summary":"  3D visual grounding consists of identifying the instance in a 3D scene which\nis referred by an accompanying language description. While several\narchitectures have been proposed within the commonly employed\ngrounding-by-selection framework, the utilized losses are comparatively\nunder-explored. In particular, most methods rely on a basic supervised\ncross-entropy loss on the predicted distribution over candidate instances,\nwhich fails to model both spatial relations between instances and the internal\nfine-grained word-level structure of the verbal referral. Sparse attempts to\nadditionally supervise verbal embeddings globally by learning the class of the\nreferred instance from the description or employing verbo-visual contrast to\nbetter separate instance embeddings do not fundamentally lift the\naforementioned limitations. Responding to these shortcomings, we introduce two\nnovel losses for 3D visual grounding: a visual-level offset loss on regressed\nvector offsets from each instance to the ground-truth referred instance and a\nlanguage-related span loss on predictions for the word-level span of the\nreferred instance in the description. In addition, we equip the verbo-visual\nfusion module of our new 3D visual grounding architecture AsphaltNet with a\ntop-down bidirectional attentive fusion block, which enables the supervisory\nsignals from our two losses to propagate to the respective converse branches of\nthe network and thus aid the latter to learn context-aware instance embeddings\nand grounding-aware verbal embeddings. AsphaltNet proposes novel auxiliary\nlosses to aid 3D visual grounding with competitive results compared to the\nstate-of-the-art on the ReferIt3D benchmark.\n","authors":["Sombit Dey","Ozan Unal","Christos Sakaridis","Luc Van Gool"],"pdf_url":"https://arxiv.org/pdf/2411.03405v1.pdf","comment":"Accepted at WACV 2025"},{"id":"http://arxiv.org/abs/2411.03403v1","updated":"2024-11-05T18:38:42Z","published":"2024-11-05T18:38:42Z","title":"Enhancing Maritime Situational Awareness through End-to-End Onboard Raw\n  Data Analysis","summary":"  Satellite-based onboard data processing is crucial for time-sensitive\napplications requiring timely and efficient rapid response. Advances in edge\nartificial intelligence are shifting computational power from ground-based\ncenters to on-orbit platforms, transforming the\n\"sensing-communication-decision-feedback\" cycle and reducing latency from\nacquisition to delivery. The current research presents a framework addressing\nthe strict bandwidth, energy, and latency constraints of small satellites,\nfocusing on maritime monitoring. The study contributes three main innovations.\nFirstly, it investigates the application of deep learning techniques for direct\nship detection and classification from raw satellite imagery. By simplifying\nthe onboard processing chain, our approach facilitates direct analyses without\nrequiring computationally intensive steps such as calibration and\northo-rectification. Secondly, to address the scarcity of raw satellite data,\nwe introduce two novel datasets, VDS2Raw and VDV2Raw, which are derived from\nraw data from Sentinel-2 and Vegetation and Environment Monitoring New Micro\nSatellite (VENuS) missions, respectively, and enriched with Automatic\nIdentification System (AIS) records. Thirdly, we characterize the tasks'\noptimal single and multiple spectral band combinations through statistical and\nfeature-based analyses validated on both datasets. In sum, we demonstrate the\nfeasibility of the proposed method through a proof-of-concept on CubeSat-like\nhardware, confirming the models' potential for operational satellite-based\nmaritime monitoring.\n","authors":["Roberto Del Prete","Manuel Salvoldi","Domenico Barretta","Nicolas Longépé","Gabriele Meoni","Arnon Karnieli","Maria Daniela Graziano","Alfredo Renga"],"pdf_url":"https://arxiv.org/pdf/2411.03403v1.pdf","comment":"38 pages"},{"id":"http://arxiv.org/abs/2407.15668v2","updated":"2024-11-05T18:38:08Z","published":"2024-07-22T14:29:36Z","title":"SLVideo: A Sign Language Video Moment Retrieval Framework","summary":"  SLVideo is a video moment retrieval system for Sign Language videos that\nincorporates facial expressions, addressing this gap in existing technology.\nThe system extracts embedding representations for the hand and face signs from\nvideo frames to capture the signs in their entirety, enabling users to search\nfor a specific sign language video segment with text queries. A collection of\neight hours of annotated Portuguese Sign Language videos is used as the\ndataset, and a CLIP model is used to generate the embeddings. The initial\nresults are promising in a zero-shot setting. In addition, SLVideo incorporates\na thesaurus that enables users to search for similar signs to those retrieved,\nusing the video segment embeddings, and also supports the edition and creation\nof video sign language annotations. Project web page:\nhttps://novasearch.github.io/SLVideo/\n","authors":["Gonçalo Vinagre Martins","João Magalhães","Afonso Quinaz","Carla Viegas","Sofia Cavaco"],"pdf_url":"https://arxiv.org/pdf/2407.15668v2.pdf","comment":"4 pages, 1 figure, 1 table"},{"id":"http://arxiv.org/abs/2405.17705v3","updated":"2024-11-05T18:02:53Z","published":"2024-05-27T23:38:10Z","title":"DC-Gaussian: Improving 3D Gaussian Splatting for Reflective Dash Cam\n  Videos","summary":"  We present DC-Gaussian, a new method for generating novel views from\nin-vehicle dash cam videos. While neural rendering techniques have made\nsignificant strides in driving scenarios, existing methods are primarily\ndesigned for videos collected by autonomous vehicles. However, these videos are\nlimited in both quantity and diversity compared to dash cam videos, which are\nmore widely used across various types of vehicles and capture a broader range\nof scenarios. Dash cam videos often suffer from severe obstructions such as\nreflections and occlusions on the windshields, which significantly impede the\napplication of neural rendering techniques. To address this challenge, we\ndevelop DC-Gaussian based on the recent real-time neural rendering technique 3D\nGaussian Splatting (3DGS). Our approach includes an adaptive image\ndecomposition module to model reflections and occlusions in a unified manner.\nAdditionally, we introduce illumination-aware obstruction modeling to manage\nreflections and occlusions under varying lighting conditions. Lastly, we employ\na geometry-guided Gaussian enhancement strategy to improve rendering details by\nincorporating additional geometry priors. Experiments on self-captured and\npublic dash cam videos show that our method not only achieves state-of-the-art\nperformance in novel view synthesis, but also accurately reconstructing\ncaptured scenes getting rid of obstructions. See the project page for code,\ndata: https://linhanwang.github.io/dcgaussian/.\n","authors":["Linhan Wang","Kai Cheng","Shuo Lei","Shengkun Wang","Wei Yin","Chenyang Lei","Xiaoxiao Long","Chang-Tien Lu"],"pdf_url":"https://arxiv.org/pdf/2405.17705v3.pdf","comment":"10 pages,7 figures;project page:\n  https://linhanwang.github.io/dcgaussian/; Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2405.15199v2","updated":"2024-11-05T16:40:01Z","published":"2024-05-24T04:10:34Z","title":"ODGEN: Domain-specific Object Detection Data Generation with Diffusion\n  Models","summary":"  Modern diffusion-based image generative models have made significant progress\nand become promising to enrich training data for the object detection task.\nHowever, the generation quality and the controllability for complex scenes\ncontaining multi-class objects and dense objects with occlusions remain\nlimited. This paper presents ODGEN, a novel method to generate high-quality\nimages conditioned on bounding boxes, thereby facilitating data synthesis for\nobject detection. Given a domain-specific object detection dataset, we first\nfine-tune a pre-trained diffusion model on both cropped foreground objects and\nentire images to fit target distributions. Then we propose to control the\ndiffusion model using synthesized visual prompts with spatial constraints and\nobject-wise textual descriptions. ODGEN exhibits robustness in handling complex\nscenes and specific domains. Further, we design a dataset synthesis pipeline to\nevaluate ODGEN on 7 domain-specific benchmarks to demonstrate its\neffectiveness. Adding training data generated by ODGEN improves up to 25.3%\nmAP@.50:.95 with object detectors like YOLOv5 and YOLOv7, outperforming prior\ncontrollable generative methods. In addition, we design an evaluation protocol\nbased on COCO-2014 to validate ODGEN in general domains and observe an\nadvantage up to 5.6% in mAP@.50:.95 against existing methods.\n","authors":["Jingyuan Zhu","Shiyu Li","Yuxuan Liu","Ping Huang","Jiulong Shan","Huimin Ma","Jian Yuan"],"pdf_url":"https://arxiv.org/pdf/2405.15199v2.pdf","comment":"Accepted by NeurIPS2024"}]},"2024-11-06T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2407.10964v2","updated":"2024-11-06T18:58:03Z","published":"2024-07-15T17:58:42Z","title":"No Train, all Gain: Self-Supervised Gradients Improve Deep Frozen\n  Representations","summary":"  This paper introduces FUNGI, Features from UNsupervised GradIents, a method\nto enhance the features of transformer encoders by leveraging self-supervised\ngradients. Our method is simple: given any pretrained model, we first compute\ngradients from various self-supervised objectives for each input. These\ngradients are projected to a lower dimension and then concatenated with the\nmodel's output embedding. The resulting features are evaluated on k-nearest\nneighbor classification over 11 datasets from vision, 5 from natural language\nprocessing, and 2 from audio. Across backbones spanning various sizes and\npretraining strategies, FUNGI features provide consistent performance\nimprovements over the embeddings. We also show that using FUNGI features can\nbenefit linear classification, clustering and image retrieval, and that they\nsignificantly improve the retrieval-based in-context scene understanding\nabilities of pretrained models, for example improving upon DINO by +17% for\nsemantic segmentation - without any training.\n","authors":["Walter Simoncini","Spyros Gidaris","Andrei Bursuc","Yuki M. Asano"],"pdf_url":"https://arxiv.org/pdf/2407.10964v2.pdf","comment":"NeurIPS 2024. Code available at\n  https://github.com/WalterSimoncini/fungivision"},{"id":"http://arxiv.org/abs/2411.04118v1","updated":"2024-11-06T18:51:02Z","published":"2024-11-06T18:51:02Z","title":"Medical Adaptation of Large Language and Vision-Language Models: Are We\n  Making Progress?","summary":"  Several recent works seek to develop foundation models specifically for\nmedical applications, adapting general-purpose large language models (LLMs) and\nvision-language models (VLMs) via continued pretraining on publicly available\nbiomedical corpora. These works typically claim that such domain-adaptive\npretraining (DAPT) improves performance on downstream medical tasks, such as\nanswering medical licensing exam questions. In this paper, we compare seven\npublic \"medical\" LLMs and two VLMs against their corresponding base models,\narriving at a different conclusion: all medical VLMs and nearly all medical\nLLMs fail to consistently improve over their base models in the zero-/few-shot\nprompting regime for medical question-answering (QA) tasks. For instance,\nacross the tasks and model pairs we consider in the 3-shot setting, medical\nLLMs only outperform their base models in 12.1% of cases, reach a (statistical)\ntie in 49.8% of cases, and are significantly worse than their base models in\nthe remaining 38.2% of cases. Our conclusions are based on (i) comparing each\nmedical model head-to-head, directly against the corresponding base model; (ii)\noptimizing the prompts for each model separately; and (iii) accounting for\nstatistical uncertainty in comparisons. While these basic practices are not\nconsistently adopted in the literature, our ablations show that they\nsubstantially impact conclusions. Our findings suggest that state-of-the-art\ngeneral-domain models may already exhibit strong medical knowledge and\nreasoning capabilities, and offer recommendations to strengthen the conclusions\nof future studies.\n","authors":["Daniel P. Jeong","Saurabh Garg","Zachary C. Lipton","Michael Oberst"],"pdf_url":"https://arxiv.org/pdf/2411.04118v1.pdf","comment":"Accepted to EMNLP 2024 Main Conference as Long Paper (Oral)"},{"id":"http://arxiv.org/abs/2411.04109v1","updated":"2024-11-06T18:36:22Z","published":"2024-11-06T18:36:22Z","title":"Self-Consistency Preference Optimization","summary":"  Self-alignment, whereby models learn to improve themselves without human\nannotation, is a rapidly growing research area. However, existing techniques\noften fail to improve complex reasoning tasks due to the difficulty of\nassigning correct rewards. An orthogonal approach that is known to improve\ncorrectness is self-consistency, a method applied at inference time based on\nmultiple sampling in order to find the most consistent answer. In this work, we\nextend the self-consistency concept to help train models. We thus introduce\nself-consistency preference optimization (ScPO), which iteratively trains\nconsistent answers to be preferred over inconsistent ones on unsupervised new\nproblems. We show ScPO leads to large improvements over conventional reward\nmodel training on reasoning tasks such as GSM8K and MATH, closing the gap with\nsupervised training with gold answers or preferences, and that combining ScPO\nwith standard supervised learning improves results even further. On ZebraLogic,\nScPO finetunes Llama-3 8B to be superior to Llama-3 70B, Gemma-2 27B, and\nClaude-3 Haiku.\n","authors":["Archiki Prasad","Weizhe Yuan","Richard Yuanzhe Pang","Jing Xu","Maryam Fazel-Zarandi","Mohit Bansal","Sainbayar Sukhbaatar","Jason Weston","Jane Yu"],"pdf_url":"https://arxiv.org/pdf/2411.04109v1.pdf","comment":"16 pages, 3 figures"},{"id":"http://arxiv.org/abs/2411.04105v1","updated":"2024-11-06T18:35:32Z","published":"2024-11-06T18:35:32Z","title":"How Transformers Solve Propositional Logic Problems: A Mechanistic\n  Analysis","summary":"  Large language models (LLMs) have shown amazing performance on tasks that\nrequire planning and reasoning. Motivated by this, we investigate the internal\nmechanisms that underpin a network's ability to perform complex logical\nreasoning. We first construct a synthetic propositional logic problem that\nserves as a concrete test-bed for network training and evaluation. Crucially,\nthis problem demands nontrivial planning to solve, but we can train a small\ntransformer to achieve perfect accuracy. Building on our set-up, we then pursue\nan understanding of precisely how a three-layer transformer, trained from\nscratch, solves this problem. We are able to identify certain \"planning\" and\n\"reasoning\" circuits in the network that necessitate cooperation between the\nattention blocks to implement the desired logic. To expand our findings, we\nthen study a larger model, Mistral 7B. Using activation patching, we\ncharacterize internal components that are critical in solving our logic\nproblem. Overall, our work systemically uncovers novel aspects of small and\nlarge transformers, and continues the study of how they plan and reason.\n","authors":["Guan Zhe Hong","Nishanth Dikkala","Enming Luo","Cyrus Rashtchian","Rina Panigrahy"],"pdf_url":"https://arxiv.org/pdf/2411.04105v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04093v1","updated":"2024-11-06T18:14:48Z","published":"2024-11-06T18:14:48Z","title":"Summarization of Opinionated Political Documents with Varied\n  Perspectives","summary":"  Global partisan hostility and polarization has increased, and this\npolarization is heightened around presidential elections. Models capable of\ngenerating accurate summaries of diverse perspectives can help reduce such\npolarization by exposing users to alternative perspectives. In this work, we\nintroduce a novel dataset and task for independently summarizing each political\nperspective in a set of passages from opinionated news articles. For this task,\nwe propose a framework for evaluating different dimensions of perspective\nsummary performance. We benchmark 10 models of varying sizes and architectures\nthrough both automatic and human evaluation. While recent models like GPT-4o\nperform well on this task, we find that all models struggle to generate\nsummaries faithful to the intended perspective. Our analysis of summaries\nfocuses on how extraction behavior depends on the features of the input\ndocuments.\n","authors":["Nicholas Deas","Kathleen McKeown"],"pdf_url":"https://arxiv.org/pdf/2411.04093v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04090v1","updated":"2024-11-06T18:08:57Z","published":"2024-11-06T18:08:57Z","title":"A Collaborative Content Moderation Framework for Toxicity Detection\n  based on Conformalized Estimates of Annotation Disagreement","summary":"  Content moderation typically combines the efforts of human moderators and\nmachine learning models.However, these systems often rely on data where\nsignificant disagreement occurs during moderation, reflecting the subjective\nnature of toxicity perception.Rather than dismissing this disagreement as\nnoise, we interpret it as a valuable signal that highlights the inherent\nambiguity of the content,an insight missed when only the majority label is\nconsidered.In this work, we introduce a novel content moderation framework that\nemphasizes the importance of capturing annotation disagreement. Our approach\nuses multitask learning, where toxicity classification serves as the primary\ntask and annotation disagreement is addressed as an auxiliary\ntask.Additionally, we leverage uncertainty estimation techniques, specifically\nConformal Prediction, to account for both the ambiguity in comment annotations\nand the model's inherent uncertainty in predicting toxicity and\ndisagreement.The framework also allows moderators to adjust thresholds for\nannotation disagreement, offering flexibility in determining when ambiguity\nshould trigger a review.We demonstrate that our joint approach enhances model\nperformance, calibration, and uncertainty estimation, while offering greater\nparameter efficiency and improving the review process in comparison to\nsingle-task methods.\n","authors":["Guillermo Villate-Castillo","Javier Del Ser","Borja Sanz"],"pdf_url":"https://arxiv.org/pdf/2411.04090v1.pdf","comment":"35 pages, 1 figure"},{"id":"http://arxiv.org/abs/2408.11832v2","updated":"2024-11-06T18:07:03Z","published":"2024-08-06T15:49:58Z","title":"OpenFactCheck: A Unified Framework for Factuality Evaluation of LLMs","summary":"  The increased use of large language models (LLMs) across a variety of\nreal-world applications calls for automatic tools to check the factual accuracy\nof their outputs, as LLMs often hallucinate. This is difficult as it requires\nassessing the factuality of free-form open-domain responses. While there has\nbeen a lot of research on this topic, different papers use different evaluation\nbenchmarks and measures, which makes them hard to compare and hampers future\nprogress. To mitigate these issues, we developed OpenFactCheck, a unified\nframework, with three modules: (i) RESPONSEEVAL, which allows users to easily\ncustomize an automatic fact-checking system and to assess the factuality of all\nclaims in an input document using that system, (ii) LLMEVAL, which assesses the\noverall factuality of an LLM, and (iii) CHECKEREVAL, a module to evaluate\nautomatic fact-checking systems. OpenFactCheck is open-sourced\n(https://github.com/mbzuai-nlp/openfactcheck) and publicly released as a Python\nlibrary (https://pypi.org/project/openfactcheck/) and also as a web service\n(http://app.openfactcheck.com). A video describing the system is available at\nhttps://youtu.be/-i9VKL0HleI.\n","authors":["Hasan Iqbal","Yuxia Wang","Minghan Wang","Georgi Georgiev","Jiahui Geng","Iryna Gurevych","Preslav Nakov"],"pdf_url":"https://arxiv.org/pdf/2408.11832v2.pdf","comment":"11 pages, 4 Figures, 3 Tables, Accepted at EMNLP 2024 System\n  Demonstration. arXiv admin note: substantial text overlap with\n  arXiv:2405.05583"},{"id":"http://arxiv.org/abs/2411.04075v1","updated":"2024-11-06T17:52:01Z","published":"2024-11-06T17:52:01Z","title":"M3SciQA: A Multi-Modal Multi-Document Scientific QA Benchmark for\n  Evaluating Foundation Models","summary":"  Existing benchmarks for evaluating foundation models mainly focus on\nsingle-document, text-only tasks. However, they often fail to fully capture the\ncomplexity of research workflows, which typically involve interpreting\nnon-textual data and gathering information across multiple documents. To\naddress this gap, we introduce M3SciQA, a multi-modal, multi-document\nscientific question answering benchmark designed for a more comprehensive\nevaluation of foundation models. M3SciQA consists of 1,452 expert-annotated\nquestions spanning 70 natural language processing paper clusters, where each\ncluster represents a primary paper along with all its cited documents,\nmirroring the workflow of comprehending a single paper by requiring multi-modal\nand multi-document data. With M3SciQA, we conduct a comprehensive evaluation of\n18 foundation models. Our results indicate that current foundation models still\nsignificantly underperform compared to human experts in multi-modal information\nretrieval and in reasoning across multiple scientific documents. Additionally,\nwe explore the implications of these findings for the future advancement of\napplying foundation models in multi-modal scientific literature analysis.\n","authors":["Chuhan Li","Ziyao Shangguan","Yilun Zhao","Deyuan Li","Yixin Liu","Arman Cohan"],"pdf_url":"https://arxiv.org/pdf/2411.04075v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17027v2","updated":"2024-11-06T17:20:42Z","published":"2024-09-25T15:30:24Z","title":"Counterfactual Token Generation in Large Language Models","summary":"  \"Sure, I am happy to generate a story for you: Captain Lyra stood at the helm\nof her trusty ship, the Maelstrom's Fury, gazing out at the endless sea. [...]\nLyra's eyes welled up with tears as she realized the bitter truth - she had\nsacrificed everything for fleeting riches, and lost the love of her crew, her\nfamily, and herself.\" Although this story, generated by a large language model,\nis captivating, one may wonder -- how would the story have unfolded if the\nmodel had chosen \"Captain Maeve\" as the protagonist instead? We cannot know.\nState-of-the-art large language models are stateless -- they maintain no\ninternal memory or state. Given a prompt, they generate a sequence of tokens as\nan output using an autoregressive process. As a consequence, they cannot reason\nabout counterfactual alternatives to tokens they have generated in the past. In\nthis work, our goal is to enhance them with this functionality. To this end, we\ndevelop a causal model of token generation that builds upon the Gumbel-Max\nstructural causal model. Our model allows any large language model to perform\ncounterfactual token generation at almost no cost in comparison with vanilla\ntoken generation, it is embarrassingly simple to implement, and it does not\nrequire any fine-tuning nor prompt engineering. We implement our model on Llama\n3 8B-Instruct and Ministral-8B-Instruct and conduct a qualitative and a\nquantitative analysis of counterfactually generated text. We conclude with a\ndemonstrative application of counterfactual token generation for bias\ndetection, unveiling interesting insights about the model of the world\nconstructed by large language models.\n","authors":["Ivi Chatzi","Nina Corvelo Benz","Eleni Straitouri","Stratis Tsirtsis","Manuel Gomez-Rodriguez"],"pdf_url":"https://arxiv.org/pdf/2409.17027v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01483v3","updated":"2024-11-06T17:04:36Z","published":"2024-11-03T08:49:55Z","title":"Teaching Models to Improve on Tape","summary":"  Large Language Models (LLMs) often struggle when prompted to generate content\nunder specific constraints. However, in such cases it is often easy to check\nwhether these constraints are satisfied or violated. Recent works have shown\nthat LLMs can benefit from such \"corrective feedback\". Here we claim that this\nskill of LLMs can be significantly enhanced via training. We introduce an RL\nframework for teaching models to use such rewards, by simulating interaction\nsessions, and rewarding the model according to its ability to satisfy the\nconstraints. We refer to our method as CORGI (Controlled Generation with RL for\nGuided Interaction), and evaluate it on a variety of controlled generation\ntasks using unlabeled training data. We find that CORGI consistently\noutperforms the baseline reinforcement learning method that does not\nincorporate conversational feedback. Furthermore, CORGI's interactive framework\nenables meta-learning, allowing the LLM to generalize better to guided\ninteraction in new tasks. Our results clearly show that conversational\noptimization, when combined with reinforcement learning, significantly improves\nthe effectiveness of LLMs in controlled generation contexts.\n","authors":["Liat Bezalel","Eyal Orgad","Amir Globerson"],"pdf_url":"https://arxiv.org/pdf/2411.01483v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14632v2","updated":"2024-11-06T16:54:48Z","published":"2024-10-18T17:32:22Z","title":"Diverging Preferences: When do Annotators Disagree and do Models Know?","summary":"  We examine diverging preferences in human-labeled preference datasets. We\ndevelop a taxonomy of disagreement sources spanning 10 categories across four\nhigh-level classes -- task underspecification, response style, refusals, and\nannotation errors. We find that the majority of disagreements are in opposition\nwith standard reward modeling approaches, which are designed with the\nassumption that annotator disagreement is noise. We then explore how these\nfindings impact two areas of LLM development: reward modeling and evaluation.\nIn our experiments, we demonstrate how standard reward modeling methods, like\nthe Bradley-Terry model, fail to differentiate whether a given preference\njudgment is the result of unanimous agreement among annotators or the majority\nopinion among diverging user preferences. We also find that these tendencies\nare also echoed by popular LLM-as-Judge evaluation methods, which consistently\nidentify a winning response in cases of diverging preferences. These findings\nhighlight remaining challenges in LLM evaluations, which are greatly influenced\nby divisive features like response style, and in developing pluralistically\naligned LLMs. To address these issues, we develop methods for identifying\ndiverging preferences to mitigate their influence on evaluation and training.\n","authors":["Michael JQ Zhang","Zhilin Wang","Jena D. Hwang","Yi Dong","Olivier Delalleau","Yejin Choi","Eunsol Choi","Xiang Ren","Valentina Pyatkin"],"pdf_url":"https://arxiv.org/pdf/2410.14632v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04032v1","updated":"2024-11-06T16:31:28Z","published":"2024-11-06T16:31:28Z","title":"Beemo: Benchmark of Expert-edited Machine-generated Outputs","summary":"  The rapid proliferation of large language models (LLMs) has increased the\nvolume of machine-generated texts (MGTs) and blurred text authorship in various\ndomains. However, most existing MGT benchmarks include single-author texts\n(human-written and machine-generated). This conventional design fails to\ncapture more practical multi-author scenarios, where the user refines the LLM\nresponse for natural flow, coherence, and factual correctness. Our paper\nintroduces the Benchmark of Expert-edited Machine-generated Outputs (Beemo),\nwhich includes 6.5k texts written by humans, generated by ten\ninstruction-finetuned LLMs, and edited by experts for various use cases,\nranging from creative writing to summarization. Beemo additionally comprises\n13.1k machine-generated and LLM-edited texts, allowing for diverse MGT\ndetection evaluation across various edit types. We document Beemo's creation\nprotocol and present the results of benchmarking 33 configurations of MGT\ndetectors in different experimental setups. We find that expert-based editing\nevades MGT detection, while LLM-edited texts are unlikely to be recognized as\nhuman-written. Beemo and all materials are publicly available.\n","authors":["Ekaterina Artemova","Jason Lucas","Saranya Venkatraman","Jooyoung Lee","Sergei Tilga","Adaku Uchendu","Vladislav Mikhailov"],"pdf_url":"https://arxiv.org/pdf/2411.04032v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04025v1","updated":"2024-11-06T16:20:37Z","published":"2024-11-06T16:20:37Z","title":"Prompt Engineering Using GPT for Word-Level Code-Mixed Language\n  Identification in Low-Resource Dravidian Languages","summary":"  Language Identification (LI) is crucial for various natural language\nprocessing tasks, serving as a foundational step in applications such as\nsentiment analysis, machine translation, and information retrieval. In\nmultilingual societies like India, particularly among the youth engaging on\nsocial media, text often exhibits code-mixing, blending local languages with\nEnglish at different linguistic levels. This phenomenon presents formidable\nchallenges for LI systems, especially when languages intermingle within single\nwords. Dravidian languages, prevalent in southern India, possess rich\nmorphological structures yet suffer from under-representation in digital\nplatforms, leading to the adoption of Roman or hybrid scripts for\ncommunication. This paper introduces a prompt based method for a shared task\naimed at addressing word-level LI challenges in Dravidian languages. In this\nwork, we leveraged GPT-3.5 Turbo to understand whether the large language\nmodels is able to correctly classify words into correct categories. Our\nfindings show that the Kannada model consistently outperformed the Tamil model\nacross most metrics, indicating a higher accuracy and reliability in\nidentifying and categorizing Kannada language instances. In contrast, the Tamil\nmodel showed moderate performance, particularly needing improvement in\nprecision and recall.\n","authors":["Aniket Deroy","Subhankar Maity"],"pdf_url":"https://arxiv.org/pdf/2411.04025v1.pdf","comment":"Accepted at FIRE 2024 (Track: Word-level Language Identification in\n  Dravidian Languages)"},{"id":"http://arxiv.org/abs/2404.08262v3","updated":"2024-11-06T16:19:24Z","published":"2024-04-12T06:21:48Z","title":"Pretraining and Updates of Domain-Specific LLM: A Case Study in the\n  Japanese Business Domain","summary":"  The development of Large Language Models (LLMs) in various languages has been\nadvancing, but the combination of non-English languages with domain-specific\ncontexts remains underexplored. This paper presents our findings from training\nand evaluating a Japanese business domain-specific LLM designed to better\nunderstand business-related documents, such as the news on current affairs,\ntechnical reports, and patents. Additionally, LLMs in this domain require\nregular updates to incorporate the most recent knowledge. Therefore, we also\nreport our findings from the first experiments and evaluations involving\nupdates to this LLM using the latest article data, which is an important\nproblem setting that has not been addressed in previous research. From our\nexperiments on a newly created benchmark dataset for question answering in the\ntarget domain, we found that (1) our pretrained model improves QA accuracy\nwithout losing general knowledge, and (2) a proper mixture of the latest and\nolder texts in the training data for the update is necessary. Our pretrained\nmodel and business domain benchmark are publicly available to support further\nstudies.\n","authors":["Kosuke Takahashi","Takahiro Omi","Kosuke Arima","Tatsuya Ishigaki"],"pdf_url":"https://arxiv.org/pdf/2404.08262v3.pdf","comment":"Accepted at PACLIC 38"},{"id":"http://arxiv.org/abs/2410.07520v2","updated":"2024-11-06T16:17:21Z","published":"2024-10-10T01:21:48Z","title":"News Reporter: A Multi-lingual LLM Framework for Broadcast T.V News","summary":"  Large Language Models (LLMs) have fast become an essential tools to many\nconversational chatbots due to their ability to provide coherent answers for\nvaried queries. Datasets used to train these LLMs are often a mix of generic\nand synthetic samples, thus lacking the verification needed to provide correct\nand verifiable answers for T.V. News.\n  We collect and share a large collection of QA pairs extracted from\ntranscripts of news recordings from various news-channels across the United\nStates. Resultant QA pairs are then used to fine-tune an off-the-shelf LLM\nmodel. Our model surpasses base models of similar size on several open LLM\nbenchmarks. We further integrate and propose a RAG method to improve\ncontextualization of our answers and also point it to a verifiable news\nrecording.\n","authors":["Tarun Jain","Yufei Gao","Sridhar Vanga","Karan Singla"],"pdf_url":"https://arxiv.org/pdf/2410.07520v2.pdf","comment":"5 pages, under review at ICASSP 2025"},{"id":"http://arxiv.org/abs/2407.14916v2","updated":"2024-11-06T16:11:18Z","published":"2024-07-20T16:05:17Z","title":"Improving Context-Aware Preference Modeling for Language Models","summary":"  While finetuning language models from pairwise preferences has proven\nremarkably effective, the underspecified nature of natural language presents\ncritical challenges. Direct preference feedback is uninterpretable, difficult\nto provide where multidimensional criteria may apply, and often inconsistent,\neither because it is based on incomplete instructions or provided by diverse\nprincipals. To address these challenges, we consider the two-step preference\nmodeling procedure that first resolves the under-specification by selecting a\ncontext, and then evaluates preference with respect to the chosen context. We\ndecompose reward modeling error according to these two steps, which suggests\nthat supervising context in addition to context-specific preference may be a\nviable approach to aligning models with diverse human preferences. For this to\nwork, the ability of models to evaluate context-specific preference is\ncritical. To this end, we contribute context-conditioned preference datasets\nand accompanying experiments that investigate the ability of language models to\nevaluate context-specific preference. We use our datasets to (1) show that\nexisting preference models benefit from, but fail to fully consider, added\ncontext, (2) finetune a context-aware reward model with context-specific\nperformance exceeding that of GPT-4 and Llama 3 70B on tested datasets, and (3)\ninvestigate the value of context-aware preference modeling.\n","authors":["Silviu Pitis","Ziang Xiao","Nicolas Le Roux","Alessandro Sordoni"],"pdf_url":"https://arxiv.org/pdf/2407.14916v2.pdf","comment":"NeurIPS 2024. 10 pages (29 with references and appendix)"},{"id":"http://arxiv.org/abs/2405.17537v3","updated":"2024-11-06T15:56:04Z","published":"2024-05-27T17:57:48Z","title":"CLIBD: Bridging Vision and Genomics for Biodiversity Monitoring at Scale","summary":"  Measuring biodiversity is crucial for understanding ecosystem health. While\nprior works have developed machine learning models for taxonomic classification\nof photographic images and DNA separately, in this work, we introduce a\nmultimodal approach combining both, using CLIP-style contrastive learning to\nalign images, barcode DNA, and text-based representations of taxonomic labels\nin a unified embedding space. This allows for accurate classification of both\nknown and unknown insect species without task-specific fine-tuning, leveraging\ncontrastive learning for the first time to fuse DNA and image data. Our method\nsurpasses previous single-modality approaches in accuracy by over 8% on\nzero-shot learning tasks, showcasing its effectiveness in biodiversity studies.\n","authors":["ZeMing Gong","Austin T. Wang","Xiaoliang Huo","Joakim Bruslund Haurum","Scott C. Lowe","Graham W. Taylor","Angel X. Chang"],"pdf_url":"https://arxiv.org/pdf/2405.17537v3.pdf","comment":"25 pages with 11 figures"},{"id":"http://arxiv.org/abs/2410.16676v3","updated":"2024-11-06T15:49:30Z","published":"2024-10-22T04:18:19Z","title":"Improving Causal Reasoning in Large Language Models: A Survey","summary":"  Causal reasoning (CR) is a crucial aspect of intelligence, essential for\nproblem-solving, decision-making, and understanding the world. While large\nlanguage models (LLMs) can generate rationales for their outputs, their ability\nto reliably perform causal reasoning remains uncertain, often falling short in\ntasks requiring a deep understanding of causality. In this survey, we provide a\ncomprehensive review of research aimed at enhancing LLMs for causal reasoning.\nWe categorize existing methods based on the role of LLMs: either as reasoning\nengines or as helpers providing knowledge or data to traditional CR methods,\nfollowed by a detailed discussion of the methodologies in each category. We\nthen evaluate the performance of LLMs on various causal reasoning tasks,\nproviding key findings and in-depth analysis. Finally, we provide insights from\ncurrent studies and highlight promising directions for future research. We aim\nfor this work to serve as a comprehensive resource, fostering further\nadvancements in causal reasoning with LLMs. Resources are available at\nhttps://github.com/chendl02/Awesome-LLM-causal-reasoning.\n","authors":["Longxuan Yu","Delin Chen","Siheng Xiong","Qingyang Wu","Qingzhen Liu","Dawei Li","Zhikai Chen","Xiaoze Liu","Liangming Pan"],"pdf_url":"https://arxiv.org/pdf/2410.16676v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03966v1","updated":"2024-11-06T15:03:47Z","published":"2024-11-06T15:03:47Z","title":"WorryWords: Norms of Anxiety Association for over 44k English Words","summary":"  Anxiety, the anticipatory unease about a potential negative outcome, is a\ncommon and beneficial human emotion. However, there is still much that is not\nknown, such as how anxiety relates to our body and how it manifests in\nlanguage. This is especially pertinent given the increasing impact of\nanxiety-related disorders. In this work, we introduce WorryWords, the first\nlarge-scale repository of manually derived word--anxiety associations for over\n44,450 English words. We show that the anxiety associations are highly\nreliable. We use WorryWords to study the relationship between anxiety and other\nemotion constructs, as well as the rate at which children acquire anxiety words\nwith age. Finally, we show that using WorryWords alone, one can accurately\ntrack the change of anxiety in streams of text. The lexicon enables a wide\nvariety of anxiety-related research in psychology, NLP, public health, and\nsocial sciences. WorryWords (and its translations to over 100 languages) is\nfreely available. http://saifmohammad.com/worrywords.html\n","authors":["Saif M. Mohammad"],"pdf_url":"https://arxiv.org/pdf/2411.03966v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03964v1","updated":"2024-11-06T14:54:19Z","published":"2024-11-06T14:54:19Z","title":"What Really is Commonsense Knowledge?","summary":"  Commonsense datasets have been well developed in Natural Language Processing,\nmainly through crowdsource human annotation. However, there are debates on the\ngenuineness of commonsense reasoning benchmarks. In specific, a significant\nportion of instances in some commonsense benchmarks do not concern commonsense\nknowledge. That problem would undermine the measurement of the true commonsense\nreasoning ability of evaluated models. It is also suggested that the problem\noriginated from a blurry concept of commonsense knowledge, as distinguished\nfrom other types of knowledge. To demystify all of the above claims, in this\nstudy, we survey existing definitions of commonsense knowledge, ground into the\nthree frameworks for defining concepts, and consolidate them into a\nmulti-framework unified definition of commonsense knowledge (so-called\nconsolidated definition). We then use the consolidated definition for\nannotations and experiments on the CommonsenseQA and CommonsenseQA 2.0 datasets\nto examine the above claims. Our study shows that there exists a large portion\nof non-commonsense-knowledge instances in the two datasets, and a large\nperformance gap on these two subsets where Large Language Models (LLMs) perform\nworse on commonsense-knowledge instances.\n","authors":["Quyet V. Do","Junze Li","Tung-Duong Vuong","Zhaowei Wang","Yangqiu Song","Xiaojuan Ma"],"pdf_url":"https://arxiv.org/pdf/2411.03964v1.pdf","comment":"Code and data will be released together with the next version of the\n  paper"},{"id":"http://arxiv.org/abs/2411.03962v1","updated":"2024-11-06T14:51:02Z","published":"2024-11-06T14:51:02Z","title":"How Does A Text Preprocessing Pipeline Affect Ontology Syntactic\n  Matching?","summary":"  The generic text preprocessing pipeline, comprising Tokenisation,\nNormalisation, Stop Words Removal, and Stemming/Lemmatisation, has been\nimplemented in many ontology matching (OM) systems. However, the lack of\nstandardisation in text preprocessing creates diversity in mapping results. In\nthis paper, we investigate the effect of the text preprocessing pipeline on OM\ntasks at syntactic levels. Our experiments on 8 Ontology Alignment Evaluation\nInitiative (OAEI) track repositories with 49 distinct alignments indicate: (1)\nTokenisation and Normalisation are currently more effective than Stop Words\nRemoval and Stemming/Lemmatisation; and (2) The selection of Lemmatisation and\nStemming is task-specific. We recommend standalone Lemmatisation or Stemming\nwith post-hoc corrections. We find that (3) Porter Stemmer and Snowball Stemmer\nperform better than Lancaster Stemmer; and that (4) Part-of-Speech (POS)\nTagging does not help Lemmatisation. To repair less effective Stop Words\nRemoval and Stemming/Lemmatisation used in OM tasks, we propose a novel\ncontext-based pipeline repair approach that significantly improves matching\ncorrectness and overall matching performance. We also discuss the use of text\npreprocessing pipeline in the new era of large language models (LLMs).\n","authors":["Zhangcheng Qiang","Kerry Taylor","Weiqing Wang"],"pdf_url":"https://arxiv.org/pdf/2411.03962v1.pdf","comment":"13 pages, 26 figures, 4 tables"},{"id":"http://arxiv.org/abs/2406.10149v2","updated":"2024-11-06T14:50:40Z","published":"2024-06-14T16:00:29Z","title":"BABILong: Testing the Limits of LLMs with Long Context\n  Reasoning-in-a-Haystack","summary":"  In recent years, the input context sizes of large language models (LLMs) have\nincreased dramatically. However, existing evaluation methods have not kept\npace, failing to comprehensively assess the efficiency of models in handling\nlong contexts. To bridge this gap, we introduce the BABILong benchmark,\ndesigned to test language models' ability to reason across facts distributed in\nextremely long documents. BABILong includes a diverse set of 20 reasoning\ntasks, including fact chaining, simple induction, deduction, counting, and\nhandling lists/sets. These tasks are challenging on their own, and even more\ndemanding when the required facts are scattered across long natural text. Our\nevaluations show that popular LLMs effectively utilize only 10-20\\% of the\ncontext and their performance declines sharply with increased reasoning\ncomplexity. Among alternatives to in-context reasoning, Retrieval-Augmented\nGeneration methods achieve a modest 60\\% accuracy on single-fact question\nanswering, independent of context length. Among context extension methods, the\nhighest performance is demonstrated by recurrent memory transformers after\nfine-tuning, enabling the processing of lengths up to 50 million tokens. The\nBABILong benchmark is extendable to any length to support the evaluation of new\nupcoming models with increased capabilities, and we provide splits up to 10\nmillion token lengths.\n","authors":["Yuri Kuratov","Aydar Bulatov","Petr Anokhin","Ivan Rodkin","Dmitry Sorokin","Artyom Sorokin","Mikhail Burtsev"],"pdf_url":"https://arxiv.org/pdf/2406.10149v2.pdf","comment":"NeurIPS 2024 Datasets and Benchmarks Track"},{"id":"http://arxiv.org/abs/2410.12656v2","updated":"2024-11-06T14:14:58Z","published":"2024-10-16T15:17:20Z","title":"Evaluating Morphological Compositional Generalization in Large Language\n  Models","summary":"  Large language models (LLMs) have demonstrated significant progress in\nvarious natural language generation and understanding tasks. However, their\nlinguistic generalization capabilities remain questionable, raising doubts\nabout whether these models learn language similarly to humans. While humans\nexhibit compositional generalization and linguistic creativity in language use,\nthe extent to which LLMs replicate these abilities, particularly in morphology,\nis under-explored. In this work, we systematically investigate the\nmorphological generalization abilities of LLMs through the lens of\ncompositionality. We define morphemes as compositional primitives and design a\nnovel suite of generative and discriminative tasks to assess morphological\nproductivity and systematicity. Focusing on agglutinative languages such as\nTurkish and Finnish, we evaluate several state-of-the-art instruction-finetuned\nmultilingual models, including GPT-4 and Gemini. Our analysis shows that LLMs\nstruggle with morphological compositional generalization particularly when\napplied to novel word roots, with performance declining sharply as\nmorphological complexity increases. While models can identify individual\nmorphological combinations better than chance, their performance lacks\nsystematicity, leading to significant accuracy gaps compared to humans.\n","authors":["Mete Ismayilzada","Defne Circi","Jonne Sälevä","Hale Sirin","Abdullatif Köksal","Bhuwan Dhingra","Antoine Bosselut","Lonneke van der Plas","Duygu Ataman"],"pdf_url":"https://arxiv.org/pdf/2410.12656v2.pdf","comment":"33 pages"},{"id":"http://arxiv.org/abs/2411.03934v1","updated":"2024-11-06T14:11:39Z","published":"2024-11-06T14:11:39Z","title":"Interactions Across Blocks in Post-Training Quantization of Large\n  Language Models","summary":"  Post-training quantization is widely employed to reduce the computational\ndemands of neural networks. Typically, individual substructures, such as layers\nor blocks of layers, are quantized with the objective of minimizing\nquantization errors in their pre-activations by fine-tuning the corresponding\nweights. Deriving this local objective from the global objective of minimizing\ntask loss involves two key simplifications: assuming substructures are mutually\nindependent and ignoring the knowledge of subsequent substructures as well as\nthe task loss. In this work, we assess the effects of these simplifications on\nweight-only quantization of large language models. We introduce two multi-block\nfine-tuning strategies and compare them against the baseline of fine-tuning\nsingle transformer blocks. The first captures correlations of weights across\nblocks by jointly optimizing multiple quantized blocks. The second incorporates\nknowledge of subsequent blocks by minimizing the error in downstream\npre-activations rather than focusing solely on the quantized block. Our\nfindings indicate that the effectiveness of these methods depends on the\nspecific network model, with no impact on some models but demonstrating\nsignificant benefits for others.\n","authors":["Khasmamad Shabanovi","Lukas Wiest","Vladimir Golkov","Daniel Cremers","Thomas Pfeil"],"pdf_url":"https://arxiv.org/pdf/2411.03934v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.07001v4","updated":"2024-11-06T13:56:28Z","published":"2024-05-11T12:33:46Z","title":"ChartInsights: Evaluating Multimodal Large Language Models for Low-Level\n  Chart Question Answering","summary":"  Chart question answering (ChartQA) tasks play a critical role in interpreting\nand extracting insights from visualization charts. While recent advancements in\nmultimodal large language models (MLLMs) like GPT-4o have shown promise in\nhigh-level ChartQA tasks, such as chart captioning, their effectiveness in\nlow-level ChartQA tasks (e.g., identifying correlations) remains underexplored.\nIn this paper, we address this gap by evaluating MLLMs on low-level ChartQA\nusing a newly curated dataset, ChartInsights, which consists of 22,347 (chart,\ntask, query, answer) covering 10 data analysis tasks across 7 chart types. We\nsystematically evaluate 19 advanced MLLMs, including 12 open-source and 7\nclosed-source models. The average accuracy rate across these models is 39.8%,\nwith GPT-4o achieving the highest accuracy at 69.17%. To further explore the\nlimitations of MLLMs in low-level ChartQA, we conduct experiments that alter\nvisual elements of charts (e.g., changing color schemes, adding image noise) to\nassess their impact on the task effectiveness. Furthermore, we propose a new\ntextual prompt strategy, Chain-of-Charts, tailored for low-level ChartQA tasks,\nwhich boosts performance by 14.41%, achieving an accuracy of 83.58%. Finally,\nincorporating a visual prompt strategy that directs attention to relevant\nvisual elements further improves accuracy to 84.32%.\n","authors":["Yifan Wu","Lutao Yan","Leixian Shen","Yunhai Wang","Nan Tang","Yuyu Luo"],"pdf_url":"https://arxiv.org/pdf/2405.07001v4.pdf","comment":"EMNLP 2024 Conference Paper"},{"id":"http://arxiv.org/abs/2411.03923v1","updated":"2024-11-06T13:54:08Z","published":"2024-11-06T13:54:08Z","title":"Evaluation data contamination in LLMs: how do we measure it and (when)\n  does it matter?","summary":"  Hampering the interpretation of benchmark scores, evaluation data\ncontamination has become a growing concern in the evaluation of LLMs, and an\nactive area of research studies its effects. While evaluation data\ncontamination is easily understood intuitively, it is surprisingly difficult to\ndefine precisely which samples should be considered contaminated and,\nconsequently, how it impacts benchmark scores. We propose that these questions\nshould be addressed together and that contamination metrics can be assessed\nbased on whether models benefit from the examples they mark contaminated. We\npropose a novel analysis method called ConTAM, and show with a large scale\nsurvey of existing and novel n-gram based contamination metrics across 13\nbenchmarks and 7 models from 2 different families that ConTAM can be used to\nbetter understand evaluation data contamination and its effects. We find that\ncontamination may have a much larger effect than reported in recent LLM\nreleases and benefits models differently at different scales. We also find that\nconsidering only the longest contaminated substring provides a better signal\nthan considering a union of all contaminated substrings, and that doing model\nand benchmark specific threshold analysis greatly increases the specificity of\nthe results. Lastly, we investigate the impact of hyperparameter choices,\nfinding that, among other things, both using larger values of n and\ndisregarding matches that are infrequent in the pre-training data lead to many\nfalse negatives. With ConTAM, we provide a method to empirically ground\nevaluation data contamination metrics in downstream effects. With our\nexploration, we shed light on how evaluation data contamination can impact LLMs\nand provide insight into the considerations important when doing contamination\nanalysis. We end our paper by discussing these in more detail and providing\nconcrete suggestions for future work.\n","authors":["Aaditya K. Singh","Muhammed Yusuf Kocyigit","Andrew Poulton","David Esiobu","Maria Lomeli","Gergely Szilvasy","Dieuwke Hupkes"],"pdf_url":"https://arxiv.org/pdf/2411.03923v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03920v1","updated":"2024-11-06T13:51:42Z","published":"2024-11-06T13:51:42Z","title":"RAGulator: Lightweight Out-of-Context Detectors for Grounded Text\n  Generation","summary":"  Real-time detection of out-of-context LLM outputs is crucial for enterprises\nlooking to safely adopt RAG applications. In this work, we train lightweight\nmodels to discriminate LLM-generated text that is semantically out-of-context\nfrom retrieved text documents. We preprocess a combination of summarisation and\nsemantic textual similarity datasets to construct training data using minimal\nresources. We find that DeBERTa is not only the best-performing model under\nthis pipeline, but it is also fast and does not require additional text\npreprocessing or feature engineering. While emerging work demonstrates that\ngenerative LLMs can also be fine-tuned and used in complex data pipelines to\nachieve state-of-the-art performance, we note that speed and resource limits\nare important considerations for on-premise deployment.\n","authors":["Ian Poey","Jiajun Liu","Qishuai Zhong","Adrien Chenailler"],"pdf_url":"https://arxiv.org/pdf/2411.03920v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02937v2","updated":"2024-11-06T13:40:25Z","published":"2024-11-05T09:27:21Z","title":"Benchmarking Multimodal Retrieval Augmented Generation with Dynamic VQA\n  Dataset and Self-adaptive Planning Agent","summary":"  Multimodal Retrieval Augmented Generation (mRAG) plays an important role in\nmitigating the \"hallucination\" issue inherent in multimodal large language\nmodels (MLLMs). Although promising, existing heuristic mRAGs typically\npredefined fixed retrieval processes, which causes two issues: (1) Non-adaptive\nRetrieval Queries. (2) Overloaded Retrieval Queries. However, these flaws\ncannot be adequately reflected by current knowledge-seeking visual question\nanswering (VQA) datasets, since the most required knowledge can be readily\nobtained with a standard two-step retrieval. To bridge the dataset gap, we\nfirst construct Dyn-VQA dataset, consisting of three types of \"dynamic\"\nquestions, which require complex knowledge retrieval strategies variable in\nquery, tool, and time: (1) Questions with rapidly changing answers. (2)\nQuestions requiring multi-modal knowledge. (3) Multi-hop questions. Experiments\non Dyn-VQA reveal that existing heuristic mRAGs struggle to provide sufficient\nand precisely relevant knowledge for dynamic questions due to their rigid\nretrieval processes. Hence, we further propose the first self-adaptive planning\nagent for multimodal retrieval, OmniSearch. The underlying idea is to emulate\nthe human behavior in question solution which dynamically decomposes complex\nmultimodal questions into sub-question chains with retrieval action. Extensive\nexperiments prove the effectiveness of our OmniSearch, also provide direction\nfor advancing mRAG. The code and dataset will be open-sourced at\nhttps://github.com/Alibaba-NLP/OmniSearch.\n","authors":["Yangning Li","Yinghui Li","Xinyu Wang","Yong Jiang","Zhen Zhang","Xinran Zheng","Hui Wang","Hai-Tao Zheng","Pengjun Xie","Philip S. Yu","Fei Huang","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2411.02937v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03906v1","updated":"2024-11-06T13:37:28Z","published":"2024-11-06T13:37:28Z","title":"Lexicalization Is All You Need: Examining the Impact of Lexical\n  Knowledge in a Compositional QALD System","summary":"  In this paper, we examine the impact of lexicalization on Question Answering\nover Linked Data (QALD). It is well known that one of the key challenges in\ninterpreting natural language questions with respect to SPARQL lies in bridging\nthe lexical gap, that is mapping the words in the query to the correct\nvocabulary elements. We argue in this paper that lexicalization, that is\nexplicit knowledge about the potential interpretations of a word with respect\nto the given vocabulary, significantly eases the task and increases the\nperformance of QA systems. Towards this goal, we present a compositional QA\nsystem that can leverage explicit lexical knowledge in a compositional manner\nto infer the meaning of a question in terms of a SPARQL query. We show that\nsuch a system, given lexical knowledge, has a performance well beyond current\nQA systems, achieving up to a $35.8\\%$ increase in the micro $F_1$ score\ncompared to the best QA system on QALD-9. This shows the importance and\npotential of including explicit lexical knowledge. In contrast, we show that\nLLMs have limited abilities to exploit lexical knowledge, with only marginal\nimprovements compared to a version without lexical knowledge. This shows that\nLLMs have no ability to compositionally interpret a question on the basis of\nthe meaning of its parts, a key feature of compositional approaches. Taken\ntogether, our work shows new avenues for QALD research, emphasizing the\nimportance of lexicalization and compositionality.\n","authors":["David Maria Schmidt","Mohammad Fazleh Elahi","Philipp Cimiano"],"pdf_url":"https://arxiv.org/pdf/2411.03906v1.pdf","comment":"24th International Conference on Knowledge Engineering and Knowledge\n  Management (EKAW 2024), November 26-28, 2024, Amsterdam, The Netherlands"},{"id":"http://arxiv.org/abs/2411.03895v1","updated":"2024-11-06T13:13:33Z","published":"2024-11-06T13:13:33Z","title":"Computational Analysis of Gender Depiction in the Comedias of Calderón\n  de la Barca","summary":"  In theatre, playwrights use the portrayal of characters to explore culturally\nbased gender norms. In this paper, we develop quantitative methods to study\ngender depiction in the non-religious works (comedias) of Pedro Calder\\'on de\nla Barca, a prolific Spanish 17th century author. We gather insights from a\ncorpus of more than 100 plays by using a gender classifier and applying model\nexplainability (attribution) methods to determine which text features are most\ninfluential in the model's decision to classify speech as 'male' or 'female',\nindicating the most gendered elements of dialogue in Calder\\'on's comedias in a\nhuman accessible manner. We find that female and male characters are portrayed\ndifferently and can be identified by the gender prediction model at practically\nuseful accuracies (up to f=0.83). Analysis reveals semantic aspects of gender\nportrayal, and demonstrates that the model is even useful in providing a\nrelatively accurate scene-by-scene prediction of cross-dressing characters.\n","authors":["Allison Keith","Antonio Rojas Castro","Sebastian Padó"],"pdf_url":"https://arxiv.org/pdf/2411.03895v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03888v1","updated":"2024-11-06T13:06:43Z","published":"2024-11-06T13:06:43Z","title":"Multi3Hate: Multimodal, Multilingual, and Multicultural Hate Speech\n  Detection with Vision-Language Models","summary":"  Warning: this paper contains content that may be offensive or upsetting\n  Hate speech moderation on global platforms poses unique challenges due to the\nmultimodal and multilingual nature of content, along with the varying cultural\nperceptions. How well do current vision-language models (VLMs) navigate these\nnuances? To investigate this, we create the first multimodal and multilingual\nparallel hate speech dataset, annotated by a multicultural set of annotators,\ncalled Multi3Hate. It contains 300 parallel meme samples across 5 languages:\nEnglish, German, Spanish, Hindi, and Mandarin. We demonstrate that cultural\nbackground significantly affects multimodal hate speech annotation in our\ndataset. The average pairwise agreement among countries is just 74%,\nsignificantly lower than that of randomly selected annotator groups. Our\nqualitative analysis indicates that the lowest pairwise label agreement-only\n67% between the USA and India-can be attributed to cultural factors. We then\nconduct experiments with 5 large VLMs in a zero-shot setting, finding that\nthese models align more closely with annotations from the US than with those\nfrom other cultures, even when the memes and prompts are presented in the\ndominant language of the other culture. Code and dataset are available at\nhttps://github.com/MinhDucBui/Multi3Hate.\n","authors":["Minh Duc Bui","Katharina von der Wense","Anne Lauscher"],"pdf_url":"https://arxiv.org/pdf/2411.03888v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2410.20746v3","updated":"2024-11-06T13:05:51Z","published":"2024-10-28T05:25:50Z","title":"ElectionSim: Massive Population Election Simulation Powered by Large\n  Language Model Driven Agents","summary":"  The massive population election simulation aims to model the preferences of\nspecific groups in particular election scenarios. It has garnered significant\nattention for its potential to forecast real-world social trends. Traditional\nagent-based modeling (ABM) methods are constrained by their ability to\nincorporate complex individual background information and provide interactive\nprediction results. In this paper, we introduce ElectionSim, an innovative\nelection simulation framework based on large language models, designed to\nsupport accurate voter simulations and customized distributions, together with\nan interactive platform to dialogue with simulated voters. We present a\nmillion-level voter pool sampled from social media platforms to support\naccurate individual simulation. We also introduce PPE, a poll-based\npresidential election benchmark to assess the performance of our framework\nunder the U.S. presidential election scenario. Through extensive experiments\nand analyses, we demonstrate the effectiveness and robustness of our framework\nin U.S. presidential election simulations.\n","authors":["Xinnong Zhang","Jiayu Lin","Libo Sun","Weihong Qi","Yihang Yang","Yue Chen","Hanjia Lyu","Xinyi Mou","Siming Chen","Jiebo Luo","Xuanjing Huang","Shiping Tang","Zhongyu Wei"],"pdf_url":"https://arxiv.org/pdf/2410.20746v3.pdf","comment":"42 pages, 14 figures"},{"id":"http://arxiv.org/abs/2411.03884v1","updated":"2024-11-06T13:00:34Z","published":"2024-11-06T13:00:34Z","title":"Polynomial Composition Activations: Unleashing the Dynamics of Large\n  Language Models","summary":"  Transformers have found extensive applications across various domains due to\nthe powerful fitting capabilities. This success can be partially attributed to\ntheir inherent nonlinearity. Thus, in addition to the ReLU function employed in\nthe original transformer architecture, researchers have explored alternative\nmodules such as GeLU and SwishGLU to enhance nonlinearity and thereby augment\nrepresentational capacity. In this paper, we propose a novel category of\npolynomial composition activations (PolyCom), designed to optimize the dynamics\nof transformers. Theoretically, we provide a comprehensive mathematical\nanalysis of PolyCom, highlighting its enhanced expressivity and efficacy\nrelative to other activation functions. Notably, we demonstrate that networks\nincorporating PolyCom achieve the $\\textbf{optimal approximation rate}$,\nindicating that PolyCom networks require minimal parameters to approximate\ngeneral smooth functions in Sobolev spaces. We conduct empirical experiments on\nthe pre-training configurations of large language models (LLMs), including both\ndense and sparse architectures. By substituting conventional activation\nfunctions with PolyCom, we enable LLMs to capture higher-order interactions\nwithin the data, thus improving performance metrics in terms of accuracy and\nconvergence rates. Extensive experimental results demonstrate the effectiveness\nof our method, showing substantial improvements over other activation\nfunctions. Code is available at https://github.com/BryceZhuo/PolyCom.\n","authors":["Zhijian Zhuo","Ya Wang","Yutao Zeng","Xiaoqing Li","Xun Zhou","Jinwen Ma"],"pdf_url":"https://arxiv.org/pdf/2411.03884v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03883v1","updated":"2024-11-06T12:57:58Z","published":"2024-11-06T12:57:58Z","title":"MEG: Medical Knowledge-Augmented Large Language Models for Question\n  Answering","summary":"  Question answering is a natural language understanding task that involves\nreasoning over both explicit context and unstated, relevant domain knowledge.\nLarge language models (LLMs), which underpin most contemporary question\nanswering systems, struggle to induce how concepts relate in specialized\ndomains such as medicine. Existing medical LLMs are also costly to train. In\nthis work, we present MEG, a parameter-efficient approach for medical\nknowledge-augmented LLMs. MEG uses a lightweight mapping network to integrate\ngraph embeddings into the LLM, enabling it to leverage external knowledge in a\ncost-effective way. We evaluate our method on four popular medical\nmultiple-choice datasets and show that LLMs greatly benefit from the factual\ngrounding provided by knowledge graph embeddings. MEG attains an average of\n+10.2% accuracy over the Mistral-Instruct baseline, and +6.7% over specialized\nmodels like BioMistral. We also show results based on Llama-3. Finally, we show\nthat MEG's performance remains robust to the choice of graph encoder.\n","authors":["Laura Cabello","Carmen Martin-Turrero","Uchenna Akujuobi","Anders Søgaard","Carlos Bobed"],"pdf_url":"https://arxiv.org/pdf/2411.03883v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10499v3","updated":"2024-11-06T12:35:37Z","published":"2024-07-15T07:43:55Z","title":"CIBench: Evaluating Your LLMs with a Code Interpreter Plugin","summary":"  While LLM-Based agents, which use external tools to solve complex problems,\nhave made significant progress, benchmarking their ability is challenging,\nthereby hindering a clear understanding of their limitations. In this paper, we\npropose an interactive evaluation framework, named CIBench, to comprehensively\nassess LLMs' ability to utilize code interpreters for data science tasks. Our\nevaluation framework includes an evaluation dataset and two evaluation modes.\nThe evaluation dataset is constructed using an LLM-human cooperative approach\nand simulates an authentic workflow by leveraging consecutive and interactive\nIPython sessions. The two evaluation modes assess LLMs' ability with and\nwithout human assistance. We conduct extensive experiments to analyze the\nability of 24 LLMs on CIBench and provide valuable insights for future LLMs in\ncode interpreter utilization.\n","authors":["Chuyu Zhang","Songyang Zhang","Yingfan Hu","Haowen Shen","Kuikun Liu","Zerun Ma","Fengzhe Zhou","Wenwei Zhang","Xuming He","Dahua Lin","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2407.10499v3.pdf","comment":"Under review. The first three authors contribute equally, and\n  Songyang Zhang is the project leader"},{"id":"http://arxiv.org/abs/2411.03866v1","updated":"2024-11-06T12:22:04Z","published":"2024-11-06T12:22:04Z","title":"Performance evaluation of SLAM-ASR: The Good, the Bad, the Ugly, and the\n  Way Forward","summary":"  Recent research has demonstrated that training a linear connector between\nspeech foundation encoders and large language models (LLMs) enables this\narchitecture to achieve strong ASR capabilities. Despite the impressive\nresults, it remains unclear whether these simple approaches are robust enough\nacross different scenarios and speech conditions, such as domain shifts and\ndifferent speech perturbations. In this paper, we address these questions by\nconducting various ablation experiments using a recent and widely adopted\napproach called SLAM-ASR. We present novel empirical findings that offer\ninsights on how to effectively utilize the SLAM-ASR architecture across a wide\nrange of settings. Our main findings indicate that the SLAM-ASR exhibits poor\nperformance in cross-domain evaluation settings. Additionally, speech\nperturbations within in-domain data, such as changes in speed or the presence\nof additive noise, can significantly impact performance. Our findings offer\ncritical insights for fine-tuning and configuring robust LLM-based ASR models,\ntailored to different data characteristics and computational resources.\n","authors":["Shashi Kumar","Iuliia Thorbecke","Sergio Burdisso","Esaú Villatoro-Tello","Manjunath K E","Kadri Hacioğlu","Pradeep Rangappa","Petr Motlicek","Aravind Ganapathiraju","Andreas Stolcke"],"pdf_url":"https://arxiv.org/pdf/2411.03866v1.pdf","comment":"Submitted to ICASSP 2025 SALMA Workshop"},{"id":"http://arxiv.org/abs/2404.01204v3","updated":"2024-11-06T12:02:52Z","published":"2024-04-01T16:00:01Z","title":"The Fine Line: Navigating Large Language Model Pretraining with\n  Down-streaming Capability Analysis","summary":"  Uncovering early-stage metrics that reflect final model performance is one\ncore principle for large-scale pretraining. The existing scaling law\ndemonstrates the power-law correlation between pretraining loss and training\nflops, which serves as an important indicator of the current training state for\nlarge language models. However, this principle only focuses on the model's\ncompression properties on the training data, resulting in an inconsistency with\nthe ability improvements on the downstream tasks. Some follow-up works\nattempted to extend the scaling-law to more complex metrics (such as\nhyperparameters), but still lacked a comprehensive analysis of the dynamic\ndifferences among various capabilities during pretraining. To address the\naforementioned limitations, this paper undertakes a comprehensive comparison of\nmodel capabilities at various pretraining intermediate checkpoints. Through\nthis analysis, we confirm that specific downstream metrics exhibit similar\ntraining dynamics across models of different sizes, up to 67 billion\nparameters. In addition to our core findings, we've reproduced Amber and\nOpenLLaMA, releasing their intermediate checkpoints. This initiative offers\nvaluable resources to the research community and facilitates the verification\nand exploration of LLM pretraining by open-source researchers. Besides, we\nprovide empirical summaries, including performance comparisons of different\nmodels and capabilities, and tuition of key metrics for different training\nphases. Based on these findings, we provide a more user-friendly strategy for\nevaluating the optimization state, offering guidance for establishing a stable\npretraining process.\n","authors":["Chen Yang","Junzhuo Li","Xinyao Niu","Xinrun Du","Songyang Gao","Haoran Zhang","Zhaoliang Chen","Xingwei Qu","Ruibin Yuan","Yizhi Li","Jiaheng Liu","Stephen W. Huang","Shawn Yue","Ge Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.01204v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03855v1","updated":"2024-11-06T11:57:55Z","published":"2024-11-06T11:57:55Z","title":"MambaPEFT: Exploring Parameter-Efficient Fine-Tuning for Mamba","summary":"  An ecosystem of Transformer-based models has been established by building\nlarge models with extensive data. Parameter-efficient fine-tuning (PEFT) is a\ncrucial technology for deploying these models to downstream tasks with minimal\ncost while achieving effective performance. Recently, Mamba, a State Space\nModel (SSM)-based model, has attracted attention as a potential alternative to\nTransformers. While many large-scale Mamba-based models have been proposed,\nefficiently adapting pre-trained Mamba-based models to downstream tasks remains\nunexplored. In this paper, we conduct an exploratory analysis of PEFT methods\nfor Mamba. We investigate the effectiveness of existing PEFT methods for\nTransformers when applied to Mamba. We also modify these methods to better\nalign with the Mamba architecture. Additionally, we propose new Mamba-specific\nPEFT methods that leverage the distinctive structure of Mamba. Our experiments\nindicate that PEFT performs more effectively for Mamba than Transformers.\nLastly, we demonstrate how to effectively combine multiple PEFT methods and\nprovide a framework that outperforms previous works. To ensure reproducibility,\nwe will release the code after publication.\n","authors":["Masakazu Yoshimura","Teruaki Hayashi","Yota Maeda"],"pdf_url":"https://arxiv.org/pdf/2411.03855v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.19453v2","updated":"2024-11-06T11:49:10Z","published":"2024-10-25T10:28:59Z","title":"ShifCon: Enhancing Non-Dominant Language Capabilities with a Shift-based\n  Contrastive Framework","summary":"  Although fine-tuning Large Language Models (LLMs) with multilingual data can\nrapidly enhance the multilingual capabilities of LLMs, they still exhibit a\nperformance gap between the dominant language (e.g., English) and non-dominant\nones due to the imbalance of training data across languages. To further enhance\nthe performance of non-dominant languages, we propose ShifCon, a Shift-based\nContrastive framework that aligns the internal forward process of other\nlanguages toward that of the dominant one. Specifically, it shifts the\nrepresentations of non-dominant languages into the dominant language subspace,\nallowing them to access relatively rich information encoded in the model\nparameters. The enriched representations are then shifted back into their\noriginal language subspace before generation. Moreover, we introduce a subspace\ndistance metric to pinpoint the optimal layer area for shifting representations\nand employ multilingual contrastive learning to further enhance the alignment\nof representations within this area. Experiments demonstrate that our ShifCon\nframework significantly enhances the performance of non-dominant languages,\nparticularly for low-resource ones. Further analysis offers extra insights to\nverify the effectiveness of ShifCon and propel future research\n","authors":["Hengyuan Zhang","Chenming Shang","Sizhe Wang","Dongdong Zhang","Renliang Sun","Yiyao Yu","Yujiu Yang","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2410.19453v2.pdf","comment":"23 pages, 11 figures"},{"id":"http://arxiv.org/abs/2411.02832v2","updated":"2024-11-06T11:19:42Z","published":"2024-11-05T06:11:17Z","title":"PersianRAG: A Retrieval-Augmented Generation System for Persian Language","summary":"  Retrieval augmented generation (RAG) models, which integrate large-scale\npre-trained generative models with external retrieval mechanisms, have shown\nsignificant success in various natural language processing (NLP) tasks.\nHowever, applying RAG models in Persian language as a low-resource language,\nposes distinct challenges. These challenges primarily involve the\npreprocessing, embedding, retrieval, prompt construction, language modeling,\nand response evaluation of the system. In this paper, we address the challenges\ntowards implementing a real-world RAG system for Persian language called\nPersianRAG. We propose novel solutions to overcome these obstacles and evaluate\nour approach using several Persian benchmark datasets. Our experimental results\ndemonstrate the capability of the PersianRAG framework to enhance question\nanswering task in Persian.\n","authors":["Hossein Hosseini","Mohammad Sobhan Zare","Amir Hossein Mohammadi","Arefeh Kazemi","Zahra Zojaji","Mohammad Ali Nematbakhsh"],"pdf_url":"https://arxiv.org/pdf/2411.02832v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01192v2","updated":"2024-11-06T11:19:01Z","published":"2024-11-02T09:39:49Z","title":"Swan and ArabicMTEB: Dialect-Aware, Arabic-Centric, Cross-Lingual, and\n  Cross-Cultural Embedding Models and Benchmarks","summary":"  We introduce {\\bf Swan}, a family of embedding models centred around the\nArabic language, addressing both small-scale and large-scale use cases. Swan\nincludes two variants: Swan-Small, based on ARBERTv2, and Swan-Large, built on\nArMistral, a pretrained Arabic large language model. To evaluate these models,\nwe propose ArabicMTEB, a comprehensive benchmark suite that assesses\ncross-lingual, multi-dialectal, multi-domain, and multi-cultural Arabic text\nembedding performance, covering eight diverse tasks and spanning 94 datasets.\nSwan-Large achieves state-of-the-art results, outperforming\nMultilingual-E5-large in most Arabic tasks, while the Swan-Small consistently\nsurpasses Multilingual-E5-base. Our extensive evaluations demonstrate that Swan\nmodels are both dialectally and culturally aware, excelling across various\nArabic domains while offering significant monetary efficiency. This work\nsignificantly advances the field of Arabic language modelling and provides\nvaluable resources for future research and applications in Arabic natural\nlanguage processing. Our models and benchmark will be made publicly accessible\nfor research.\n","authors":["Gagan Bhatia","El Moatez Billah Nagoudi","Abdellah El Mekki","Fakhraddin Alwajih","Muhammad Abdul-Mageed"],"pdf_url":"https://arxiv.org/pdf/2411.01192v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03817v1","updated":"2024-11-06T10:35:11Z","published":"2024-11-06T10:35:11Z","title":"From Novice to Expert: LLM Agent Policy Optimization via Step-wise\n  Reinforcement Learning","summary":"  The outstanding capabilities of large language models (LLMs) render them a\ncrucial component in various autonomous agent systems. While traditional\nmethods depend on the inherent knowledge of LLMs without fine-tuning, more\nrecent approaches have shifted toward the reinforcement learning strategy to\nfurther enhance agents' ability to solve complex interactive tasks with\nenvironments and tools. However, previous approaches are constrained by the\nsparse reward issue, where existing datasets solely provide a final scalar\nreward for each multi-step reasoning chain, potentially leading to\nineffectiveness and inefficiency in policy learning. In this paper, we\nintroduce StepAgent, which utilizes step-wise reward to optimize the agent's\nreinforcement learning process. Inheriting the spirit of novice-to-expert\ntheory, we first compare the actions of the expert and the agent to\nautomatically generate intermediate rewards for fine-grained optimization.\nAdditionally, we propose implicit-reward and inverse reinforcement learning\ntechniques to facilitate agent reflection and policy adjustment. Further\ntheoretical analysis demonstrates that the action distribution of the agent can\nconverge toward the expert action distribution over multiple training cycles.\nExperimental results across various datasets indicate that StepAgent\noutperforms existing baseline methods.\n","authors":["Zhirui Deng","Zhicheng Dou","Yutao Zhu","Ji-Rong Wen","Ruibin Xiong","Mang Wang","Weipeng Chen"],"pdf_url":"https://arxiv.org/pdf/2411.03817v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03814v1","updated":"2024-11-06T10:32:09Z","published":"2024-11-06T10:32:09Z","title":"MRJ-Agent: An Effective Jailbreak Agent for Multi-Round Dialogue","summary":"  Large Language Models (LLMs) demonstrate outstanding performance in their\nreservoir of knowledge and understanding capabilities, but they have also been\nshown to be prone to illegal or unethical reactions when subjected to jailbreak\nattacks. To ensure their responsible deployment in critical applications, it is\ncrucial to understand the safety capabilities and vulnerabilities of LLMs.\nPrevious works mainly focus on jailbreak in single-round dialogue, overlooking\nthe potential jailbreak risks in multi-round dialogues, which are a vital way\nhumans interact with and extract information from LLMs. Some studies have\nincreasingly concentrated on the risks associated with jailbreak in multi-round\ndialogues. These efforts typically involve the use of manually crafted\ntemplates or prompt engineering techniques. However, due to the inherent\ncomplexity of multi-round dialogues, their jailbreak performance is limited. To\nsolve this problem, we propose a novel multi-round dialogue jailbreaking agent,\nemphasizing the importance of stealthiness in identifying and mitigating\npotential threats to human values posed by LLMs. We propose a risk\ndecomposition strategy that distributes risks across multiple rounds of queries\nand utilizes psychological strategies to enhance attack strength. Extensive\nexperiments show that our proposed method surpasses other attack methods and\nachieves state-of-the-art attack success rate. We will make the corresponding\ncode and dataset available for future research. The code will be released soon.\n","authors":["Fengxiang Wang","Ranjie Duan","Peng Xiao","Xiaojun Jia","YueFeng Chen","Chongwen Wang","Jialing Tao","Hang Su","Jun Zhu","Hui Xue"],"pdf_url":"https://arxiv.org/pdf/2411.03814v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.18680v3","updated":"2024-11-06T10:27:05Z","published":"2024-09-27T12:06:53Z","title":"Beyond Single-Audio: Advancing Multi-Audio Processing in Audio Large\n  Language Models","summary":"  Various audio-LLMs (ALLMs) have been explored recently for tackling different\naudio tasks simultaneously using a single, unified model. While existing\nevaluations of ALLMs primarily focus on single-audio tasks, real-world\napplications often involve processing multiple audio streams simultaneously. To\nbridge this gap, we propose the first multi-audio evaluation (MAE) benchmark\nthat consists of 20 datasets from 11 multi-audio tasks encompassing both speech\nand sound scenarios. Comprehensive experiments on MAE demonstrate that the\nexisting ALLMs, while being powerful in comprehending primary audio elements in\nindividual audio inputs, struggling to handle multi-audio scenarios. To this\nend, we propose a novel multi-audio-LLM (MALLM) to capture audio context among\nmultiple similar audios using discriminative learning on our proposed synthetic\ndata. The results demonstrate that the proposed MALLM outperforms all baselines\nand achieves high data efficiency using synthetic data without requiring human\nannotations. The proposed MALLM opens the door for ALLMs towards multi-audio\nprocessing era and brings us closer to replicating human auditory capabilities\nin machines.\n","authors":["Yiming Chen","Xianghu Yue","Xiaoxue Gao","Chen Zhang","Luis Fernando D'Haro","Robby T. Tan","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2409.18680v3.pdf","comment":"EMNLP24 Findings. Data available at\n  https://github.com/MatthewCYM/MALLM"},{"id":"http://arxiv.org/abs/2411.03811v1","updated":"2024-11-06T10:14:58Z","published":"2024-11-06T10:14:58Z","title":"The natural stability of autonomous morphology","summary":"  Autonomous morphology, such as inflection class systems and paradigmatic\ndistribution patterns, is widespread and diachronically resilient in natural\nlanguage. Why this should be so has remained unclear given that autonomous\nmorphology imposes learning costs, offers no clear benefit relative to its\nabsence and could easily be removed by the analogical forces which are\nconstantly reshaping it. Here we propose an explanation for the resilience of\nautonomous morphology, in terms of a diachronic dynamic of attraction and\nrepulsion between morphomic categories, which emerges spontaneously from a\nsimple paradigm cell filling process. Employing computational evolutionary\nmodels, our key innovation is to bring to light the role of `dissociative\nevidence', i.e., evidence for inflectional distinctiveness which a rational\nreasoner will have access to during analogical inference. Dissociative evidence\ncreates a repulsion dynamic which prevents morphomic classes from collapsing\ntogether entirely, i.e., undergoing complete levelling. As we probe alternative\nmodels, we reveal the limits of conditional entropy as a measure for\npredictability in systems that are undergoing change. Finally, we demonstrate\nthat autonomous morphology, far from being `unnatural' (e.g.\n\\citealt{Aronoff1994}), is rather the natural (emergent) consequence of a\nnatural (rational) process of inference applied to inflectional systems.\n","authors":["Erich Round","Louise Esher","Sacha Beniamine"],"pdf_url":"https://arxiv.org/pdf/2411.03811v1.pdf","comment":"Accepted for publication by the journal Morphology"},{"id":"http://arxiv.org/abs/2411.03806v1","updated":"2024-11-06T10:06:21Z","published":"2024-11-06T10:06:21Z","title":"Understanding the Effects of Human-written Paraphrases in LLM-generated\n  Text Detection","summary":"  Natural Language Generation has been rapidly developing with the advent of\nlarge language models (LLMs). While their usage has sparked significant\nattention from the general public, it is important for readers to be aware when\na piece of text is LLM-generated. This has brought about the need for building\nmodels that enable automated LLM-generated text detection, with the aim of\nmitigating potential negative outcomes of such content. Existing LLM-generated\ndetectors show competitive performances in telling apart LLM-generated and\nhuman-written text, but this performance is likely to deteriorate when\nparaphrased texts are considered. In this study, we devise a new data\ncollection strategy to collect Human & LLM Paraphrase Collection (HLPC), a\nfirst-of-its-kind dataset that incorporates human-written texts and\nparaphrases, as well as LLM-generated texts and paraphrases. With the aim of\nunderstanding the effects of human-written paraphrases on the performance of\nstate-of-the-art LLM-generated text detectors OpenAI RoBERTa and watermark\ndetectors, we perform classification experiments that incorporate human-written\nparaphrases, watermarked and non-watermarked LLM-generated documents from GPT\nand OPT, and LLM-generated paraphrases from DIPPER and BART. The results show\nthat the inclusion of human-written paraphrases has a significant impact of\nLLM-generated detector performance, promoting TPR@1%FPR with a possible\ntrade-off of AUROC and accuracy.\n","authors":["Hiu Ting Lau","Arkaitz Zubiaga"],"pdf_url":"https://arxiv.org/pdf/2411.03806v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03805v1","updated":"2024-11-06T10:02:50Z","published":"2024-11-06T10:02:50Z","title":"A Comparative Study of Recent Large Language Models on Generating\n  Hospital Discharge Summaries for Lung Cancer Patients","summary":"  Generating discharge summaries is a crucial yet time-consuming task in\nclinical practice, essential for conveying pertinent patient information and\nfacilitating continuity of care. Recent advancements in large language models\n(LLMs) have significantly enhanced their capability in understanding and\nsummarizing complex medical texts. This research aims to explore how LLMs can\nalleviate the burden of manual summarization, streamline workflow efficiencies,\nand support informed decision-making in healthcare settings. Clinical notes\nfrom a cohort of 1,099 lung cancer patients were utilized, with a subset of 50\npatients for testing purposes, and 102 patients used for model fine-tuning.\nThis study evaluates the performance of multiple LLMs, including GPT-3.5,\nGPT-4, GPT-4o, and LLaMA 3 8b, in generating discharge summaries. Evaluation\nmetrics included token-level analysis (BLEU, ROUGE-1, ROUGE-2, ROUGE-L) and\nsemantic similarity scores between model-generated summaries and\nphysician-written gold standards. LLaMA 3 8b was further tested on clinical\nnotes of varying lengths to examine the stability of its performance. The study\nfound notable variations in summarization capabilities among LLMs. GPT-4o and\nfine-tuned LLaMA 3 demonstrated superior token-level evaluation metrics, while\nLLaMA 3 consistently produced concise summaries across different input lengths.\nSemantic similarity scores indicated GPT-4o and LLaMA 3 as leading models in\ncapturing clinical relevance. This study contributes insights into the efficacy\nof LLMs for generating discharge summaries, highlighting LLaMA 3's robust\nperformance in maintaining clarity and relevance across varying clinical\ncontexts. These findings underscore the potential of automated summarization\ntools to enhance documentation precision and efficiency, ultimately improving\npatient care and operational capability in healthcare settings.\n","authors":["Yiming Li","Fang Li","Kirk Roberts","Licong Cui","Cui Tao","Hua Xu"],"pdf_url":"https://arxiv.org/pdf/2411.03805v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.15306v3","updated":"2024-11-06T09:49:31Z","published":"2024-05-24T07:48:35Z","title":"DeTikZify: Synthesizing Graphics Programs for Scientific Figures and\n  Sketches with TikZ","summary":"  Creating high-quality scientific figures can be time-consuming and\nchallenging, even though sketching ideas on paper is relatively easy.\nFurthermore, recreating existing figures that are not stored in formats\npreserving semantic information is equally complex. To tackle this problem, we\nintroduce DeTikZify, a novel multimodal language model that automatically\nsynthesizes scientific figures as semantics-preserving TikZ graphics programs\nbased on sketches and existing figures. To achieve this, we create three new\ndatasets: DaTikZv2, the largest TikZ dataset to date, containing over 360k\nhuman-created TikZ graphics; SketchFig, a dataset that pairs hand-drawn\nsketches with their corresponding scientific figures; and MetaFig, a collection\nof diverse scientific figures and associated metadata. We train DeTikZify on\nMetaFig and DaTikZv2, along with synthetically generated sketches learned from\nSketchFig. We also introduce an MCTS-based inference algorithm that enables\nDeTikZify to iteratively refine its outputs without the need for additional\ntraining. Through both automatic and human evaluation, we demonstrate that\nDeTikZify outperforms commercial Claude 3 and GPT-4V in synthesizing TikZ\nprograms, with the MCTS algorithm effectively boosting its performance. We make\nour code, models, and datasets publicly available.\n","authors":["Jonas Belouadi","Simone Paolo Ponzetto","Steffen Eger"],"pdf_url":"https://arxiv.org/pdf/2405.15306v3.pdf","comment":"Accepted at NeurIPS 2024 (spotlight); Project page:\n  https://github.com/potamides/DeTikZify"},{"id":"http://arxiv.org/abs/2411.03039v2","updated":"2024-11-06T09:28:25Z","published":"2024-11-05T12:22:51Z","title":"Self-Compositional Data Augmentation for Scientific Keyphrase Generation","summary":"  State-of-the-art models for keyphrase generation require large amounts of\ntraining data to achieve good performance. However, obtaining keyphrase-labeled\ndocuments can be challenging and costly. To address this issue, we present a\nself-compositional data augmentation method. More specifically, we measure the\nrelatedness of training documents based on their shared keyphrases, and combine\nsimilar documents to generate synthetic samples. The advantage of our method\nlies in its ability to create additional training samples that keep domain\ncoherence, without relying on external data or resources. Our results on\nmultiple datasets spanning three different domains, demonstrate that our method\nconsistently improves keyphrase generation. A qualitative analysis of the\ngenerated keyphrases for the Computer Science domain confirms this improvement\ntowards their representativity property.\n","authors":["Mael Houbre","Florian Boudin","Beatrice Daille","Akiko Aizawa"],"pdf_url":"https://arxiv.org/pdf/2411.03039v2.pdf","comment":"Accepted to JCDL 2024. This is the author's version of the work. It\n  is posted here for your personal use. Not for redistribution. The definitive\n  version was published in the proceedings of the 2024 ACM/IEEE Joint\n  Conference on Digital Libraries (JCDL 24)\n  https://doi.org/10.1145/3677389.3702504"},{"id":"http://arxiv.org/abs/2411.02265v3","updated":"2024-11-06T09:15:27Z","published":"2024-11-04T16:56:26Z","title":"Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated\n  Parameters by Tencent","summary":"  In this paper, we introduce Hunyuan-Large, which is currently the largest\nopen-source Transformer-based mixture of experts model, with a total of 389\nbillion parameters and 52 billion activation parameters, capable of handling up\nto 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior\nperformance across various benchmarks including language understanding and\ngeneration, logical reasoning, mathematical problem-solving, coding,\nlong-context, and aggregated tasks, where it outperforms LLama3.1-70B and\nexhibits comparable performance when compared to the significantly larger\nLLama3.1-405B model. Key practice of Hunyuan-Large include large-scale\nsynthetic data that is orders larger than in previous literature, a mixed\nexpert routing strategy, a key-value cache compression technique, and an\nexpert-specific learning rate strategy. Additionally, we also investigate the\nscaling laws and learning rate schedule of mixture of experts models, providing\nvaluable insights and guidances for future model development and optimization.\nThe code and checkpoints of Hunyuan-Large are released to facilitate future\ninnovations and applications.\n  Codes: https://github.com/Tencent/Hunyuan-Large\n  Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large\n","authors":["Xingwu Sun","Yanfeng Chen","Yiqing Huang","Ruobing Xie","Jiaqi Zhu","Kai Zhang","Shuaipeng Li","Zhen Yang","Jonny Han","Xiaobo Shu","Jiahao Bu","Zhongzhi Chen","Xuemeng Huang","Fengzong Lian","Saiyong Yang","Jianfeng Yan","Yuyuan Zeng","Xiaoqin Ren","Chao Yu","Lulu Wu","Yue Mao","Jun Xia","Tao Yang","Suncong Zheng","Kan Wu","Dian Jiao","Jinbao Xue","Xipeng Zhang","Decheng Wu","Kai Liu","Dengpeng Wu","Guanghui Xu","Shaohua Chen","Shuang Chen","Xiao Feng","Yigeng Hong","Junqiang Zheng","Chengcheng Xu","Zongwei Li","Xiong Kuang","Jianglu Hu","Yiqi Chen","Yuchi Deng","Guiyang Li","Ao Liu","Chenchen Zhang","Shihui Hu","Zilong Zhao","Zifan Wu","Yao Ding","Weichao Wang","Han Liu","Roberts Wang","Hao Fei","Peijie Yu","Ze Zhao","Xun Cao","Hai Wang","Fusheng Xiang","Mengyuan Huang","Zhiyuan Xiong","Bin Hu","Xuebin Hou","Lei Jiang","Jianqiang Ma","Jiajia Wu","Yaping Deng","Yi Shen","Qian Wang","Weijie Liu","Jie Liu","Meng Chen","Liang Dong","Weiwen Jia","Hu Chen","Feifei Liu","Rui Yuan","Huilin Xu","Zhenxiang Yan","Tengfei Cao","Zhichao Hu","Xinhua Feng","Dong Du","Tinghao Yu","Yangyu Tao","Feng Zhang","Jianchen Zhu","Chengzhong Xu","Xirui Li","Chong Zha","Wen Ouyang","Yinben Xia","Xiang Li","Zekun He","Rongpeng Chen","Jiawei Song","Ruibin Chen","Fan Jiang","Chongqing Zhao","Bo Wang","Hao Gong","Rong Gan","Winston Hu","Zhanhui Kang","Yong Yang","Yuhong Liu","Di Wang","Jie Jiang"],"pdf_url":"https://arxiv.org/pdf/2411.02265v3.pdf","comment":"17 pages, 4 Figures"},{"id":"http://arxiv.org/abs/2411.03769v1","updated":"2024-11-06T09:05:17Z","published":"2024-11-06T09:05:17Z","title":"No Culture Left Behind: ArtELingo-28, a Benchmark of WikiArt with\n  Captions in 28 Languages","summary":"  Research in vision and language has made considerable progress thanks to\nbenchmarks such as COCO. COCO captions focused on unambiguous facts in English;\nArtEmis introduced subjective emotions and ArtELingo introduced some\nmultilinguality (Chinese and Arabic). However we believe there should be more\nmultilinguality. Hence, we present ArtELingo-28, a vision-language benchmark\nthat spans $\\textbf{28}$ languages and encompasses approximately\n$\\textbf{200,000}$ annotations ($\\textbf{140}$ annotations per image).\nTraditionally, vision research focused on unambiguous class labels, whereas\nArtELingo-28 emphasizes diversity of opinions over languages and cultures. The\nchallenge is to build machine learning systems that assign emotional captions\nto images. Baseline results will be presented for three novel conditions:\nZero-Shot, Few-Shot and One-vs-All Zero-Shot. We find that cross-lingual\ntransfer is more successful for culturally-related languages. Data and code are\nprovided at www.artelingo.org.\n","authors":["Youssef Mohamed","Runjia Li","Ibrahim Said Ahmad","Kilichbek Haydarov","Philip Torr","Kenneth Ward Church","Mohamed Elhoseiny"],"pdf_url":"https://arxiv.org/pdf/2411.03769v1.pdf","comment":"9 pages, Accepted at EMNLP 24, for more details see www.artelingo.org"},{"id":"http://arxiv.org/abs/2411.03766v1","updated":"2024-11-06T08:59:44Z","published":"2024-11-06T08:59:44Z","title":"Number Cookbook: Number Understanding of Language Models and How to\n  Improve It","summary":"  Large language models (LLMs) can solve an increasing number of complex\nreasoning tasks while making surprising mistakes in basic numerical\nunderstanding and processing (such as 9.11 > 9.9). The latter ability is\nessential for tackling complex arithmetic and mathematical problems and serves\nas a foundation for most reasoning tasks, but previous work paid little\nattention to it or only discussed several restricted tasks (like integer\naddition). In this paper, we comprehensively investigate the numerical\nunderstanding and processing ability (NUPA) of LLMs. Firstly, we introduce a\nbenchmark covering four common numerical representations and 17 distinct\nnumerical tasks in four major categories, resulting in 41 meaningful\ncombinations in total. These tasks are derived from primary and secondary\neducation curricula, encompassing nearly all everyday numerical understanding\nand processing scenarios, and the rules of these tasks are very simple and\nclear. Through the benchmark, we find that current LLMs fail frequently in many\nof the tasks. To study the problem, we train small models with existing and\npotential techniques for enhancing NUPA (such as special tokenizers, PEs, and\nnumber formats), comprehensively evaluating their effectiveness using our\ntestbed. We also finetune practical-scale LLMs on our proposed NUPA tasks and\nfind that 1) naive finetuning can improve NUPA a lot on many but not all tasks,\nand 2) surprisingly, techniques designed to enhance NUPA prove ineffective for\nfinetuning pretrained models. We further explore the impact of chain-of-thought\ntechniques on NUPA. Our work takes a preliminary step towards understanding and\nimproving NUPA of LLMs. Our benchmark and code are released at\nhttps://github.com/GraphPKU/number_cookbook.\n","authors":["Haotong Yang","Yi Hu","Shijia Kang","Zhouchen Lin","Muhan Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.03766v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02603v2","updated":"2024-11-06T08:51:52Z","published":"2024-11-04T20:53:04Z","title":"FactTest: Factuality Testing in Large Language Models with Finite-Sample\n  and Distribution-Free Guarantees","summary":"  The propensity of Large Language Models (LLMs) to generate hallucinations and\nnon-factual content undermines their reliability in high-stakes domains, where\nrigorous control over Type I errors (the conditional probability of incorrectly\nclassifying hallucinations as truthful content) is essential. Despite its\nimportance, formal verification of LLM factuality with such guarantees remains\nlargely unexplored. In this paper, we introduce FactTest, a novel framework\nthat statistically assesses whether an LLM can confidently provide correct\nanswers to given questions with high-probability correctness guarantees. We\nformulate factuality testing as hypothesis testing problem to enforce an upper\nbound of Type I errors at user-specified significance levels. Notably, we prove\nthat our framework also ensures strong Type II error control under mild\nconditions and can be extended to maintain its effectiveness when covariate\nshifts exist. %These analyses are amenable to the principled NP framework. Our\napproach is distribution-free and works for any number of human-annotated\nsamples. It is model-agnostic and applies to any black-box or white-box LM.\nExtensive experiments on question-answering (QA) and multiple-choice benchmarks\ndemonstrate that \\approach effectively detects hallucinations and improves the\nmodel's ability to abstain from answering unknown questions, leading to an over\n40% accuracy improvement.\n","authors":["Fan Nie","Xiaotian Hou","Shuhang Lin","James Zou","Huaxiu Yao","Linjun Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.02603v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13824v3","updated":"2024-11-06T08:29:22Z","published":"2024-10-17T17:48:54Z","title":"Harnessing Webpage UIs for Text-Rich Visual Understanding","summary":"  Text-rich visual understanding-the ability to process environments where\ndense textual content is integrated with visuals-is crucial for multimodal\nlarge language models (MLLMs) to interact effectively with structured\nenvironments. To enhance this capability, we propose synthesizing general\nmultimodal instructions from webpage UIs using text-based large language models\n(LLMs). Despite lacking direct visual input, text-based LLMs are able to\nprocess structured text representations from webpage accessibility trees. These\ninstructions are then paired with UI screenshots to train multimodal models. We\nintroduce MultiUI, a dataset containing 7.3 million samples from 1 million\nwebsites, covering diverse multimodal tasks and UI layouts. Models trained on\nMultiUI not only excel in web UI tasks-achieving up to a 48% improvement on\nVisualWebBench and a 19.1% boost in element accuracy on a web agent dataset\nMind2Web-but also generalize surprisingly well to non-web UI tasks and even to\nnon-UI domains, such as document understanding, OCR, and chart interpretation.\nThese results highlight the broad applicability of web UI data for advancing\ntext-rich visual understanding across various scenarios.\n","authors":["Junpeng Liu","Tianyue Ou","Yifan Song","Yuxiao Qu","Wai Lam","Chenyan Xiong","Wenhu Chen","Graham Neubig","Xiang Yue"],"pdf_url":"https://arxiv.org/pdf/2410.13824v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08081v3","updated":"2024-11-06T07:31:28Z","published":"2024-10-10T16:25:34Z","title":"Packing Analysis: Packing Is More Appropriate for Large Models or\n  Datasets in Supervised Fine-tuning","summary":"  Packing, initially utilized in the pre-training phase, is an optimization\ntechnique designed to maximize hardware resource efficiency by combining\ndifferent training sequences to fit the model's maximum input length. Although\nit has demonstrated effectiveness during pre-training, there remains a lack of\ncomprehensive analysis for the supervised fine-tuning (SFT) stage on the\nfollowing points: (1) whether packing can effectively enhance training\nefficiency while maintaining performance, (2) the suitable size of the model\nand dataset for fine-tuning with the packing method, and (3) whether packing\nunrelated or related training samples might cause the model to either\nexcessively disregard or over-rely on the context.\n  In this paper, we perform extensive comparisons between SFT methods using\npadding and packing, covering SFT datasets ranging from 69K to 1.2M and models\nfrom 8B to 70B. This provides the first comprehensive analysis of the\nadvantages and limitations of packing versus padding, as well as practical\nconsiderations for implementing packing in various training scenarios. Our\nanalysis covers various benchmarks, including knowledge, reasoning, and coding,\nas well as GPT-based evaluations, time efficiency, and other fine-tuning\nparameters. We also open-source our code for fine-tuning and evaluation and\nprovide checkpoints fine-tuned on datasets of different sizes, aiming to\nadvance future research on packing methods. Code is available at:\nhttps://github.com/ShuheWang1998/Packing-Analysis?tab=readme-ov-file.\n","authors":["Shuhe Wang","Guoyin Wang","Yizhong Wang","Jiwei Li","Eduard Hovy","Chen Guo"],"pdf_url":"https://arxiv.org/pdf/2410.08081v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03700v1","updated":"2024-11-06T06:50:50Z","published":"2024-11-06T06:50:50Z","title":"The Root Shapes the Fruit: On the Persistence of Gender-Exclusive Harms\n  in Aligned Language Models","summary":"  Natural-language assistants are designed to provide users with helpful\nresponses while avoiding harmful outputs, largely achieved through alignment to\nhuman preferences. Yet there is limited understanding of whether alignment\ntechniques may inadvertently perpetuate or even amplify harmful biases\ninherited from their pre-aligned base models. This issue is compounded by the\nchoice of bias evaluation benchmarks in popular preference-finetuned models,\nwhich predominantly focus on dominant social categories, such as binary gender,\nthereby limiting insights into biases affecting underrepresented groups.\nTowards addressing this gap, we center transgender, nonbinary, and other\ngender-diverse identities to investigate how alignment procedures interact with\npre-existing gender-diverse bias in LLMs. Our key contributions include: 1) a\ncomprehensive survey of bias evaluation modalities across leading\npreference-finetuned LLMs, highlighting critical gaps in gender-diverse\nrepresentation, 2) systematic evaluation of gender-diverse biases across 12\nmodels spanning Direct Preference Optimization (DPO) stages, uncovering harms\npopular bias benchmarks fail to detect, and 3) a flexible framework for\nmeasuring harmful biases in implicit reward signals applicable to other social\ncontexts. Our findings reveal that DPO-aligned models are particularly\nsensitive to supervised finetuning (SFT), and can amplify two forms of\nreal-world gender-diverse harms from their base models: stigmatization and\ngender non-affirmative language. We conclude with recommendations tailored to\nDPO and broader alignment practices, advocating for the adoption of\ncommunity-informed bias evaluation frameworks to more effectively identify and\naddress underrepresented harms in LLMs.\n","authors":["Anaelia Ovalle","Krunoslav Lehman Pavasovic","Louis Martin","Luke Zettlemoyer","Eric Michael Smith","Adina Williams","Levent Sagun"],"pdf_url":"https://arxiv.org/pdf/2411.03700v1.pdf","comment":"Accepted to 2024 Neurips Queer in AI Workshop"},{"id":"http://arxiv.org/abs/2405.01345v3","updated":"2024-11-06T06:28:45Z","published":"2024-05-02T14:49:50Z","title":"The Power of Question Translation Training in Multilingual Reasoning:\n  Broadened Scope and Deepened Insights","summary":"  Bridging the significant gap between large language model's English and\nnon-English performance presents a great challenge. While some previous studies\nattempt to mitigate this gap with translated training data, the recently\nproposed question alignment framework leverages the model's English expertise\nto improve multilingual performance with minimum usage of expensive,\nerror-prone translation. In this paper, we explore how broadly this method can\nbe applied by examining its effects in reasoning with and without\nchain-of-thought, as well as with program-of-thought. We also explore applying\nthis framework to extremely large language models in an efficient manner, such\nas through proxy-tuning. Experiment results on multilingual reasoning\nbenchmarks mGSM, mSVAMP, xCSQA and xNLI demonstrate that we can extend question\nalignment framework to boost multilingual performance across diverse reasoning\nscenarios, model families, and sizes. For instance, when applied to the LLaMA2\nmodels, it brings an average accuracy improvements of 12.2% on mGSM even with\nthe 70B model. To understand the mechanism of its success, we analyze\nrepresentation space, generated response and data scales, and reveal how\nquestion translation training strengthens language alignment within LLMs and\nshapes their working patterns.\n","authors":["Wenhao Zhu","Shujian Huang","Fei Yuan","Cheng Chen","Jiajun Chen","Alexandra Birch"],"pdf_url":"https://arxiv.org/pdf/2405.01345v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17812v2","updated":"2024-11-06T05:33:16Z","published":"2024-02-27T14:51:11Z","title":"DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping\n  Backward Propagation","summary":"  Large language models (LLMs) have achieved significant success across various\ndomains. However, training these LLMs typically involves substantial memory and\ncomputational costs during both forward and backward propagation. While\nparameter-efficient fine-tuning (PEFT) considerably reduces the training memory\nassociated with parameters, it does not address the significant computational\ncosts and activation memory. In this paper, we propose Dropping Backward\nPropagation (DropBP), a novel approach designed to reduce computational costs\nand activation memory while maintaining accuracy. DropBP randomly drops layers\nduring backward propagation, which is essentially equivalent to training\nshallow submodules generated by undropped layers and residual connections.\nAdditionally, DropBP calculates the sensitivity of each layer to assign an\nappropriate drop rate, thereby stabilizing the training process. DropBP is not\nonly applicable to full fine-tuning but can also be orthogonally integrated\nwith all types of PEFT by dropping layers during backward propagation.\nSpecifically, DropBP can reduce training time by 44% with comparable accuracy\nto the baseline, accelerate convergence to the same perplexity by 1.5x, and\nenable training with a sequence length 6.2x larger on a single NVIDIA-A100 GPU.\nFurthermore, our DropBP enabled a throughput increase of 79% on a NVIDIA A100\nGPU and 117% on an Intel Gaudi2 HPU. The code is available at\nhttps://github.com/WooSunghyeon/dropbp.\n","authors":["Sunghyeon Woo","Baeseong Park","Byeongwook Kim","Minjung Jo","Se Jung Kwon","Dongsuk Jeon","Dongsoo Lee"],"pdf_url":"https://arxiv.org/pdf/2402.17812v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03675v1","updated":"2024-11-06T05:24:09Z","published":"2024-11-06T05:24:09Z","title":"QUILL: Quotation Generation Enhancement of Large Language Models","summary":"  While Large language models (LLMs) have become excellent writing assistants,\nthey still struggle with quotation generation. This is because they either\nhallucinate when providing factual quotations or fail to provide quotes that\nexceed human expectations. To bridge the gap, we systematically study how to\nevaluate and improve LLMs' performance in quotation generation tasks. We first\nestablish a holistic and automatic evaluation system for quotation generation\ntask, which consists of five criteria each with corresponding automatic metric.\nTo improve the LLMs' quotation generation abilities, we construct a bilingual\nknowledge base that is broad in scope and rich in dimensions, containing up to\n32,022 quotes. Moreover, guided by our critiria, we further design a\nquotation-specific metric to rerank the retrieved quotations from the knowledge\nbase. Extensive experiments show that our metrics strongly correlate with human\npreferences. Existing LLMs struggle to generate desired quotes, but our\nquotation knowledge base and reranking metric help narrow this gap. Our dataset\nand code are publicly available at https://github.com/GraceXiaoo/QUILL.\n","authors":["Jin Xiao","Bowei Zhang","Qianyu He","Jiaqing Liang","Feng Wei","Jinglei Chen","Zujie Liang","Deqing Yang","Yanghua Xiao"],"pdf_url":"https://arxiv.org/pdf/2411.03675v1.pdf","comment":"17 pages, 6 figures"},{"id":"http://arxiv.org/abs/2411.03665v1","updated":"2024-11-06T04:52:38Z","published":"2024-11-06T04:52:38Z","title":"Evaluating Moral Beliefs across LLMs through a Pluralistic Framework","summary":"  Proper moral beliefs are fundamental for language models, yet assessing these\nbeliefs poses a significant challenge. This study introduces a novel\nthree-module framework to evaluate the moral beliefs of four prominent large\nlanguage models. Initially, we constructed a dataset containing 472 moral\nchoice scenarios in Chinese, derived from moral words. The decision-making\nprocess of the models in these scenarios reveals their moral principle\npreferences. By ranking these moral choices, we discern the varying moral\nbeliefs held by different language models. Additionally, through moral debates,\nwe investigate the firmness of these models to their moral choices. Our\nfindings indicate that English language models, namely ChatGPT and Gemini,\nclosely mirror moral decisions of the sample of Chinese university students,\ndemonstrating strong adherence to their choices and a preference for\nindividualistic moral beliefs. In contrast, Chinese models such as Ernie and\nChatGLM lean towards collectivist moral beliefs, exhibiting ambiguity in their\nmoral choices and debates. This study also uncovers gender bias embedded within\nthe moral beliefs of all examined language models. Our methodology offers an\ninnovative means to assess moral beliefs in both artificial and human\nintelligence, facilitating a comparison of moral values across different\ncultures.\n","authors":["Xuelin Liu","Yanfei Zhu","Shucheng Zhu","Pengyuan Liu","Ying Liu","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2411.03665v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09324v3","updated":"2024-11-06T04:43:57Z","published":"2024-06-13T17:01:40Z","title":"Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs","summary":"  Although Large Language Models (LLMs) have demonstrated significant\ncapabilities in executing complex tasks in a zero-shot manner, they are\nsusceptible to jailbreak attacks and can be manipulated to produce harmful\noutputs. Recently, a growing body of research has categorized jailbreak attacks\ninto token-level and prompt-level attacks. However, previous work primarily\noverlooks the diverse key factors of jailbreak attacks, with most studies\nconcentrating on LLM vulnerabilities and lacking exploration of\ndefense-enhanced LLMs. To address these issues, we introduced\n$\\textbf{JailTrickBench}$ to evaluate the impact of various attack settings on\nLLM performance and provide a baseline for jailbreak attacks, encouraging the\nadoption of a standardized evaluation framework. Specifically, we evaluate the\neight key factors of implementing jailbreak attacks on LLMs from both\ntarget-level and attack-level perspectives. We further conduct seven\nrepresentative jailbreak attacks on six defense methods across two widely used\ndatasets, encompassing approximately 354 experiments with about 55,000 GPU\nhours on A800-80G. Our experimental results highlight the need for standardized\nbenchmarking to evaluate these attacks on defense-enhanced LLMs. Our code is\navailable at https://github.com/usail-hkust/JailTrickBench.\n","authors":["Zhao Xu","Fan Liu","Hao Liu"],"pdf_url":"https://arxiv.org/pdf/2406.09324v3.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2401.11505v2","updated":"2024-11-06T04:11:14Z","published":"2024-01-21T14:30:20Z","title":"CheX-GPT: Harnessing Large Language Models for Enhanced Chest X-ray\n  Report Labeling","summary":"  Free-text radiology reports present a rich data source for various medical\ntasks, but effectively labeling these texts remains challenging. Traditional\nrule-based labeling methods fall short of capturing the nuances of diverse\nfree-text patterns. Moreover, models using expert-annotated data are limited by\ndata scarcity and pre-defined classes, impacting their performance, flexibility\nand scalability. To address these issues, our study offers three main\ncontributions: 1) We demonstrate the potential of GPT as an adept labeler using\ncarefully designed prompts. 2) Utilizing only the data labeled by GPT, we\ntrained a BERT-based labeler, CheX-GPT, which operates faster and more\nefficiently than its GPT counterpart. 3) To benchmark labeler performance, we\nintroduced a publicly available expert-annotated test set, MIMIC-500,\ncomprising 500 cases from the MIMIC validation set. Our findings demonstrate\nthat CheX-GPT not only excels in labeling accuracy over existing models, but\nalso showcases superior efficiency, flexibility, and scalability, supported by\nour introduction of the MIMIC-500 dataset for robust benchmarking. Code and\nmodels are available at https://github.com/Soombit-ai/CheXGPT.\n","authors":["Jawook Gu","Kihyun You","Han-Cheol Cho","Jiho Kim","Eun Kyoung Hong","Byungseok Roh"],"pdf_url":"https://arxiv.org/pdf/2401.11505v2.pdf","comment":"16 pages, 3 figures"},{"id":"http://arxiv.org/abs/2411.03644v1","updated":"2024-11-06T03:48:41Z","published":"2024-11-06T03:48:41Z","title":"Deploying Multi-task Online Server with Large Language Model","summary":"  In the industry, numerous tasks are deployed online. Traditional approaches\noften tackle each task separately by its own network, which leads to excessive\ncosts for developing and scaling models, especially in the context of large\nlanguage models. Although multi-task methods can save costs through parameter\nsharing, they often struggle to outperform single-task methods in real-world\napplications. To tackle these challenges, we present a three-stage multi-task\nlearning framework for large language models. It involves task filtering,\nfollowed by fine-tuning on high-resource tasks, and finally fine-tuning on all\ntasks. We conducted comprehensive experiments in single-task and multi-task\nsettings. Our approach, exemplified on different benchmarks, demonstrates that\nit is able to achieve performance comparable to the single-task method while\nreducing up to 90.9\\% of its overhead.\n","authors":["Yincen Qu","Chao Ma","Yiting Wu","Xiangying Dai","Hui Zhou","Hengyue Liu"],"pdf_url":"https://arxiv.org/pdf/2411.03644v1.pdf","comment":"COLING2025 under submission"},{"id":"http://arxiv.org/abs/2411.02674v2","updated":"2024-11-06T03:11:35Z","published":"2024-11-04T23:21:12Z","title":"Wave Network: An Ultra-Small Language Model","summary":"  We propose an innovative token representation and update method in a new\nultra-small language model: the Wave network. Specifically, we use a complex\nvector to represent each token, encoding both global and local semantics of the\ninput text. A complex vector consists of two components: a magnitude vector\nrepresenting the global semantics of the input text, and a phase vector\ncapturing the relationships between individual tokens and global semantics.\nExperiments on the AG News text classification task demonstrate that, when\ngenerating complex vectors from randomly initialized token embeddings, our\nsingle-layer Wave Network achieves 90.91% accuracy with wave interference and\n91.66% with wave modulation - outperforming a single Transformer layer using\nBERT pre-trained embeddings by 19.23% and 19.98%, respectively, and approaching\nthe accuracy of the pre-trained and fine-tuned BERT base model (94.64%).\nAdditionally, compared to BERT base, the Wave Network reduces video memory\nusage and training time by 77.34% and 85.62% during wave modulation. In\nsummary, we used a 2.4-million-parameter small language model to achieve\naccuracy comparable to a 100-million-parameter BERT model in text\nclassification.\n","authors":["Xin Zhang","Victor S. Sheng"],"pdf_url":"https://arxiv.org/pdf/2411.02674v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.01574v6","updated":"2024-11-06T02:54:00Z","published":"2024-06-03T17:53:00Z","title":"MMLU-Pro: A More Robust and Challenging Multi-Task Language\n  Understanding Benchmark","summary":"  In the age of large-scale language models, benchmarks like the Massive\nMultitask Language Understanding (MMLU) have been pivotal in pushing the\nboundaries of what AI can achieve in language comprehension and reasoning\nacross diverse domains. However, as models continue to improve, their\nperformance on these benchmarks has begun to plateau, making it increasingly\ndifficult to discern differences in model capabilities. This paper introduces\nMMLU-Pro, an enhanced dataset designed to extend the mostly knowledge-driven\nMMLU benchmark by integrating more challenging, reasoning-focused questions and\nexpanding the choice set from four to ten options. Additionally, MMLU-Pro\neliminates the trivial and noisy questions in MMLU. Our experimental results\nshow that MMLU-Pro not only raises the challenge, causing a significant drop in\naccuracy by 16% to 33% compared to MMLU but also demonstrates greater stability\nunder varying prompts. With 24 different prompt styles tested, the sensitivity\nof model scores to prompt variations decreased from 4-5% in MMLU to just 2% in\nMMLU-Pro. Additionally, we found that models utilizing Chain of Thought (CoT)\nreasoning achieved better performance on MMLU-Pro compared to direct answering,\nwhich is in stark contrast to the findings on the original MMLU, indicating\nthat MMLU-Pro includes more complex reasoning questions. Our assessments\nconfirm that MMLU-Pro is a more discriminative benchmark to better track\nprogress in the field.\n","authors":["Yubo Wang","Xueguang Ma","Ge Zhang","Yuansheng Ni","Abhranil Chandra","Shiguang Guo","Weiming Ren","Aaran Arulraj","Xuan He","Ziyan Jiang","Tianle Li","Max Ku","Kai Wang","Alex Zhuang","Rongqi Fan","Xiang Yue","Wenhu Chen"],"pdf_url":"https://arxiv.org/pdf/2406.01574v6.pdf","comment":"This version has been accepted and published at NeurIPS 2024 Track\n  Datasets and Benchmarks (Spotlight)"},{"id":"http://arxiv.org/abs/2409.19272v2","updated":"2024-11-06T01:58:20Z","published":"2024-09-28T07:13:33Z","title":"Perception Compressor:A training-free prompt compression method in long\n  context scenarios","summary":"  Large Language Models (LLMs) demonstrate exceptional capabilities in various\nscenarios. However, they suffer from much redundant information and are\nsensitive to the position of key information (relevant to the input question)\nin long context scenarios, leading to inferior performance. To address these\nchallenges, we present Perception Compressor, a training-free prompt\ncompression method. It includes a perception retriever that leverages guiding\nquestions and instruction to retrieve the most relevant demonstrations, a\ndual-slope ratio allocator to dynamically allocate compression ratios and\nopen-book ratios, and a semi-guided iterative compression that retains key\ninformation at the token level while removing tokens that distract the LLM. We\nconduct extensive experiments on long context benchmarks, i.e.,\nNaturalQuestions, LongBench, and MuSiQue. Experiment results show that\nPerception Compressor outperforms existing methods by a large margin, achieving\nstate-of-the-art performance.\n","authors":["Jiwei Tang","Jin Xu","Tingwei Lu","Zhicheng Zhang","Yiming Zhao","Lin Hai","Hai-Tao Zheng"],"pdf_url":"https://arxiv.org/pdf/2409.19272v2.pdf","comment":"15 pages, 5 figures"},{"id":"http://arxiv.org/abs/2406.16020v4","updated":"2024-11-06T01:49:36Z","published":"2024-06-23T05:40:26Z","title":"AudioBench: A Universal Benchmark for Audio Large Language Models","summary":"  We introduce AudioBench, a universal benchmark designed to evaluate Audio\nLarge Language Models (AudioLLMs). It encompasses 8 distinct tasks and 26\ndatasets, among which, 7 are newly proposed datasets. The evaluation targets\nthree main aspects: speech understanding, audio scene understanding, and voice\nunderstanding (paralinguistic). Despite recent advancements, there lacks a\ncomprehensive benchmark for AudioLLMs on instruction following capabilities\nconditioned on audio signals. AudioBench addresses this gap by setting up\ndatasets as well as desired evaluation metrics. Besides, we also evaluated the\ncapabilities of five popular models and found that no single model excels\nconsistently across all tasks. We outline the research outlook for AudioLLMs\nand anticipate that our open-sourced evaluation toolkit, data, and leaderboard\nwill offer a robust testbed for future model developments.\n","authors":["Bin Wang","Xunlong Zou","Geyu Lin","Shuo Sun","Zhuohan Liu","Wenyu Zhang","Zhengyuan Liu","AiTi Aw","Nancy F. Chen"],"pdf_url":"https://arxiv.org/pdf/2406.16020v4.pdf","comment":"v4 - Add acknowledgment and slight update on structure; Code:\n  https://github.com/AudioLLMs/AudioBench"},{"id":"http://arxiv.org/abs/2411.01222v2","updated":"2024-11-06T01:40:27Z","published":"2024-11-02T12:01:44Z","title":"$B^4$: A Black-Box Scrubbing Attack on LLM Watermarks","summary":"  Watermarking has emerged as a prominent technique for LLM-generated content\ndetection by embedding imperceptible patterns. Despite supreme performance, its\nrobustness against adversarial attacks remains underexplored. Previous work\ntypically considers a grey-box attack setting, where the specific type of\nwatermark is already known. Some even necessitates knowledge about\nhyperparameters of the watermarking method. Such prerequisites are unattainable\nin real-world scenarios. Targeting at a more realistic black-box threat model\nwith fewer assumptions, we here propose $\\mathcal{B}^4$, a black-box scrubbing\nattack on watermarks. Specifically, we formulate the watermark scrubbing attack\nas a constrained optimization problem by capturing its objectives with two\ndistributions, a Watermark Distribution and a Fidelity Distribution. This\noptimization problem can be approximately solved using two proxy distributions.\nExperimental results across 12 different settings demonstrate the superior\nperformance of $\\mathcal{B}^4$ compared with other baselines.\n","authors":["Baizhou Huang","Xiao Pu","Xiaojun Wan"],"pdf_url":"https://arxiv.org/pdf/2411.01222v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23956v2","updated":"2024-11-06T01:35:36Z","published":"2024-10-31T14:09:50Z","title":"Multilingual Pretraining Using a Large Corpus Machine-Translated from a\n  Single Source Language","summary":"  English, as a very high-resource language, enables the pretraining of\nhigh-quality large language models (LLMs). The same cannot be said for most\nother languages, as leading LLMs still underperform for non-English languages,\nlikely due to a gap in the quality and diversity of the available multilingual\npretraining corpora. In this work, we find that machine-translated text from a\nsingle high-quality source language can contribute significantly to the\npretraining of multilingual LLMs. We translate FineWeb-Edu, a high-quality\nEnglish web dataset, into French, German, and Spanish, resulting in a final\n300B-token dataset, which we call TransWeb-Edu, and pretrain a 1.3B-parameter\nmodel, CuatroLLM, from scratch on this dataset. Across five non-English\nreasoning tasks, we show that CuatroLLM matches or outperforms state-of-the-art\nmultilingual models trained using closed data, such as Llama3.2 and Gemma2,\ndespite using an order of magnitude less data, such as about 6% of the tokens\nused for Llama3.2's training. We further demonstrate that with additional\ndomain-specific pretraining, amounting to less than 1% of TransWeb-Edu,\nCuatroLLM surpasses the state of the art in multilingual reasoning. To promote\nreproducibility, we release our corpus, models, and training pipeline under\nopen licenses at hf.co/britllm/CuatroLLM.\n","authors":["Jiayi Wang","Yao Lu","Maurice Weber","Max Ryabinin","Yihong Chen","Raphael Tang","Pontus Stenetorp"],"pdf_url":"https://arxiv.org/pdf/2410.23956v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.19687v3","updated":"2024-11-06T01:20:07Z","published":"2024-07-29T04:10:13Z","title":"Efficiently and Effectively: A Two-stage Approach to Balance Plaintext\n  and Encrypted Text for Traffic Classification","summary":"  Encrypted traffic classification is the task of identifying the application\nor service associated with encrypted network traffic. One effective approach\nfor this task is to use deep learning methods to encode the raw traffic bytes\ndirectly and automatically extract features for classification (byte-based\nmodels). However, current byte-based models input raw traffic bytes, whether\nplaintext or encrypted text, for automated feature extraction, neglecting the\ndistinct impacts of plaintext and encrypted text on downstream tasks.\nAdditionally, these models primarily focus on improving classification\naccuracy, with little emphasis on the efficiency of models. In this paper, for\nthe first time, we analyze the impact of plaintext and encrypted text on the\nmodel's effectiveness and efficiency. Based on our observations and findings,\nwe propose a two-phase approach to balance the trade-off between plaintext and\nencrypted text in traffic classification. Specifically, Stage one is to\nDetermine whether the Plain text is enough to be accurately Classified (DPC)\nusing the proposed DPC Selector. This stage quickly identifies samples that can\nbe classified using plaintext, leveraging explicit byte features in plaintext\nto enhance model's efficiency. Stage two aims to adaptively make a\nclassification with the result from stage one. This stage incorporates\nencrypted text information for samples that cannot be classified using\nplaintext alone, ensuring the model's effectiveness on traffic classification\ntasks. Experiments on two datasets demonstrate that our proposed model achieves\nstate-of-the-art results in both effectiveness and efficiency.\n","authors":["Wei Peng","Lei Cui","Wei Cai","Zhenquan Ding","Zhiyu Hao","Xiaochun Yun"],"pdf_url":"https://arxiv.org/pdf/2407.19687v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03590v1","updated":"2024-11-06T01:09:17Z","published":"2024-11-06T01:09:17Z","title":"From Medprompt to o1: Exploration of Run-Time Strategies for Medical\n  Challenge Problems and Beyond","summary":"  Run-time steering strategies like Medprompt are valuable for guiding large\nlanguage models (LLMs) to top performance on challenging tasks. Medprompt\ndemonstrates that a general LLM can be focused to deliver state-of-the-art\nperformance on specialized domains like medicine by using a prompt to elicit a\nrun-time strategy involving chain of thought reasoning and ensembling. OpenAI's\no1-preview model represents a new paradigm, where a model is designed to do\nrun-time reasoning before generating final responses. We seek to understand the\nbehavior of o1-preview on a diverse set of medical challenge problem\nbenchmarks. Following on the Medprompt study with GPT-4, we systematically\nevaluate the o1-preview model across various medical benchmarks. Notably, even\nwithout prompting techniques, o1-preview largely outperforms the GPT-4 series\nwith Medprompt. We further systematically study the efficacy of classic prompt\nengineering strategies, as represented by Medprompt, within the new paradigm of\nreasoning models. We found that few-shot prompting hinders o1's performance,\nsuggesting that in-context learning may no longer be an effective steering\napproach for reasoning-native models. While ensembling remains viable, it is\nresource-intensive and requires careful cost-performance optimization. Our cost\nand accuracy analysis across run-time strategies reveals a Pareto frontier,\nwith GPT-4o representing a more affordable option and o1-preview achieving\nstate-of-the-art performance at higher cost. Although o1-preview offers top\nperformance, GPT-4o with steering strategies like Medprompt retains value in\nspecific contexts. Moreover, we note that the o1-preview model has reached\nnear-saturation on many existing medical benchmarks, underscoring the need for\nnew, challenging benchmarks. We close with reflections on general directions\nfor inference-time computation with LLMs.\n","authors":["Harsha Nori","Naoto Usuyama","Nicholas King","Scott Mayer McKinney","Xavier Fernandes","Sheng Zhang","Eric Horvitz"],"pdf_url":"https://arxiv.org/pdf/2411.03590v1.pdf","comment":"25 pages"},{"id":"http://arxiv.org/abs/2410.20763v2","updated":"2024-11-06T00:20:32Z","published":"2024-10-28T05:56:51Z","title":"Evaluating LLMs for Targeted Concept Simplification for Domain-Specific\n  Texts","summary":"  One useful application of NLP models is to support people in reading complex\ntext from unfamiliar domains (e.g., scientific articles). Simplifying the\nentire text makes it understandable but sometimes removes important details. On\nthe contrary, helping adult readers understand difficult concepts in context\ncan enhance their vocabulary and knowledge. In a preliminary human study, we\nfirst identify that lack of context and unfamiliarity with difficult concepts\nis a major reason for adult readers' difficulty with domain-specific text. We\nthen introduce \"targeted concept simplification,\" a simplification task for\nrewriting text to help readers comprehend text containing unfamiliar concepts.\nWe also introduce WikiDomains, a new dataset of 22k definitions from 13\nacademic domains paired with a difficult concept within each definition. We\nbenchmark the performance of open-source and commercial LLMs and a simple\ndictionary baseline on this task across human judgments of ease of\nunderstanding and meaning preservation. Interestingly, our human judges\npreferred explanations about the difficult concept more than simplification of\nthe concept phrase. Further, no single model achieved superior performance\nacross all quality dimensions, and automated metrics also show low correlations\nwith human evaluations of concept simplification ($\\sim0.2$), opening up rich\navenues for research on personalized human reading comprehension support.\n","authors":["Sumit Asthana","Hannah Rashkin","Elizabeth Clark","Fantine Huot","Mirella Lapata"],"pdf_url":"https://arxiv.org/pdf/2410.20763v2.pdf","comment":"to appear in proceedings of EMNLP 2024"},{"id":"http://arxiv.org/abs/2411.03568v1","updated":"2024-11-06T00:16:16Z","published":"2024-11-06T00:16:16Z","title":"The American Sign Language Knowledge Graph: Infusing ASL Models with\n  Linguistic Knowledge","summary":"  Language models for American Sign Language (ASL) could make language\ntechnologies substantially more accessible to those who sign. To train models\non tasks such as isolated sign recognition (ISR) and ASL-to-English\ntranslation, datasets provide annotated video examples of ASL signs. To\nfacilitate the generalizability and explainability of these models, we\nintroduce the American Sign Language Knowledge Graph (ASLKG), compiled from\ntwelve sources of expert linguistic knowledge. We use the ASLKG to train\nneuro-symbolic models for 3 ASL understanding tasks, achieving accuracies of\n91% on ISR, 14% for predicting the semantic features of unseen signs, and 36%\nfor classifying the topic of Youtube-ASL videos.\n","authors":["Lee Kezar","Nidhi Munikote","Zian Zeng","Zed Sehyr","Naomi Caselli","Jesse Thomason"],"pdf_url":"https://arxiv.org/pdf/2411.03568v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21169v3","updated":"2024-11-06T00:11:08Z","published":"2024-10-28T16:11:35Z","title":"Document Parsing Unveiled: Techniques, Challenges, and Prospects for\n  Structured Information Extraction","summary":"  Document parsing is essential for converting unstructured and semi-structured\ndocuments-such as contracts, academic papers, and invoices-into structured,\nmachine-readable data. Document parsing extract reliable structured data from\nunstructured inputs, providing huge convenience for numerous applications.\nEspecially with recent achievements in Large Language Models, document parsing\nplays an indispensable role in both knowledge base construction and training\ndata generation. This survey presents a comprehensive review of the current\nstate of document parsing, covering key methodologies, from modular pipeline\nsystems to end-to-end models driven by large vision-language models. Core\ncomponents such as layout detection, content extraction (including text,\ntables, and mathematical expressions), and multi-modal data integration are\nexamined in detail. Additionally, this paper discusses the challenges faced by\nmodular document parsing systems and vision-language models in handling complex\nlayouts, integrating multiple modules, and recognizing high-density text. It\nemphasizes the importance of developing larger and more diverse datasets and\noutlines future research directions.\n","authors":["Qintong Zhang","Victor Shea-Jay Huang","Bin Wang","Junyuan Zhang","Zhengren Wang","Hao Liang","Shawn Wang","Matthieu Lin","Conghui He","Wentao Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.21169v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04316v1","updated":"2024-11-06T23:41:18Z","published":"2024-11-06T23:41:18Z","title":"A Multilingual Sentiment Lexicon for Low-Resource Language Translation\n  using Large Languages Models and Explainable AI","summary":"  South Africa and the Democratic Republic of Congo (DRC) present a complex\nlinguistic landscape with languages such as Zulu, Sepedi, Afrikaans, French,\nEnglish, and Tshiluba (Ciluba), which creates unique challenges for AI-driven\ntranslation and sentiment analysis systems due to a lack of accurately labeled\ndata. This study seeks to address these challenges by developing a multilingual\nlexicon designed for French and Tshiluba, now expanded to include translations\nin English, Afrikaans, Sepedi, and Zulu. The lexicon enhances cultural\nrelevance in sentiment classification by integrating language-specific\nsentiment scores. A comprehensive testing corpus is created to support\ntranslation and sentiment analysis tasks, with machine learning models such as\nRandom Forest, Support Vector Machine (SVM), Decision Trees, and Gaussian Naive\nBayes (GNB) trained to predict sentiment across low resource languages (LRLs).\nAmong them, the Random Forest model performed particularly well, capturing\nsentiment polarity and handling language-specific nuances effectively.\nFurthermore, Bidirectional Encoder Representations from Transformers (BERT), a\nLarge Language Model (LLM), is applied to predict context-based sentiment with\nhigh accuracy, achieving 99% accuracy and 98% precision, outperforming other\nmodels. The BERT predictions were clarified using Explainable AI (XAI),\nimproving transparency and fostering confidence in sentiment classification.\nOverall, findings demonstrate that the proposed lexicon and machine learning\nmodels significantly enhance translation and sentiment analysis for LRLs in\nSouth Africa and the DRC, laying a foundation for future AI models that support\nunderrepresented languages, with applications across education, governance, and\nbusiness in multilingual contexts.\n","authors":["Melusi Malinga","Isaac Lupanda","Mike Wa Nkongolo","Phil van Deventer"],"pdf_url":"https://arxiv.org/pdf/2411.04316v1.pdf","comment":"This work is part of a PhD proposal in Information Technology at the\n  University of Pretoria, supervised by Dr. Mike Wa Nkongolo and co-supervised\n  by Dr. Phil van Deventer, under the Low-Resource Language Processing Lab in\n  the Department of Informatics"},{"id":"http://arxiv.org/abs/2411.02316v2","updated":"2024-11-06T23:27:24Z","published":"2024-11-04T17:40:39Z","title":"Evaluating Creative Short Story Generation in Humans and Large Language\n  Models","summary":"  Storytelling is a fundamental aspect of human communication, relying heavily\non creativity to produce narratives that are novel, appropriate, and\nsurprising. While large language models (LLMs) have recently demonstrated the\nability to generate high-quality stories, their creative capabilities remain\nunderexplored. Previous research has either focused on creativity tests\nrequiring short responses or primarily compared model performance in story\ngeneration to that of professional writers. However, the question of whether\nLLMs exhibit creativity in writing short stories on par with the average human\nremains unanswered. In this work, we conduct a systematic analysis of\ncreativity in short story generation across LLMs and everyday people. Using a\nfive-sentence creative story task, commonly employed in psychology to assess\nhuman creativity, we automatically evaluate model- and human-generated stories\nacross several dimensions of creativity, including novelty, surprise, and\ndiversity. Our findings reveal that while LLMs can generate stylistically\ncomplex stories, they tend to fall short in terms of creativity when compared\nto average human writers.\n","authors":["Mete Ismayilzada","Claire Stevenson","Lonneke van der Plas"],"pdf_url":"https://arxiv.org/pdf/2411.02316v2.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2411.04308v1","updated":"2024-11-06T23:16:25Z","published":"2024-11-06T23:16:25Z","title":"Improving Bilingual Capabilities of Language Models to Support Diverse\n  Linguistic Practices in Education","summary":"  Large language models (LLMs) offer promise in generating educational content,\nproviding instructor feedback, and reducing teacher workload on assessments.\nWhile prior studies have focused on studying LLM-powered learning analytics,\nlimited research has examined how effective LLMs are in a bilingual context. In\nthis paper, we study the effectiveness of multilingual large language models\n(MLLMs) across monolingual (English-only, Spanish-only) and bilingual\n(Spanglish) student writing. We present a learning analytics use case that\ndetails LLM performance in assessing acceptable and unacceptable explanations\nof Science and Social Science concepts. Our findings reveal a significant bias\nin the grading performance of pre-trained models for bilingual writing compared\nto English-only and Spanish-only writing. Following this, we fine-tune\nopen-source MLLMs including Llama 3.1 and Mistral NeMo using synthetic datasets\ngenerated in English, Spanish, and Spanglish. Our experiments indicate that the\nmodels perform significantly better for all three languages after fine-tuning\nwith bilingual data. This study highlights the potential of enhancing MLLM\neffectiveness to support authentic language practices amongst bilingual\nlearners. It also aims to illustrate the value of incorporating non-English\nlanguages into the design and implementation of language models in education.\n","authors":["Anand Syamkumar","Nora Tseng","Kaycie Barron","Shanglin Yang","Shamya Karumbaiah","Rheeya Uppal","Junjie Hu"],"pdf_url":"https://arxiv.org/pdf/2411.04308v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04298v1","updated":"2024-11-06T22:46:13Z","published":"2024-11-06T22:46:13Z","title":"A Capabilities Approach to Studying Bias and Harm in Language\n  Technologies","summary":"  Mainstream Natural Language Processing (NLP) research has ignored the\nmajority of the world's languages. In moving from excluding the majority of the\nworld's languages to blindly adopting what we make for English, we first risk\nimporting the same harms we have at best mitigated and at least measured for\nEnglish. However, in evaluating and mitigating harms arising from adopting new\ntechnologies into such contexts, we often disregard (1) the actual community\nneeds of Language Technologies, and (2) biases and fairness issues within the\ncontext of the communities. In this extended abstract, we consider fairness,\nbias, and inclusion in Language Technologies through the lens of the\nCapabilities Approach. The Capabilities Approach centers on what people are\ncapable of achieving, given their intersectional social, political, and\neconomic contexts instead of what resources are (theoretically) available to\nthem. We detail the Capabilities Approach, its relationship to multilingual and\nmulticultural evaluation, and how the framework affords meaningful\ncollaboration with community members in defining and measuring the harms of\nLanguage Technologies.\n","authors":["Hellina Hailu Nigatu","Zeerak Talat"],"pdf_url":"https://arxiv.org/pdf/2411.04298v1.pdf","comment":"Accepted to the New Perspectives on Bias and Discrimination in\n  Language Technology workshop"},{"id":"http://arxiv.org/abs/2411.00369v2","updated":"2024-11-06T22:41:31Z","published":"2024-11-01T05:14:03Z","title":"GRSQA -- Graph Reasoning-Structured Question Answering Dataset","summary":"  Large Language Models (LLMs) have excelled in multi-hop question-answering\n(M-QA) due to their advanced reasoning abilities. However, the impact of the\ninherent reasoning structures on LLM M-QA performance remains unclear, largely\ndue to the absence of QA datasets that provide fine-grained reasoning\nstructures. To address this gap, we introduce the Graph Reasoning-Structured\nQuestion Answering Dataset (GRS-QA), which includes both semantic contexts and\nreasoning structures for QA pairs. Unlike existing M-QA datasets, where\ndifferent reasoning structures are entangled together, GRS-QA explicitly\ncaptures intricate reasoning pathways by constructing reasoning graphs, where\nnodes represent textual contexts and edges denote logical flows. These\nreasoning graphs of different structures enable a fine-grained evaluation of\nLLM reasoning capabilities across various reasoning structures. Our empirical\nanalysis reveals that LLMs perform differently when handling questions with\nvarying reasoning structures. This finding facilitates the exploration of\ntextual structures as compared with semantics.\n","authors":["Anish Pahilajani","Devasha Trivedi","Jincen Shuai","Khin S. Yone","Samyak Rajesh Jain","Namyong Park","Ryan A. Rossi","Nesreen K. Ahmed","Franck Dernoncourt","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2411.00369v2.pdf","comment":"15 pages, 24 figures, 10 tables"},{"id":"http://arxiv.org/abs/2406.11944v2","updated":"2024-11-06T22:37:30Z","published":"2024-06-17T17:49:00Z","title":"Transcoders Find Interpretable LLM Feature Circuits","summary":"  A key goal in mechanistic interpretability is circuit analysis: finding\nsparse subgraphs of models corresponding to specific behaviors or capabilities.\nHowever, MLP sublayers make fine-grained circuit analysis on transformer-based\nlanguage models difficult. In particular, interpretable features -- such as\nthose found by sparse autoencoders (SAEs) -- are typically linear combinations\nof extremely many neurons, each with its own nonlinearity to account for.\nCircuit analysis in this setting thus either yields intractably large circuits\nor fails to disentangle local and global behavior. To address this we explore\ntranscoders, which seek to faithfully approximate a densely activating MLP\nlayer with a wider, sparsely-activating MLP layer. We introduce a novel method\nfor using transcoders to perform weights-based circuit analysis through MLP\nsublayers. The resulting circuits neatly factorize into input-dependent and\ninput-invariant terms. We then successfully train transcoders on language\nmodels with 120M, 410M, and 1.4B parameters, and find them to perform at least\non par with SAEs in terms of sparsity, faithfulness, and\nhuman-interpretability. Finally, we apply transcoders to reverse-engineer\nunknown circuits in the model, and we obtain novel insights regarding the\n\"greater-than circuit\" in GPT2-small. Our results suggest that transcoders can\nprove effective in decomposing model computations involving MLPs into\ninterpretable circuits. Code is available at\nhttps://github.com/jacobdunefsky/transcoder_circuits/.\n","authors":["Jacob Dunefsky","Philippe Chlenski","Neel Nanda"],"pdf_url":"https://arxiv.org/pdf/2406.11944v2.pdf","comment":"29 pages, 6 figures, 4 tables, 2 algorithms. NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.04291v1","updated":"2024-11-06T22:19:32Z","published":"2024-11-06T22:19:32Z","title":"Unfair Alignment: Examining Safety Alignment Across Vision Encoder\n  Layers in Vision-Language Models","summary":"  Vision-language models (VLMs) have improved significantly in multi-modal\ntasks, but their more complex architecture makes their safety alignment more\nchallenging than the alignment of large language models (LLMs). In this paper,\nwe reveal an unfair distribution of safety across the layers of VLM's vision\nencoder, with earlier and middle layers being disproportionately vulnerable to\nmalicious inputs compared to the more robust final layers. This 'cross-layer'\nvulnerability stems from the model's inability to generalize its safety\ntraining from the default architectural settings used during training to unseen\nor out-of-distribution scenarios, leaving certain layers exposed. We conduct a\ncomprehensive analysis by projecting activations from various intermediate\nlayers and demonstrate that these layers are more likely to generate harmful\noutputs when exposed to malicious inputs. Our experiments with LLaVA-1.5 and\nLlama 3.2 show discrepancies in attack success rates and toxicity scores across\nlayers, indicating that current safety alignment strategies focused on a single\ndefault layer are insufficient.\n","authors":["Saketh Bachu","Erfan Shayegani","Trishna Chakraborty","Rohit Lal","Arindam Dutta","Chengyu Song","Yue Dong","Nael Abu-Ghazaleh","Amit K. Roy-Chowdhury"],"pdf_url":"https://arxiv.org/pdf/2411.04291v1.pdf","comment":"Preprint, Under Review"},{"id":"http://arxiv.org/abs/2406.15708v2","updated":"2024-11-06T22:07:17Z","published":"2024-06-22T02:07:10Z","title":"Teach Better or Show Smarter? On Instructions and Exemplars in Automatic\n  Prompt Optimization","summary":"  Large language models have demonstrated remarkable capabilities, but their\nperformance is heavily reliant on effective prompt engineering. Automatic\nprompt optimization (APO) methods are designed to automate this and can be\nbroadly categorized into those targeting instructions (instruction\noptimization, IO) vs. those targeting exemplars (exemplar optimization, EO).\nDespite their shared objective, these have evolved rather independently, with\nIO receiving more research attention recently. This paper seeks to bridge this\ngap by comprehensively comparing the performance of representative IO and EO\ntechniques both isolation and combination on a diverse set of challenging\ntasks. Our findings reveal that intelligently reusing model-generated\ninput-output pairs obtained from evaluating prompts on the validation set as\nexemplars, consistently improves performance on top of IO methods but is\ncurrently under-investigated. We also find that despite the recent focus on IO,\nhow we select exemplars can outweigh how we optimize instructions, with EO\nstrategies as simple as random search outperforming state-of-the-art IO methods\nwith seed instructions without any optimization. Moreover, we observe a synergy\nbetween EO and IO, with optimal combinations surpassing the individual\ncontributions. We conclude that studying exemplar optimization both as a\nstandalone method and its optimal combination with instruction optimization\nremain a crucial aspect of APO and deserve greater consideration in future\nresearch, even in the era of highly capable instruction-following models.\n","authors":["Xingchen Wan","Ruoxi Sun","Hootan Nakhost","Sercan O. Arik"],"pdf_url":"https://arxiv.org/pdf/2406.15708v2.pdf","comment":"Expanded version of the NeurIPS 2024 paper"},{"id":"http://arxiv.org/abs/2407.06004v3","updated":"2024-11-06T22:07:06Z","published":"2024-07-08T14:58:29Z","title":"Perceptions to Beliefs: Exploring Precursory Inferences for Theory of\n  Mind in Large Language Models","summary":"  While humans naturally develop theory of mind (ToM), the capability to\nunderstand other people's mental states and beliefs, state-of-the-art large\nlanguage models (LLMs) underperform on simple ToM benchmarks. We posit that we\ncan extend our understanding of LLMs' ToM abilities by evaluating key human ToM\nprecursors$-$perception inference and perception-to-belief inference$-$in LLMs.\nWe introduce two datasets, Percept-ToMi and Percept-FANToM, to evaluate these\nprecursory inferences for ToM in LLMs by annotating characters' perceptions on\nToMi and FANToM, respectively. Our evaluation of eight state-of-the-art LLMs\nreveals that the models generally perform well in perception inference while\nexhibiting limited capability in perception-to-belief inference (e.g., lack of\ninhibitory control). Based on these results, we present PercepToM, a novel ToM\nmethod leveraging LLMs' strong perception inference capability while\nsupplementing their limited perception-to-belief inference. Experimental\nresults demonstrate that PercepToM significantly enhances LLM's performance,\nespecially in false belief scenarios.\n","authors":["Chani Jung","Dongkwan Kim","Jiho Jin","Jiseon Kim","Yeon Seonwoo","Yejin Choi","Alice Oh","Hyunwoo Kim"],"pdf_url":"https://arxiv.org/pdf/2407.06004v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07330v2","updated":"2024-11-06T22:03:18Z","published":"2024-07-10T02:58:37Z","title":"Interpretable Differential Diagnosis with Dual-Inference Large Language\n  Models","summary":"  Automatic differential diagnosis (DDx) is an essential medical task that\ngenerates a list of potential diseases as differentials based on patient\nsymptom descriptions. In practice, interpreting these differential diagnoses\nyields significant value but remains under-explored. Given the powerful\ncapabilities of large language models (LLMs), we investigated using LLMs for\ninterpretable DDx. Specifically, we curated the first DDx dataset with\nexpert-derived interpretation on 570 clinical notes. Besides, we proposed\nDual-Inf, a novel framework that enabled LLMs to conduct bidirectional\ninference (i.e., from symptoms to diagnoses and vice versa) for DDx\ninterpretation. Both human and automated evaluation validated its efficacy in\npredicting and elucidating differentials across four base LLMs. In addition,\nDual-Inf could reduce interpretation errors and hold promise for rare disease\nexplanations. To the best of our knowledge, it is the first work that\ncustomizes LLMs for DDx explanation and comprehensively evaluates their\ninterpretation performance. Overall, our study bridges a critical gap in DDx\ninterpretation and enhances clinical decision-making.\n","authors":["Shuang Zhou","Mingquan Lin","Sirui Ding","Jiashuo Wang","Genevieve B. Melton","James Zou","Rui Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.07330v2.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2411.04282v1","updated":"2024-11-06T22:02:30Z","published":"2024-11-06T22:02:30Z","title":"Language Models are Hidden Reasoners: Unlocking Latent Reasoning\n  Capabilities via Self-Rewarding","summary":"  Large language models (LLMs) have shown impressive capabilities, but still\nstruggle with complex reasoning tasks requiring multiple steps. While\nprompt-based methods like Chain-of-Thought (CoT) can improve LLM reasoning at\ninference time, optimizing reasoning capabilities during training remains\nchallenging. We introduce LaTent Reasoning Optimization (LaTRO), a principled\nframework that formulates reasoning as sampling from a latent distribution and\noptimizes it via variational approaches. LaTRO enables LLMs to concurrently\nimprove both their reasoning process and ability to evaluate reasoning quality,\nwithout requiring external feedback or reward models. We validate LaTRO through\nexperiments on GSM8K and ARC-Challenge datasets using multiple model\narchitectures. On GSM8K, LaTRO improves zero-shot accuracy by an average of\n12.5% over base models and 9.6% over supervised fine-tuning across\nPhi-3.5-mini, Mistral-7B, and Llama-3.1-8B. Our findings suggest that\npre-trained LLMs possess latent reasoning capabilities that can be unlocked and\nenhanced through our proposed optimization approach in a self-improvement\nmanner. The code of LaTRO is available at\n\\url{https://github.com/SalesforceAIResearch/LaTRO}.\n","authors":["Haolin Chen","Yihao Feng","Zuxin Liu","Weiran Yao","Akshara Prabhakar","Shelby Heinecke","Ricky Ho","Phil Mui","Silvio Savarese","Caiming Xiong","Huan Wang"],"pdf_url":"https://arxiv.org/pdf/2411.04282v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.15405v3","updated":"2024-11-06T21:18:59Z","published":"2023-05-24T17:59:05Z","title":"Textless Speech-to-Speech Translation With Limited Parallel Data","summary":"  Existing speech-to-speech translation (S2ST) models fall into two camps: they\neither leverage text as an intermediate step or require hundreds of hours of\nparallel speech data. Both approaches are incompatible with textless languages\nor language pairs with limited parallel data. We present PFB, a framework for\ntraining textless S2ST models that require just dozens of hours of parallel\nspeech data. We first pretrain a model on large-scale monolingual speech data,\nfinetune it with a small amount of parallel speech data (20-60 hours), and\nlastly train with an unsupervised backtranslation objective. We train and\nevaluate our models for English-to-German, German-to-English and\nMarathi-to-English translation on three different domains (European Parliament,\nCommon Voice, and All India Radio) with single-speaker synthesized speech.\nEvaluated using the ASR-BLEU metric, our models achieve reasonable performance\non all three domains, with some being within 1-2 points of our higher-resourced\ntopline.\n","authors":["Anuj Diwan","Anirudh Srinivasan","David Harwath","Eunsol Choi"],"pdf_url":"https://arxiv.org/pdf/2305.15405v3.pdf","comment":"Accepted to EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2407.12176v3","updated":"2024-11-06T20:38:48Z","published":"2024-07-16T21:03:14Z","title":"GPT-4V Cannot Generate Radiology Reports Yet","summary":"  GPT-4V's purported strong multimodal abilities raise interests in using it to\nautomate radiology report writing, but there lacks thorough evaluations. In\nthis work, we perform a systematic evaluation of GPT-4V in generating radiology\nreports on two chest X-ray report datasets: MIMIC-CXR and IU X-Ray. We attempt\nto directly generate reports using GPT-4V through different prompting\nstrategies and find that it fails terribly in both lexical metrics and clinical\nefficacy metrics. To understand the low performance, we decompose the task into\ntwo steps: 1) the medical image reasoning step of predicting medical condition\nlabels from images; and 2) the report synthesis step of generating reports from\n(groundtruth) conditions. We show that GPT-4V's performance in image reasoning\nis consistently low across different prompts. In fact, the distributions of\nmodel-predicted labels remain constant regardless of which groundtruth\nconditions are present on the image, suggesting that the model is not\ninterpreting chest X-rays meaningfully. Even when given groundtruth conditions\nin report synthesis, its generated reports are less correct and less\nnatural-sounding than a finetuned LLaMA-2. Altogether, our findings cast doubt\non the viability of using GPT-4V in a radiology workflow.\n","authors":["Yuyang Jiang","Chacha Chen","Dang Nguyen","Benjamin M. Mervak","Chenhao Tan"],"pdf_url":"https://arxiv.org/pdf/2407.12176v3.pdf","comment":"24 pages, 3 figures, code:\n  https://github.com/ChicagoHAI/cxr-eval-gpt-4v"},{"id":"http://arxiv.org/abs/2411.04223v1","updated":"2024-11-06T19:39:48Z","published":"2024-11-06T19:39:48Z","title":"Diversity Helps Jailbreak Large Language Models","summary":"  We have uncovered a powerful jailbreak technique that leverages large\nlanguage models' ability to diverge from prior context, enabling them to bypass\nsafety constraints and generate harmful outputs. By simply instructing the LLM\nto deviate and obfuscate previous attacks, our method dramatically outperforms\nexisting approaches, achieving up to a 62% higher success rate in compromising\nnine leading chatbots, including GPT-4, Gemini, and Llama, while using only 13%\nof the queries. This revelation exposes a critical flaw in current LLM safety\ntraining, suggesting that existing methods may merely mask vulnerabilities\nrather than eliminate them. Our findings sound an urgent alarm for the need to\nrevolutionize testing methodologies to ensure robust and reliable LLM security.\n","authors":["Weiliang Zhao","Daniel Ben-Levi","Junfeng Yang","Chengzhi Mao"],"pdf_url":"https://arxiv.org/pdf/2411.04223v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2312.02119"},{"id":"http://arxiv.org/abs/2411.02537v2","updated":"2024-11-06T19:27:10Z","published":"2024-11-04T19:16:53Z","title":"INQUIRE: A Natural World Text-to-Image Retrieval Benchmark","summary":"  We introduce INQUIRE, a text-to-image retrieval benchmark designed to\nchallenge multimodal vision-language models on expert-level queries. INQUIRE\nincludes iNaturalist 2024 (iNat24), a new dataset of five million natural world\nimages, along with 250 expert-level retrieval queries. These queries are paired\nwith all relevant images comprehensively labeled within iNat24, comprising\n33,000 total matches. Queries span categories such as species identification,\ncontext, behavior, and appearance, emphasizing tasks that require nuanced image\nunderstanding and domain expertise. Our benchmark evaluates two core retrieval\ntasks: (1) INQUIRE-Fullrank, a full dataset ranking task, and (2)\nINQUIRE-Rerank, a reranking task for refining top-100 retrievals. Detailed\nevaluation of a range of recent multimodal models demonstrates that INQUIRE\nposes a significant challenge, with the best models failing to achieve an\nmAP@50 above 50%. In addition, we show that reranking with more powerful\nmultimodal models can enhance retrieval performance, yet there remains a\nsignificant margin for improvement. By focusing on scientifically-motivated\necological challenges, INQUIRE aims to bridge the gap between AI capabilities\nand the needs of real-world scientific inquiry, encouraging the development of\nretrieval systems that can assist with accelerating ecological and biodiversity\nresearch. Our dataset and code are available at\nhttps://inquire-benchmark.github.io\n","authors":["Edward Vendrow","Omiros Pantazis","Alexander Shepard","Gabriel Brostow","Kate E. Jones","Oisin Mac Aodha","Sara Beery","Grant Van Horn"],"pdf_url":"https://arxiv.org/pdf/2411.02537v2.pdf","comment":"Published in NeurIPS 2024, Datasets and Benchmarks Track"},{"id":"http://arxiv.org/abs/2411.04158v1","updated":"2024-11-06T13:50:50Z","published":"2024-11-06T13:50:50Z","title":"Analyzing Multimodal Features of Spontaneous Voice Assistant Commands\n  for Mild Cognitive Impairment Detection","summary":"  Mild cognitive impairment (MCI) is a major public health concern due to its\nhigh risk of progressing to dementia. This study investigates the potential of\ndetecting MCI with spontaneous voice assistant (VA) commands from 35 older\nadults in a controlled setting. Specifically, a command-generation task is\ndesigned with pre-defined intents for participants to freely generate commands\nthat are more associated with cognitive ability than read commands. We develop\nMCI classification and regression models with audio, textual, intent, and\nmultimodal fusion features. We find the command-generation task outperforms the\ncommand-reading task with an average classification accuracy of 82%, achieved\nby leveraging multimodal fusion features. In addition, generated commands\ncorrelate more strongly with memory and attention subdomains than read\ncommands. Our results confirm the effectiveness of the command-generation task\nand imply the promise of using longitudinal in-home commands for MCI detection.\n","authors":["Nana Lin","Youxiang Zhu","Xiaohui Liang","John A. Batsis","Caroline Summerour"],"pdf_url":"https://arxiv.org/pdf/2411.04158v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03823v1","updated":"2024-11-06T10:44:15Z","published":"2024-11-06T10:44:15Z","title":"Both Text and Images Leaked! A Systematic Analysis of Multimodal LLM\n  Data Contamination","summary":"  The rapid progression of multimodal large language models (MLLMs) has\ndemonstrated superior performance on various multimodal benchmarks. However,\nthe issue of data contamination during training creates challenges in\nperformance evaluation and comparison. While numerous methods exist for\ndetecting dataset contamination in large language models (LLMs), they are less\neffective for MLLMs due to their various modalities and multiple training\nphases. In this study, we introduce a multimodal data contamination detection\nframework, MM-Detect, designed for MLLMs. Our experimental results indicate\nthat MM-Detect is sensitive to varying degrees of contamination and can\nhighlight significant performance improvements due to leakage of the training\nset of multimodal benchmarks. Furthermore, We also explore the possibility of\ncontamination originating from the pre-training phase of LLMs used by MLLMs and\nthe fine-tuning phase of MLLMs, offering new insights into the stages at which\ncontamination may be introduced.\n","authors":["Dingjie Song","Sicheng Lai","Shunian Chen","Lichao Sun","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2411.03823v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04156v1","updated":"2024-11-06T10:28:46Z","published":"2024-11-06T10:28:46Z","title":"Crystal: Illuminating LLM Abilities on Language and Code","summary":"  Large Language Models (LLMs) specializing in code generation (which are also\noften referred to as code LLMs), e.g., StarCoder and Code Llama, play\nincreasingly critical roles in various software development scenarios. It is\nalso crucial for code LLMs to possess both code generation and natural language\nabilities for many specific applications, such as code snippet retrieval using\nnatural language or code explanations. The intricate interaction between\nacquiring language and coding skills complicates the development of strong code\nLLMs. Furthermore, there is a lack of thorough prior studies on the LLM\npretraining strategy that mixes code and natural language. In this work, we\npropose a pretraining strategy to enhance the integration of natural language\nand coding capabilities within a single LLM. Specifically, it includes two\nphases of training with appropriately adjusted code/language ratios. The\nresulting model, Crystal, demonstrates remarkable capabilities in both domains.\nSpecifically, it has natural language and coding performance comparable to that\nof Llama 2 and Code Llama, respectively. Crystal exhibits better data\nefficiency, using 1.4 trillion tokens compared to the more than 2 trillion\ntokens used by Llama 2 and Code Llama. We verify our pretraining strategy by\nanalyzing the training process and observe consistent improvements in most\nbenchmarks. We also adopted a typical application adaptation phase with a\ncode-centric data mixture, only to find that it did not lead to enhanced\nperformance or training efficiency, underlining the importance of a carefully\ndesigned data recipe. To foster research within the community, we commit to\nopen-sourcing every detail of the pretraining, including our training datasets,\ncode, loggings and 136 checkpoints throughout the training.\n","authors":["Tianhua Tao","Junbo Li","Bowen Tan","Hongyi Wang","William Marshall","Bhargav M Kanakiya","Joel Hestness","Natalia Vassilieva","Zhiqiang Shen","Eric P. Xing","Zhengzhong Liu"],"pdf_url":"https://arxiv.org/pdf/2411.04156v1.pdf","comment":"Published as a conference paper at COLM 2024"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2411.04125v1","updated":"2024-11-06T18:59:41Z","published":"2024-11-06T18:59:41Z","title":"Community Forensics: Using Thousands of Generators to Train Fake Image\n  Detectors","summary":"  One of the key challenges of detecting AI-generated images is spotting images\nthat have been created by previously unseen generative models. We argue that\nthe limited diversity of the training data is a major obstacle to addressing\nthis problem, and we propose a new dataset that is significantly larger and\nmore diverse than prior work. As part of creating this dataset, we\nsystematically download thousands of text-to-image latent diffusion models and\nsample images from them. We also collect images from dozens of popular open\nsource and commercial models. The resulting dataset contains 2.7M images that\nhave been sampled from 4803 different models. These images collectively capture\na wide range of scene content, generator architectures, and image processing\nsettings. Using this dataset, we study the generalization abilities of fake\nimage detectors. Our experiments suggest that detection performance improves as\nthe number of models in the training set increases, even when these models have\nsimilar architectures. We also find that detection performance improves as the\ndiversity of the models increases, and that our trained detectors generalize\nbetter than those trained on other datasets.\n","authors":["Jeongsoo Park","Andrew Owens"],"pdf_url":"https://arxiv.org/pdf/2411.04125v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2407.10964v2","updated":"2024-11-06T18:58:03Z","published":"2024-07-15T17:58:42Z","title":"No Train, all Gain: Self-Supervised Gradients Improve Deep Frozen\n  Representations","summary":"  This paper introduces FUNGI, Features from UNsupervised GradIents, a method\nto enhance the features of transformer encoders by leveraging self-supervised\ngradients. Our method is simple: given any pretrained model, we first compute\ngradients from various self-supervised objectives for each input. These\ngradients are projected to a lower dimension and then concatenated with the\nmodel's output embedding. The resulting features are evaluated on k-nearest\nneighbor classification over 11 datasets from vision, 5 from natural language\nprocessing, and 2 from audio. Across backbones spanning various sizes and\npretraining strategies, FUNGI features provide consistent performance\nimprovements over the embeddings. We also show that using FUNGI features can\nbenefit linear classification, clustering and image retrieval, and that they\nsignificantly improve the retrieval-based in-context scene understanding\nabilities of pretrained models, for example improving upon DINO by +17% for\nsemantic segmentation - without any training.\n","authors":["Walter Simoncini","Spyros Gidaris","Andrei Bursuc","Yuki M. Asano"],"pdf_url":"https://arxiv.org/pdf/2407.10964v2.pdf","comment":"NeurIPS 2024. Code available at\n  https://github.com/WalterSimoncini/fungivision"},{"id":"http://arxiv.org/abs/2411.04112v1","updated":"2024-11-06T18:44:09Z","published":"2024-11-06T18:44:09Z","title":"Fed-EC: Bandwidth-Efficient Clustering-Based Federated Learning For\n  Autonomous Visual Robot Navigation","summary":"  Centralized learning requires data to be aggregated at a central server,\nwhich poses significant challenges in terms of data privacy and bandwidth\nconsumption. Federated learning presents a compelling alternative, however,\nvanilla federated learning methods deployed in robotics aim to learn a single\nglobal model across robots that works ideally for all. But in practice one\nmodel may not be well suited for robots deployed in various environments. This\npaper proposes Federated-EmbedCluster (Fed-EC), a clustering-based federated\nlearning framework that is deployed with vision based autonomous robot\nnavigation in diverse outdoor environments. The framework addresses the key\nfederated learning challenge of deteriorating model performance of a single\nglobal model due to the presence of non-IID data across real-world robots.\nExtensive real-world experiments validate that Fed-EC reduces the communication\nsize by 23x for each robot while matching the performance of centralized\nlearning for goal-oriented navigation and outperforms local learning. Fed-EC\ncan transfer previously learnt models to new robots that join the cluster.\n","authors":["Shreya Gummadi","Mateus V. Gasparino","Deepak Vasisht","Girish Chowdhary"],"pdf_url":"https://arxiv.org/pdf/2411.04112v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19863v4","updated":"2024-11-06T18:29:38Z","published":"2024-03-28T22:17:19Z","title":"DeNetDM: Debiasing by Network Depth Modulation","summary":"  Neural networks trained on biased datasets tend to inadvertently learn\nspurious correlations, hindering generalization. We formally prove that (1)\nsamples that exhibit spurious correlations lie on a lower rank manifold\nrelative to the ones that do not; and (2) the depth of a network acts as an\nimplicit regularizer on the rank of the attribute subspace that is encoded in\nits representations. Leveraging these insights, we present DeNetDM, a novel\ndebiasing method that uses network depth modulation as a way of developing\nrobustness to spurious correlations. Using a training paradigm derived from\nProduct of Experts, we create both biased and debiased branches with deep and\nshallow architectures and then distill knowledge to produce the target debiased\nmodel. Our method requires no bias annotations or explicit data augmentation\nwhile performing on par with approaches that require either or both. We\ndemonstrate that DeNetDM outperforms existing debiasing techniques on both\nsynthetic and real-world datasets by 5\\%. The project page is available at\nhttps://vssilpa.github.io/denetdm/.\n","authors":["Silpa Vadakkeeveetil Sreelatha","Adarsh Kappiyath","Abhra Chaudhuri","Anjan Dutta"],"pdf_url":"https://arxiv.org/pdf/2403.19863v4.pdf","comment":"Camera-ready version : NeurIPS 2024, * indicates these authors\n  contributed equally"},{"id":"http://arxiv.org/abs/2411.04097v1","updated":"2024-11-06T18:25:00Z","published":"2024-11-06T18:25:00Z","title":"RaVL: Discovering and Mitigating Spurious Correlations in Fine-Tuned\n  Vision-Language Models","summary":"  Fine-tuned vision-language models (VLMs) often capture spurious correlations\nbetween image features and textual attributes, resulting in degraded zero-shot\nperformance at test time. Existing approaches for addressing spurious\ncorrelations (i) primarily operate at the global image-level rather than\nintervening directly on fine-grained image features and (ii) are predominantly\ndesigned for unimodal settings. In this work, we present RaVL, which takes a\nfine-grained perspective on VLM robustness by discovering and mitigating\nspurious correlations using local image features rather than operating at the\nglobal image level. Given a fine-tuned VLM, RaVL first discovers spurious\ncorrelations by leveraging a region-level clustering approach to identify\nprecise image features contributing to zero-shot classification errors. Then,\nRaVL mitigates the identified spurious correlation with a novel region-aware\nloss function that enables the VLM to focus on relevant regions and ignore\nspurious relationships during fine-tuning. We evaluate RaVL on 654 VLMs with\nvarious model architectures, data domains, and learned spurious correlations.\nOur results show that RaVL accurately discovers (191% improvement over the\nclosest baseline) and mitigates (8.2% improvement on worst-group image\nclassification accuracy) spurious correlations. Qualitative evaluations on\ngeneral-domain and medical-domain VLMs confirm our findings.\n","authors":["Maya Varma","Jean-Benoit Delbrouck","Zhihong Chen","Akshay Chaudhari","Curtis Langlotz"],"pdf_url":"https://arxiv.org/pdf/2411.04097v1.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2409.16147v3","updated":"2024-11-06T18:08:23Z","published":"2024-09-23T00:11:30Z","title":"Gaussian Deja-vu: Creating Controllable 3D Gaussian Head-Avatars with\n  Enhanced Generalization and Personalization Abilities","summary":"  Recent advancements in 3D Gaussian Splatting (3DGS) have unlocked significant\npotential for modeling 3D head avatars, providing greater flexibility than\nmesh-based methods and more efficient rendering compared to NeRF-based\napproaches. Despite these advancements, the creation of controllable 3DGS-based\nhead avatars remains time-intensive, often requiring tens of minutes to hours.\nTo expedite this process, we here introduce the \"Gaussian Deja-vu\" framework,\nwhich first obtains a generalized model of the head avatar and then\npersonalizes the result. The generalized model is trained on large 2D\n(synthetic and real) image datasets. This model provides a well-initialized 3D\nGaussian head that is further refined using a monocular video to achieve the\npersonalized head avatar. For personalizing, we propose learnable\nexpression-aware rectification blendmaps to correct the initial 3D Gaussians,\nensuring rapid convergence without the reliance on neural networks. Experiments\ndemonstrate that the proposed method meets its objectives. It outperforms\nstate-of-the-art 3D Gaussian head avatars in terms of photorealistic quality as\nwell as reduces training time consumption to at least a quarter of the existing\nmethods, producing the avatar in minutes.\n","authors":["Peizhi Yan","Rabab Ward","Qiang Tang","Shan Du"],"pdf_url":"https://arxiv.org/pdf/2409.16147v3.pdf","comment":"11 pages, Accepted by WACV 2025 in Round 1"},{"id":"http://arxiv.org/abs/2411.04079v1","updated":"2024-11-06T17:57:43Z","published":"2024-11-06T17:57:43Z","title":"Textual Decomposition Then Sub-motion-space Scattering for\n  Open-Vocabulary Motion Generation","summary":"  Text-to-motion generation is a crucial task in computer vision, which\ngenerates the target 3D motion by the given text. The existing annotated\ndatasets are limited in scale, resulting in most existing methods overfitting\nto the small datasets and unable to generalize to the motions of the open\ndomain. Some methods attempt to solve the open-vocabulary motion generation\nproblem by aligning to the CLIP space or using the Pretrain-then-Finetuning\nparadigm. However, the current annotated dataset's limited scale only allows\nthem to achieve mapping from sub-text-space to sub-motion-space, instead of\nmapping between full-text-space and full-motion-space (full mapping), which is\nthe key to attaining open-vocabulary motion generation. To this end, this paper\nproposes to leverage the atomic motion (simple body part motions over a short\ntime period) as an intermediate representation, and leverage two orderly\ncoupled steps, i.e., Textual Decomposition and Sub-motion-space Scattering, to\naddress the full mapping problem. For Textual Decomposition, we design a\nfine-grained description conversion algorithm, and combine it with the\ngeneralization ability of a large language model to convert any given motion\ntext into atomic texts. Sub-motion-space Scattering learns the compositional\nprocess from atomic motions to the target motions, to make the learned\nsub-motion-space scattered to form the full-motion-space. For a given motion of\nthe open domain, it transforms the extrapolation into interpolation and thereby\nsignificantly improves generalization. Our network, $DSO$-Net, combines textual\n$d$ecomposition and sub-motion-space $s$cattering to solve the\n$o$pen-vocabulary motion generation. Extensive experiments demonstrate that our\nDSO-Net achieves significant improvements over the state-of-the-art methods on\nopen-vocabulary motion generation. Code is available at\nhttps://vankouf.github.io/DSONet/.\n","authors":["Ke Fan","Jiangning Zhang","Ran Yi","Jingyu Gong","Yabiao Wang","Yating Wang","Xin Tan","Chengjie Wang","Lizhuang Ma"],"pdf_url":"https://arxiv.org/pdf/2411.04079v1.pdf","comment":"project page: https://vankouf.github.io/DSONet/"},{"id":"http://arxiv.org/abs/2411.04077v1","updated":"2024-11-06T17:55:37Z","published":"2024-11-06T17:55:37Z","title":"H-POPE: Hierarchical Polling-based Probing Evaluation of Hallucinations\n  in Large Vision-Language Models","summary":"  By leveraging both texts and images, large vision language models (LVLMs)\nhave shown significant progress in various multi-modal tasks. Nevertheless,\nthese models often suffer from hallucinations, e.g., they exhibit\ninconsistencies between the visual input and the textual output. To address\nthis, we propose H-POPE, a coarse-to-fine-grained benchmark that systematically\nassesses hallucination in object existence and attributes. Our evaluation shows\nthat models are prone to hallucinations on object existence, and even more so\non fine-grained attributes. We further investigate whether these models rely on\nvisual input to formulate the output texts.\n","authors":["Nhi Pham","Michael Schott"],"pdf_url":"https://arxiv.org/pdf/2411.04077v1.pdf","comment":"Poster at https://sites.google.com/berkeley.edu/bb-stat/home"},{"id":"http://arxiv.org/abs/2411.04059v1","updated":"2024-11-06T17:11:44Z","published":"2024-11-06T17:11:44Z","title":"Pseudo-labeling with Keyword Refining for Few-Supervised Video\n  Captioning","summary":"  Video captioning generate a sentence that describes the video content.\nExisting methods always require a number of captions (\\eg, 10 or 20) per video\nto train the model, which is quite costly. In this work, we explore the\npossibility of using only one or very few ground-truth sentences, and introduce\na new task named few-supervised video captioning. Specifically, we propose a\nfew-supervised video captioning framework that consists of lexically\nconstrained pseudo-labeling module and keyword-refined captioning module.\nUnlike the random sampling in natural language processing that may cause\ninvalid modifications (\\ie, edit words), the former module guides the model to\nedit words using some actions (\\eg, copy, replace, insert, and delete) by a\npretrained token-level classifier, and then fine-tunes candidate sentences by a\npretrained language model. Meanwhile, the former employs the repetition\npenalized sampling to encourage the model to yield concise pseudo-labeled\nsentences with less repetition, and selects the most relevant sentences upon a\npretrained video-text model. Moreover, to keep semantic consistency between\npseudo-labeled sentences and video content, we develop the transformer-based\nkeyword refiner with the video-keyword gated fusion strategy to emphasize more\non relevant words. Extensive experiments on several benchmarks demonstrate the\nadvantages of the proposed approach in both few-supervised and fully-supervised\nscenarios. The code implementation is available at\nhttps://github.com/mlvccn/PKG_VidCap\n","authors":["Ping Li","Tao Wang","Xinkui Zhao","Xianghua Xu","Mingli Song"],"pdf_url":"https://arxiv.org/pdf/2411.04059v1.pdf","comment":"12 figures, Accepted in Pattern Recognition"},{"id":"http://arxiv.org/abs/2410.23247v2","updated":"2024-11-06T17:07:53Z","published":"2024-10-30T17:30:35Z","title":"bit2bit: 1-bit quanta video reconstruction via self-supervised photon\n  prediction","summary":"  Quanta image sensors, such as SPAD arrays, are an emerging sensor technology,\nproducing 1-bit arrays representing photon detection events over exposures as\nshort as a few nanoseconds. In practice, raw data are post-processed using\nheavy spatiotemporal binning to create more useful and interpretable images at\nthe cost of degrading spatiotemporal resolution. In this work, we propose\nbit2bit, a new method for reconstructing high-quality image stacks at the\noriginal spatiotemporal resolution from sparse binary quanta image data.\nInspired by recent work on Poisson denoising, we developed an algorithm that\ncreates a dense image sequence from sparse binary photon data by predicting the\nphoton arrival location probability distribution. However, due to the binary\nnature of the data, we show that the assumption of a Poisson distribution is\ninadequate. Instead, we model the process with a Bernoulli lattice process from\nthe truncated Poisson. This leads to the proposal of a novel self-supervised\nsolution based on a masked loss function. We evaluate our method using both\nsimulated and real data. On simulated data from a conventional video, we\nachieve 34.35 mean PSNR with extremely photon-sparse binary input (<0.06\nphotons per pixel per frame). We also present a novel dataset containing a wide\nrange of real SPAD high-speed videos under various challenging imaging\nconditions. The scenes cover strong/weak ambient light, strong motion,\nultra-fast events, etc., which will be made available to the community, on\nwhich we demonstrate the promise of our approach. Both reconstruction quality\nand throughput substantially surpass the state-of-the-art methods (e.g., Quanta\nBurst Photography (QBP)). Our approach significantly enhances the visualization\nand usability of the data, enabling the application of existing analysis\ntechniques.\n","authors":["Yehe Liu","Alexander Krull","Hector Basevi","Ales Leonardis","Michael W. Jenkins"],"pdf_url":"https://arxiv.org/pdf/2410.23247v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.04055v1","updated":"2024-11-06T16:59:51Z","published":"2024-11-06T16:59:51Z","title":"Multi-branch Spatio-Temporal Graph Neural Network For Efficient Ice\n  Layer Thickness Prediction","summary":"  Understanding spatio-temporal patterns in polar ice layers is essential for\ntracking changes in ice sheet balance and assessing ice dynamics. While\nconvolutional neural networks are widely used in learning ice layer patterns\nfrom raw echogram images captured by airborne snow radar sensors, noise in the\nechogram images prevents researchers from getting high-quality results.\nInstead, we focus on geometric deep learning using graph neural networks,\naiming to build a spatio-temporal graph neural network that learns from\nthickness information of the top ice layers and predicts for deeper layers. In\nthis paper, we developed a novel multi-branch spatio-temporal graph neural\nnetwork that used the GraphSAGE framework for spatio features learning and a\ntemporal convolution operation to capture temporal changes, enabling different\nbranches of the network to be more specialized and focusing on a single\nlearning task. We found that our proposed multi-branch network can consistently\noutperform the current fused spatio-temporal graph neural network in both\naccuracy and efficiency.\n","authors":["Zesheng Liu","Maryam Rahnemoonfar"],"pdf_url":"https://arxiv.org/pdf/2411.04055v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.08774v3","updated":"2024-11-06T16:57:36Z","published":"2024-02-13T20:16:31Z","title":"LDTrack: Dynamic People Tracking by Service Robots using Diffusion\n  Models","summary":"  Tracking of dynamic people in cluttered and crowded human-centered\nenvironments is a challenging robotics problem due to the presence of\nintraclass variations including occlusions, pose deformations, and lighting\nvariations. This paper introduces a novel deep learning architecture, using\nconditional latent diffusion models, the Latent Diffusion Track (LDTrack), for\ntracking multiple dynamic people under intraclass variations. By uniquely\nutilizing conditional latent diffusion models to capture temporal person\nembeddings, our architecture can adapt to appearance changes of people over\ntime. We incorporated a latent feature encoder network which enables the\ndiffusion process to operate within a high-dimensional latent space to allow\nfor the extraction and spatial-temporal refinement of such rich features as\nperson appearance, motion, location, identity, and contextual information.\nExtensive experiments demonstrate the effectiveness of LDTrack over other\nstate-of-the-art tracking methods in cluttered and crowded human-centered\nenvironments under intraclass variations. Namely, the results show our method\noutperforms existing deep learning robotic people tracking methods in both\ntracking accuracy and tracking precision with statistical significance.\nAdditionally, a comprehensive multi-object tracking comparison study was\nperformed against the state-of-the-art methods in urban environments,\ndemonstrating the generalizability of LDTrack. An ablation study was performed\nto validate the design choices of LDTrack.\n","authors":["Angus Fung","Beno Benhabib","Goldie Nejat"],"pdf_url":"https://arxiv.org/pdf/2402.08774v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.16749v2","updated":"2024-11-06T16:55:39Z","published":"2024-05-27T01:38:30Z","title":"DMPlug: A Plug-in Method for Solving Inverse Problems with Diffusion\n  Models","summary":"  Pretrained diffusion models (DMs) have recently been popularly used in\nsolving inverse problems (IPs). The existing methods mostly interleave\niterative steps in the reverse diffusion process and iterative steps to bring\nthe iterates closer to satisfying the measurement constraint. However, such\ninterleaving methods struggle to produce final results that look like natural\nobjects of interest (i.e., manifold feasibility) and fit the measurement (i.e.,\nmeasurement feasibility), especially for nonlinear IPs. Moreover, their\ncapabilities to deal with noisy IPs with unknown types and levels of\nmeasurement noise are unknown. In this paper, we advocate viewing the reverse\nprocess in DMs as a function and propose a novel plug-in method for solving IPs\nusing pretrained DMs, dubbed DMPlug. DMPlug addresses the issues of manifold\nfeasibility and measurement feasibility in a principled manner, and also shows\ngreat potential for being robust to unknown types and levels of noise. Through\nextensive experiments across various IP tasks, including two linear and three\nnonlinear IPs, we demonstrate that DMPlug consistently outperforms\nstate-of-the-art methods, often by large margins especially for nonlinear IPs.\nThe code is available at https://github.com/sun-umn/DMPlug.\n","authors":["Hengkang Wang","Xu Zhang","Taihui Li","Yuxiang Wan","Tiancong Chen","Ju Sun"],"pdf_url":"https://arxiv.org/pdf/2405.16749v2.pdf","comment":"Published in NeurIPS 2024\n  (https://openreview.net/forum?id=81IFFsfQUj)"},{"id":"http://arxiv.org/abs/2410.05969v2","updated":"2024-11-06T16:28:58Z","published":"2024-10-08T12:16:30Z","title":"Deep neural network-based detection of counterfeit products from\n  smartphone images","summary":"  Counterfeit products such as drugs and vaccines as well as luxury items such\nas high-fashion handbags, watches, jewelry, garments, and cosmetics, represent\nsignificant direct losses of revenue to legitimate manufacturers and vendors,\nas well as indirect costs to societies at large. We present the world's first\npurely computer-vision-based system to combat such counterfeiting-one that does\nnot require special security tags or other alterations to the products or\nmodifications to supply chain tracking. Our deep neural network system shows\nhigh accuracy on branded garments from our first manufacturer tested (99.71%\nafter 3.06% rejections) using images captured under natural, weakly controlled\nconditions, such as in retail stores, customs checkpoints, warehouses, and\noutdoors. Our system, suitably transfer trained on a small number of fake and\ngenuine articles, should find application in additional product categories as\nwell, for example fashion accessories, perfume boxes, medicines, and more.\n","authors":["Hugo Garcia-Cotte","Dorra Mellouli","Abdul Rehman","Li Wang","David G. Stork"],"pdf_url":"https://arxiv.org/pdf/2410.05969v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17537v3","updated":"2024-11-06T15:56:04Z","published":"2024-05-27T17:57:48Z","title":"CLIBD: Bridging Vision and Genomics for Biodiversity Monitoring at Scale","summary":"  Measuring biodiversity is crucial for understanding ecosystem health. While\nprior works have developed machine learning models for taxonomic classification\nof photographic images and DNA separately, in this work, we introduce a\nmultimodal approach combining both, using CLIP-style contrastive learning to\nalign images, barcode DNA, and text-based representations of taxonomic labels\nin a unified embedding space. This allows for accurate classification of both\nknown and unknown insect species without task-specific fine-tuning, leveraging\ncontrastive learning for the first time to fuse DNA and image data. Our method\nsurpasses previous single-modality approaches in accuracy by over 8% on\nzero-shot learning tasks, showcasing its effectiveness in biodiversity studies.\n","authors":["ZeMing Gong","Austin T. Wang","Xiaoliang Huo","Joakim Bruslund Haurum","Scott C. Lowe","Graham W. Taylor","Angel X. Chang"],"pdf_url":"https://arxiv.org/pdf/2405.17537v3.pdf","comment":"25 pages with 11 figures"},{"id":"http://arxiv.org/abs/2411.04008v1","updated":"2024-11-06T15:47:18Z","published":"2024-11-06T15:47:18Z","title":"Aligning Characteristic Descriptors with Images for Human-Expert-like\n  Explainability","summary":"  In mission-critical domains such as law enforcement and medical diagnosis,\nthe ability to explain and interpret the outputs of deep learning models is\ncrucial for ensuring user trust and supporting informed decision-making.\nDespite advancements in explainability, existing methods often fall short in\nproviding explanations that mirror the depth and clarity of those given by\nhuman experts. Such expert-level explanations are essential for the dependable\napplication of deep learning models in law enforcement and medical contexts.\nAdditionally, we recognize that most explanations in real-world scenarios are\ncommunicated primarily through natural language. Addressing these needs, we\npropose a novel approach that utilizes characteristic descriptors to explain\nmodel decisions by identifying their presence in images, thereby generating\nexpert-like explanations. Our method incorporates a concept bottleneck layer\nwithin the model architecture, which calculates the similarity between image\nand descriptor encodings to deliver inherent and faithful explanations. Through\nexperiments in face recognition and chest X-ray diagnosis, we demonstrate that\nour approach offers a significant contrast over existing techniques, which are\noften limited to the use of saliency maps. We believe our approach represents a\nsignificant step toward making deep learning systems more accountable,\ntransparent, and trustworthy in the critical domains of face recognition and\nmedical diagnosis.\n","authors":["Bharat Chandra Yalavarthi","Nalini Ratha"],"pdf_url":"https://arxiv.org/pdf/2411.04008v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04004v1","updated":"2024-11-06T15:43:51Z","published":"2024-11-06T15:43:51Z","title":"Synomaly Noise and Multi-Stage Diffusion: A Novel Approach for\n  Unsupervised Anomaly Detection in Ultrasound Imaging","summary":"  Ultrasound (US) imaging is widely used in routine clinical practice due to\nits advantages of being radiation-free, cost-effective, and portable. However,\nthe low reproducibility and quality of US images, combined with the scarcity of\nexpert-level annotation, make the training of fully supervised segmentation\nmodels challenging. To address these issues, we propose a novel unsupervised\nanomaly detection framework based on a diffusion model that incorporates a\nsynthetic anomaly (Synomaly) noise function and a multi-stage diffusion\nprocess. Synomaly noise introduces synthetic anomalies into healthy images\nduring training, allowing the model to effectively learn anomaly removal. The\nmulti-stage diffusion process is introduced to progressively denoise images,\npreserving fine details while improving the quality of anomaly-free\nreconstructions. The generated high-fidelity counterfactual healthy images can\nfurther enhance the interpretability of the segmentation models, as well as\nprovide a reliable baseline for evaluating the extent of anomalies and\nsupporting clinical decision-making. Notably, the unsupervised anomaly\ndetection model is trained purely on healthy images, eliminating the need for\nanomalous training samples and pixel-level annotations. We validate the\nproposed approach on carotid US, brain MRI, and liver CT datasets. The\nexperimental results demonstrate that the proposed framework outperforms\nexisting state-of-the-art unsupervised anomaly detection methods, achieving\nperformance comparable to fully supervised segmentation models in the US\ndataset. Additionally, ablation studies underline the importance of\nhyperparameter selection for Synomaly noise and the effectiveness of the\nmulti-stage diffusion process in enhancing model performance.\n","authors":["Yuan Bi","Lucie Huang","Ricarda Clarenbach","Reza Ghotbi","Angelos Karlas","Nassir Navab","Zhongliang Jiang"],"pdf_url":"https://arxiv.org/pdf/2411.04004v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03993v1","updated":"2024-11-06T15:34:57Z","published":"2024-11-06T15:34:57Z","title":"Local vs distributed representations: What is the right basis for\n  interpretability?","summary":"  Much of the research on the interpretability of deep neural networks has\nfocused on studying the visual features that maximally activate individual\nneurons. However, recent work has cast doubts on the usefulness of such local\nrepresentations for understanding the behavior of deep neural networks because\nindividual neurons tend to respond to multiple unrelated visual patterns, a\nphenomenon referred to as \"superposition\". A promising alternative to\ndisentangle these complex patterns is learning sparsely distributed vector\nrepresentations from entire network layers, as the resulting basis vectors\nseemingly encode single identifiable visual patterns consistently. Thus, one\nwould expect the resulting code to align better with human perceivable visual\npatterns, but supporting evidence remains, at best, anecdotal. To fill this\ngap, we conducted three large-scale psychophysics experiments collected from a\npool of 560 participants. Our findings provide (i) strong evidence that\nfeatures obtained from sparse distributed representations are easier to\ninterpret by human observers and (ii) that this effect is more pronounced in\nthe deepest layers of a neural network. Complementary analyses also reveal that\n(iii) features derived from sparse distributed representations contribute more\nto the model's decision. Overall, our results highlight that distributed\nrepresentations constitute a superior basis for interpretability, underscoring\na need for the field to move beyond the interpretation of local neural codes in\nfavor of sparsely distributed ones.\n","authors":["Julien Colin","Lore Goetschalckx","Thomas Fel","Victor Boutin","Jay Gopal","Thomas Serre","Nuria Oliver"],"pdf_url":"https://arxiv.org/pdf/2411.03993v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03990v1","updated":"2024-11-06T15:30:42Z","published":"2024-11-06T15:30:42Z","title":"ET-SEED: Efficient Trajectory-Level SE(3) Equivariant Diffusion Policy","summary":"  Imitation learning, e.g., diffusion policy, has been proven effective in\nvarious robotic manipulation tasks. However, extensive demonstrations are\nrequired for policy robustness and generalization. To reduce the demonstration\nreliance, we leverage spatial symmetry and propose ET-SEED, an efficient\ntrajectory-level SE(3) equivariant diffusion model for generating action\nsequences in complex robot manipulation tasks. Further, previous equivariant\ndiffusion models require the per-step equivariance in the Markov process,\nmaking it difficult to learn policy under such strong constraints. We\ntheoretically extend equivariant Markov kernels and simplify the condition of\nequivariant diffusion process, thereby significantly improving training\nefficiency for trajectory-level SE(3) equivariant diffusion policy in an\nend-to-end manner. We evaluate ET-SEED on representative robotic manipulation\ntasks, involving rigid body, articulated and deformable object. Experiments\ndemonstrate superior data efficiency and manipulation proficiency of our\nproposed method, as well as its ability to generalize to unseen configurations\nwith only a few demonstrations. Website: https://et-seed.github.io/\n","authors":["Chenrui Tie","Yue Chen","Ruihai Wu","Boxuan Dong","Zeyi Li","Chongkai Gao","Hao Dong"],"pdf_url":"https://arxiv.org/pdf/2411.03990v1.pdf","comment":"Accept to CoRL 2024 Workshop on X-Embodiment Robot Learning"},{"id":"http://arxiv.org/abs/2411.03982v1","updated":"2024-11-06T15:19:24Z","published":"2024-11-06T15:19:24Z","title":"ReEdit: Multimodal Exemplar-Based Image Editing with Diffusion Models","summary":"  Modern Text-to-Image (T2I) Diffusion models have revolutionized image editing\nby enabling the generation of high-quality photorealistic images. While the de\nfacto method for performing edits with T2I models is through text instructions,\nthis approach non-trivial due to the complex many-to-many mapping between\nnatural language and images. In this work, we address exemplar-based image\nediting -- the task of transferring an edit from an exemplar pair to a content\nimage(s). We propose ReEdit, a modular and efficient end-to-end framework that\ncaptures edits in both text and image modalities while ensuring the fidelity of\nthe edited image. We validate the effectiveness of ReEdit through extensive\ncomparisons with state-of-the-art baselines and sensitivity analyses of key\ndesign choices. Our results demonstrate that ReEdit consistently outperforms\ncontemporary approaches both qualitatively and quantitatively. Additionally,\nReEdit boasts high practical applicability, as it does not require any\ntask-specific optimization and is four times faster than the next best\nbaseline.\n","authors":["Ashutosh Srivastava","Tarun Ram Menta","Abhinav Java","Avadhoot Jadhav","Silky Singh","Surgan Jandial","Balaji Krishnamurthy"],"pdf_url":"https://arxiv.org/pdf/2411.03982v1.pdf","comment":"First three authors contributed equally to this work"},{"id":"http://arxiv.org/abs/2411.03976v1","updated":"2024-11-06T15:13:31Z","published":"2024-11-06T15:13:31Z","title":"HRDecoder: High-Resolution Decoder Network for Fundus Image Lesion\n  Segmentation","summary":"  High resolution is crucial for precise segmentation in fundus images, yet\nhandling high-resolution inputs incurs considerable GPU memory costs, with\ndiminishing performance gains as overhead increases. To address this issue\nwhile tackling the challenge of segmenting tiny objects, recent studies have\nexplored local-global fusion methods. These methods preserve fine details using\nlocal regions and capture long-range context information from downscaled global\nimages. However, the necessity of multiple forward passes inevitably incurs\nsignificant computational overhead, adversely affecting inference speed. In\nthis paper, we propose HRDecoder, a simple High-Resolution Decoder network for\nfundus lesion segmentation. It integrates a high-resolution representation\nlearning module to capture fine-grained local features and a high-resolution\nfusion module to fuse multi-scale predictions. Our method effectively improves\nthe overall segmentation accuracy of fundus lesions while consuming reasonable\nmemory and computational overhead, and maintaining satisfying inference speed.\nExperimental results on the IDRID and DDR datasets demonstrate the\neffectiveness of our method. Code is available at\nhttps://github.com/CVIU-CSU/HRDecoder.\n","authors":["Ziyuan Ding","Yixiong Liang","Shichao Kan","Qing Liu"],"pdf_url":"https://arxiv.org/pdf/2411.03976v1.pdf","comment":"11 pages, 3 figures, accepted by MICCAI 2024, the revised version"},{"id":"http://arxiv.org/abs/2407.17952v2","updated":"2024-11-06T14:58:17Z","published":"2024-07-25T11:16:37Z","title":"BetterDepth: Plug-and-Play Diffusion Refiner for Zero-Shot Monocular\n  Depth Estimation","summary":"  By training over large-scale datasets, zero-shot monocular depth estimation\n(MDE) methods show robust performance in the wild but often suffer from\ninsufficient detail. Although recent diffusion-based MDE approaches exhibit a\nsuperior ability to extract details, they struggle in geometrically complex\nscenes that challenge their geometry prior, trained on less diverse 3D data. To\nleverage the complementary merits of both worlds, we propose BetterDepth to\nachieve geometrically correct affine-invariant MDE while capturing fine\ndetails. Specifically, BetterDepth is a conditional diffusion-based refiner\nthat takes the prediction from pre-trained MDE models as depth conditioning, in\nwhich the global depth layout is well-captured, and iteratively refines details\nbased on the input image. For the training of such a refiner, we propose global\npre-alignment and local patch masking methods to ensure BetterDepth remains\nfaithful to the depth conditioning while learning to add fine-grained scene\ndetails. With efficient training on small-scale synthetic datasets, BetterDepth\nachieves state-of-the-art zero-shot MDE performance on diverse public datasets\nand on in-the-wild scenes. Moreover, BetterDepth can improve the performance of\nother MDE models in a plug-and-play manner without further re-training.\n","authors":["Xiang Zhang","Bingxin Ke","Hayko Riemenschneider","Nando Metzger","Anton Obukhov","Markus Gross","Konrad Schindler","Christopher Schroers"],"pdf_url":"https://arxiv.org/pdf/2407.17952v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2408.00738v3","updated":"2024-11-06T14:45:58Z","published":"2024-08-01T17:35:58Z","title":"Virchow2: Scaling Self-Supervised Mixed Magnification Models in\n  Pathology","summary":"  Foundation models are rapidly being developed for computational pathology\napplications. However, it remains an open question which factors are most\nimportant for downstream performance with data scale and diversity, model size,\nand training algorithm all playing a role. In this work, we propose algorithmic\nmodifications, tailored for pathology, and we present the result of scaling\nboth data and model size, surpassing previous studies in both dimensions. We\nintroduce three new models: Virchow2, a 632 million parameter vision\ntransformer, Virchow2G, a 1.9 billion parameter vision transformer, and\nVirchow2G Mini, a 22 million parameter distillation of Virchow2G, each trained\nwith 3.1 million histopathology whole slide images, with diverse tissues,\noriginating institutions, and stains. We achieve state of the art performance\non 12 tile-level tasks, as compared to the top performing competing models. Our\nresults suggest that data diversity and domain-specific methods can outperform\nmodels that only scale in the number of parameters, but, on average,\nperformance benefits from the combination of domain-specific methods, data\nscale, and model scale.\n","authors":["Eric Zimmermann","Eugene Vorontsov","Julian Viret","Adam Casson","Michal Zelechowski","George Shaikovski","Neil Tenenholtz","James Hall","David Klimstra","Razik Yousfi","Thomas Fuchs","Nicolo Fusi","Siqi Liu","Kristen Severson"],"pdf_url":"https://arxiv.org/pdf/2408.00738v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03960v1","updated":"2024-11-06T14:45:41Z","published":"2024-11-06T14:45:41Z","title":"Face Reconstruction from Face Embeddings using Adapter to a Face\n  Foundation Model","summary":"  Face recognition systems extract embedding vectors from face images and use\nthese embeddings to verify or identify individuals. Face reconstruction attack\n(also known as template inversion) refers to reconstructing face images from\nface embeddings and using the reconstructed face image to enter a face\nrecognition system. In this paper, we propose to use a face foundation model to\nreconstruct face images from the embeddings of a blackbox face recognition\nmodel. The foundation model is trained with 42M images to generate face images\nfrom the facial embeddings of a fixed face recognition model. We propose to use\nan adapter to translate target embeddings into the embedding space of the\nfoundation model. The generated images are evaluated on different face\nrecognition models and different datasets, demonstrating the effectiveness of\nour method to translate embeddings of different face recognition models. We\nalso evaluate the transferability of reconstructed face images when attacking\ndifferent face recognition models. Our experimental results show that our\nreconstructed face images outperform previous reconstruction attacks against\nface recognition models.\n","authors":["Hatef Otroshi Shahreza","Anjith George","Sébastien Marcel"],"pdf_url":"https://arxiv.org/pdf/2411.03960v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03959v1","updated":"2024-11-06T14:45:16Z","published":"2024-11-06T14:45:16Z","title":"Energy Score-based Pseudo-Label Filtering and Adaptive Loss for\n  Imbalanced Semi-supervised SAR target recognition","summary":"  Automatic target recognition (ATR) is an important use case for synthetic\naperture radar (SAR) image interpretation. Recent years have seen significant\nadvancements in SAR ATR technology based on semi-supervised learning. However,\nexisting semi-supervised SAR ATR algorithms show low recognition accuracy in\nthe case of class imbalance. This work offers a non-balanced semi-supervised\nSAR target recognition approach using dynamic energy scores and adaptive loss.\nFirst, an energy score-based method is developed to dynamically select\nunlabeled samples near to the training distribution as pseudo-labels during\ntraining, assuring pseudo-label reliability in long-tailed distribution\ncircumstances. Secondly, loss functions suitable for class imbalances are\nproposed, including adaptive margin perception loss and adaptive hard triplet\nloss, the former offsets inter-class confusion of classifiers, alleviating the\nimbalance issue inherent in pseudo-label generation. The latter effectively\ntackles the model's preference for the majority class by focusing on complex\ndifficult samples during training. Experimental results on extremely imbalanced\nSAR datasets demonstrate that the proposed method performs well under the dual\nconstraints of scarce labels and data imbalance, effectively overcoming the\nmodel bias caused by data imbalance and achieving high-precision target\nrecognition.\n","authors":["Xinzheng Zhang","Yuqing Luo","Guopeng Li"],"pdf_url":"https://arxiv.org/pdf/2411.03959v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07724v2","updated":"2024-11-06T14:29:36Z","published":"2024-04-11T13:16:47Z","title":"Applying Guidance in a Limited Interval Improves Sample and Distribution\n  Quality in Diffusion Models","summary":"  Guidance is a crucial technique for extracting the best performance out of\nimage-generating diffusion models. Traditionally, a constant guidance weight\nhas been applied throughout the sampling chain of an image. We show that\nguidance is clearly harmful toward the beginning of the chain (high noise\nlevels), largely unnecessary toward the end (low noise levels), and only\nbeneficial in the middle. We thus restrict it to a specific range of noise\nlevels, improving both the inference speed and result quality. This limited\nguidance interval improves the record FID in ImageNet-512 significantly, from\n1.81 to 1.40. We show that it is quantitatively and qualitatively beneficial\nacross different sampler parameters, network architectures, and datasets,\nincluding the large-scale setting of Stable Diffusion XL. We thus suggest\nexposing the guidance interval as a hyperparameter in all diffusion models that\nuse guidance.\n","authors":["Tuomas Kynkäänniemi","Miika Aittala","Tero Karras","Samuli Laine","Timo Aila","Jaakko Lehtinen"],"pdf_url":"https://arxiv.org/pdf/2404.07724v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2407.09786v2","updated":"2024-11-06T14:22:28Z","published":"2024-07-13T06:53:39Z","title":"Self-supervised 3D Point Cloud Completion via Multi-view Adversarial\n  Learning","summary":"  In real-world scenarios, scanned point clouds are often incomplete due to\nocclusion issues. The task of self-supervised point cloud completion involves\nreconstructing missing regions of these incomplete objects without the\nsupervision of complete ground truth. Current self-supervised methods either\nrely on multiple views of partial observations for supervision or overlook the\nintrinsic geometric similarity that can be identified and utilized from the\ngiven partial point clouds. In this paper, we propose MAL-SPC, a framework that\neffectively leverages both object-level and category-specific geometric\nsimilarities to complete missing structures. Our MAL-SPC does not require any\n3D complete supervision and only necessitates a single partial point cloud for\neach object. Specifically, we first introduce a Pattern Retrieval Network to\nretrieve similar position and curvature patterns between the partial input and\nthe predicted shape, then leverage these similarities to densify and refine the\nreconstructed results. Additionally, we render the reconstructed complete shape\ninto multi-view depth maps and design an adversarial learning module to learn\nthe geometry of the target shape from category-specific single-view depth\nimages. To achieve anisotropic rendering, we design a density-aware radius\nestimation algorithm to improve the quality of the rendered images. Our MAL-SPC\nyields the best results compared to current state-of-the-art methods.We will\nmake the source code publicly available at \\url{https://github.com/ltwu6/malspc\n","authors":["Lintai Wu","Xianjing Cheng","Yong Xu","Huanqiang Zeng","Junhui Hou"],"pdf_url":"https://arxiv.org/pdf/2407.09786v2.pdf","comment":"14 pages,10 figures"},{"id":"http://arxiv.org/abs/2411.03926v1","updated":"2024-11-06T13:57:53Z","published":"2024-11-06T13:57:53Z","title":"Act in Collusion: A Persistent Distributed Multi-Target Backdoor in\n  Federated Learning","summary":"  Federated learning, a novel paradigm designed to protect data privacy, is\nvulnerable to backdoor attacks due to its distributed nature. Current research\noften designs attacks based on a single attacker with a single backdoor,\noverlooking more realistic and complex threats in federated learning. We\npropose a more practical threat model for federated learning: the distributed\nmulti-target backdoor. In this model, multiple attackers control different\nclients, embedding various triggers and targeting different classes,\ncollaboratively implanting backdoors into the global model via central\naggregation. Empirical validation shows that existing methods struggle to\nmaintain the effectiveness of multiple backdoors in the global model. Our key\ninsight is that similar backdoor triggers cause parameter conflicts and\ninjecting new backdoors disrupts gradient directions, significantly weakening\nsome backdoors performance. To solve this, we propose a Distributed\nMulti-Target Backdoor Attack (DMBA), ensuring efficiency and persistence of\nbackdoors from different malicious clients. To avoid parameter conflicts, we\ndesign a multi-channel dispersed frequency trigger strategy to maximize trigger\ndifferences. To mitigate gradient interference, we introduce backdoor replay in\nlocal training to neutralize conflicting gradients. Extensive validation shows\nthat 30 rounds after the attack, Attack Success Rates of three different\nbackdoors from various clients remain above 93%. The code will be made publicly\navailable after the review period.\n","authors":["Tao Liu","Wu Yang","Chen Xu","Jiguang Lv","Huanran Wang","Yuhang Zhang","Shuchun Xu","Dapeng Man"],"pdf_url":"https://arxiv.org/pdf/2411.03926v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.07001v4","updated":"2024-11-06T13:56:28Z","published":"2024-05-11T12:33:46Z","title":"ChartInsights: Evaluating Multimodal Large Language Models for Low-Level\n  Chart Question Answering","summary":"  Chart question answering (ChartQA) tasks play a critical role in interpreting\nand extracting insights from visualization charts. While recent advancements in\nmultimodal large language models (MLLMs) like GPT-4o have shown promise in\nhigh-level ChartQA tasks, such as chart captioning, their effectiveness in\nlow-level ChartQA tasks (e.g., identifying correlations) remains underexplored.\nIn this paper, we address this gap by evaluating MLLMs on low-level ChartQA\nusing a newly curated dataset, ChartInsights, which consists of 22,347 (chart,\ntask, query, answer) covering 10 data analysis tasks across 7 chart types. We\nsystematically evaluate 19 advanced MLLMs, including 12 open-source and 7\nclosed-source models. The average accuracy rate across these models is 39.8%,\nwith GPT-4o achieving the highest accuracy at 69.17%. To further explore the\nlimitations of MLLMs in low-level ChartQA, we conduct experiments that alter\nvisual elements of charts (e.g., changing color schemes, adding image noise) to\nassess their impact on the task effectiveness. Furthermore, we propose a new\ntextual prompt strategy, Chain-of-Charts, tailored for low-level ChartQA tasks,\nwhich boosts performance by 14.41%, achieving an accuracy of 83.58%. Finally,\nincorporating a visual prompt strategy that directs attention to relevant\nvisual elements further improves accuracy to 84.32%.\n","authors":["Yifan Wu","Lutao Yan","Leixian Shen","Yunhai Wang","Nan Tang","Yuyu Luo"],"pdf_url":"https://arxiv.org/pdf/2405.07001v4.pdf","comment":"EMNLP 2024 Conference Paper"},{"id":"http://arxiv.org/abs/2411.03924v1","updated":"2024-11-06T13:54:26Z","published":"2024-11-06T13:54:26Z","title":"Self-supervised Representation Learning for Cell Event Recognition\n  through Time Arrow Prediction","summary":"  The spatio-temporal nature of live-cell microscopy data poses challenges in\nthe analysis of cell states which is fundamental in bioimaging. Deep-learning\nbased segmentation or tracking methods rely on large amount of high quality\nannotations to work effectively. In this work, we explore an alternative\nsolution: using feature maps obtained from self-supervised representation\nlearning (SSRL) on time arrow prediction (TAP) for the downstream supervised\ntask of cell event recognition. We demonstrate through extensive experiments\nand analysis that this approach can achieve better performance with limited\nannotation compared to models trained from end to end using fully supervised\napproach. Our analysis also provides insight into applications of the SSRL\nusing TAP in live-cell microscopy.\n","authors":["Cangxiong Chen","Vinay P. Namboodiri","Julia E. Sero"],"pdf_url":"https://arxiv.org/pdf/2411.03924v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06629v4","updated":"2024-11-06T13:29:57Z","published":"2023-10-10T13:48:18Z","title":"EViT: An Eagle Vision Transformer with Bi-Fovea Self-Attention","summary":"  Owing to advancements in deep learning technology, Vision Transformers (ViTs)\nhave demonstrated impressive performance in various computer vision tasks.\nNonetheless, ViTs still face some challenges, such as high computational\ncomplexity and the absence of desirable inductive biases. To alleviate these\nissues, {the potential advantages of combining eagle vision with ViTs are\nexplored. We summarize a Bi-Fovea Visual Interaction (BFVI) structure inspired\nby the unique physiological and visual characteristics of eagle eyes. A novel\nBi-Fovea Self-Attention (BFSA) mechanism and Bi-Fovea Feedforward Network\n(BFFN) are proposed based on this structural design approach, which can be used\nto mimic the hierarchical and parallel information processing scheme of the\nbiological visual cortex, enabling networks to learn feature representations of\ntargets in a coarse-to-fine manner. Furthermore, a Bionic Eagle Vision (BEV)\nblock is designed as the basic building unit based on the BFSA mechanism and\nBFFN. By stacking BEV blocks, a unified and efficient family of pyramid\nbackbone networks called Eagle Vision Transformers (EViTs) is developed.\nExperimental results show that EViTs exhibit highly competitive performance in\nvarious computer vision tasks, such as image classification, object detection\nand semantic segmentation. Compared with other approaches, EViTs have\nsignificant advantages, especially in terms of performance and computational\nefficiency. Code is available at https://github.com/nkusyl/EViT\n","authors":["Yulong Shi","Mingwei Sun","Yongshuai Wang","Jiahao Ma","Zengqiang Chen"],"pdf_url":"https://arxiv.org/pdf/2310.06629v4.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2411.00393v3","updated":"2024-11-06T13:25:42Z","published":"2024-11-01T06:40:47Z","title":"Advantages of Neural Population Coding for Deep Learning","summary":"  Scalar variables, e.g., the orientation of a shape in an image, are commonly\npredicted using a single output neuron in a neural network. In contrast, the\nmammalian cortex represents variables with a population of neurons. In this\npopulation code, each neuron is most active at its preferred value and shows\npartial activity for other values. Here, we investigate the benefit of using a\npopulation code for the output layer of a neural network. We compare population\ncodes against single-neuron outputs and one-hot vectors. First, we show\ntheoretically and in experiments with synthetic data that population codes\nimprove robustness to input noise in networks of stacked linear layers. Second,\nwe demonstrate the benefit of using population codes to encode ambiguous\noutputs, such as the pose of symmetric objects. Using the T-LESS dataset of\nfeature-less real-world objects, we show that population codes improve the\naccuracy of predicting 3D object orientation from image input.\n","authors":["Heiko Hoffmann"],"pdf_url":"https://arxiv.org/pdf/2411.00393v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03055v2","updated":"2024-11-06T13:24:10Z","published":"2024-11-05T12:42:42Z","title":"ATM: Improving Model Merging by Alternating Tuning and Merging","summary":"  Model merging has recently emerged as a cost-efficient paradigm for\nmulti-task learning. Among current approaches, task arithmetic stands out for\nits simplicity and effectiveness. In this paper, we motivate the effectiveness\nof task vectors by linking them to multi-task gradients. We show that in a\nsingle-epoch scenario, task vectors are mathematically equivalent to the\ngradients obtained via gradient descent in a multi-task setting, and still\napproximate these gradients in subsequent epochs. Furthermore, we show that\ntask vectors perform optimally when equality is maintained, and their\neffectiveness is largely driven by the first epoch's gradient. Building on this\ninsight, we propose viewing model merging as a single step in an iterative\nprocess that Alternates between Tuning and Merging (ATM). This method acts as a\nbridge between model merging and multi-task gradient descent, achieving\nstate-of-the-art results with the same data and computational requirements. We\nextensively evaluate ATM across diverse settings, achieving up to 20% higher\naccuracy in computer vision and NLP tasks, compared to the best baselines.\nFinally, we provide both empirical and theoretical support for its\neffectiveness, demonstrating increased orthogonality between task vectors and\nproving that ATM minimizes an upper bound on the loss obtained by jointly\nfinetuning all tasks.\n","authors":["Luca Zhou","Daniele Solombrino","Donato Crisostomi","Maria Sofia Bucarelli","Fabrizio Silvestri","Emanuele Rodolà"],"pdf_url":"https://arxiv.org/pdf/2411.03055v2.pdf","comment":"Main paper: 10 Pages, 11 figures, 2 tables"},{"id":"http://arxiv.org/abs/2409.08240v3","updated":"2024-11-06T13:03:20Z","published":"2024-09-12T17:39:23Z","title":"IFAdapter: Instance Feature Control for Grounded Text-to-Image\n  Generation","summary":"  While Text-to-Image (T2I) diffusion models excel at generating visually\nappealing images of individual instances, they struggle to accurately position\nand control the features generation of multiple instances. The Layout-to-Image\n(L2I) task was introduced to address the positioning challenges by\nincorporating bounding boxes as spatial control signals, but it still falls\nshort in generating precise instance features. In response, we propose the\nInstance Feature Generation (IFG) task, which aims to ensure both positional\naccuracy and feature fidelity in generated instances. To address the IFG task,\nwe introduce the Instance Feature Adapter (IFAdapter). The IFAdapter enhances\nfeature depiction by incorporating additional appearance tokens and utilizing\nan Instance Semantic Map to align instance-level features with spatial\nlocations. The IFAdapter guides the diffusion process as a plug-and-play\nmodule, making it adaptable to various community models. For evaluation, we\ncontribute an IFG benchmark and develop a verification pipeline to objectively\ncompare models' abilities to generate instances with accurate positioning and\nfeatures. Experimental results demonstrate that IFAdapter outperforms other\nmodels in both quantitative and qualitative evaluations.\n","authors":["Yinwei Wu","Xianpan Zhou","Bing Ma","Xuefeng Su","Kai Ma","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2409.08240v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01853v2","updated":"2024-11-06T12:27:27Z","published":"2024-11-04T07:07:31Z","title":"GVKF: Gaussian Voxel Kernel Functions for Highly Efficient Surface\n  Reconstruction in Open Scenes","summary":"  In this paper we present a novel method for efficient and effective 3D\nsurface reconstruction in open scenes. Existing Neural Radiance Fields (NeRF)\nbased works typically require extensive training and rendering time due to the\nadopted implicit representations. In contrast, 3D Gaussian splatting (3DGS)\nuses an explicit and discrete representation, hence the reconstructed surface\nis built by the huge number of Gaussian primitives, which leads to excessive\nmemory consumption and rough surface details in sparse Gaussian areas. To\naddress these issues, we propose Gaussian Voxel Kernel Functions (GVKF), which\nestablish a continuous scene representation based on discrete 3DGS through\nkernel regression. The GVKF integrates fast 3DGS rasterization and highly\neffective scene implicit representations, achieving high-fidelity open scene\nsurface reconstruction. Experiments on challenging scene datasets demonstrate\nthe efficiency and effectiveness of our proposed GVKF, featuring with high\nreconstruction quality, real-time rendering speed, significant savings in\nstorage and training memory consumption.\n","authors":["Gaochao Song","Chong Cheng","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2411.01853v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.03862v1","updated":"2024-11-06T12:14:23Z","published":"2024-11-06T12:14:23Z","title":"ROBIN: Robust and Invisible Watermarks for Diffusion Models with\n  Adversarial Optimization","summary":"  Watermarking generative content serves as a vital tool for authentication,\nownership protection, and mitigation of potential misuse. Existing watermarking\nmethods face the challenge of balancing robustness and concealment. They\nempirically inject a watermark that is both invisible and robust and passively\nachieve concealment by limiting the strength of the watermark, thus reducing\nthe robustness. In this paper, we propose to explicitly introduce a watermark\nhiding process to actively achieve concealment, thus allowing the embedding of\nstronger watermarks. To be specific, we implant a robust watermark in an\nintermediate diffusion state and then guide the model to hide the watermark in\nthe final generated image. We employ an adversarial optimization algorithm to\nproduce the optimal hiding prompt guiding signal for each watermark. The prompt\nembedding is optimized to minimize artifacts in the generated image, while the\nwatermark is optimized to achieve maximum strength. The watermark can be\nverified by reversing the generation process. Experiments on various diffusion\nmodels demonstrate the watermark remains verifiable even under significant\nimage tampering and shows superior invisibility compared to other\nstate-of-the-art robust watermarking methods.\n","authors":["Huayang Huang","Yu Wu","Qian Wang"],"pdf_url":"https://arxiv.org/pdf/2411.03862v1.pdf","comment":"Accept to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.03861v1","updated":"2024-11-06T12:14:11Z","published":"2024-11-06T12:14:11Z","title":"FedRISE: Rating Induced Sign Election of Gradients for Byzantine\n  Tolerant Federated Aggregation","summary":"  One of the most common defense strategies against model poisoning in\nfederated learning is to employ a robust aggregator mechanism that makes the\ntraining more resilient. Many of the existing Byzantine robust aggregators\nprovide theoretical guarantees and are empirically effective against certain\ncategories of attacks. However, we observe that certain high-strength attacks\ncan subvert the aggregator and collapse the training. In addition, most\naggregators require identifying tolerant settings to converge. Impact of\nattacks becomes more pronounced when the number of Byzantines is near-majority,\nand becomes harder to evade if the attacker is omniscient with access to data,\nhonest updates and aggregation methods. Motivated by these observations, we\ndevelop a robust aggregator called FedRISE for cross-silo FL that is consistent\nand less susceptible to poisoning updates by an omniscient attacker. The\nproposed method explicitly determines the optimal direction of each gradient\nthrough a sign-voting strategy that uses variance-reduced sparse gradients. We\nargue that vote weighting based on the cosine similarity of raw gradients is\nmisleading, and we introduce a sign-based gradient valuation function that\nignores the gradient magnitude. We compare our method against 8 robust\naggregators under 6 poisoning attacks on 3 datasets and architectures. Our\nresults show that existing robust aggregators collapse for at least some\nattacks under severe settings, while FedRISE demonstrates better robustness\nbecause of a stringent gradient inclusion formulation.\n","authors":["Joseph Geo Benjamin","Mothilal Asokan","Mohammad Yaqub","Karthik Nandakumar"],"pdf_url":"https://arxiv.org/pdf/2411.03861v1.pdf","comment":"This is a work under submission/review process"},{"id":"http://arxiv.org/abs/2408.13800v3","updated":"2024-11-06T12:10:54Z","published":"2024-08-25T10:42:07Z","title":"BCDNet: A Fast Residual Neural Network For Invasive Ductal Carcinoma\n  Detection","summary":"  It is of great significance to diagnose Invasive Ductal Carcinoma (IDC) in\nearly stage, which is the most common subtype of breast cancer. Although the\npowerful models in the Computer-Aided Diagnosis (CAD) systems provide promising\nresults, it is still difficult to integrate them into other medical devices or\nuse them without sufficient computation resource. In this paper, we propose\nBCDNet, which firstly upsamples the input image by the residual block and use\nsmaller convolutional block and a special MLP to learn features. BCDNet is\nproofed to effectively detect IDC in histopathological RGB images with an\naverage accuracy of 91.6% and reduce training consumption effectively compared\nto ResNet 50 and ViT-B-16.\n","authors":["Yujia Lin","Aiwei Lian","Mingyu Liao","Shuangjie Yuan"],"pdf_url":"https://arxiv.org/pdf/2408.13800v3.pdf","comment":"5 pages, 3 figures"},{"id":"http://arxiv.org/abs/2411.03313v2","updated":"2024-11-06T12:07:08Z","published":"2024-11-05T18:58:15Z","title":"Classification Done Right for Vision-Language Pre-Training","summary":"  We introduce SuperClass, a super simple classification method for\nvision-language pre-training on image-text data. Unlike its contrastive\ncounterpart CLIP who contrast with a text encoder, SuperClass directly utilizes\ntokenized raw text as supervised classification labels, without the need for\nadditional text filtering or selection. Due to the absence of the text encoding\nas contrastive target, SuperClass does not require a text encoder and does not\nneed to maintain a large batch size as CLIP does. SuperClass demonstrated\nsuperior performance on various downstream tasks, including classic computer\nvision benchmarks and vision language downstream tasks. We further explored the\nscaling behavior of SuperClass on model size, training length, or data size,\nand reported encouraging results and comparisons to CLIP.\nhttps://github.com/x-cls/superclass\n","authors":["Zilong Huang","Qinghao Ye","Bingyi Kang","Jiashi Feng","Haoqi Fan"],"pdf_url":"https://arxiv.org/pdf/2411.03313v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2404.05997v2","updated":"2024-11-06T12:06:03Z","published":"2024-04-09T04:04:50Z","title":"Concept-Attention Whitening for Interpretable Skin Lesion Diagnosis","summary":"  The black-box nature of deep learning models has raised concerns about their\ninterpretability for successful deployment in real-world clinical applications.\nTo address the concerns, eXplainable Artificial Intelligence (XAI) aims to\nprovide clear and understandable explanations of the decision-making process.\nIn the medical domain, concepts such as attributes of lesions or abnormalities\nserve as key evidence for deriving diagnostic results. Existing concept-based\nmodels mainly depend on concepts that appear independently and require\nfine-grained concept annotations such as bounding boxes. However, a medical\nimage usually contains multiple concepts, and the fine-grained concept\nannotations are difficult to acquire. In this paper, we aim to interpret\nrepresentations in deep neural networks by aligning the axes of the latent\nspace with known concepts of interest. We propose a novel Concept-Attention\nWhitening (CAW) framework for interpretable skin lesion diagnosis. CAW is\ncomprised of a disease diagnosis branch and a concept alignment branch. In the\nformer branch, we train a convolutional neural network (CNN) with an inserted\nCAW layer to perform skin lesion diagnosis. The CAW layer decorrelates features\nand aligns image features to conceptual meanings via an orthogonal matrix. In\nthe latter branch, the orthogonal matrix is calculated under the guidance of\nthe concept attention mask. We particularly introduce a weakly-supervised\nconcept mask generator that only leverages coarse concept labels for filtering\nlocal regions that are relevant to certain concepts, improving the optimization\nof the orthogonal matrix. Extensive experiments on two public skin lesion\ndiagnosis datasets demonstrated that CAW not only enhanced interpretability but\nalso maintained a state-of-the-art diagnostic performance.\n","authors":["Junlin Hou","Jilan Xu","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2404.05997v2.pdf","comment":"MICCAI 2024"},{"id":"http://arxiv.org/abs/2410.11666v3","updated":"2024-11-06T12:00:44Z","published":"2024-10-15T14:53:07Z","title":"Degradation Oriented and Regularized Network for Blind Depth\n  Super-Resolution","summary":"  Recent RGB-guided depth super-resolution methods have achieved impressive\nperformance under the assumption of fixed and known degradation (e.g., bicubic\ndownsampling). However, in real-world scenarios, captured depth data often\nsuffer from unconventional and unknown degradation due to sensor limitations\nand complex imaging environments (e.g., low reflective surfaces, varying\nillumination). Consequently, the performance of these methods significantly\ndeclines when real-world degradation deviate from their assumptions. In this\npaper, we propose the Degradation Oriented and Regularized Network (DORNet), a\nnovel framework designed to adaptively address unknown degradation in\nreal-world scenes through implicit degradation representations. Our approach\nbegins with the development of a self-supervised degradation learning strategy,\nwhich models the degradation representations of low-resolution depth data using\nrouting selection-based degradation regularization. To facilitate effective\nRGB-D fusion, we further introduce a degradation-oriented feature\ntransformation module that selectively propagates RGB content into the depth\ndata based on the learned degradation priors. Extensive experimental results on\nboth real and synthetic datasets demonstrate the superiority of our DORNet in\nhandling unknown degradation, outperforming existing methods. The code is\navailable at https://github.com/yanzq95/DORNet.\n","authors":["Zhengxue Wang","Zhiqiang Yan","Jinshan Pan","Guangwei Gao","Kai Zhang","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2410.11666v3.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2411.03855v1","updated":"2024-11-06T11:57:55Z","published":"2024-11-06T11:57:55Z","title":"MambaPEFT: Exploring Parameter-Efficient Fine-Tuning for Mamba","summary":"  An ecosystem of Transformer-based models has been established by building\nlarge models with extensive data. Parameter-efficient fine-tuning (PEFT) is a\ncrucial technology for deploying these models to downstream tasks with minimal\ncost while achieving effective performance. Recently, Mamba, a State Space\nModel (SSM)-based model, has attracted attention as a potential alternative to\nTransformers. While many large-scale Mamba-based models have been proposed,\nefficiently adapting pre-trained Mamba-based models to downstream tasks remains\nunexplored. In this paper, we conduct an exploratory analysis of PEFT methods\nfor Mamba. We investigate the effectiveness of existing PEFT methods for\nTransformers when applied to Mamba. We also modify these methods to better\nalign with the Mamba architecture. Additionally, we propose new Mamba-specific\nPEFT methods that leverage the distinctive structure of Mamba. Our experiments\nindicate that PEFT performs more effectively for Mamba than Transformers.\nLastly, we demonstrate how to effectively combine multiple PEFT methods and\nprovide a framework that outperforms previous works. To ensure reproducibility,\nwe will release the code after publication.\n","authors":["Masakazu Yoshimura","Teruaki Hayashi","Yota Maeda"],"pdf_url":"https://arxiv.org/pdf/2411.03855v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.11124v2","updated":"2024-11-06T11:40:50Z","published":"2024-01-20T05:31:47Z","title":"Cross-Task Affinity Learning for Multitask Dense Scene Predictions","summary":"  Multitask learning (MTL) has become prominent for its ability to predict\nmultiple tasks jointly, achieving better per-task performance with fewer\nparameters than single-task learning. Recently, decoder-focused architectures\nhave significantly improved multitask performance by refining task predictions\nusing features from related tasks. However, most refinement methods struggle to\nefficiently capture both local and long-range dependencies between\ntask-specific representations and cross-task patterns. In this paper, we\nintroduce the Cross-Task Affinity Learning (CTAL) module, a lightweight\nframework that enhances task refinement in multitask networks. CTAL effectively\ncaptures local and long-range cross-task interactions by optimizing task\naffinity matrices for parameter-efficient grouped convolutions without concern\nfor information loss. Our results demonstrate state-of-the-art MTL performance\nfor both CNN and transformer backbones, using significantly fewer parameters\nthan single-task learning. Our code is publicly available at\nhttps://github.com/Armanfard-Lab/EMA-Net.\n","authors":["Dimitrios Sinodinos","Narges Armanfard"],"pdf_url":"https://arxiv.org/pdf/2401.11124v2.pdf","comment":"Accepted for publication at the IEEE Winter Conference on\n  Applications of Computer Vision (WACV) 2025"},{"id":"http://arxiv.org/abs/2411.03835v1","updated":"2024-11-06T11:14:49Z","published":"2024-11-06T11:14:49Z","title":"An Edge Computing-Based Solution for Real-Time Leaf Disease\n  Classification using Thermal Imaging","summary":"  Deep learning (DL) technologies can transform agriculture by improving crop\nhealth monitoring and management, thus improving food safety. In this paper, we\nexplore the potential of edge computing for real-time classification of leaf\ndiseases using thermal imaging. We present a thermal image dataset for plant\ndisease classification and evaluate deep learning models, including\nInceptionV3, MobileNetV1, MobileNetV2, and VGG-16, on resource-constrained\ndevices like the Raspberry Pi 4B. Using pruning and quantization-aware\ntraining, these models achieve inference times up to 1.48x faster on Edge TPU\nMax for VGG16, and up to 2.13x faster with precision reduction on Intel NCS2\nfor MobileNetV1, compared to high-end GPUs like the RTX 3090, while maintaining\nstate-of-the-art accuracy.\n","authors":["Públio Elon Correa da Silva","Jurandy Almeida"],"pdf_url":"https://arxiv.org/pdf/2411.03835v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03831v1","updated":"2024-11-06T11:03:34Z","published":"2024-11-06T11:03:34Z","title":"An Enhancement of Haar Cascade Algorithm Applied to Face Recognition for\n  Gate Pass Security","summary":"  This study is focused on enhancing the Haar Cascade Algorithm to decrease the\nfalse positive and false negative rate in face matching and face detection to\nincrease the accuracy rate even under challenging conditions. The face\nrecognition library was implemented with Haar Cascade Algorithm in which the\n128-dimensional vectors representing the unique features of a face are encoded.\nA subprocess was applied where the grayscale image from Haar Cascade was\nconverted to RGB to improve the face encoding. Logical process and face\nfiltering are also used to decrease non-face detection. The Enhanced Haar\nCascade Algorithm produced a 98.39% accuracy rate (21.39% increase), 63.59%\nprecision rate, 98.30% recall rate, and 72.23% in F1 Score. In comparison, the\nHaar Cascade Algorithm achieved a 46.70% to 77.00% accuracy rate, 44.15%\nprecision rate, 98.61% recall rate, and 47.01% in F1 Score. Both algorithms\nused the Confusion Matrix Test with 301,950 comparisons using the same dataset\nof 550 images. The 98.39% accuracy rate shows a significant decrease in false\npositive and false negative rates in facial recognition. Face matching and face\ndetection are more accurate in images with complex backgrounds, lighting\nvariations, and occlusions, or even those with similar attributes.\n","authors":["Clarence A. Antipona","Romeo R. Magsino","Raymund M. Dioses","Khatalyn E. Mata"],"pdf_url":"https://arxiv.org/pdf/2411.03831v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03829v1","updated":"2024-11-06T11:03:02Z","published":"2024-11-06T11:03:02Z","title":"Generalize or Detect? Towards Robust Semantic Segmentation Under\n  Multiple Distribution Shifts","summary":"  In open-world scenarios, where both novel classes and domains may exist, an\nideal segmentation model should detect anomaly classes for safety and\ngeneralize to new domains. However, existing methods often struggle to\ndistinguish between domain-level and semantic-level distribution shifts,\nleading to poor out-of-distribution (OOD) detection or domain generalization\nperformance. In this work, we aim to equip the model to generalize effectively\nto covariate-shift regions while precisely identifying semantic-shift regions.\nTo achieve this, we design a novel generative augmentation method to produce\ncoherent images that incorporate both anomaly (or novel) objects and various\ncovariate shifts at both image and object levels. Furthermore, we introduce a\ntraining strategy that recalibrates uncertainty specifically for semantic\nshifts and enhances the feature extractor to align features associated with\ndomain shifts. We validate the effectiveness of our method across benchmarks\nfeaturing both semantic and domain shifts. Our method achieves state-of-the-art\nperformance across all benchmarks for both OOD detection and domain\ngeneralization. Code is available at\nhttps://github.com/gaozhitong/MultiShiftSeg.\n","authors":["Zhitong Gao","Bingnan Li","Mathieu Salzmann","Xuming He"],"pdf_url":"https://arxiv.org/pdf/2411.03829v1.pdf","comment":"Published in NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.03823v1","updated":"2024-11-06T10:44:15Z","published":"2024-11-06T10:44:15Z","title":"Both Text and Images Leaked! A Systematic Analysis of Multimodal LLM\n  Data Contamination","summary":"  The rapid progression of multimodal large language models (MLLMs) has\ndemonstrated superior performance on various multimodal benchmarks. However,\nthe issue of data contamination during training creates challenges in\nperformance evaluation and comparison. While numerous methods exist for\ndetecting dataset contamination in large language models (LLMs), they are less\neffective for MLLMs due to their various modalities and multiple training\nphases. In this study, we introduce a multimodal data contamination detection\nframework, MM-Detect, designed for MLLMs. Our experimental results indicate\nthat MM-Detect is sensitive to varying degrees of contamination and can\nhighlight significant performance improvements due to leakage of the training\nset of multimodal benchmarks. Furthermore, We also explore the possibility of\ncontamination originating from the pre-training phase of LLMs used by MLLMs and\nthe fine-tuning phase of MLLMs, offering new insights into the stages at which\ncontamination may be introduced.\n","authors":["Dingjie Song","Sicheng Lai","Shunian Chen","Lichao Sun","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2411.03823v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03819v1","updated":"2024-11-06T10:39:00Z","published":"2024-11-06T10:39:00Z","title":"SA3DIP: Segment Any 3D Instance with Potential 3D Priors","summary":"  The proliferation of 2D foundation models has sparked research into adapting\nthem for open-world 3D instance segmentation. Recent methods introduce a\nparadigm that leverages superpoints as geometric primitives and incorporates 2D\nmulti-view masks from Segment Anything model (SAM) as merging guidance,\nachieving outstanding zero-shot instance segmentation results. However, the\nlimited use of 3D priors restricts the segmentation performance. Previous\nmethods calculate the 3D superpoints solely based on estimated normal from\nspatial coordinates, resulting in under-segmentation for instances with similar\ngeometry. Besides, the heavy reliance on SAM and hand-crafted algorithms in 2D\nspace suffers from over-segmentation due to SAM's inherent part-level\nsegmentation tendency. To address these issues, we propose SA3DIP, a novel\nmethod for Segmenting Any 3D Instances via exploiting potential 3D Priors.\nSpecifically, on one hand, we generate complementary 3D primitives based on\nboth geometric and textural priors, which reduces the initial errors that\naccumulate in subsequent procedures. On the other hand, we introduce\nsupplemental constraints from the 3D space by using a 3D detector to guide a\nfurther merging process. Furthermore, we notice a considerable portion of\nlow-quality ground truth annotations in ScanNetV2 benchmark, which affect the\nfair evaluations. Thus, we present ScanNetV2-INS with complete ground truth\nlabels and supplement additional instances for 3D class-agnostic instance\nsegmentation. Experimental evaluations on various 2D-3D datasets demonstrate\nthe effectiveness and robustness of our approach. Our code and proposed\nScanNetV2-INS dataset are available HERE.\n","authors":["Xi Yang","Xu Gu","Xingyilang Yin","Xinbo Gao"],"pdf_url":"https://arxiv.org/pdf/2411.03819v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03807v1","updated":"2024-11-06T10:07:46Z","published":"2024-11-06T10:07:46Z","title":"GS2Pose: Tow-stage 6D Object Pose Estimation Guided by Gaussian\n  Splatting","summary":"  This paper proposes a new method for accurate and robust 6D pose estimation\nof novel objects, named GS2Pose. By introducing 3D Gaussian splatting, GS2Pose\ncan utilize the reconstruction results without requiring a high-quality CAD\nmodel, which means it only requires segmented RGBD images as input.\nSpecifically, GS2Pose employs a two-stage structure consisting of coarse\nestimation followed by refined estimation. In the coarse stage, a lightweight\nU-Net network with a polarization attention mechanism, called Pose-Net, is\ndesigned. By using the 3DGS model for supervised training, Pose-Net can\ngenerate NOCS images to compute a coarse pose. In the refinement stage, GS2Pose\nformulates a pose regression algorithm following the idea of reprojection or\nBundle Adjustment (BA), referred to as GS-Refiner. By leveraging Lie algebra to\nextend 3DGS, GS-Refiner obtains a pose-differentiable rendering pipeline that\nrefines the coarse pose by comparing the input images with the rendered images.\nGS-Refiner also selectively updates parameters in the 3DGS model to achieve\nenvironmental adaptation, thereby enhancing the algorithm's robustness and\nflexibility to illuminative variation, occlusion, and other challenging\ndisruptive factors. GS2Pose was evaluated through experiments conducted on the\nLineMod dataset, where it was compared with similar algorithms, yielding highly\ncompetitive results. The code for GS2Pose will soon be released on GitHub.\n","authors":["Jilan Mei","Junbo Li","Cai Meng"],"pdf_url":"https://arxiv.org/pdf/2411.03807v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17459v2","updated":"2024-11-06T09:50:06Z","published":"2024-09-26T01:34:42Z","title":"TFS-NeRF: Template-Free NeRF for Semantic 3D Reconstruction of Dynamic\n  Scene","summary":"  Despite advancements in Neural Implicit models for 3D surface reconstruction,\nhandling dynamic environments with arbitrary rigid, non-rigid, or deformable\nentities remains challenging. Many template-based methods are entity-specific,\nfocusing on humans, while generic reconstruction methods adaptable to such\ndynamic scenes often require additional inputs like depth or optical flow or\nrely on pre-trained image features for reasonable outcomes. These methods\ntypically use latent codes to capture frame-by-frame deformations. In contrast,\nsome template-free methods bypass these requirements and adopt traditional LBS\n(Linear Blend Skinning) weights for a detailed representation of deformable\nobject motions, although they involve complex optimizations leading to lengthy\ntraining times. To this end, as a remedy, this paper introduces TFS-NeRF, a\ntemplate-free 3D semantic NeRF for dynamic scenes captured from sparse or\nsingle-view RGB videos, featuring interactions among various entities and more\ntime-efficient than other LBS-based approaches. Our framework uses an\nInvertible Neural Network (INN) for LBS prediction, simplifying the training\nprocess. By disentangling the motions of multiple entities and optimizing\nper-entity skinning weights, our method efficiently generates accurate,\nsemantically separable geometries. Extensive experiments demonstrate that our\napproach produces high-quality reconstructions of both deformable and\nnon-deformable objects in complex interactions, with improved training\nefficiency compared to existing methods.\n","authors":["Sandika Biswas","Qianyi Wu","Biplab Banerjee","Hamid Rezatofighi"],"pdf_url":"https://arxiv.org/pdf/2409.17459v2.pdf","comment":"Accepted in NeurIPS 2024"},{"id":"http://arxiv.org/abs/2405.15306v3","updated":"2024-11-06T09:49:31Z","published":"2024-05-24T07:48:35Z","title":"DeTikZify: Synthesizing Graphics Programs for Scientific Figures and\n  Sketches with TikZ","summary":"  Creating high-quality scientific figures can be time-consuming and\nchallenging, even though sketching ideas on paper is relatively easy.\nFurthermore, recreating existing figures that are not stored in formats\npreserving semantic information is equally complex. To tackle this problem, we\nintroduce DeTikZify, a novel multimodal language model that automatically\nsynthesizes scientific figures as semantics-preserving TikZ graphics programs\nbased on sketches and existing figures. To achieve this, we create three new\ndatasets: DaTikZv2, the largest TikZ dataset to date, containing over 360k\nhuman-created TikZ graphics; SketchFig, a dataset that pairs hand-drawn\nsketches with their corresponding scientific figures; and MetaFig, a collection\nof diverse scientific figures and associated metadata. We train DeTikZify on\nMetaFig and DaTikZv2, along with synthetically generated sketches learned from\nSketchFig. We also introduce an MCTS-based inference algorithm that enables\nDeTikZify to iteratively refine its outputs without the need for additional\ntraining. Through both automatic and human evaluation, we demonstrate that\nDeTikZify outperforms commercial Claude 3 and GPT-4V in synthesizing TikZ\nprograms, with the MCTS algorithm effectively boosting its performance. We make\nour code, models, and datasets publicly available.\n","authors":["Jonas Belouadi","Simone Paolo Ponzetto","Steffen Eger"],"pdf_url":"https://arxiv.org/pdf/2405.15306v3.pdf","comment":"Accepted at NeurIPS 2024 (spotlight); Project page:\n  https://github.com/potamides/DeTikZify"},{"id":"http://arxiv.org/abs/2411.03795v1","updated":"2024-11-06T09:39:52Z","published":"2024-11-06T09:39:52Z","title":"VQA$^2$:Visual Question Answering for Video Quality Assessment","summary":"  The advent and proliferation of large multi-modal models (LMMs) have\nintroduced a new paradigm to video-related computer vision fields, including\ntraining and inference methods based on visual question answering (VQA). These\nmethods enable models to handle multiple downstream tasks robustly. Video\nQuality Assessment (VQA), a classic field in low-level visual quality\nevaluation, originally focused on quantitative video quality scoring. However,\ndriven by advances in LMMs, it is now evolving towards more comprehensive\nvisual quality understanding tasks. Visual question answering has significantly\nimproved low-level visual evaluation within the image domain recently. However,\nrelated work is almost nonexistent in the video domain, leaving substantial\nroom for improvement. To address this gap, we introduce the VQA2 Instruction\nDataset the first visual question answering instruction dataset entirely\nfocuses on video quality assessment, and based on it, we propose the VQA2\nseries models The VQA2 Instruction Dataset consists of three stages and covers\nvarious video types, containing 157,735 instruction question-answer pairs,\nincluding both manually annotated and synthetic data. We conduct extensive\nexperiments on both video quality scoring and video quality understanding\ntasks. Results demonstrate that the VQA2 series models achieve state-of-the-art\n(SOTA) performance in quality scoring tasks, and their performance in visual\nquality question answering surpasses the renowned GPT-4o. Additionally, our\nfinal model, the VQA2-Assistant, performs well across both scoring and\nquestion-answering tasks, validating its versatility.\n","authors":["Ziheng Jia","Zicheng Zhang","Jiaying Qian","Haoning Wu","Wei Sun","Chunyi Li","Xiaohong Liu","Weisi Lin","Guangtao Zhai","Xiongkuo Min"],"pdf_url":"https://arxiv.org/pdf/2411.03795v1.pdf","comment":"10 pages 3 figures"},{"id":"http://arxiv.org/abs/2411.03794v1","updated":"2024-11-06T09:39:25Z","published":"2024-11-06T09:39:25Z","title":"Harmformer: Harmonic Networks Meet Transformers for Continuous\n  Roto-Translation Equivariance","summary":"  CNNs exhibit inherent equivariance to image translation, leading to efficient\nparameter and data usage, faster learning, and improved robustness. The concept\nof translation equivariant networks has been successfully extended to rotation\ntransformation using group convolution for discrete rotation groups and\nharmonic functions for the continuous rotation group encompassing $360^\\circ$.\nWe explore the compatibility of the SA mechanism with full rotation\nequivariance, in contrast to previous studies that focused on discrete\nrotation. We introduce the Harmformer, a harmonic transformer with a\nconvolutional stem that achieves equivariance for both translation and\ncontinuous rotation. Accompanied by an end-to-end equivariance proof, the\nHarmformer not only outperforms previous equivariant transformers, but also\ndemonstrates inherent stability under any continuous rotation, even without\nseeing rotated samples during training.\n","authors":["Tomáš Karella","Adam Harmanec","Jan Kotera","Jan Blažek","Filip Šroubek"],"pdf_url":"https://arxiv.org/pdf/2411.03794v1.pdf","comment":"Appears in NeurIPS 2024 Workshop on Symmetry and Geometry in Neural\n  Representations"},{"id":"http://arxiv.org/abs/2404.03202v5","updated":"2024-11-06T09:26:23Z","published":"2024-04-04T05:10:26Z","title":"OmniGS: Fast Radiance Field Reconstruction using Omnidirectional\n  Gaussian Splatting","summary":"  Photorealistic reconstruction relying on 3D Gaussian Splatting has shown\npromising potential in various domains. However, the current 3D Gaussian\nSplatting system only supports radiance field reconstruction using undistorted\nperspective images. In this paper, we present OmniGS, a novel omnidirectional\nGaussian splatting system, to take advantage of omnidirectional images for fast\nradiance field reconstruction. Specifically, we conduct a theoretical analysis\nof spherical camera model derivatives in 3D Gaussian Splatting. According to\nthe derivatives, we then implement a new GPU-accelerated omnidirectional\nrasterizer that directly splats 3D Gaussians onto the equirectangular screen\nspace for omnidirectional image rendering. We realize differentiable\noptimization of the omnidirectional radiance field without the requirement of\ncube-map rectification or tangent-plane approximation. Extensive experiments\nconducted in egocentric and roaming scenarios demonstrate that our method\nachieves state-of-the-art reconstruction quality and high rendering speed using\nomnidirectional images. The code will be publicly available.\n","authors":["Longwei Li","Huajian Huang","Sai-Kit Yeung","Hui Cheng"],"pdf_url":"https://arxiv.org/pdf/2404.03202v5.pdf","comment":"8 pages, 6 figures, accepted by WACV 2025, project page:\n  https://liquorleaf.github.io/research/OmniGS/"},{"id":"http://arxiv.org/abs/2411.03223v2","updated":"2024-11-06T09:10:46Z","published":"2024-11-05T16:12:12Z","title":"Beyond Grid Data: Exploring Graph Neural Networks for Earth Observation","summary":"  Earth Observation (EO) data analysis has been significantly revolutionized by\ndeep learning (DL), with applications typically limited to grid-like data\nstructures. Graph Neural Networks (GNNs) emerge as an important innovation,\npropelling DL into the non-Euclidean domain. Naturally, GNNs can effectively\ntackle the challenges posed by diverse modalities, multiple sensors, and the\nheterogeneous nature of EO data. To introduce GNNs in the related domains, our\nreview begins by offering fundamental knowledge on GNNs. Then, we summarize the\ngeneric problems in EO, to which GNNs can offer potential solutions. Following\nthis, we explore a broad spectrum of GNNs' applications to scientific problems\nin Earth systems, covering areas such as weather and climate analysis, disaster\nmanagement, air quality monitoring, agriculture, land cover classification,\nhydrological process modeling, and urban modeling. The rationale behind\nadopting GNNs in these fields is explained, alongside methodologies for\norganizing graphs and designing favorable architectures for various tasks.\nFurthermore, we highlight methodological challenges of implementing GNNs in\nthese domains and possible solutions that could guide future research. While\nacknowledging that GNNs are not a universal solution, we conclude the paper by\ncomparing them with other popular architectures like transformers and analyzing\ntheir potential synergies.\n","authors":["Shan Zhao","Zhaiyu Chen","Zhitong Xiong","Yilei Shi","Sudipan Saha","Xiao Xiang Zhu"],"pdf_url":"https://arxiv.org/pdf/2411.03223v2.pdf","comment":"Accepted for publication in Geoscience and Remote Sensing Magazine\n  (GRSM)"},{"id":"http://arxiv.org/abs/2404.09633v2","updated":"2024-11-06T09:04:35Z","published":"2024-04-15T10:05:36Z","title":"In-Context Translation: Towards Unifying Image Recognition, Processing,\n  and Generation","summary":"  We propose In-Context Translation (ICT), a general learning framework to\nunify visual recognition (e.g., semantic segmentation), low-level image\nprocessing (e.g., denoising), and conditional image generation (e.g.,\nedge-to-image synthesis). Thanks to unification, ICT significantly reduces the\ninherent inductive bias that comes with designing models for specific tasks,\nand it maximizes mutual enhancement across similar tasks. However, the\nunification across a large number of tasks is non-trivial due to various data\nformats and training pipelines. To this end, ICT introduces two designs.\nFirstly, it standardizes input-output data of different tasks into RGB image\npairs, e.g., semantic segmentation data pairs an RGB image with its\nsegmentation mask in the same RGB format. This turns different tasks into a\ngeneral translation task between two RGB images. Secondly, it standardizes the\ntraining of different tasks into a general in-context learning, where\n\"in-context\" means the input comprises an example input-output pair of the\ntarget task and a query image. The learning objective is to generate the\n\"missing\" data paired with the query. The implicit translation process is thus\nbetween the query and the generated image. In experiments, ICT unifies ten\nvision tasks and showcases impressive performance on their respective\nbenchmarks. Notably, ICT performs well across three major categories of\ncomputer vision tasks, while its two competitors (Painter and PromptDiffusion)\nare only effective in at most two of these task categories. In addition,\ncompared to its competitors, ICT trained on only 4 RTX 3090 GPUs is shown to be\nmore efficient and less costly in training.\n","authors":["Han Xue","Qianru Sun","Li Song","Wenjun Zhang","Zhiwu Huang"],"pdf_url":"https://arxiv.org/pdf/2404.09633v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03758v1","updated":"2024-11-06T08:33:07Z","published":"2024-11-06T08:33:07Z","title":"Sub-DM:Subspace Diffusion Model with Orthogonal Decomposition for MRI\n  Reconstruction","summary":"  Diffusion model-based approaches recently achieved re-markable success in MRI\nreconstruction, but integration into clinical routine remains challenging due\nto its time-consuming convergence. This phenomenon is partic-ularly notable\nwhen directly apply conventional diffusion process to k-space data without\nconsidering the inherent properties of k-space sampling, limiting k-space\nlearning efficiency and image reconstruction quality. To tackle these\nchallenges, we introduce subspace diffusion model with orthogonal\ndecomposition, a method (referred to as Sub-DM) that restrict the diffusion\nprocess via projections onto subspace as the k-space data distribution evolves\ntoward noise. Particularly, the subspace diffusion model circumvents the\ninference challenges posed by the com-plex and high-dimensional characteristics\nof k-space data, so the highly compact subspace ensures that diffusion process\nrequires only a few simple iterations to produce accurate prior information.\nFurthermore, the orthogonal decomposition strategy based on wavelet transform\nhin-ders the information loss during the migration of the vanilla diffusion\nprocess to the subspace. Considering the strate-gy is approximately reversible,\nsuch that the entire pro-cess can be reversed. As a result, it allows the\ndiffusion processes in different spaces to refine models through a mutual\nfeedback mechanism, enabling the learning of ac-curate prior even when dealing\nwith complex k-space data. Comprehensive experiments on different datasets\nclearly demonstrate that the superiority of Sub-DM against state of-the-art\nmethods in terms of reconstruction speed and quality.\n","authors":["Yu Guan","Qinrong Cai","Wei Li","Qiuyun Fan","Dong Liang","Qiegen Liu"],"pdf_url":"https://arxiv.org/pdf/2411.03758v1.pdf","comment":"10 pages, 11 figures"},{"id":"http://arxiv.org/abs/2410.13824v3","updated":"2024-11-06T08:29:22Z","published":"2024-10-17T17:48:54Z","title":"Harnessing Webpage UIs for Text-Rich Visual Understanding","summary":"  Text-rich visual understanding-the ability to process environments where\ndense textual content is integrated with visuals-is crucial for multimodal\nlarge language models (MLLMs) to interact effectively with structured\nenvironments. To enhance this capability, we propose synthesizing general\nmultimodal instructions from webpage UIs using text-based large language models\n(LLMs). Despite lacking direct visual input, text-based LLMs are able to\nprocess structured text representations from webpage accessibility trees. These\ninstructions are then paired with UI screenshots to train multimodal models. We\nintroduce MultiUI, a dataset containing 7.3 million samples from 1 million\nwebsites, covering diverse multimodal tasks and UI layouts. Models trained on\nMultiUI not only excel in web UI tasks-achieving up to a 48% improvement on\nVisualWebBench and a 19.1% boost in element accuracy on a web agent dataset\nMind2Web-but also generalize surprisingly well to non-web UI tasks and even to\nnon-UI domains, such as document understanding, OCR, and chart interpretation.\nThese results highlight the broad applicability of web UI data for advancing\ntext-rich visual understanding across various scenarios.\n","authors":["Junpeng Liu","Tianyue Ou","Yifan Song","Yuxiao Qu","Wai Lam","Chenyan Xiong","Wenhu Chen","Graham Neubig","Xiang Yue"],"pdf_url":"https://arxiv.org/pdf/2410.13824v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03752v1","updated":"2024-11-06T08:27:49Z","published":"2024-11-06T08:27:49Z","title":"Deferred Poisoning: Making the Model More Vulnerable via Hessian\n  Singularization","summary":"  Recent studies have shown that deep learning models are very vulnerable to\npoisoning attacks. Many defense methods have been proposed to address this\nissue. However, traditional poisoning attacks are not as threatening as\ncommonly believed. This is because they often cause differences in how the\nmodel performs on the training set compared to the validation set. Such\ninconsistency can alert defenders that their data has been poisoned, allowing\nthem to take the necessary defensive actions. In this paper, we introduce a\nmore threatening type of poisoning attack called the Deferred Poisoning Attack.\nThis new attack allows the model to function normally during the training and\nvalidation phases but makes it very sensitive to evasion attacks or even\nnatural noise. We achieve this by ensuring the poisoned model's loss function\nhas a similar value as a normally trained model at each input sample but with a\nlarge local curvature. A similar model loss ensures that there is no obvious\ninconsistency between the training and validation accuracy, demonstrating high\nstealthiness. On the other hand, the large curvature implies that a small\nperturbation may cause a significant increase in model loss, leading to\nsubstantial performance degradation, which reflects a worse robustness. We\nfulfill this purpose by making the model have singular Hessian information at\nthe optimal point via our proposed Singularization Regularization term. We have\nconducted both theoretical and empirical analyses of the proposed method and\nvalidated its effectiveness through experiments on image classification tasks.\nFurthermore, we have confirmed the hazards of this form of poisoning attack\nunder more general scenarios using natural noise, offering a new perspective\nfor research in the field of security.\n","authors":["Yuhao He","Jinyu Tian","Xianwei Zheng","Li Dong","Yuanman Li","Leo Yu Zhang","Jiantao Zhou"],"pdf_url":"https://arxiv.org/pdf/2411.03752v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17331v2","updated":"2024-11-06T08:22:33Z","published":"2024-07-24T14:54:16Z","title":"Multi-label Cluster Discrimination for Visual Representation Learning","summary":"  Contrastive Language Image Pre-training (CLIP) has recently demonstrated\nsuccess across various tasks due to superior feature representation empowered\nby image-text contrastive learning. However, the instance discrimination method\nused by CLIP can hardly encode the semantic structure of training data. To\nhandle this limitation, cluster discrimination has been proposed through\niterative cluster assignment and classification. Nevertheless, most cluster\ndiscrimination approaches only define a single pseudo-label for each image,\nneglecting multi-label signals in the image. In this paper, we propose a novel\nMulti-Label Cluster Discrimination method named MLCD to enhance representation\nlearning. In the clustering step, we first cluster the large-scale LAION-400M\ndataset into one million centers based on off-the-shelf embedding features.\nConsidering that natural images frequently contain multiple visual objects or\nattributes, we select the multiple closest centers as auxiliary class labels.\nIn the discrimination step, we design a novel multi-label classification loss,\nwhich elegantly separates losses from positive classes and negative classes,\nand alleviates ambiguity on decision boundary. We validate the proposed\nmulti-label cluster discrimination method with experiments on different scales\nof models and pre-training datasets. Experimental results show that our method\nachieves state-of-the-art performance on multiple downstream tasks including\nlinear probe, zero-shot classification, and image-text retrieval. Code and\nmodels have been released at https://github.com/deepglint/unicom .\n","authors":["Xiang An","Kaicheng Yang","Xiangzi Dai","Ziyong Feng","Jiankang Deng"],"pdf_url":"https://arxiv.org/pdf/2407.17331v2.pdf","comment":"Accepted by ECCV2024"},{"id":"http://arxiv.org/abs/2411.03745v1","updated":"2024-11-06T08:22:00Z","published":"2024-11-06T08:22:00Z","title":"Homotopy Continuation Made Easy: Regression-based Online Simulation of\n  Starting Problem-Solution Pairs","summary":"  While automatically generated polynomial elimination templates have sparked\ngreat progress in the field of 3D computer vision, there remain many problems\nfor which the degree of the constraints or the number of unknowns leads to\nintractability. In recent years, homotopy continuation has been introduced as a\nplausible alternative. However, the method currently depends on expensive\nparallel tracking of all possible solutions in the complex domain, or a\nclassification network for starting problem-solution pairs trained over a\nlimited set of real-world examples. Our innovation consists of employing a\nregression network trained in simulation to directly predict a solution from\ninput correspondences, followed by an online simulator that invents a\nconsistent problem-solution pair. Subsequently, homotopy continuation is\napplied to track that single solution back to the original problem. We apply\nthis elegant combination to generalized camera resectioning, and also introduce\na new solution to the challenging generalized relative pose and scale problem.\nAs demonstrated, the proposed method successfully compensates the raw error\ncommitted by the regressor alone, and leads to state-of-the-art efficiency and\nsuccess rates while running on CPU resources, only.\n","authors":["Xinyue Zhang","Zijia Dai","Wanting Xu","Laurent Kneip"],"pdf_url":"https://arxiv.org/pdf/2411.03745v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03730v1","updated":"2024-11-06T07:51:19Z","published":"2024-11-06T07:51:19Z","title":"NeurIPS 2023 Competition: Privacy Preserving Federated Learning Document\n  VQA","summary":"  The Privacy Preserving Federated Learning Document VQA (PFL-DocVQA)\ncompetition challenged the community to develop provably private and\ncommunication-efficient solutions in a federated setting for a real-life use\ncase: invoice processing. The competition introduced a dataset of real invoice\ndocuments, along with associated questions and answers requiring information\nextraction and reasoning over the document images. Thereby, it brings together\nresearchers and expertise from the document analysis, privacy, and federated\nlearning communities. Participants fine-tuned a pre-trained, state-of-the-art\nDocument Visual Question Answering model provided by the organizers for this\nnew domain, mimicking a typical federated invoice processing setup. The base\nmodel is a multi-modal generative language model, and sensitive information\ncould be exposed through either the visual or textual input modality.\nParticipants proposed elegant solutions to reduce communication costs while\nmaintaining a minimum utility threshold in track 1 and to protect all\ninformation from each document provider using differential privacy in track 2.\nThe competition served as a new testbed for developing and testing private\nfederated learning methods, simultaneously raising awareness about privacy\nwithin the document image analysis and recognition community. Ultimately, the\ncompetition analysis provides best practices and recommendations for\nsuccessfully running privacy-focused federated learning challenges in the\nfuture.\n","authors":["Marlon Tobaben","Mohamed Ali Souibgui","Rubèn Tito","Khanh Nguyen","Raouf Kerkouche","Kangsoo Jung","Joonas Jälkö","Lei Kang","Andrey Barsky","Vincent Poulain d'Andecy","Aurélie Joseph","Aashiq Muhamed","Kevin Kuo","Virginia Smith","Yusuke Yamasaki","Takumi Fukami","Kenta Niwa","Iifan Tyou","Hiro Ishii","Rio Yokota","Ragul N","Rintu Kutum","Josep Llados","Ernest Valveny","Antti Honkela","Mario Fritz","Dimosthenis Karatzas"],"pdf_url":"https://arxiv.org/pdf/2411.03730v1.pdf","comment":"27 pages, 6 figures"},{"id":"http://arxiv.org/abs/2411.03729v1","updated":"2024-11-06T07:48:30Z","published":"2024-11-06T07:48:30Z","title":"Relation Learning and Aggregate-attention for Multi-person Motion\n  Prediction","summary":"  Multi-person motion prediction is an emerging and intricate task with broad\nreal-world applications. Unlike single person motion prediction, it considers\nnot just the skeleton structures or human trajectories but also the\ninteractions between others. Previous methods use various networks to achieve\nimpressive predictions but often overlook that the joints relations within an\nindividual (intra-relation) and interactions among groups (inter-relation) are\ndistinct types of representations. These methods often lack explicit\nrepresentation of inter&intra-relations, and inevitably introduce undesired\ndependencies. To address this issue, we introduce a new collaborative framework\nfor multi-person motion prediction that explicitly modeling these relations:a\nGCN-based network for intra-relations and a novel reasoning network for\ninter-relations.Moreover, we propose a novel plug-and-play aggregation module\ncalled the Interaction Aggregation Module (IAM), which employs an\naggregate-attention mechanism to seamlessly integrate these relations.\nExperiments indicate that the module can also be applied to other dual-path\nmodels. Extensive experiments on the 3DPW, 3DPW-RC, CMU-Mocap, MuPoTS-3D, as\nwell as synthesized datasets Mix1 & Mix2 (9 to 15 persons), demonstrate that\nour method achieves state-of-the-art performance.\n","authors":["Kehua Qu","Rui Ding","Jin Tang"],"pdf_url":"https://arxiv.org/pdf/2411.03729v1.pdf","comment":"Submitted to IEEE Transactions on Multimedia"},{"id":"http://arxiv.org/abs/2411.03728v1","updated":"2024-11-06T07:46:34Z","published":"2024-11-06T07:46:34Z","title":"Efficient Fourier Filtering Network with Contrastive Learning for\n  UAV-based Unaligned Bi-modal Salient Object Detection","summary":"  Unmanned aerial vehicle (UAV)-based bi-modal salient object detection (BSOD)\naims to segment salient objects in a scene utilizing complementary cues in\nunaligned RGB and thermal image pairs. However, the high computational expense\nof existing UAV-based BSOD models limits their applicability to real-world UAV\ndevices. To address this problem, we propose an efficient Fourier filter\nnetwork with contrastive learning that achieves both real-time and accurate\nperformance. Specifically, we first design a semantic contrastive alignment\nloss to align the two modalities at the semantic level, which facilitates\nmutual refinement in a parameter-free way. Second, inspired by the fast Fourier\ntransform that obtains global relevance in linear complexity, we propose\nsynchronized alignment fusion, which aligns and fuses bi-modal features in the\nchannel and spatial dimensions by a hierarchical filtering mechanism. Our\nproposed model, AlignSal, reduces the number of parameters by 70.0%, decreases\nthe floating point operations by 49.4%, and increases the inference speed by\n152.5% compared to the cutting-edge BSOD model (i.e., MROS). Extensive\nexperiments on the UAV RGB-T 2400 and three weakly aligned datasets demonstrate\nthat AlignSal achieves both real-time inference speed and better performance\nand generalizability compared to sixteen state-of-the-art BSOD models across\nmost evaluation metrics. In addition, our ablation studies further verify\nAlignSal's potential in boosting the performance of existing aligned BSOD\nmodels on UAV-based unaligned data. The code is available at:\nhttps://github.com/JoshuaLPF/AlignSal.\n","authors":["Pengfei Lyu","Pak-Hei Yeung","Xiufei Cheng","Xiaosheng Yu","Chengdong Wu","Jagath C. Rajapakse"],"pdf_url":"https://arxiv.org/pdf/2411.03728v1.pdf","comment":"11 pages, 7 figures"},{"id":"http://arxiv.org/abs/2411.03725v1","updated":"2024-11-06T07:44:04Z","published":"2024-11-06T07:44:04Z","title":"PX2Tooth: Reconstructing the 3D Point Cloud Teeth from a Single\n  Panoramic X-ray","summary":"  Reconstructing the 3D anatomical structures of the oral cavity, which\noriginally reside in the cone-beam CT (CBCT), from a single 2D Panoramic\nX-ray(PX) remains a critical yet challenging task, as it can effectively reduce\nradiation risks and treatment costs during the diagnostic in digital dentistry.\nHowever, current methods are either error-prone or only trained/evaluated on\nsmall-scale datasets (less than 50 cases), resulting in compromised\ntrustworthiness. In this paper, we propose PX2Tooth, a novel approach to\nreconstruct 3D teeth using a single PX image with a two-stage framework. First,\nwe design the PXSegNet to segment the permanent teeth from the PX images,\nproviding clear positional, morphological, and categorical information for each\ntooth. Subsequently, we design a novel tooth generation network (TGNet) that\nlearns to transform random point clouds into 3D teeth. TGNet integrates the\nsegmented patch information and introduces a Prior Fusion Module (PFM) to\nenhance the generation quality, especially in the root apex region. Moreover,\nwe construct a dataset comprising 499 pairs of CBCT and Panoramic X-rays.\nExtensive experiments demonstrate that PX2Tooth can achieve an Intersection\nover Union (IoU) of 0.793, significantly surpassing previous methods,\nunderscoring the great potential of artificial intelligence in digital\ndentistry.\n","authors":["Wen Ma","Huikai Wu","Zikai Xiao","Yang Feng","Jian Wu","Zuozhu Liu"],"pdf_url":"https://arxiv.org/pdf/2411.03725v1.pdf","comment":"Ma W, Wu H, Xiao Z, et al. PX2Tooth: Reconstructing the 3D Point\n  Cloud Teeth from a Single Panoramic X-Ray[C]//International Conference on\n  Medical Image Computing and Computer-Assisted Intervention. Cham: Springer\n  Nature Switzerland, 2024: 411-421"},{"id":"http://arxiv.org/abs/2411.03724v1","updated":"2024-11-06T07:43:40Z","published":"2024-11-06T07:43:40Z","title":"Estimation of Psychosocial Work Environment Exposures Through Video\n  Object Detection. Proof of Concept Using CCTV Footage","summary":"  This paper examines the use of computer vision algorithms to estimate aspects\nof the psychosocial work environment using CCTV footage. We present a proof of\nconcept for a methodology that detects and tracks people in video footage and\nestimates interactions between customers and employees by estimating their\nposes and calculating the duration of their encounters. We propose a pipeline\nthat combines existing object detection and tracking algorithms (YOLOv8 and\nDeepSORT) with pose estimation algorithms (BlazePose) to estimate the number of\ncustomers and employees in the footage as well as the duration of their\nencounters. We use a simple rule-based approach to classify the interactions as\npositive, neutral or negative based on three different criteria: distance,\nduration and pose. The proposed methodology is tested on a small dataset of\nCCTV footage. While the data is quite limited in particular with respect to the\nquality of the footage, we have chosen this case as it represents a typical\nsetting where the method could be applied. The results show that the object\ndetection and tracking part of the pipeline has a reasonable performance on the\ndataset with a high degree of recall and reasonable accuracy. At this stage,\nthe pose estimation is still limited to fully detect the type of interactions\ndue to difficulties in tracking employees in the footage. We conclude that the\nmethod is a promising alternative to self-reported measures of the psychosocial\nwork environment and could be used in future studies to obtain external\nobservations of the work environment.\n","authors":["Claus D. Hansen","Thuy Hai Le","David Campos"],"pdf_url":"https://arxiv.org/pdf/2411.03724v1.pdf","comment":"11 pages, 9 figures, presented at IWOAR 9th International Workshop on\n  Sensor-Based Activity Recognition and Artificial Intelligence, September\n  26-27, Potsdam, Germany"},{"id":"http://arxiv.org/abs/2411.03723v1","updated":"2024-11-06T07:40:27Z","published":"2024-11-06T07:40:27Z","title":"Zero-shot Dynamic MRI Reconstruction with Global-to-local Diffusion\n  Model","summary":"  Diffusion models have recently demonstrated considerable advancement in the\ngeneration and reconstruction of magnetic resonance imaging (MRI) data. These\nmodels exhibit great potential in handling unsampled data and reducing noise,\nhighlighting their promise as generative models. However, their application in\ndynamic MRI remains relatively underexplored. This is primarily due to the\nsubstantial amount of fully-sampled data typically required for training, which\nis difficult to obtain in dynamic MRI due to its spatio-temporal complexity and\nhigh acquisition costs. To address this challenge, we propose a dynamic MRI\nreconstruction method based on a time-interleaved acquisition scheme, termed\nthe Glob-al-to-local Diffusion Model. Specifically, fully encoded\nfull-resolution reference data are constructed by merging under-sampled k-space\ndata from adjacent time frames, generating two distinct bulk training datasets\nfor global and local models. The global-to-local diffusion framework\nalternately optimizes global information and local image details, enabling\nzero-shot reconstruction. Extensive experiments demonstrate that the proposed\nmethod performs well in terms of noise reduction and detail preservation,\nachieving reconstruction quality comparable to that of supervised approaches.\n","authors":["Yu Guan","Kunlong Zhang","Qi Qi","Dong Wang","Ziwen Ke","Shaoyu Wang","Dong Liang","Qiegen Liu"],"pdf_url":"https://arxiv.org/pdf/2411.03723v1.pdf","comment":"11 pages, 9 figures"},{"id":"http://arxiv.org/abs/2411.03717v1","updated":"2024-11-06T07:30:34Z","published":"2024-11-06T07:30:34Z","title":"These Maps Are Made by Propagation: Adapting Deep Stereo Networks to\n  Road Scenarios with Decisive Disparity Diffusion","summary":"  Stereo matching has emerged as a cost-effective solution for road surface 3D\nreconstruction, garnering significant attention towards improving both\ncomputational efficiency and accuracy. This article introduces decisive\ndisparity diffusion (D3Stereo), marking the first exploration of dense deep\nfeature matching that adapts pre-trained deep convolutional neural networks\n(DCNNs) to previously unseen road scenarios. A pyramid of cost volumes is\ninitially created using various levels of learned representations.\nSubsequently, a novel recursive bilateral filtering algorithm is employed to\naggregate these costs. A key innovation of D3Stereo lies in its alternating\ndecisive disparity diffusion strategy, wherein intra-scale diffusion is\nemployed to complete sparse disparity images, while inter-scale inheritance\nprovides valuable prior information for higher resolutions. Extensive\nexperiments conducted on our created UDTIRI-Stereo and Stereo-Road datasets\nunderscore the effectiveness of D3Stereo strategy in adapting pre-trained DCNNs\nand its superior performance compared to all other explicit programming-based\nalgorithms designed specifically for road surface 3D reconstruction. Additional\nexperiments conducted on the Middlebury dataset with backbone DCNNs pre-trained\non the ImageNet database further validate the versatility of D3Stereo strategy\nin tackling general stereo matching problems.\n","authors":["Chuang-Wei Liu","Yikang Zhang","Qijun Chen","Ioannis Pitas","Rui Fan"],"pdf_url":"https://arxiv.org/pdf/2411.03717v1.pdf","comment":"13 pages, 7 figures"},{"id":"http://arxiv.org/abs/2411.03714v1","updated":"2024-11-06T07:28:57Z","published":"2024-11-06T07:28:57Z","title":"Explaining Human Activity Recognition with SHAP: Validating Insights\n  with Perturbation and Quantitative Measures","summary":"  In Human Activity Recognition (HAR), understanding the intricacy of body\nmovements within high-risk applications is essential. This study uses SHapley\nAdditive exPlanations (SHAP) to explain the decision-making process of Graph\nConvolution Networks (GCNs) when classifying activities with skeleton data. We\nemploy SHAP to explain two real-world datasets: one for cerebral palsy (CP)\nclassification and the widely used NTU RGB+D 60 action recognition dataset. To\ntest the explanation, we introduce a novel perturbation approach that modifies\nthe model's edge importance matrix, allowing us to evaluate the impact of\nspecific body key points on prediction outcomes. To assess the fidelity of our\nexplanations, we employ informed perturbation, targeting body key points\nidentified as important by SHAP and comparing them against random perturbation\nas a control condition. This perturbation enables a judgment on whether the\nbody key points are truly influential or non-influential based on the SHAP\nvalues. Results on both datasets show that body key points identified as\nimportant through SHAP have the largest influence on the accuracy, specificity,\nand sensitivity metrics. Our findings highlight that SHAP can provide granular\ninsights into the input feature contribution to the prediction outcome of GCNs\nin HAR tasks. This demonstrates the potential for more interpretable and\ntrustworthy models in high-stakes applications like healthcare or\nrehabilitation.\n","authors":["Felix Tempel","Espen Alexander F. Ihlen","Lars Adde","Inga Strümke"],"pdf_url":"https://arxiv.org/pdf/2411.03714v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.05824v2","updated":"2024-11-06T07:23:52Z","published":"2022-09-13T09:00:58Z","title":"CPnP: Consistent Pose Estimator for Perspective-n-Point Problem with\n  Bias Elimination","summary":"  The Perspective-n-Point (PnP) problem has been widely studied in both\ncomputer vision and photogrammetry societies. With the development of feature\nextraction techniques, a large number of feature points might be available in a\nsingle shot. It is promising to devise a consistent estimator, i.e., the\nestimate can converge to the true camera pose as the number of points\nincreases. To this end, we propose a consistent PnP solver, named \\emph{CPnP},\nwith bias elimination. Specifically, linear equations are constructed from the\noriginal projection model via measurement model modification and variable\nelimination, based on which a closed-form least-squares solution is obtained.\nWe then analyze and subtract the asymptotic bias of this solution, resulting in\na consistent estimate. Additionally, Gauss-Newton (GN) iterations are executed\nto refine the consistent solution. Our proposed estimator is efficient in terms\nof computations -- it has $O(n)$ computational complexity. Experimental tests\non both synthetic data and real images show that our proposed estimator is\nsuperior to some well-known ones for images with dense visual features, in\nterms of estimation precision and computing time.\n","authors":["Guangyang Zeng","Shiyu Chen","Biqiang Mu","Guodong Shi","Junfeng Wu"],"pdf_url":"https://arxiv.org/pdf/2209.05824v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03707v1","updated":"2024-11-06T07:11:15Z","published":"2024-11-06T07:11:15Z","title":"Fine-Tuning Vision-Language Model for Automated Engineering Drawing\n  Information Extraction","summary":"  Geometric Dimensioning and Tolerancing (GD&T) plays a critical role in\nmanufacturing by defining acceptable variations in part features to ensure\ncomponent quality and functionality. However, extracting GD&T information from\n2D engineering drawings is a time-consuming and labor-intensive task, often\nrelying on manual efforts or semi-automated tools. To address these challenges,\nthis study proposes an automated and computationally efficient GD&T extraction\nmethod by fine-tuning Florence-2, an open-source vision-language model (VLM).\nThe model is trained on a dataset of 400 drawings with ground truth annotations\nprovided by domain experts. For comparison, two state-of-the-art closed-source\nVLMs, GPT-4o and Claude-3.5-Sonnet, are evaluated on the same dataset. All\nmodels are assessed using precision, recall, F1-score, and hallucination\nmetrics. Due to the computational cost and impracticality of fine-tuning large\nclosed-source VLMs for domain-specific tasks, GPT-4o and Claude-3.5-Sonnet are\nevaluated in a zero-shot setting. In contrast, Florence-2, a smaller model with\n0.23 billion parameters, is optimized through full-parameter fine-tuning across\nthree distinct experiments, each utilizing datasets augmented to different\nlevels. The results show that Florence-2 achieves a 29.95% increase in\nprecision, a 37.75% increase in recall, a 52.40% improvement in F1-score, and a\n43.15% reduction in hallucination rate compared to the best-performing\nclosed-source model. These findings highlight the effectiveness of fine-tuning\nsmaller, open-source VLMs like Florence-2, offering a practical and efficient\nsolution for automated GD&T extraction to support downstream manufacturing\ntasks.\n","authors":["Muhammad Tayyab Khan","Lequn Chen","Ye Han Ng","Wenhe Feng","Nicholas Yew Jin Tan","Seung Ki Moon"],"pdf_url":"https://arxiv.org/pdf/2411.03707v1.pdf","comment":"Paper has been submitted to the 9th International Conference on\n  Innovation in Artificial Intelligence (ICIAI 2025)"},{"id":"http://arxiv.org/abs/2403.20213v3","updated":"2024-11-06T07:09:03Z","published":"2024-03-29T14:50:43Z","title":"VHM: Versatile and Honest Vision Language Model for Remote Sensing Image\n  Analysis","summary":"  This paper develops a Versatile and Honest vision language Model (VHM) for\nremote sensing image analysis. VHM is built on a large-scale remote sensing\nimage-text dataset with rich-content captions (VersaD), and an honest\ninstruction dataset comprising both factual and deceptive questions (HnstD).\nUnlike prevailing remote sensing image-text datasets, in which image captions\nfocus on a few prominent objects and their relationships, VersaD captions\nprovide detailed information about image properties, object attributes, and the\noverall scene. This comprehensive captioning enables VHM to thoroughly\nunderstand remote sensing images and perform diverse remote sensing tasks.\nMoreover, different from existing remote sensing instruction datasets that only\ninclude factual questions, HnstD contains additional deceptive questions\nstemming from the non-existence of objects. This feature prevents VHM from\nproducing affirmative answers to nonsense queries, thereby ensuring its\nhonesty. In our experiments, VHM significantly outperforms various vision\nlanguage models on common tasks of scene classification, visual question\nanswering, and visual grounding. Additionally, VHM achieves competent\nperformance on several unexplored tasks, such as building vectorizing,\nmulti-label classification and honest question answering. We will release the\ncode, data and model weights at https://github.com/opendatalab/VHM .\n","authors":["Chao Pang","Xingxing Weng","Jiang Wu","Jiayu Li","Yi Liu","Jiaxing Sun","Weijia Li","Shuai Wang","Litong Feng","Gui-Song Xia","Conghui He"],"pdf_url":"https://arxiv.org/pdf/2403.20213v3.pdf","comment":"Equal contribution: Chao Pang, Xingxing Weng, Jiang Wu; Corresponding\n  author: Gui-Song Xia, Conghui He"},{"id":"http://arxiv.org/abs/2403.05408v2","updated":"2024-11-06T07:08:58Z","published":"2024-03-08T16:06:54Z","title":"FedFMS: Exploring Federated Foundation Models for Medical Image\n  Segmentation","summary":"  Medical image segmentation is crucial for clinical diagnosis. The\nSegmentation Anything Model (SAM) serves as a powerful foundation model for\nvisual segmentation and can be adapted for medical image segmentation. However,\nmedical imaging data typically contain privacy-sensitive information, making it\nchallenging to train foundation models with centralized storage and sharing. To\ndate, there are few foundation models tailored for medical image deployment\nwithin the federated learning framework, and the segmentation performance, as\nwell as the efficiency of communication and training, remain unexplored. In\nresponse to these issues, we developed Federated Foundation models for Medical\nimage Segmentation (FedFMS), which includes the Federated SAM (FedSAM) and a\ncommunication and training-efficient Federated SAM with Medical SAM Adapter\n(FedMSA). Comprehensive experiments on diverse datasets are conducted to\ninvestigate the performance disparities between centralized training and\nfederated learning across various configurations of FedFMS. The experiments\nrevealed that FedFMS could achieve performance comparable to models trained via\ncentralized training methods while maintaining privacy. Furthermore, FedMSA\ndemonstrated the potential to enhance communication and training efficiency.\nOur model implementation codes are available at\nhttps://github.com/LIU-YUXI/FedFMS.\n","authors":["Yuxi Liu","Guibo Luo","Yuesheng Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.05408v2.pdf","comment":"Accepted by MICCAI'2024"},{"id":"http://arxiv.org/abs/2411.03706v1","updated":"2024-11-06T07:08:41Z","published":"2024-11-06T07:08:41Z","title":"3DGS-CD: 3D Gaussian Splatting-based Change Detection for Physical\n  Object Rearrangement","summary":"  We present 3DGS-CD, the first 3D Gaussian Splatting (3DGS)-based method for\ndetecting physical object rearrangements in 3D scenes. Our approach estimates\n3D object-level changes by comparing two sets of unaligned images taken at\ndifferent times. Leveraging 3DGS's novel view rendering and EfficientSAM's\nzero-shot segmentation capabilities, we detect 2D object-level changes, which\nare then associated and fused across views to estimate 3D changes. Our method\ncan detect changes in cluttered environments using sparse post-change images\nwithin as little as 18s, using as few as a single new image. It does not rely\non depth input, user instructions, object classes, or object models -- An\nobject is recognized simply if it has been re-arranged. Our approach is\nevaluated on both public and self-collected real-world datasets, achieving up\nto 14% higher accuracy and three orders of magnitude faster performance\ncompared to the state-of-the-art radiance-field-based change detection method.\nThis significant performance boost enables a broad range of downstream\napplications, where we highlight three key use cases: object reconstruction,\nrobot workspace reset, and 3DGS model update. Our code and data will be made\navailable at https://github.com/520xyxyzq/3DGS-CD.\n","authors":["Ziqi Lu","Jianbo Ye","John Leonard"],"pdf_url":"https://arxiv.org/pdf/2411.03706v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03702v1","updated":"2024-11-06T06:58:17Z","published":"2024-11-06T06:58:17Z","title":"Graph-Based Multi-Modal Sensor Fusion for Autonomous Driving","summary":"  The growing demand for robust scene understanding in mobile robotics and\nautonomous driving has highlighted the importance of integrating multiple\nsensing modalities. By combining data from diverse sensors like cameras and\nLIDARs, fusion techniques can overcome the limitations of individual sensors,\nenabling a more complete and accurate perception of the environment. We\nintroduce a novel approach to multi-modal sensor fusion, focusing on developing\na graph-based state representation that supports critical decision-making\nprocesses in autonomous driving. We present a Sensor-Agnostic Graph-Aware\nKalman Filter [3], the first online state estimation technique designed to fuse\nmulti-modal graphs derived from noisy multi-sensor data. The estimated\ngraph-based state representations serve as a foundation for advanced\napplications like Multi-Object Tracking (MOT), offering a comprehensive\nframework for enhancing the situational awareness and safety of autonomous\nsystems. We validate the effectiveness of our proposed framework through\nextensive experiments conducted on both synthetic and real-world driving\ndatasets (nuScenes). Our results showcase an improvement in MOTA and a\nreduction in estimated position errors (MOTP) and identity switches (IDS) for\ntracked objects using the SAGA-KF. Furthermore, we highlight the capability of\nsuch a framework to develop methods that can leverage heterogeneous information\n(like semantic objects and geometric structures) from various sensing\nmodalities, enabling a more holistic approach to scene understanding and\nenhancing the safety and effectiveness of autonomous systems.\n","authors":["Depanshu Sani","Saket Anand"],"pdf_url":"https://arxiv.org/pdf/2411.03702v1.pdf","comment":"An extended abstract accepted at Young Researchers' Symposium, ICVGIP\n  '24. This extended abstract contains the following: 1. Short summary of our\n  work, SAGA-KF, accepted at ICPR'24. 2. A proposal that was awarded the\n  Qualcomm Innovation Fellowship'24"},{"id":"http://arxiv.org/abs/2411.02188v3","updated":"2024-11-06T06:38:47Z","published":"2024-11-04T15:42:22Z","title":"Digi2Real: Bridging the Realism Gap in Synthetic Data Face Recognition\n  via Foundation Models","summary":"  The accuracy of face recognition systems has improved significantly in the\npast few years, thanks to the large amount of data collected and the\nadvancement in neural network architectures. However, these large-scale\ndatasets are often collected without explicit consent, raising ethical and\nprivacy concerns. To address this, there have been proposals to use synthetic\ndatasets for training face recognition models. Yet, such models still rely on\nreal data to train the generative models and generally exhibit inferior\nperformance compared to those trained on real datasets. One of these datasets,\nDigiFace, uses a graphics pipeline to generate different identities and\ndifferent intra-class variations without using real data in training the\nmodels. However, the performance of this approach is poor on face recognition\nbenchmarks, possibly due to the lack of realism in the images generated from\nthe graphics pipeline. In this work, we introduce a novel framework for realism\ntransfer aimed at enhancing the realism of synthetically generated face images.\nOur method leverages the large-scale face foundation model, and we adapt the\npipeline for realism enhancement. By integrating the controllable aspects of\nthe graphics pipeline with our realism enhancement technique, we generate a\nlarge amount of realistic variations-combining the advantages of both\napproaches. Our empirical evaluations demonstrate that models trained using our\nenhanced dataset significantly improve the performance of face recognition\nsystems over the baseline. The source code and datasets will be made available\npublicly: https://www.idiap.ch/paper/digi2real\n","authors":["Anjith George","Sebastien Marcel"],"pdf_url":"https://arxiv.org/pdf/2411.02188v3.pdf","comment":"The dataset would be available here:\n  https://www.idiap.ch/paper/digi2real"},{"id":"http://arxiv.org/abs/2411.03696v1","updated":"2024-11-06T06:34:27Z","published":"2024-11-06T06:34:27Z","title":"OccLoff: Learning Optimized Feature Fusion for 3D Occupancy Prediction","summary":"  3D semantic occupancy prediction is crucial for finely representing the\nsurrounding environment, which is essential for ensuring the safety in\nautonomous driving. Existing fusion-based occupancy methods typically involve\nperforming a 2D-to-3D view transformation on image features, followed by\ncomputationally intensive 3D operations to fuse these with LiDAR features,\nleading to high computational costs and reduced accuracy. Moreover, current\nresearch on occupancy prediction predominantly focuses on designing specific\nnetwork architectures, often tailored to particular models, with limited\nattention given to the more fundamental aspect of semantic feature learning.\nThis gap hinders the development of more transferable methods that could\nenhance the performance of various occupancy models. To address these\nchallenges, we propose OccLoff, a framework that Learns to Optimize Feature\nFusion for 3D occupancy prediction. Specifically, we introduce a sparse fusion\nencoder with entropy masks that directly fuses 3D and 2D features, improving\nmodel accuracy while reducing computational overhead. Additionally, we propose\na transferable proxy-based loss function and an adaptive hard sample weighting\nalgorithm, which enhance the performance of several state-of-the-art methods.\nExtensive evaluations on the nuScenes and SemanticKITTI benchmarks demonstrate\nthe superiority of our framework, and ablation studies confirm the\neffectiveness of each proposed module.\n","authors":["Ji Zhang","Yiran Ding","Zixin Liu"],"pdf_url":"https://arxiv.org/pdf/2411.03696v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03695v1","updated":"2024-11-06T06:33:55Z","published":"2024-11-06T06:33:55Z","title":"AMNCutter: Affinity-Attention-Guided Multi-View Normalized Cutter for\n  Unsupervised Surgical Instrument Segmentation","summary":"  Surgical instrument segmentation (SIS) is pivotal for robotic-assisted\nminimally invasive surgery, assisting surgeons by identifying surgical\ninstruments in endoscopic video frames. Recent unsupervised surgical instrument\nsegmentation (USIS) methods primarily rely on pseudo-labels derived from\nlow-level features such as color and optical flow, but these methods show\nlimited effectiveness and generalizability in complex and unseen endoscopic\nscenarios. In this work, we propose a label-free unsupervised model featuring a\nnovel module named Multi-View Normalized Cutter (m-NCutter). Different from\nprevious USIS works, our model is trained using a graph-cutting loss function\nthat leverages patch affinities for supervision, eliminating the need for\npseudo-labels. The framework adaptively determines which affinities from which\nlevels should be prioritized. Therefore, the low- and high-level features and\ntheir affinities are effectively integrated to train a label-free unsupervised\nmodel, showing superior effectiveness and generalization ability. We conduct\ncomprehensive experiments across multiple SIS datasets to validate our\napproach's state-of-the-art (SOTA) performance, robustness, and exceptional\npotential as a pre-trained model. Our code is released at\nhttps://github.com/MingyuShengSMY/AMNCutter.\n","authors":["Mingyu Sheng","Jianan Fan","Dongnan Liu","Ron Kikinis","Weidong Cai"],"pdf_url":"https://arxiv.org/pdf/2411.03695v1.pdf","comment":"This paper was accepted by the 2025 IEEE Winter Conference on\n  Applications of Computer Vision (WACV)"},{"id":"http://arxiv.org/abs/2411.03688v1","updated":"2024-11-06T06:14:24Z","published":"2024-11-06T06:14:24Z","title":"Where Do We Stand with Implicit Neural Representations? A Technical and\n  Performance Survey","summary":"  Implicit Neural Representations (INRs) have emerged as a paradigm in\nknowledge representation, offering exceptional flexibility and performance\nacross a diverse range of applications. INRs leverage multilayer perceptrons\n(MLPs) to model data as continuous implicit functions, providing critical\nadvantages such as resolution independence, memory efficiency, and\ngeneralisation beyond discretised data structures. Their ability to solve\ncomplex inverse problems makes them particularly effective for tasks including\naudio reconstruction, image representation, 3D object reconstruction, and\nhigh-dimensional data synthesis. This survey provides a comprehensive review of\nstate-of-the-art INR methods, introducing a clear taxonomy that categorises\nthem into four key areas: activation functions, position encoding, combined\nstrategies, and network structure optimisation. We rigorously analyse their\ncritical properties, such as full differentiability, smoothness, compactness,\nand adaptability to varying resolutions while also examining their strengths\nand limitations in addressing locality biases and capturing fine details. Our\nexperimental comparison offers new insights into the trade-offs between\ndifferent approaches, showcasing the capabilities and challenges of the latest\nINR techniques across various tasks. In addition to identifying areas where\ncurrent methods excel, we highlight key limitations and potential avenues for\nimprovement, such as developing more expressive activation functions, enhancing\npositional encoding mechanisms, and improving scalability for complex,\nhigh-dimensional data. This survey serves as a roadmap for researchers,\noffering practical guidance for future exploration in the field of INRs. We aim\nto foster new methodologies by outlining promising research directions for INRs\nand applications.\n","authors":["Amer Essakine","Yanqi Cheng","Chun-Wun Cheng","Lipei Zhang","Zhongying Deng","Lei Zhu","Carola-Bibiane Schönlieb","Angelica I Aviles-Rivero"],"pdf_url":"https://arxiv.org/pdf/2411.03688v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.04315v4","updated":"2024-11-06T05:35:40Z","published":"2023-11-07T19:41:19Z","title":"A Data Perspective on Enhanced Identity Preservation for Diffusion\n  Personalization","summary":"  Large text-to-image models have revolutionized the ability to generate\nimagery using natural language. However, particularly unique or personal visual\nconcepts, such as pets and furniture, will not be captured by the original\nmodel. This has led to interest in how to personalize a text-to-image model.\nDespite significant progress, this task remains a formidable challenge,\nparticularly in preserving the subject's identity. Most researchers attempt to\naddress this issue by modifying model architectures. These methods are capable\nof keeping the subject structure and color but fail to preserve identity\ndetails. Towards this issue, our approach takes a data-centric perspective. We\nintroduce a novel regularization dataset generation strategy on both the text\nand image level. This strategy enables the model to preserve fine details of\nthe desired subjects, such as text and logos. Our method is\narchitecture-agnostic and can be flexibly applied on various text-to-image\nmodels. We show on established benchmarks that our data-centric approach forms\nthe new state of the art in terms of identity preservation and text alignment.\n","authors":["Xingzhe He","Zhiwen Cao","Nicholas Kolkin","Lantao Yu","Kun Wan","Helge Rhodin","Ratheesh Kalarot"],"pdf_url":"https://arxiv.org/pdf/2311.04315v4.pdf","comment":"WACV 2025"},{"id":"http://arxiv.org/abs/2410.13147v5","updated":"2024-11-06T05:18:04Z","published":"2024-10-17T02:04:57Z","title":"Utilizing Large Language Models in an iterative paradigm with Domain\n  feedback for Zero-shot Molecule optimization","summary":"  Molecule optimization is a critical task in drug discovery to optimize\ndesired properties of a given molecule through chemical modification. Despite\nLarge Language Models (LLMs) holding the potential to efficiently simulate this\ntask by using natural language to direct the optimization, straightforwardly\nutilizing shows limited performance. In this work, we facilitate utilizing LLMs\nin an iterative paradigm by proposing a simple yet highly effective domain\nfeedback provider, namely $\\text{Re}^3$DF. In detail, $\\text{Re}^3$DF harnesses\nan external toolkit, RDKit, to handle the molecule hallucination, if the\nmodified molecule is chemically invalid. Otherwise, its desired properties are\ncomputed and compared to the original one, establishing reliable domain\nfeedback with correct direction and distance towards the objective, followed by\na retrieved example, to explicitly guide the LLM to refine the modified\nmolecule. We conduct experiments across both single- and multi-property\nobjectives with 2 thresholds, where $\\text{Re}^3$DF shows significant\nimprovements. Particularly, for 20 single-property objectives, $\\text{Re}^3$DF\nenhances Hit ratio by 16.95% and 20.76% under loose and strict thresholds,\nrespectively. For 32 multi-property objectives, $\\text{Re}^3$DF enhances Hit\nratio by 6.04% and 5.25%.\n","authors":["Khiem Le","Nitesh V. Chawla"],"pdf_url":"https://arxiv.org/pdf/2410.13147v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09774v3","updated":"2024-11-06T05:16:59Z","published":"2024-09-15T15:46:03Z","title":"Generalizing Alignment Paradigm of Text-to-Image Generation with\n  Preferences through $f$-divergence Minimization","summary":"  Direct Preference Optimization (DPO) has recently expanded its successful\napplication from aligning large language models (LLMs) to aligning\ntext-to-image models with human preferences, which has generated considerable\ninterest within the community. However, we have observed that these approaches\nrely solely on minimizing the reverse Kullback-Leibler divergence during\nalignment process between the fine-tuned model and the reference model,\nneglecting the incorporation of other divergence constraints. In this study, we\nfocus on extending reverse Kullback-Leibler divergence in the alignment\nparadigm of text-to-image models to $f$-divergence, which aims to garner better\nalignment performance as well as good generation diversity. We provide the\ngeneralized formula of the alignment paradigm under the $f$-divergence\ncondition and thoroughly analyze the impact of different divergence constraints\non alignment process from the perspective of gradient fields. We conduct\ncomprehensive evaluation on image-text alignment performance, human value\nalignment performance and generation diversity performance under different\ndivergence constraints, and the results indicate that alignment based on\nJensen-Shannon divergence achieves the best trade-off among them. The option of\ndivergence employed for aligning text-to-image models significantly impacts the\ntrade-off between alignment performance (especially human value alignment) and\ngeneration diversity, which highlights the necessity of selecting an\nappropriate divergence for practical applications.\n","authors":["Haoyuan Sun","Bo Xia","Yongzhe Chang","Xueqian Wang"],"pdf_url":"https://arxiv.org/pdf/2409.09774v3.pdf","comment":"34 pages"},{"id":"http://arxiv.org/abs/2411.03672v1","updated":"2024-11-06T05:11:25Z","published":"2024-11-06T05:11:25Z","title":"Towards 3D Semantic Scene Completion for Autonomous Driving: A\n  Meta-Learning Framework Empowered by Deformable Large-Kernel Attention and\n  Mamba Model","summary":"  Semantic scene completion (SSC) is essential for achieving comprehensive\nperception in autonomous driving systems. However, existing SSC methods often\noverlook the high deployment costs in real-world applications. Traditional\narchitectures, such as 3D Convolutional Neural Networks (3D CNNs) and\nself-attention mechanisms, face challenges in efficiently capturing long-range\ndependencies within 3D voxel grids, limiting their effectiveness. To address\nthese issues, we introduce MetaSSC, a novel meta-learning-based framework for\nSSC that leverages deformable convolution, large-kernel attention, and the\nMamba (D-LKA-M) model. Our approach begins with a voxel-based semantic\nsegmentation (SS) pretraining task, aimed at exploring the semantics and\ngeometry of incomplete regions while acquiring transferable meta-knowledge.\nUsing simulated cooperative perception datasets, we supervise the perception\ntraining of a single vehicle using aggregated sensor data from multiple nearby\nconnected autonomous vehicles (CAVs), generating richer and more comprehensive\nlabels. This meta-knowledge is then adapted to the target domain through a\ndual-phase training strategy that does not add extra model parameters, enabling\nefficient deployment. To further enhance the model's capability in capturing\nlong-sequence relationships within 3D voxel grids, we integrate Mamba blocks\nwith deformable convolution and large-kernel attention into the backbone\nnetwork. Extensive experiments demonstrate that MetaSSC achieves\nstate-of-the-art performance, significantly outperforming competing models\nwhile also reducing deployment costs.\n","authors":["Yansong Qu","Zilin Huang","Zihao Sheng","Tiantian Chen","Sikai Chen"],"pdf_url":"https://arxiv.org/pdf/2411.03672v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.02077v4","updated":"2024-11-06T05:11:24Z","published":"2024-07-02T09:11:17Z","title":"Hierarchical Temporal Context Learning for Camera-based Semantic Scene\n  Completion","summary":"  Camera-based 3D semantic scene completion (SSC) is pivotal for predicting\ncomplicated 3D layouts with limited 2D image observations. The existing\nmainstream solutions generally leverage temporal information by roughly\nstacking history frames to supplement the current frame, such straightforward\ntemporal modeling inevitably diminishes valid clues and increases learning\ndifficulty. To address this problem, we present HTCL, a novel Hierarchical\nTemporal Context Learning paradigm for improving camera-based semantic scene\ncompletion. The primary innovation of this work involves decomposing temporal\ncontext learning into two hierarchical steps: (a) cross-frame affinity\nmeasurement and (b) affinity-based dynamic refinement. Firstly, to separate\ncritical relevant context from redundant information, we introduce the pattern\naffinity with scale-aware isolation and multiple independent learners for\nfine-grained contextual correspondence modeling. Subsequently, to dynamically\ncompensate for incomplete observations, we adaptively refine the feature\nsampling locations based on initially identified locations with high affinity\nand their neighboring relevant regions. Our method ranks $1^{st}$ on the\nSemanticKITTI benchmark and even surpasses LiDAR-based methods in terms of mIoU\non the OpenOccupancy benchmark. Our code is available on\nhttps://github.com/Arlo0o/HTCL.\n","authors":["Bohan Li","Jiajun Deng","Wenyao Zhang","Zhujin Liang","Dalong Du","Xin Jin","Wenjun Zeng"],"pdf_url":"https://arxiv.org/pdf/2407.02077v4.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2411.03670v1","updated":"2024-11-06T05:09:34Z","published":"2024-11-06T05:09:34Z","title":"Touchstone Benchmark: Are We on the Right Way for Evaluating AI\n  Algorithms for Medical Segmentation?","summary":"  How can we test AI performance? This question seems trivial, but it isn't.\nStandard benchmarks often have problems such as in-distribution and small-size\ntest sets, oversimplified metrics, unfair comparisons, and short-term outcome\npressure. As a consequence, good performance on standard benchmarks does not\nguarantee success in real-world scenarios. To address these problems, we\npresent Touchstone, a large-scale collaborative segmentation benchmark of 9\ntypes of abdominal organs. This benchmark is based on 5,195 training CT scans\nfrom 76 hospitals around the world and 5,903 testing CT scans from 11\nadditional hospitals. This diverse test set enhances the statistical\nsignificance of benchmark results and rigorously evaluates AI algorithms across\nvarious out-of-distribution scenarios. We invited 14 inventors of 19 AI\nalgorithms to train their algorithms, while our team, as a third party,\nindependently evaluated these algorithms on three test sets. In addition, we\nalso evaluated pre-existing AI frameworks--which, differing from algorithms,\nare more flexible and can support different algorithms--including MONAI from\nNVIDIA, nnU-Net from DKFZ, and numerous other open-source frameworks. We are\ncommitted to expanding this benchmark to encourage more innovation of AI\nalgorithms for the medical domain.\n","authors":["Pedro R. A. S. Bassi","Wenxuan Li","Yucheng Tang","Fabian Isensee","Zifu Wang","Jieneng Chen","Yu-Cheng Chou","Yannick Kirchhoff","Maximilian Rokuss","Ziyan Huang","Jin Ye","Junjun He","Tassilo Wald","Constantin Ulrich","Michael Baumgartner","Saikat Roy","Klaus H. Maier-Hein","Paul Jaeger","Yiwen Ye","Yutong Xie","Jianpeng Zhang","Ziyang Chen","Yong Xia","Zhaohu Xing","Lei Zhu","Yousef Sadegheih","Afshin Bozorgpour","Pratibha Kumari","Reza Azad","Dorit Merhof","Pengcheng Shi","Ting Ma","Yuxin Du","Fan Bai","Tiejun Huang","Bo Zhao","Haonan Wang","Xiaomeng Li","Hanxue Gu","Haoyu Dong","Jichen Yang","Maciej A. Mazurowski","Saumya Gupta","Linshan Wu","Jiaxin Zhuang","Hao Chen","Holger Roth","Daguang Xu","Matthew B. Blaschko","Sergio Decherchi","Andrea Cavalli","Alan L. Yuille","Zongwei Zhou"],"pdf_url":"https://arxiv.org/pdf/2411.03670v1.pdf","comment":"Accepted to NeurIPS-2024"},{"id":"http://arxiv.org/abs/2409.13941v2","updated":"2024-11-06T05:05:12Z","published":"2024-09-20T23:04:21Z","title":"TalkMosaic: Interactive PhotoMosaic with Multi-modal LLM Q&A\n  Interactions","summary":"  We use images of cars of a wide range of varieties to compose an image of an\nanimal such as a bird or a lion for the theme of environmental protection to\nmaximize the information about cars in a single composed image and to raise the\nawareness about environmental challenges. We present a novel way of image\ninteraction with an artistically-composed photomosaic image, in which a simple\noperation of \"click and display\" is used to demonstrate the interactive switch\nbetween a tile image in a photomosaic image and the corresponding original car\nimage, which will be automatically saved on the Desktop. We build a multimodal\ncustom GPT named TalkMosaic by incorporating car images information and the\nrelated knowledge to ChatGPT. By uploading the original car image to\nTalkMosaic, we can ask questions about the given car image and get the\ncorresponding answers efficiently and effectively such as where to buy the tire\nin the car image that satisfies high environmental standards. We give an\nin-depth analysis on how to speed up the inference of multimodal LLM using\nsparse attention and quantization techniques with presented probabilistic\nFlashAttention (PrFlashAttention) and Staircase Adaptive Quantization (SAQ)\nmethods. The implemented prototype demonstrates the feasibility and\neffectiveness of the presented approach.\n","authors":["Kevin Li","Fulu Li"],"pdf_url":"https://arxiv.org/pdf/2409.13941v2.pdf","comment":"6 pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.14789v2","updated":"2024-11-06T04:41:48Z","published":"2024-08-27T05:31:30Z","title":"Revisiting Surgical Instrument Segmentation Without Human Intervention:\n  A Graph Partitioning View","summary":"  Surgical instrument segmentation (SIS) on endoscopic images stands as a\nlong-standing and essential task in the context of computer-assisted\ninterventions for boosting minimally invasive surgery. Given the recent surge\nof deep learning methodologies and their data-hungry nature, training a neural\npredictive model based on massive expert-curated annotations has been\ndominating and served as an off-the-shelf approach in the field, which could,\nhowever, impose prohibitive burden to clinicians for preparing fine-grained\npixel-wise labels corresponding to the collected surgical video frames. In this\nwork, we propose an unsupervised method by reframing the video frame\nsegmentation as a graph partitioning problem and regarding image pixels as\ngraph nodes, which is significantly different from the previous efforts. A\nself-supervised pre-trained model is firstly leveraged as a feature extractor\nto capture high-level semantic features. Then, Laplacian matrixs are computed\nfrom the features and are eigendecomposed for graph partitioning. On the \"deep\"\neigenvectors, a surgical video frame is meaningfully segmented into different\nmodules such as tools and tissues, providing distinguishable semantic\ninformation like locations, classes, and relations. The segmentation problem\ncan then be naturally tackled by applying clustering or threshold on the\neigenvectors. Extensive experiments are conducted on various datasets (e.g.,\nEndoVis2017, EndoVis2018, UCL, etc.) for different clinical endpoints. Across\nall the challenging scenarios, our method demonstrates outstanding performance\nand robustness higher than unsupervised state-of-the-art (SOTA) methods. The\ncode is released at https://github.com/MingyuShengSMY/GraphClusteringSIS.git.\n","authors":["Mingyu Sheng","Jianan Fan","Dongnan Liu","Ron Kikinis","Weidong Cai"],"pdf_url":"https://arxiv.org/pdf/2408.14789v2.pdf","comment":"This paper is accepted by The 32nd ACM International Conference on\n  Multimedia (ACM MM 2024) Workshop on Multimedia Computing for Health and\n  Medicine (MCHM)"},{"id":"http://arxiv.org/abs/2406.15735v3","updated":"2024-11-06T03:53:13Z","published":"2024-06-22T04:56:16Z","title":"Identifying and Solving Conditional Image Leakage in Image-to-Video\n  Diffusion Model","summary":"  Diffusion models have obtained substantial progress in image-to-video\ngeneration. However, in this paper, we find that these models tend to generate\nvideos with less motion than expected. We attribute this to the issue called\nconditional image leakage, where the image-to-video diffusion models (I2V-DMs)\ntend to over-rely on the conditional image at large time steps. We further\naddress this challenge from both inference and training aspects. First, we\npropose to start the generation process from an earlier time step to avoid the\nunreliable large-time steps of I2V-DMs, as well as an initial noise\ndistribution with optimal analytic expressions (Analytic-Init) by minimizing\nthe KL divergence between it and the actual marginal distribution to bridge the\ntraining-inference gap. Second, we design a time-dependent noise distribution\n(TimeNoise) for the conditional image during training, applying higher noise\nlevels at larger time steps to disrupt it and reduce the model's dependency on\nit. We validate these general strategies on various I2V-DMs on our collected\nopen-domain image benchmark and the UCF101 dataset. Extensive results show that\nour methods outperform baselines by producing higher motion scores with lower\nerrors while maintaining image alignment and temporal consistency, thereby\nyielding superior overall performance and enabling more accurate motion\ncontrol. The project page: \\url{https://cond-image-leak.github.io/}.\n","authors":["Min Zhao","Hongzhou Zhu","Chendong Xiang","Kaiwen Zheng","Chongxuan Li","Jun Zhu"],"pdf_url":"https://arxiv.org/pdf/2406.15735v3.pdf","comment":"NeurIPS 2024. Project page: https://cond-image-leak.github.io/"},{"id":"http://arxiv.org/abs/2411.01797v2","updated":"2024-11-06T03:45:13Z","published":"2024-11-04T04:45:45Z","title":"AIWR: Aerial Image Water Resource Dataset for Segmentation Analysis","summary":"  Effective water resource management is crucial in agricultural regions like\nnortheastern Thailand, where limited water retention in sandy soils poses\nsignificant challenges. In response to this issue, the Aerial Image Water\nResource (AIWR) dataset was developed, comprising 800 aerial images focused on\nnatural and artificial water bodies in this region. The dataset was created\nusing Bing Maps and follows the standards of the Fundamental Geographic Data\nSet (FGDS). It includes ground truth annotations validated by experts in remote\nsensing, making it an invaluable resource for researchers in geoinformatics,\ncomputer vision, and artificial intelligence. The AIWR dataset presents\nconsiderable challenges, such as segmentation due to variations in the size,\ncolor, shape, and similarity of water bodies, which often resemble other land\nuse categories. The objective of the proposed dataset is to explore advanced\nAI-driven methods for water body segmentation, addressing the unique challenges\nposed by the dataset complexity and limited size. This dataset and related\nresearch contribute to the development of novel algorithms for water\nmanagement, supporting sustainable agricultural practices in regions facing\nsimilar challenges.\n","authors":["Sangdaow Noppitak","Emmanuel Okafor","Olarik Surinta"],"pdf_url":"https://arxiv.org/pdf/2411.01797v2.pdf","comment":"12 pages, 8 figures"},{"id":"http://arxiv.org/abs/2411.03638v1","updated":"2024-11-06T03:30:46Z","published":"2024-11-06T03:30:46Z","title":"Adaptive Stereo Depth Estimation with Multi-Spectral Images Across All\n  Lighting Conditions","summary":"  Depth estimation under adverse conditions remains a significant challenge.\nRecently, multi-spectral depth estimation, which integrates both visible light\nand thermal images, has shown promise in addressing this issue. However,\nexisting algorithms struggle with precise pixel-level feature matching,\nlimiting their ability to fully exploit geometric constraints across different\nspectra. To address this, we propose a novel framework incorporating stereo\ndepth estimation to enforce accurate geometric constraints. In particular, we\ntreat the visible light and thermal images as a stereo pair and utilize a\nCross-modal Feature Matching (CFM) Module to construct a cost volume for\npixel-level matching. To mitigate the effects of poor lighting on stereo\nmatching, we introduce Degradation Masking, which leverages robust monocular\nthermal depth estimation in degraded regions. Our method achieves\nstate-of-the-art (SOTA) performance on the Multi-Spectral Stereo (MS2) dataset,\nwith qualitative evaluations demonstrating high-quality depth maps under\nvarying lighting conditions.\n","authors":["Zihan Qin","Jialei Xu","Wenbo Zhao","Junjun Jiang","Xianming Liu"],"pdf_url":"https://arxiv.org/pdf/2411.03638v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03637v1","updated":"2024-11-06T03:28:06Z","published":"2024-11-06T03:28:06Z","title":"Structure Consistent Gaussian Splatting with Matching Prior for Few-shot\n  Novel View Synthesis","summary":"  Despite the substantial progress of novel view synthesis, existing methods,\neither based on the Neural Radiance Fields (NeRF) or more recently 3D Gaussian\nSplatting (3DGS), suffer significant degradation when the input becomes sparse.\nNumerous efforts have been introduced to alleviate this problem, but they still\nstruggle to synthesize satisfactory results efficiently, especially in the\nlarge scene. In this paper, we propose SCGaussian, a Structure Consistent\nGaussian Splatting method using matching priors to learn 3D consistent scene\nstructure. Considering the high interdependence of Gaussian attributes, we\noptimize the scene structure in two folds: rendering geometry and, more\nimportantly, the position of Gaussian primitives, which is hard to be directly\nconstrained in the vanilla 3DGS due to the non-structure property. To achieve\nthis, we present a hybrid Gaussian representation. Besides the ordinary\nnon-structure Gaussian primitives, our model also consists of ray-based\nGaussian primitives that are bound to matching rays and whose optimization of\ntheir positions is restricted along the ray. Thus, we can utilize the matching\ncorrespondence to directly enforce the position of these Gaussian primitives to\nconverge to the surface points where rays intersect. Extensive experiments on\nforward-facing, surrounding, and complex large scenes show the effectiveness of\nour approach with state-of-the-art performance and high efficiency. Code is\navailable at https://github.com/prstrive/SCGaussian.\n","authors":["Rui Peng","Wangze Xu","Luyang Tang","Liwei Liao","Jianbo Jiao","Ronggang Wang"],"pdf_url":"https://arxiv.org/pdf/2411.03637v1.pdf","comment":"NeurIPS 2024 Accepted"},{"id":"http://arxiv.org/abs/2410.21872v2","updated":"2024-11-06T02:52:47Z","published":"2024-10-29T09:08:57Z","title":"Advancing Efficient Brain Tumor Multi-Class Classification -- New\n  Insights from the Vision Mamba Model in Transfer Learning","summary":"  Early and accurate diagnosis of brain tumors is crucial for improving patient\nsurvival rates. However, the detection and classification of brain tumors are\nchallenging due to their diverse types and complex morphological\ncharacteristics. This study investigates the application of pre-trained models\nfor brain tumor classification, with a particular focus on deploying the Mamba\nmodel. We fine-tuned several mainstream transfer learning models and applied\nthem to the multi-class classification of brain tumors. By comparing these\nmodels to those trained from scratch, we demonstrated the significant\nadvantages of transfer learning, especially in the medical imaging field, where\nannotated data is often limited. Notably, we introduced the Vision Mamba (Vim),\na novel network architecture, and applied it for the first time in brain tumor\nclassification, achieving exceptional classification accuracy. Experimental\nresults indicate that the Vim model achieved 100% classification accuracy on an\nindependent test set, emphasizing its potential for tumor classification tasks.\nThese findings underscore the effectiveness of transfer learning in brain tumor\nclassification and reveal that, compared to existing state-of-the-art models,\nthe Vim model is lightweight, efficient, and highly accurate, offering a new\nperspective for clinical applications. Furthermore, the framework proposed in\nthis study for brain tumor classification, based on transfer learning and the\nVision Mamba model, is broadly applicable to other medical imaging\nclassification problems.\n","authors":["Yinyi Lai","Anbo Cao","Yuan Gao","Jiaqi Shang","Zongyu Li","Jia Guo"],"pdf_url":"https://arxiv.org/pdf/2410.21872v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03628v1","updated":"2024-11-06T02:50:30Z","published":"2024-11-06T02:50:30Z","title":"StreamingBench: Assessing the Gap for MLLMs to Achieve Streaming Video\n  Understanding","summary":"  The rapid development of Multimodal Large Language Models (MLLMs) has\nexpanded their capabilities from image comprehension to video understanding.\nHowever, most of these MLLMs focus primarily on offline video comprehension,\nnecessitating extensive processing of all video frames before any queries can\nbe made. This presents a significant gap compared to the human ability to\nwatch, listen, think, and respond to streaming inputs in real time,\nhighlighting the limitations of current MLLMs. In this paper, we introduce\nStreamingBench, the first comprehensive benchmark designed to evaluate the\nstreaming video understanding capabilities of MLLMs. StreamingBench assesses\nthree core aspects of streaming video understanding: (1) real-time visual\nunderstanding, (2) omni-source understanding, and (3) contextual understanding.\nThe benchmark consists of 18 tasks, featuring 900 videos and 4,500\nhuman-curated QA pairs. Each video features five questions presented at\ndifferent time points to simulate a continuous streaming scenario. We conduct\nexperiments on StreamingBench with 13 open-source and proprietary MLLMs and\nfind that even the most advanced proprietary MLLMs like Gemini 1.5 Pro and\nGPT-4o perform significantly below human-level streaming video understanding\ncapabilities. We hope our work can facilitate further advancements for MLLMs,\nempowering them to approach human-level video comprehension and interaction in\nmore realistic scenarios.\n","authors":["Junming Lin","Zheng Fang","Chi Chen","Zihao Wan","Fuwen Luo","Peng Li","Yang Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2411.03628v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03618v1","updated":"2024-11-06T02:23:38Z","published":"2024-11-06T02:23:38Z","title":"Cross Feature Fusion of Fundus Image and Generated Lesion Map for\n  Referable Diabetic Retinopathy Classification","summary":"  Diabetic Retinopathy (DR) is a primary cause of blindness, necessitating\nearly detection and diagnosis. This paper focuses on referable DR\nclassification to enhance the applicability of the proposed method in clinical\npractice. We develop an advanced cross-learning DR classification method\nleveraging transfer learning and cross-attention mechanisms. The proposed\nmethod employs the Swin U-Net architecture to segment lesion maps from DR\nfundus images. The Swin U-Net segmentation model, enriched with DR lesion\ninsights, is transferred to generate a lesion map. Both the fundus image and\nits segmented lesion map are used as complementary inputs for the\nclassification model. A cross-attention mechanism is deployed to improve the\nmodel's ability to capture fine-grained details from the input pairs. Our\nexperiments, utilizing two public datasets, FGADR and EyePACS, demonstrate a\nsuperior accuracy of 94.6%, surpassing current state-of-the-art methods by\n4.4%. To this end, we aim for the proposed method to be seamlessly integrated\ninto clinical workflows, enhancing accuracy and efficiency in identifying\nreferable DR.\n","authors":["Dahyun Mok","Junghyun Bum","Le Duc Tai","Hyunseung Choo"],"pdf_url":"https://arxiv.org/pdf/2411.03618v1.pdf","comment":"ACCV 2024 accepted"},{"id":"http://arxiv.org/abs/2406.16473v2","updated":"2024-11-06T02:17:05Z","published":"2024-06-24T09:25:02Z","title":"D2SP: Dynamic Dual-Stage Purification Framework for Dual Noise\n  Mitigation in Vision-based Affective Recognition","summary":"  The contemporary state-of-the-art of Dynamic Facial Expression Recognition\n(DFER) technology facilitates remarkable progress by deriving emotional\nmappings of facial expressions from video content, underpinned by training on\nvoluminous datasets. Yet, the DFER datasets encompass a substantial volume of\nnoise data. Noise arises from low-quality captures that defy logical labeling,\nand instances that suffer from mislabeling due to annotation bias, engendering\ntwo principal types of uncertainty: the uncertainty regarding data usability\nand the uncertainty concerning label reliability. Addressing the two types of\nuncertainty, we have meticulously crafted a two-stage framework aiming at\n\\textbf{S}eeking \\textbf{C}ertain data \\textbf{I}n extensive \\textbf{U}ncertain\ndata (SCIU). This initiative aims to purge the DFER datasets of these\nuncertainties, thereby ensuring that only clean, verified data is employed in\ntraining processes. To mitigate the issue of low-quality samples, we introduce\nthe Coarse-Grained Pruning (CGP) stage, which assesses sample weights and\nprunes those deemed unusable due to their low weight. For samples with\nincorrect annotations, the Fine-Grained Correction (FGC) stage evaluates\nprediction stability to rectify mislabeled data. Moreover, SCIU is conceived as\na universally compatible, plug-and-play framework, tailored to integrate\nseamlessly with prevailing DFER methodologies. Rigorous experiments across\nprevalent DFER datasets and against numerous benchmark methods substantiates\nSCIU's capacity to markedly elevate performance metrics.\n","authors":["Haoran Wang","Xinji Mai","Zeng Tao","Xuan Tong","Junxiong Lin","Yan Wang","Jiawen Yu","Boyang Wang","Shaoqi Yan","Qing Zhao","Ziheng Zhou","Shuyong Gao","Wenqiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.16473v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03615v1","updated":"2024-11-06T02:16:34Z","published":"2024-11-06T02:16:34Z","title":"ADMIRE: a locally adaptive single-image, non-uniformity correction and\n  denoising algorithm: application to uncooled IR camera","summary":"  We propose a new way to correct for the non-uniformity (NU) and the noise in\nuncooled infrared-type images. This method works on static images, needs no\nregistration, no camera motion and no model for the non uniformity. The\nproposed method uses an hybrid scheme including an automatic locally-adaptive\ncontrast adjustment and a state-of-the-art image denoising method. It permits\nto correct for a fully non-linear NU and the noise efficiently using only one\nimage. We compared it with total variation on real raw and simulated NU\ninfrared images. The strength of this approach lies in its simplicity, low\ncomputational cost. It needs no test-pattern or calibration and produces no\n\"ghost-artefact\".\n","authors":["Yohann Tendero","Jerome Gilles"],"pdf_url":"https://arxiv.org/pdf/2411.03615v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03610v1","updated":"2024-11-06T02:05:44Z","published":"2024-11-06T02:05:44Z","title":"LCP-Fusion: A Neural Implicit SLAM with Enhanced Local Constraints and\n  Computable Prior","summary":"  Recently the dense Simultaneous Localization and Mapping (SLAM) based on\nneural implicit representation has shown impressive progress in hole filling\nand high-fidelity mapping. Nevertheless, existing methods either heavily rely\non known scene bounds or suffer inconsistent reconstruction due to drift in\npotential loop-closure regions, or both, which can be attributed to the\ninflexible representation and lack of local constraints. In this paper, we\npresent LCP-Fusion, a neural implicit SLAM system with enhanced local\nconstraints and computable prior, which takes the sparse voxel octree structure\ncontaining feature grids and SDF priors as hybrid scene representation,\nenabling the scalability and robustness during mapping and tracking. To enhance\nthe local constraints, we propose a novel sliding window selection strategy\nbased on visual overlap to address the loop-closure, and a practical warping\nloss to constrain relative poses. Moreover, we estimate SDF priors as coarse\ninitialization for implicit features, which brings additional explicit\nconstraints and robustness, especially when a light but efficient adaptive\nearly ending is adopted. Experiments demonstrate that our method achieve better\nlocalization accuracy and reconstruction consistency than existing RGB-D\nimplicit SLAM, especially in challenging real scenes (ScanNet) as well as\nself-captured scenes with unknown scene bounds. The code is available at\nhttps://github.com/laliwang/LCP-Fusion.\n","authors":["Jiahui Wang","Yinan Deng","Yi Yang","Yufeng Yue"],"pdf_url":"https://arxiv.org/pdf/2411.03610v1.pdf","comment":"Accepted by 2024 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS 2024)"},{"id":"http://arxiv.org/abs/2411.03576v1","updated":"2024-11-06T00:34:26Z","published":"2024-11-06T00:34:26Z","title":"Hybrid Attention for Robust RGB-T Pedestrian Detection in Real-World\n  Conditions","summary":"  Multispectral pedestrian detection has gained significant attention in recent\nyears, particularly in autonomous driving applications. To address the\nchallenges posed by adversarial illumination conditions, the combination of\nthermal and visible images has demonstrated its advantages. However, existing\nfusion methods rely on the critical assumption that the RGB-Thermal (RGB-T)\nimage pairs are fully overlapping. These assumptions often do not hold in\nreal-world applications, where only partial overlap between images can occur\ndue to sensors configuration. Moreover, sensor failure can cause loss of\ninformation in one modality. In this paper, we propose a novel module called\nthe Hybrid Attention (HA) mechanism as our main contribution to mitigate\nperformance degradation caused by partial overlap and sensor failure, i.e. when\nat least part of the scene is acquired by only one sensor. We propose an\nimproved RGB-T fusion algorithm, robust against partial overlap and sensor\nfailure encountered during inference in real-world applications. We also\nleverage a mobile-friendly backbone to cope with resource constraints in\nembedded systems. We conducted experiments by simulating various partial\noverlap and sensor failure scenarios to evaluate the performance of our\nproposed method. The results demonstrate that our approach outperforms\nstate-of-the-art methods, showcasing its superiority in handling real-world\nchallenges.\n","authors":["Arunkumar Rathinam","Leo Pauly","Abd El Rahman Shabayek","Wassim Rharbaoui","Anis Kacem","Vincent Gaudillière","Djamila Aouada"],"pdf_url":"https://arxiv.org/pdf/2411.03576v1.pdf","comment":"Accepted for publication in IEEE Robotics and Automation Letters,\n  October 2024"},{"id":"http://arxiv.org/abs/2411.03569v1","updated":"2024-11-06T00:17:36Z","published":"2024-11-06T00:17:36Z","title":"Towards Personalized Federated Learning via Comprehensive Knowledge\n  Distillation","summary":"  Federated learning is a distributed machine learning paradigm designed to\nprotect data privacy. However, data heterogeneity across various clients\nresults in catastrophic forgetting, where the model rapidly forgets previous\nknowledge while acquiring new knowledge. To address this challenge,\npersonalized federated learning has emerged to customize a personalized model\nfor each client. However, the inherent limitation of this mechanism is its\nexcessive focus on personalization, potentially hindering the generalization of\nthose models. In this paper, we present a novel personalized federated learning\nmethod that uses global and historical models as teachers and the local model\nas the student to facilitate comprehensive knowledge distillation. The\nhistorical model represents the local model from the last round of client\ntraining, containing historical personalized knowledge, while the global model\nrepresents the aggregated model from the last round of server aggregation,\ncontaining global generalized knowledge. By applying knowledge distillation, we\neffectively transfer global generalized knowledge and historical personalized\nknowledge to the local model, thus mitigating catastrophic forgetting and\nenhancing the general performance of personalized models. Extensive\nexperimental results demonstrate the significant advantages of our method.\n","authors":["Pengju Wang","Bochao Liu","Weijia Guo","Yong Li","Shiming Ge"],"pdf_url":"https://arxiv.org/pdf/2411.03569v1.pdf","comment":"Accepted by IEEE SMC 2024"},{"id":"http://arxiv.org/abs/2411.03568v1","updated":"2024-11-06T00:16:16Z","published":"2024-11-06T00:16:16Z","title":"The American Sign Language Knowledge Graph: Infusing ASL Models with\n  Linguistic Knowledge","summary":"  Language models for American Sign Language (ASL) could make language\ntechnologies substantially more accessible to those who sign. To train models\non tasks such as isolated sign recognition (ISR) and ASL-to-English\ntranslation, datasets provide annotated video examples of ASL signs. To\nfacilitate the generalizability and explainability of these models, we\nintroduce the American Sign Language Knowledge Graph (ASLKG), compiled from\ntwelve sources of expert linguistic knowledge. We use the ASLKG to train\nneuro-symbolic models for 3 ASL understanding tasks, achieving accuracies of\n91% on ISR, 14% for predicting the semantic features of unseen signs, and 36%\nfor classifying the topic of Youtube-ASL videos.\n","authors":["Lee Kezar","Nidhi Munikote","Zian Zeng","Zed Sehyr","Naomi Caselli","Jesse Thomason"],"pdf_url":"https://arxiv.org/pdf/2411.03568v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21169v3","updated":"2024-11-06T00:11:08Z","published":"2024-10-28T16:11:35Z","title":"Document Parsing Unveiled: Techniques, Challenges, and Prospects for\n  Structured Information Extraction","summary":"  Document parsing is essential for converting unstructured and semi-structured\ndocuments-such as contracts, academic papers, and invoices-into structured,\nmachine-readable data. Document parsing extract reliable structured data from\nunstructured inputs, providing huge convenience for numerous applications.\nEspecially with recent achievements in Large Language Models, document parsing\nplays an indispensable role in both knowledge base construction and training\ndata generation. This survey presents a comprehensive review of the current\nstate of document parsing, covering key methodologies, from modular pipeline\nsystems to end-to-end models driven by large vision-language models. Core\ncomponents such as layout detection, content extraction (including text,\ntables, and mathematical expressions), and multi-modal data integration are\nexamined in detail. Additionally, this paper discusses the challenges faced by\nmodular document parsing systems and vision-language models in handling complex\nlayouts, integrating multiple modules, and recognizing high-density text. It\nemphasizes the importance of developing larger and more diverse datasets and\noutlines future research directions.\n","authors":["Qintong Zhang","Victor Shea-Jay Huang","Bin Wang","Junyuan Zhang","Zhengren Wang","Hao Liang","Shawn Wang","Matthieu Lin","Conghui He","Wentao Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.21169v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.12507v3","updated":"2024-11-06T23:44:52Z","published":"2023-04-25T01:12:47Z","title":"Learning Task-Specific Strategies for Accelerated MRI","summary":"  Compressed sensing magnetic resonance imaging (CS-MRI) seeks to recover\nvisual information from subsampled measurements for diagnostic tasks.\nTraditional CS-MRI methods often separately address measurement subsampling,\nimage reconstruction, and task prediction, resulting in a suboptimal end-to-end\nperformance. In this work, we propose TACKLE as a unified co-design framework\nfor jointly optimizing subsampling, reconstruction, and prediction strategies\nfor the performance on downstream tasks. The na\\\"ive approach of simply\nappending a task prediction module and training with a task-specific loss leads\nto suboptimal downstream performance. Instead, we develop a training procedure\nwhere a backbone architecture is first trained for a generic pre-training task\n(image reconstruction in our case), and then fine-tuned for different\ndownstream tasks with a prediction head. Experimental results on multiple\npublic MRI datasets show that TACKLE achieves an improved performance on\nvarious tasks over traditional CS-MRI methods. We also demonstrate that TACKLE\nis robust to distribution shifts by showing that it generalizes to a new\ndataset we experimentally collected using different acquisition setups from the\ntraining data. Without additional fine-tuning, TACKLE leads to both numerical\nand visual improvements compared to existing baselines. We have further\nimplemented a learned 4$\\times$-accelerated sequence on a Siemens 3T MRI Skyra\nscanner. Compared to the fully-sampling scan that takes 335 seconds, our\noptimized sequence only takes 84 seconds, achieving a four-fold time reduction\nas desired, while maintaining high performance.\n","authors":["Zihui Wu","Tianwei Yin","Yu Sun","Robert Frost","Andre van der Kouwe","Adrian V. Dalca","Katherine L. Bouman"],"pdf_url":"https://arxiv.org/pdf/2304.12507v3.pdf","comment":"Our code is available at https://github.com/zihuiwu/TACKLE. More\n  information can be found at http://imaging.cms.caltech.edu/tackle/"},{"id":"http://arxiv.org/abs/2304.04901v2","updated":"2024-11-06T23:32:27Z","published":"2023-04-11T00:17:28Z","title":"Efficiently Collecting Training Dataset for 2D Object Detection by\n  Online Visual Feedback","summary":"  Training deep-learning-based vision systems require the manual annotation of\na significant number of images. Such manual annotation is highly time-consuming\nand labor-intensive. Although previous studies have attempted to eliminate the\neffort required for annotation, the effort required for image collection was\nretained. To address this, we propose a human-in-the-loop dataset collection\nmethod that uses a web application. To counterbalance the workload and\nperformance by encouraging the collection of multi-view object image datasets\nin an enjoyable manner, thereby amplifying motivation, we propose three types\nof online visual feedback features to track the progress of the collection\nstatus. Our experiments thoroughly investigated the impact of each feature on\ncollection performance and quality of operation. The results suggested the\nfeasibility of annotation and object detection.\n","authors":["Takuya Kiyokawa","Naoki Shirakura","Hiroki Katayama","Keita Tomochika","Jun Takamatsu"],"pdf_url":"https://arxiv.org/pdf/2304.04901v2.pdf","comment":"13 pages, 14 figures"},{"id":"http://arxiv.org/abs/2402.03478v2","updated":"2024-11-06T23:02:47Z","published":"2024-02-05T19:39:52Z","title":"Estimating Epistemic and Aleatoric Uncertainty with a Single Model","summary":"  Estimating and disentangling epistemic uncertainty, uncertainty that is\nreducible with more training data, and aleatoric uncertainty, uncertainty that\nis inherent to the task at hand, is critically important when applying machine\nlearning to high-stakes applications such as medical imaging and weather\nforecasting. Conditional diffusion models' breakthrough ability to accurately\nand efficiently sample from the posterior distribution of a dataset now makes\nuncertainty estimation conceptually straightforward: One need only train and\nsample from a large ensemble of diffusion models. Unfortunately, training such\nan ensemble becomes computationally intractable as the complexity of the model\narchitecture grows. In this work we introduce a new approach to ensembling,\nhyper-diffusion models (HyperDM), which allows one to accurately estimate both\nepistemic and aleatoric uncertainty with a single model. Unlike existing\nsingle-model uncertainty methods like Monte-Carlo dropout and Bayesian neural\nnetworks, HyperDM offers prediction accuracy on par with, and in some cases\nsuperior to, multi-model ensembles. Furthermore, our proposed approach scales\nto modern network architectures such as Attention U-Net and yields more\naccurate uncertainty estimates compared to existing methods. We validate our\nmethod on two distinct real-world tasks: x-ray computed tomography\nreconstruction and weather temperature forecasting.\n","authors":["Matthew A. Chan","Maria J. Molina","Christopher A. Metzler"],"pdf_url":"https://arxiv.org/pdf/2402.03478v2.pdf","comment":"19 pages, 11 figures. To be published in Conference on Neural\n  Information Processing Systems (NeurIPS) 2024"},{"id":"http://arxiv.org/abs/2312.02985v2","updated":"2024-11-06T23:02:02Z","published":"2023-11-15T13:28:02Z","title":"FocalPose++: Focal Length and Object Pose Estimation via Render and\n  Compare","summary":"  We introduce FocalPose++, a neural render-and-compare method for jointly\nestimating the camera-object 6D pose and camera focal length given a single RGB\ninput image depicting a known object. The contributions of this work are\nthreefold. First, we derive a focal length update rule that extends an existing\nstate-of-the-art render-and-compare 6D pose estimator to address the joint\nestimation task. Second, we investigate several different loss functions for\njointly estimating the object pose and focal length. We find that a combination\nof direct focal length regression with a reprojection loss disentangling the\ncontribution of translation, rotation, and focal length leads to improved\nresults. Third, we explore the effect of different synthetic training data on\nthe performance of our method. Specifically, we investigate different\ndistributions used for sampling object's 6D pose and camera's focal length when\nrendering the synthetic images, and show that parametric distribution fitted on\nreal training data works the best. We show results on three challenging\nbenchmark datasets that depict known 3D models in uncontrolled settings. We\ndemonstrate that our focal length and 6D pose estimates have lower error than\nthe existing state-of-the-art methods.\n","authors":["Martin Cífka","Georgy Ponimatkin","Yann Labbé","Bryan Russell","Mathieu Aubry","Vladimir Petrik","Josef Sivic"],"pdf_url":"https://arxiv.org/pdf/2312.02985v2.pdf","comment":"25 pages, 22 figures. IEEE TPAMI, 2024. Extended version of the\n  conference paper arXiv:2204.05145"},{"id":"http://arxiv.org/abs/2411.04291v1","updated":"2024-11-06T22:19:32Z","published":"2024-11-06T22:19:32Z","title":"Unfair Alignment: Examining Safety Alignment Across Vision Encoder\n  Layers in Vision-Language Models","summary":"  Vision-language models (VLMs) have improved significantly in multi-modal\ntasks, but their more complex architecture makes their safety alignment more\nchallenging than the alignment of large language models (LLMs). In this paper,\nwe reveal an unfair distribution of safety across the layers of VLM's vision\nencoder, with earlier and middle layers being disproportionately vulnerable to\nmalicious inputs compared to the more robust final layers. This 'cross-layer'\nvulnerability stems from the model's inability to generalize its safety\ntraining from the default architectural settings used during training to unseen\nor out-of-distribution scenarios, leaving certain layers exposed. We conduct a\ncomprehensive analysis by projecting activations from various intermediate\nlayers and demonstrate that these layers are more likely to generate harmful\noutputs when exposed to malicious inputs. Our experiments with LLaVA-1.5 and\nLlama 3.2 show discrepancies in attack success rates and toxicity scores across\nlayers, indicating that current safety alignment strategies focused on a single\ndefault layer are insufficient.\n","authors":["Saketh Bachu","Erfan Shayegani","Trishna Chakraborty","Rohit Lal","Arindam Dutta","Chengyu Song","Yue Dong","Nael Abu-Ghazaleh","Amit K. Roy-Chowdhury"],"pdf_url":"https://arxiv.org/pdf/2411.04291v1.pdf","comment":"Preprint, Under Review"},{"id":"http://arxiv.org/abs/2401.03115v2","updated":"2024-11-06T22:09:09Z","published":"2024-01-06T03:03:28Z","title":"Transferable Learned Image Compression-Resistant Adversarial\n  Perturbations","summary":"  Adversarial attacks can readily disrupt the image classification system,\nrevealing the vulnerability of DNN-based recognition tasks. While existing\nadversarial perturbations are primarily applied to uncompressed images or\ncompressed images by the traditional image compression method, i.e., JPEG,\nlimited studies have investigated the robustness of models for image\nclassification in the context of DNN-based image compression. With the rapid\nevolution of advanced image compression, DNN-based learned image compression\nhas emerged as the promising approach for transmitting images in many\nsecurity-critical applications, such as cloud-based face recognition and\nautonomous driving, due to its superior performance over traditional\ncompression. Therefore, there is a pressing need to fully investigate the\nrobustness of a classification system post-processed by learned image\ncompression. To bridge this research gap, we explore the adversarial attack on\na new pipeline that targets image classification models that utilize learned\nimage compressors as pre-processing modules. Furthermore, to enhance the\ntransferability of perturbations across various quality levels and\narchitectures of learned image compression models, we introduce a saliency\nscore-based sampling method to enable the fast generation of transferable\nperturbation. Extensive experiments with popular attack methods demonstrate the\nenhanced transferability of our proposed method when attacking images that have\nbeen post-processed with different learned image compression models.\n","authors":["Yang Sui","Zhuohang Li","Ding Ding","Xiang Pan","Xiaozhong Xu","Shan Liu","Zhenzhong Chen"],"pdf_url":"https://arxiv.org/pdf/2401.03115v2.pdf","comment":"Accepted by BMVC 2024"},{"id":"http://arxiv.org/abs/2410.12692v2","updated":"2024-11-06T21:46:48Z","published":"2024-10-16T15:52:32Z","title":"Machine learning approach to brain tumor detection and classification","summary":"  Brain tumor detection and classification are critical tasks in medical image\nanalysis, particularly in early-stage diagnosis, where accurate and timely\ndetection can significantly improve treatment outcomes. In this study, we apply\nvarious statistical and machine learning models to detect and classify brain\ntumors using brain MRI images. We explore a variety of statistical models\nincluding linear, logistic, and Bayesian regressions, and the machine learning\nmodels including decision tree, random forest, single-layer perceptron,\nmulti-layer perceptron, convolutional neural network (CNN), recurrent neural\nnetwork, and long short-term memory. Our findings show that CNN outperforms\nother models, achieving the best performance. Additionally, we confirm that the\nCNN model can also work for multi-class classification, distinguishing between\nfour categories of brain MRI images such as normal, glioma, meningioma, and\npituitary tumor images. This study demonstrates that machine learning\napproaches are suitable for brain tumor detection and classification,\nfacilitating real-world medical applications in assisting radiologists with\nearly and accurate diagnosis.\n","authors":["Alice Oh","Inyoung Noh","Jian Choo","Jihoo Lee","Justin Park","Kate Hwang","Sanghyeon Kim","Soo Min Oh"],"pdf_url":"https://arxiv.org/pdf/2410.12692v2.pdf","comment":"7 pages, 2 figures, 2 tables"},{"id":"http://arxiv.org/abs/2411.04269v1","updated":"2024-11-06T21:22:46Z","published":"2024-11-06T21:22:46Z","title":"Increasing the scalability of graph convolution for FPGA-implemented\n  event-based vision","summary":"  Event cameras are becoming increasingly popular as an alternative to\ntraditional frame-based vision sensors, especially in mobile robotics. Taking\nfull advantage of their high temporal resolution, high dynamic range, low power\nconsumption and sparsity of event data, which only reflects changes in the\nobserved scene, requires both an efficient algorithm and a specialised hardware\nplatform. A recent trend involves using Graph Convolutional Neural Networks\n(GCNNs) implemented on a heterogeneous SoC FPGA. In this paper we focus on\noptimising hardware modules for graph convolution to allow flexible selection\nof the FPGA resource (BlockRAM, DSP and LUT) for their implementation. We\npropose a ''two-step convolution'' approach that utilises additional BRAM\nbuffers in order to reduce up to 94% of LUT usage for multiplications. This\nmethod significantly improves the scalability of GCNNs, enabling the deployment\nof models with more layers, larger graphs sizes and their application for more\ndynamic scenarios.\n","authors":["Piotr Wzorek","Kamil Jeziorek","Tomasz Kryjak","Andrea Pinna"],"pdf_url":"https://arxiv.org/pdf/2411.04269v1.pdf","comment":"Accepted for the PhD forum during FPT 2024 (International Conference\n  on Field Programmable Technology), 10-12 December 2024, Sydney, Australia"},{"id":"http://arxiv.org/abs/2411.04263v1","updated":"2024-11-06T21:16:02Z","published":"2024-11-06T21:16:02Z","title":"Object Recognition in Human Computer Interaction:- A Comparative\n  Analysis","summary":"  Human-computer interaction (HCI) has been a widely researched area for many\nyears, with continuous advancements in technology leading to the development of\nnew techniques that change the way we interact with computers. With the recent\nadvent of powerful computers, we recognize human actions and interact\naccordingly, thus revolutionizing the way we interact with computers. The\npurpose of this paper is to provide a comparative analysis of various\nalgorithms used for recognizing user faces and gestures in the context of\ncomputer vision and HCI. This study aims to explore and evaluate the\nperformance of different algorithms in terms of accuracy, robustness, and\nefficiency. This study aims to provide a comprehensive analysis of algorithms\nfor face and gesture recognition in the context of computer vision and HCI,\nwith the goal of improving the design and development of interactive systems\nthat are more intuitive, efficient, and user-friendly.\n","authors":["Kaushik Ranade","Tanmay Khule","Riddhi More"],"pdf_url":"https://arxiv.org/pdf/2411.04263v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04255v1","updated":"2024-11-06T20:55:30Z","published":"2024-11-06T20:55:30Z","title":"Pose-Transformation and Radial Distance Clustering for Unsupervised\n  Person Re-identification","summary":"  Person re-identification (re-ID) aims to tackle the problem of matching\nidentities across non-overlapping cameras. Supervised approaches require\nidentity information that may be difficult to obtain and are inherently biased\ntowards the dataset they are trained on, making them unscalable across domains.\nTo overcome these challenges, we propose an unsupervised approach to the person\nre-ID setup. Having zero knowledge of true labels, our proposed method enhances\nthe discriminating ability of the learned features via a novel two-stage\ntraining strategy. The first stage involves training a deep network on an\nexpertly designed pose-transformed dataset obtained by generating multiple\nperturbations for each original image in the pose space. Next, the network\nlearns to map similar features closer in the feature space using the proposed\ndiscriminative clustering algorithm. We introduce a novel radial distance loss,\nthat attends to the fundamental aspects of feature learning - compact clusters\nwith low intra-cluster and high inter-cluster variation. Extensive experiments\non several large-scale re-ID datasets demonstrate the superiority of our method\ncompared to state-of-the-art approaches.\n","authors":["Siddharth Seth","Akash Sonth","Anirban Chakraborty"],"pdf_url":"https://arxiv.org/pdf/2411.04255v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04249v1","updated":"2024-11-06T20:42:13Z","published":"2024-11-06T20:42:13Z","title":"PocoLoco: A Point Cloud Diffusion Model of Human Shape in Loose Clothing","summary":"  Modeling a human avatar that can plausibly deform to articulations is an\nactive area of research. We present PocoLoco -- the first template-free,\npoint-based, pose-conditioned generative model for 3D humans in loose clothing.\nWe motivate our work by noting that most methods require a parametric model of\nthe human body to ground pose-dependent deformations. Consequently, they are\nrestricted to modeling clothing that is topologically similar to the naked body\nand do not extend well to loose clothing. The few methods that attempt to model\nloose clothing typically require either canonicalization or a\nUV-parameterization and need to address the challenging problem of explicitly\nestimating correspondences for the deforming clothes. In this work, we\nformulate avatar clothing deformation as a conditional point-cloud generation\ntask within the denoising diffusion framework. Crucially, our framework\noperates directly on unordered point clouds, eliminating the need for a\nparametric model or a clothing template. This also enables a variety of\npractical applications, such as point-cloud completion and pose-based editing\n-- important features for virtual human animation. As current datasets for\nhuman avatars in loose clothing are far too small for training diffusion\nmodels, we release a dataset of two subjects performing various poses in loose\nclothing with a total of 75K point clouds. By contributing towards tackling the\nchallenging task of effectively modeling loose clothing and expanding the\navailable data for training these models, we aim to set the stage for further\ninnovation in digital humans. The source code is available at\nhttps://github.com/sidsunny/pocoloco .\n","authors":["Siddharth Seth","Rishabh Dabral","Diogo Luvizon","Marc Habermann","Ming-Hsuan Yang","Christian Theobalt","Adam Kortylewski"],"pdf_url":"https://arxiv.org/pdf/2411.04249v1.pdf","comment":"WACV 2025"},{"id":"http://arxiv.org/abs/2410.22233v2","updated":"2024-11-06T19:52:58Z","published":"2024-10-29T17:01:05Z","title":"ContextIQ: A Multimodal Expert-Based Video Retrieval System for\n  Contextual Advertising","summary":"  Contextual advertising serves ads that are aligned to the content that the\nuser is viewing. The rapid growth of video content on social platforms and\nstreaming services, along with privacy concerns, has increased the need for\ncontextual advertising. Placing the right ad in the right context creates a\nseamless and pleasant ad viewing experience, resulting in higher audience\nengagement and, ultimately, better ad monetization. From a technology\nstandpoint, effective contextual advertising requires a video retrieval system\ncapable of understanding complex video content at a very granular level.\nCurrent text-to-video retrieval models based on joint multimodal training\ndemand large datasets and computational resources, limiting their practicality\nand lacking the key functionalities required for ad ecosystem integration. We\nintroduce ContextIQ, a multimodal expert-based video retrieval system designed\nspecifically for contextual advertising. ContextIQ utilizes modality-specific\nexperts-video, audio, transcript (captions), and metadata such as objects,\nactions, emotion, etc.-to create semantically rich video representations. We\nshow that our system, without joint training, achieves better or comparable\nresults to state-of-the-art models and commercial solutions on multiple\ntext-to-video retrieval benchmarks. Our ablation studies highlight the benefits\nof leveraging multiple modalities for enhanced video retrieval accuracy instead\nof using a vision-language model alone. Furthermore, we show how video\nretrieval systems such as ContextIQ can be used for contextual advertising in\nan ad ecosystem while also addressing concerns related to brand safety and\nfiltering inappropriate content.\n","authors":["Ashutosh Chaubey","Anoubhav Agarwaal","Sartaki Sinha Roy","Aayush Agrawal","Susmita Ghose"],"pdf_url":"https://arxiv.org/pdf/2410.22233v2.pdf","comment":"Accepted at WACV 2025"},{"id":"http://arxiv.org/abs/2411.04224v1","updated":"2024-11-06T19:44:36Z","published":"2024-11-06T19:44:36Z","title":"WiFlexFormer: Efficient WiFi-Based Person-Centric Sensing","summary":"  We propose WiFlexFormer, a highly efficient Transformer-based architecture\ndesigned for WiFi Channel State Information (CSI)-based person-centric sensing.\nWe benchmark WiFlexFormer against state-of-the-art vision and specialized\narchitectures for processing radio frequency data and demonstrate that it\nachieves comparable Human Activity Recognition (HAR) performance while offering\na significantly lower parameter count and faster inference times. With an\ninference time of just 10 ms on an Nvidia Jetson Orin Nano, WiFlexFormer is\noptimized for real-time inference. Additionally, its low parameter count\ncontributes to improved cross-domain generalization, where it often outperforms\nlarger models. Our comprehensive evaluation shows that WiFlexFormer is a\npotential solution for efficient, scalable WiFi-based sensing applications. The\nPyTorch implementation of WiFlexFormer is publicly available at:\nhttps://github.com/StrohmayerJ/WiFlexFormer.\n","authors":["Julian Strohmayer","Matthias Wödlinger","Martin Kampel"],"pdf_url":"https://arxiv.org/pdf/2411.04224v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02537v2","updated":"2024-11-06T19:27:10Z","published":"2024-11-04T19:16:53Z","title":"INQUIRE: A Natural World Text-to-Image Retrieval Benchmark","summary":"  We introduce INQUIRE, a text-to-image retrieval benchmark designed to\nchallenge multimodal vision-language models on expert-level queries. INQUIRE\nincludes iNaturalist 2024 (iNat24), a new dataset of five million natural world\nimages, along with 250 expert-level retrieval queries. These queries are paired\nwith all relevant images comprehensively labeled within iNat24, comprising\n33,000 total matches. Queries span categories such as species identification,\ncontext, behavior, and appearance, emphasizing tasks that require nuanced image\nunderstanding and domain expertise. Our benchmark evaluates two core retrieval\ntasks: (1) INQUIRE-Fullrank, a full dataset ranking task, and (2)\nINQUIRE-Rerank, a reranking task for refining top-100 retrievals. Detailed\nevaluation of a range of recent multimodal models demonstrates that INQUIRE\nposes a significant challenge, with the best models failing to achieve an\nmAP@50 above 50%. In addition, we show that reranking with more powerful\nmultimodal models can enhance retrieval performance, yet there remains a\nsignificant margin for improvement. By focusing on scientifically-motivated\necological challenges, INQUIRE aims to bridge the gap between AI capabilities\nand the needs of real-world scientific inquiry, encouraging the development of\nretrieval systems that can assist with accelerating ecological and biodiversity\nresearch. Our dataset and code are available at\nhttps://inquire-benchmark.github.io\n","authors":["Edward Vendrow","Omiros Pantazis","Alexander Shepard","Gabriel Brostow","Kate E. Jones","Oisin Mac Aodha","Sara Beery","Grant Van Horn"],"pdf_url":"https://arxiv.org/pdf/2411.02537v2.pdf","comment":"Published in NeurIPS 2024, Datasets and Benchmarks Track"},{"id":"http://arxiv.org/abs/2411.04168v1","updated":"2024-11-06T18:59:17Z","published":"2024-11-06T18:59:17Z","title":"DiMSUM: Diffusion Mamba -- A Scalable and Unified Spatial-Frequency\n  Method for Image Generation","summary":"  We introduce a novel state-space architecture for diffusion models,\neffectively harnessing spatial and frequency information to enhance the\ninductive bias towards local features in input images for image generation\ntasks. While state-space networks, including Mamba, a revolutionary advancement\nin recurrent neural networks, typically scan input sequences from left to\nright, they face difficulties in designing effective scanning strategies,\nespecially in the processing of image data. Our method demonstrates that\nintegrating wavelet transformation into Mamba enhances the local structure\nawareness of visual inputs and better captures long-range relations of\nfrequencies by disentangling them into wavelet subbands, representing both low-\nand high-frequency components. These wavelet-based outputs are then processed\nand seamlessly fused with the original Mamba outputs through a cross-attention\nfusion layer, combining both spatial and frequency information to optimize the\norder awareness of state-space models which is essential for the details and\noverall quality of image generation. Besides, we introduce a globally-shared\ntransformer to supercharge the performance of Mamba, harnessing its exceptional\npower to capture global relationships. Through extensive experiments on\nstandard benchmarks, our method demonstrates superior results compared to DiT\nand DIFFUSSM, achieving faster training convergence and delivering high-quality\noutputs. The codes and pretrained models are released at\nhttps://github.com/VinAIResearch/DiMSUM.git.\n","authors":["Hao Phung","Quan Dao","Trung Dao","Hoang Phan","Dimitris Metaxas","Anh Tran"],"pdf_url":"https://arxiv.org/pdf/2411.04168v1.pdf","comment":"Accepted to NeurIPS 2024. Project page:\n  https://hao-pt.github.io/dimsum/"}]},"2024-11-07T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2406.00922v3","updated":"2024-11-07T18:59:30Z","published":"2024-06-03T01:32:52Z","title":"MediQ: Question-Asking LLMs and a Benchmark for Reliable Interactive\n  Clinical Reasoning","summary":"  Users typically engage with LLMs interactively, yet most existing benchmarks\nevaluate them in a static, single-turn format, posing reliability concerns in\ninteractive scenarios. We identify a key obstacle towards reliability: LLMs are\ntrained to answer any question, even with incomplete context or insufficient\nknowledge. In this paper, we propose to change the static paradigm to an\ninteractive one, develop systems that proactively ask questions to gather more\ninformation and respond reliably, and introduce an benchmark - MediQ - to\nevaluate question-asking ability in LLMs. MediQ simulates clinical interactions\nconsisting of a Patient System and an adaptive Expert System; with potentially\nincomplete initial information, the Expert refrains from making diagnostic\ndecisions when unconfident, and instead elicits missing details via follow-up\nquestions. We provide a pipeline to convert single-turn medical benchmarks into\nan interactive format. Our results show that directly prompting\nstate-of-the-art LLMs to ask questions degrades performance, indicating that\nadapting LLMs to proactive information-seeking settings is nontrivial. We\nexperiment with abstention strategies to better estimate model confidence and\ndecide when to ask questions, improving diagnostic accuracy by 22.3%; however,\nperformance still lags compared to an (unrealistic in practice) upper bound\nwith complete information upfront. Further analyses show improved interactive\nperformance with filtering irrelevant contexts and reformatting conversations.\nOverall, we introduce a novel problem towards LLM reliability, an interactive\nMediQ benchmark and a novel question-asking system, and highlight directions to\nextend LLMs' information-seeking abilities in critical domains.\n","authors":["Shuyue Stella Li","Vidhisha Balachandran","Shangbin Feng","Jonathan S. Ilgen","Emma Pierson","Pang Wei Koh","Yulia Tsvetkov"],"pdf_url":"https://arxiv.org/pdf/2406.00922v3.pdf","comment":"29 pages, 12 figures"},{"id":"http://arxiv.org/abs/2411.05001v1","updated":"2024-11-07T18:59:28Z","published":"2024-11-07T18:59:28Z","title":"Analyzing The Language of Visual Tokens","summary":"  With the introduction of transformer-based models for vision and language\ntasks, such as LLaVA and Chameleon, there has been renewed interest in the\ndiscrete tokenized representation of images. These models often treat image\npatches as discrete tokens, analogous to words in natural language, learning\njoint alignments between visual and human languages. However, little is known\nabout the statistical behavior of these visual languages - whether they follow\nsimilar frequency distributions, grammatical structures, or topologies as\nnatural languages. In this paper, we take a natural-language-centric approach\nto analyzing discrete visual languages and uncover striking similarities and\nfundamental differences. We demonstrate that, although visual languages adhere\nto Zipfian distributions, higher token innovation drives greater entropy and\nlower compression, with tokens predominantly representing object parts,\nindicating intermediate granularity. We also show that visual languages lack\ncohesive grammatical structures, leading to higher perplexity and weaker\nhierarchical organization compared to natural languages. Finally, we\ndemonstrate that, while vision models align more closely with natural languages\nthan other models, this alignment remains significantly weaker than the\ncohesion found within natural languages. Through these experiments, we\ndemonstrate how understanding the statistical properties of discrete visual\nlanguages can inform the design of more effective computer vision models.\n","authors":["David M. Chan","Rodolfo Corona","Joonyong Park","Cheol Jun Cho","Yutong Bai","Trevor Darrell"],"pdf_url":"https://arxiv.org/pdf/2411.05001v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.05000v1","updated":"2024-11-07T18:59:27Z","published":"2024-11-07T18:59:27Z","title":"Needle Threading: Can LLMs Follow Threads through Near-Million-Scale\n  Haystacks?","summary":"  As the context limits of Large Language Models (LLMs) increase, the range of\npossible applications and downstream functions broadens. In many real-world\ntasks, decisions depend on details scattered across collections of often\ndisparate documents containing mostly irrelevant information. Long-context LLMs\nappear well-suited to this form of complex information retrieval and reasoning,\nwhich has traditionally proven costly and time-consuming. However, although the\ndevelopment of longer context models has seen rapid gains in recent years, our\nunderstanding of how effectively LLMs use their context has not kept pace. To\naddress this, we conduct a set of retrieval experiments designed to evaluate\nthe capabilities of 17 leading LLMs, such as their ability to follow threads of\ninformation through the context window. Strikingly, we find that many models\nare remarkably threadsafe: capable of simultaneously following multiple threads\nwithout significant loss in performance. Still, for many models, we find the\neffective context limit is significantly shorter than the supported context\nlength, with accuracy decreasing as the context window grows. Our study also\nhighlights the important point that token counts from different tokenizers\nshould not be directly compared -- they often correspond to substantially\ndifferent numbers of written characters. We release our code and long-context\nexperimental data.\n","authors":["Jonathan Roberts","Kai Han","Samuel Albanie"],"pdf_url":"https://arxiv.org/pdf/2411.05000v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04997v1","updated":"2024-11-07T18:59:16Z","published":"2024-11-07T18:59:16Z","title":"LLM2CLIP: Powerful Language Model Unlock Richer Visual Representation","summary":"  CLIP is one of the most important multimodal foundational models today. What\npowers CLIP's capabilities? The rich supervision signals provided by natural\nlanguage, the carrier of human knowledge, shape a powerful cross-modal\nrepresentation space. However, with the rapid advancements in large language\nmodels LLMs like GPT-4 and LLaMA, the boundaries of language comprehension and\ngeneration are continually being pushed. This raises an intriguing question:\ncan the capabilities of LLMs be harnessed to further improve multimodal\nrepresentation learning? The potential benefits of incorporating LLMs into CLIP\nare clear. LLMs' strong textual understanding can fundamentally improve CLIP's\nability to handle image captions, drastically enhancing its ability to process\nlong and complex texts, a well-known limitation of vanilla CLIP. Moreover, LLMs\nare trained on a vast corpus of text, possessing open-world knowledge. This\nallows them to expand on caption information during training, increasing the\nefficiency of the learning process. In this paper, we propose LLM2CLIP, a novel\napproach that embraces the power of LLMs to unlock CLIP's potential. By\nfine-tuning the LLM in the caption space with contrastive learning, we extract\nits textual capabilities into the output embeddings, significantly improving\nthe output layer's textual discriminability. We then design an efficient\ntraining process where the fine-tuned LLM acts as a powerful teacher for CLIP's\nvisual encoder. Thanks to the LLM's presence, we can now incorporate longer and\nmore complex captions without being restricted by vanilla CLIP's text encoder's\ncontext window and ability limitations. Our experiments demonstrate that this\napproach brings substantial improvements in cross-modal tasks.\n","authors":["Weiquan Huang","Aoqi Wu","Yifan Yang","Xufang Luo","Yuqing Yang","Liang Hu","Qi Dai","Xiyang Dai","Dongdong Chen","Chong Luo","Lili Qiu"],"pdf_url":"https://arxiv.org/pdf/2411.04997v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04996v1","updated":"2024-11-07T18:59:06Z","published":"2024-11-07T18:59:06Z","title":"Mixture-of-Transformers: A Sparse and Scalable Architecture for\n  Multi-Modal Foundation Models","summary":"  The development of large language models (LLMs) has expanded to multi-modal\nsystems capable of processing text, images, and speech within a unified\nframework. Training these models demands significantly larger datasets and\ncomputational resources compared to text-only LLMs. To address the scaling\nchallenges, we introduce Mixture-of-Transformers (MoT), a sparse multi-modal\ntransformer architecture that significantly reduces pretraining computational\ncosts. MoT decouples non-embedding parameters of the model by modality --\nincluding feed-forward networks, attention matrices, and layer normalization --\nenabling modality-specific processing with global self-attention over the full\ninput sequence. We evaluate MoT across multiple settings and model scales. In\nthe Chameleon 7B setting (autoregressive text-and-image generation), MoT\nmatches the dense baseline's performance using only 55.8\\% of the FLOPs. When\nextended to include speech, MoT reaches speech performance comparable to the\ndense baseline with only 37.2\\% of the FLOPs. In the Transfusion setting, where\ntext and image are trained with different objectives, a 7B MoT model matches\nthe image modality performance of the dense baseline with one third of the\nFLOPs, and a 760M MoT model outperforms a 1.4B dense baseline across key image\ngeneration metrics. System profiling further highlights MoT's practical\nbenefits, achieving dense baseline image quality in 47.2\\% of the wall-clock\ntime and text quality in 75.6\\% of the wall-clock time (measured on AWS\np4de.24xlarge instances with NVIDIA A100 GPUs).\n","authors":["Weixin Liang","Lili Yu","Liang Luo","Srinivasan Iyer","Ning Dong","Chunting Zhou","Gargi Ghosh","Mike Lewis","Wen-tau Yih","Luke Zettlemoyer","Xi Victoria Lin"],"pdf_url":"https://arxiv.org/pdf/2411.04996v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04986v1","updated":"2024-11-07T18:55:09Z","published":"2024-11-07T18:55:09Z","title":"The Semantic Hub Hypothesis: Language Models Share Semantic\n  Representations Across Languages and Modalities","summary":"  Modern language models can process inputs across diverse languages and\nmodalities. We hypothesize that models acquire this capability through learning\na shared representation space across heterogeneous data types (e.g., different\nlanguages and modalities), which places semantically similar inputs near one\nanother, even if they are from different modalities/languages. We term this the\nsemantic hub hypothesis, following the hub-and-spoke model from neuroscience\n(Patterson et al., 2007) which posits that semantic knowledge in the human\nbrain is organized through a transmodal semantic \"hub\" which integrates\ninformation from various modality-specific \"spokes\" regions. We first show that\nmodel representations for semantically equivalent inputs in different languages\nare similar in the intermediate layers, and that this space can be interpreted\nusing the model's dominant pretraining language via the logit lens. This\ntendency extends to other data types, including arithmetic expressions, code,\nand visual/audio inputs. Interventions in the shared representation space in\none data type also predictably affect model outputs in other data types,\nsuggesting that this shared representations space is not simply a vestigial\nbyproduct of large-scale training on broad data, but something that is actively\nutilized by the model during input processing.\n","authors":["Zhaofeng Wu","Xinyan Velocity Yu","Dani Yogatama","Jiasen Lu","Yoon Kim"],"pdf_url":"https://arxiv.org/pdf/2411.04986v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04975v1","updated":"2024-11-07T18:49:33Z","published":"2024-11-07T18:49:33Z","title":"SuffixDecoding: A Model-Free Approach to Speeding Up Large Language\n  Model Inference","summary":"  We present SuffixDecoding, a novel model-free approach to accelerating large\nlanguage model (LLM) inference through speculative decoding. Unlike existing\nmethods that rely on draft models or specialized decoding heads, SuffixDecoding\nleverages suffix trees built from previously generated outputs to efficiently\npredict candidate token sequences. Our approach enables flexible\ntree-structured speculation without the overhead of maintaining and\norchestrating additional models. SuffixDecoding builds and dynamically updates\nsuffix trees to capture patterns in the generated text, using them to construct\nspeculation trees through a principled scoring mechanism based on empirical\ntoken frequencies. SuffixDecoding requires only CPU memory which is plentiful\nand underutilized on typical LLM serving nodes. We demonstrate that\nSuffixDecoding achieves competitive speedups compared to model-based approaches\nacross diverse workloads including open-domain chat, code generation, and\ntext-to-SQL tasks. For open-ended chat and code generation tasks,\nSuffixDecoding achieves up to $1.4\\times$ higher output throughput than\nSpecInfer and up to $1.1\\times$ lower time-per-token (TPOT) latency. For a\nproprietary multi-LLM text-to-SQL application, SuffixDecoding achieves up to\n$2.9\\times$ higher output throughput and $3\\times$ lower latency than\nspeculative decoding. Our evaluation shows that SuffixDecoding maintains high\nacceptance rates even with small reference corpora of 256 examples, while\ncontinuing to improve performance as more historical outputs are incorporated.\n","authors":["Gabriele Oliaro","Zhihao Jia","Daniel Campos","Aurick Qiao"],"pdf_url":"https://arxiv.org/pdf/2411.04975v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04965v1","updated":"2024-11-07T18:41:50Z","published":"2024-11-07T18:41:50Z","title":"BitNet a4.8: 4-bit Activations for 1-bit LLMs","summary":"  Recent research on the 1-bit Large Language Models (LLMs), such as BitNet\nb1.58, presents a promising direction for reducing the inference cost of LLMs\nwhile maintaining their performance. In this work, we introduce BitNet a4.8,\nenabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid\nquantization and sparsification strategy to mitigate the quantization errors\nintroduced by the outlier channels. Specifically, we utilize 4-bit activations\nfor inputs to the attention and feed-forward network layers, while sparsifying\nintermediate states followed with 8-bit quantization. Extensive experiments\ndemonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58\nwith equivalent training costs, while being faster in inference with enabling\n4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of\nparameters and supports 3-bit KV cache, further enhancing the efficiency of\nlarge-scale LLM deployment and inference.\n","authors":["Hongyu Wang","Shuming Ma","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2411.04965v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2411.04962v1","updated":"2024-11-07T18:39:04Z","published":"2024-11-07T18:39:04Z","title":"Position Paper On Diagnostic Uncertainty Estimation from Large Language\n  Models: Next-Word Probability Is Not Pre-test Probability","summary":"  Large language models (LLMs) are being explored for diagnostic decision\nsupport, yet their ability to estimate pre-test probabilities, vital for\nclinical decision-making, remains limited. This study evaluates two LLMs,\nMistral-7B and Llama3-70B, using structured electronic health record data on\nthree diagnosis tasks. We examined three current methods of extracting LLM\nprobability estimations and revealed their limitations. We aim to highlight the\nneed for improved techniques in LLM confidence estimation.\n","authors":["Yanjun Gao","Skatje Myers","Shan Chen","Dmitriy Dligach","Timothy A Miller","Danielle Bitterman","Guanhua Chen","Anoop Mayampurath","Matthew Churpek","Majid Afshar"],"pdf_url":"https://arxiv.org/pdf/2411.04962v1.pdf","comment":"Accepted to GenAI4Health Workshop at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.04952v1","updated":"2024-11-07T18:29:38Z","published":"2024-11-07T18:29:38Z","title":"M3DocRAG: Multi-modal Retrieval is What You Need for Multi-page\n  Multi-document Understanding","summary":"  Document visual question answering (DocVQA) pipelines that answer questions\nfrom documents have broad applications. Existing methods focus on handling\nsingle-page documents with multi-modal language models (MLMs), or rely on\ntext-based retrieval-augmented generation (RAG) that uses text extraction tools\nsuch as optical character recognition (OCR). However, there are difficulties in\napplying these methods in real-world scenarios: (a) questions often require\ninformation across different pages or documents, where MLMs cannot handle many\nlong documents; (b) documents often have important information in visual\nelements such as figures, but text extraction tools ignore them. We introduce\nM3DocRAG, a novel multi-modal RAG framework that flexibly accommodates various\ndocument contexts (closed-domain and open-domain), question hops (single-hop\nand multi-hop), and evidence modalities (text, chart, figure, etc.). M3DocRAG\nfinds relevant documents and answers questions using a multi-modal retriever\nand an MLM, so that it can efficiently handle single or many documents while\npreserving visual information. Since previous DocVQA datasets ask questions in\nthe context of a specific document, we also present M3DocVQA, a new benchmark\nfor evaluating open-domain DocVQA over 3,000+ PDF documents with 40,000+ pages.\nIn three benchmarks (M3DocVQA/MMLongBench-Doc/MP-DocVQA), empirical results\nshow that M3DocRAG with ColPali and Qwen2-VL 7B achieves superior performance\nthan many strong baselines, including state-of-the-art performance in\nMP-DocVQA. We provide comprehensive analyses of different indexing, MLMs, and\nretrieval models. Lastly, we qualitatively show that M3DocRAG can successfully\nhandle various scenarios, such as when relevant information exists across\nmultiple pages and when answer evidence only exists in images.\n","authors":["Jaemin Cho","Debanjan Mahata","Ozan Irsoy","Yujie He","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2411.04952v1.pdf","comment":"Project webpage: https://m3docrag.github.io"},{"id":"http://arxiv.org/abs/2411.04950v1","updated":"2024-11-07T18:28:40Z","published":"2024-11-07T18:28:40Z","title":"Estimating the Influence of Sequentially Correlated Literary Properties\n  in Textual Classification: A Data-Centric Hypothesis-Testing Approach","summary":"  Stylometry aims to distinguish authors by analyzing literary traits assumed\nto reflect semi-conscious choices distinct from elements like genre or theme.\nHowever, these components often overlap, complicating text classification based\nsolely on feature distributions. While some literary properties, such as\nthematic content, are likely to manifest as correlations between adjacent text\nunits, others, like authorial style, may be independent thereof. We introduce a\nhypothesis-testing approach to evaluate the influence of sequentially\ncorrelated literary properties on text classification, aiming to determine when\nthese correlations drive classification. Using a multivariate binary\ndistribution, our method models sequential correlations between text units as a\nstochastic process, assessing the likelihood of clustering across varying\nadjacency scales. This enables us to examine whether classification is\ndominated by sequentially correlated properties or remains independent. In\nexperiments on a diverse English prose corpus, our analysis integrates\ntraditional and neural embeddings within supervised and unsupervised\nframeworks. Results demonstrate that our approach effectively identifies when\ntextual classification is not primarily influenced by sequentially correlated\nliterary properties, particularly in cases where texts differ in authorial\nstyle or genre rather than by a single author within a similar genre.\n","authors":["Gideon Yoffe","Nachum Dershowitz","Ariel Vishne","Barak Sober"],"pdf_url":"https://arxiv.org/pdf/2411.04950v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14894v2","updated":"2024-11-07T18:15:23Z","published":"2024-06-21T06:30:16Z","title":"Talking the Talk Does Not Entail Walking the Walk: On the Limits of\n  Large Language Models in Lexical Entailment Recognition","summary":"  Verbs form the backbone of language, providing the structure and meaning to\nsentences. Yet, their intricate semantic nuances pose a longstanding challenge.\nUnderstanding verb relations through the concept of lexical entailment is\ncrucial for comprehending sentence meanings and grasping verb dynamics. This\nwork investigates the capabilities of eight Large Language Models in\nrecognizing lexical entailment relations among verbs through differently\ndevised prompting strategies and zero-/few-shot settings over verb pairs from\ntwo lexical databases, namely WordNet and HyperLex. Our findings unveil that\nthe models can tackle the lexical entailment recognition task with moderately\ngood performance, although at varying degree of effectiveness and under\ndifferent conditions. Also, utilizing few-shot prompting can enhance the\nmodels' performance. However, perfectly solving the task arises as an unmet\nchallenge for all examined LLMs, which raises an emergence for further research\ndevelopments on this topic.\n","authors":["Candida M. Greco","Lucio La Cava","Andrea Tagarelli"],"pdf_url":"https://arxiv.org/pdf/2406.14894v2.pdf","comment":"Accepted for publication at The 2024 Conference on Empirical Methods\n  in Natural Language Processing (EMNLP-2024) - Findings"},{"id":"http://arxiv.org/abs/2411.04920v1","updated":"2024-11-07T17:57:03Z","published":"2024-11-07T17:57:03Z","title":"GPTKB: Building Very Large Knowledge Bases from Language Models","summary":"  General-domain knowledge bases (KB), in particular the \"big three\" --\nWikidata, Yago and DBpedia -- are the backbone of many intelligent\napplications. While these three have seen steady development, comprehensive KB\nconstruction at large has seen few fresh attempts. In this work, we propose to\nbuild a large general-domain KB entirely from a large language model (LLM). We\ndemonstrate the feasibility of large-scale KB construction from LLMs, while\nhighlighting specific challenges arising around entity recognition, entity and\nproperty canonicalization, and taxonomy construction. As a prototype, we use\nGPT-4o-mini to construct GPTKB, which contains 105 million triples for more\nthan 2.9 million entities, at a cost 100x less than previous KBC projects. Our\nwork is a landmark for two fields: For NLP, for the first time, it provides\n\\textit{constructive} insights into the knowledge (or beliefs) of LLMs. For the\nSemantic Web, it shows novel ways forward for the long-standing challenge of\ngeneral-domain KB construction. GPTKB is accessible at https://gptkb.org.\n","authors":["Yujia Hu","Shrestha Ghosh","Tuan-Phong Nugyen","Simon Razniewski"],"pdf_url":"https://arxiv.org/pdf/2411.04920v1.pdf","comment":"11 pages, 4 tables"},{"id":"http://arxiv.org/abs/2406.15586v2","updated":"2024-11-07T17:56:19Z","published":"2024-06-21T18:41:22Z","title":"TinyStyler: Efficient Few-Shot Text Style Transfer with Authorship\n  Embeddings","summary":"  The goal of text style transfer is to transform the style of texts while\npreserving their original meaning, often with only a few examples of the target\nstyle. Existing style transfer methods generally rely on the few-shot\ncapabilities of large language models or on complex controllable text\ngeneration approaches that are inefficient and underperform on fluency metrics.\nWe introduce TinyStyler, a lightweight but effective approach, which leverages\na small language model (800M params) and pre-trained authorship embeddings to\nperform efficient, few-shot text style transfer. We evaluate on the challenging\ntask of authorship style transfer and find TinyStyler outperforms strong\napproaches such as GPT-4. We also evaluate TinyStyler's ability to perform text\nattribute style transfer (formal $\\leftrightarrow$ informal) with automatic and\nhuman evaluations and find that the approach outperforms recent controllable\ntext generation methods. Our model has been made publicly available at\nhttps://huggingface.co/tinystyler/tinystyler .\n","authors":["Zachary Horvitz","Ajay Patel","Kanishk Singh","Chris Callison-Burch","Kathleen McKeown","Zhou Yu"],"pdf_url":"https://arxiv.org/pdf/2406.15586v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04914v1","updated":"2024-11-07T17:53:47Z","published":"2024-11-07T17:53:47Z","title":"GASE: Generatively Augmented Sentence Encoding","summary":"  We propose an approach to enhance sentence embeddings by applying generative\ntext models for data augmentation at inference time. Unlike conventional data\naugmentation that utilises synthetic training data, our approach does not\nrequire access to model parameters or the computational resources typically\nrequired for fine-tuning state-of-the-art models. Generatively Augmented\nSentence Encoding uses diverse linguistic synthetic variants of input texts\ngenerated by paraphrasing, summarising, or extracting keywords, followed by\npooling the original and synthetic embeddings. Experimental results on the\nMassive Text Embedding Benchmark for Semantic Textual Similarity (STS)\ndemonstrate performance improvements across a range of embedding models using\ndifferent generative models for augmentation. We find that generative\naugmentation leads to larger performance improvements for embedding models with\nlower baseline performance. These findings suggest that integrating generative\naugmentation at inference time adds semantic diversity and can enhance the\nrobustness and generalizability of sentence embeddings for embedding models.\nOur results show that the degree to which generative augmentation can improve\nSTS performance depends not only on the embedding model but also on the\ndataset. From a broader perspective, the approach allows trading training for\ninference compute.\n","authors":["Manuel Frank","Haithem Afli"],"pdf_url":"https://arxiv.org/pdf/2411.04914v1.pdf","comment":"12 pages, 3 figures"},{"id":"http://arxiv.org/abs/2411.04905v1","updated":"2024-11-07T17:47:25Z","published":"2024-11-07T17:47:25Z","title":"OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models","summary":"  Large language models (LLMs) for code have become indispensable in various\ndomains, including code generation, reasoning tasks and agent systems.While\nopen-access code LLMs are increasingly approaching the performance levels of\nproprietary models, high-quality code LLMs suitable for rigorous scientific\ninvestigation, particularly those with reproducible data processing pipelines\nand transparent training protocols, remain limited. The scarcity is due to\nvarious challenges, including resource constraints, ethical considerations, and\nthe competitive advantages of keeping models advanced. To address the gap, we\nintroduce OpenCoder, a top-tier code LLM that not only achieves performance\ncomparable to leading models but also serves as an ``open cookbook'' for the\nresearch community. Unlike most prior efforts, we release not only model\nweights and inference code, but also the reproducible training data, complete\ndata processing pipeline, rigorous experimental ablation results, and detailed\ntraining protocols for open scientific research. Through this comprehensive\nrelease, we identify the key ingredients for building a top-tier code LLM: (1)\ncode optimized heuristic rules for data cleaning and methods for data\ndeduplication, (2) recall of text corpus related to code and (3) high-quality\nsynthetic data in both annealing and supervised fine-tuning stages. By offering\nthis level of openness, we aim to broaden access to all aspects of a top-tier\ncode LLM, with OpenCoder serving as both a powerful model and an open\nfoundation to accelerate research, and enable reproducible advancements in code\nAI.\n","authors":["Siming Huang","Tianhao Cheng","Jason Klein Liu","Jiaran Hao","Liuyihan Song","Yang Xu","J. Yang","J. H. Liu","Chenchen Zhang","Linzheng Chai","Ruifeng Yuan","Zhaoxiang Zhang","Jie Fu","Qian Liu","Ge Zhang","Zili Wang","Yuan Qi","Yinghui Xu","Wei Chu"],"pdf_url":"https://arxiv.org/pdf/2411.04905v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15814v2","updated":"2024-11-07T17:33:37Z","published":"2024-07-22T17:26:12Z","title":"Perceptions of Linguistic Uncertainty by Language Models and Humans","summary":"  _Uncertainty expressions_ such as \"probably\" or \"highly unlikely\" are\npervasive in human language. While prior work has established that there is\npopulation-level agreement in terms of how humans quantitatively interpret\nthese expressions, there has been little inquiry into the abilities of language\nmodels in the same context. In this paper, we investigate how language models\nmap linguistic expressions of uncertainty to numerical responses. Our approach\nassesses whether language models can employ theory of mind in this setting:\nunderstanding the uncertainty of another agent about a particular statement,\nindependently of the model's own certainty about that statement. We find that 7\nout of 10 models are able to map uncertainty expressions to probabilistic\nresponses in a human-like manner. However, we observe systematically different\nbehavior depending on whether a statement is actually true or false. This\nsensitivity indicates that language models are substantially more susceptible\nto bias based on their prior knowledge (as compared to humans). These findings\nraise important questions and have broad implications for human-AI and AI-AI\ncommunication.\n","authors":["Catarina G Belem","Markelle Kelly","Mark Steyvers","Sameer Singh","Padhraic Smyth"],"pdf_url":"https://arxiv.org/pdf/2407.15814v2.pdf","comment":"Accepted at EMNLP 2024 (Main)"},{"id":"http://arxiv.org/abs/2410.04981v2","updated":"2024-11-07T17:25:45Z","published":"2024-10-07T12:22:06Z","title":"On the Rigour of Scientific Writing: Criteria, Analysis, and Insights","summary":"  Rigour is crucial for scientific research as it ensures the reproducibility\nand validity of results and findings. Despite its importance, little work\nexists on modelling rigour computationally, and there is a lack of analysis on\nwhether these criteria can effectively signal or measure the rigour of\nscientific papers in practice. In this paper, we introduce a bottom-up,\ndata-driven framework to automatically identify and define rigour criteria and\nassess their relevance in scientific writing. Our framework includes rigour\nkeyword extraction, detailed rigour definition generation, and salient criteria\nidentification. Furthermore, our framework is domain-agnostic and can be\ntailored to the evaluation of scientific rigour for different areas,\naccommodating the distinct salient criteria across fields. We conducted\ncomprehensive experiments based on datasets collected from two high impact\nvenues for Machine Learning and NLP (i.e., ICLR and ACL) to demonstrate the\neffectiveness of our framework in modelling rigour. In addition, we analyse\nlinguistic patterns of rigour, revealing that framing certainty is crucial for\nenhancing the perception of scientific rigour, while suggestion certainty and\nprobability uncertainty diminish it.\n","authors":["Joseph James","Chenghao Xiao","Yucheng Li","Chenghua Lin"],"pdf_url":"https://arxiv.org/pdf/2410.04981v2.pdf","comment":"Accepted Findings at EMNLP 2024"},{"id":"http://arxiv.org/abs/2411.04862v1","updated":"2024-11-07T16:53:09Z","published":"2024-11-07T16:53:09Z","title":"Sentiment Analysis of Spanish Political Party Tweets Using Pre-trained\n  Language Models","summary":"  Title: Sentiment Analysis of Spanish Political Party Communications on\nTwitter Using Pre-trained Language Models\n  Authors: Chuqiao Song, Shunzhang Chen, Xinyi Cai, Hao Chen\n  Comments: 21 pages, 6 figures\n  Abstract: This study investigates sentiment patterns within Spanish political\nparty communications on Twitter by leveraging BETO and RoBERTuito, two\npre-trained language models optimized for Spanish text. Using a dataset of\ntweets from major Spanish political parties: PSOE, PP, Vox, Podemos, and\nCiudadanos, spanning 2019 to 2024, this research analyzes sentiment\ndistributions and explores the relationship between sentiment expression and\nparty ideology. The findings indicate that both models consistently identify a\npredominant Neutral sentiment across all parties, with significant variations\nin Negative and Positive sentiments that align with ideological distinctions.\nSpecifically, Vox exhibits higher levels of Negative sentiment, while PSOE\ndemonstrates relatively high Positive sentiment, supporting the hypothesis that\nemotional appeals in political messaging reflect ideological stances. This\nstudy underscores the potential of pre-trained language models for non-English\nsentiment analysis on social media, providing insights into sentiment dynamics\nthat shape public discourse within Spain's multi-party political system.\n  Keywords: Spanish politics, sentiment analysis, pre-trained language models,\nTwitter, BETO, RoBERTuito, political ideology, multi-party system\n","authors":["Chuqiao Song","Shunzhang Chen","Xinyi Cai","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2411.04862v1.pdf","comment":"21 pages, 6 figures"},{"id":"http://arxiv.org/abs/2402.09269v2","updated":"2024-11-07T16:43:01Z","published":"2024-02-14T15:55:30Z","title":"Personalized Large Language Models","summary":"  Large language models (LLMs) have significantly advanced Natural Language\nProcessing (NLP) tasks in recent years. However, their universal nature poses\nlimitations in scenarios requiring personalized responses, such as\nrecommendation systems and chatbots. This paper investigates methods to\npersonalize LLMs, comparing fine-tuning and zero-shot reasoning approaches on\nsubjective tasks. Results demonstrate that personalized fine-tuning improves\nmodel reasoning compared to non-personalized models. Experiments on datasets\nfor emotion recognition and hate speech detection show consistent performance\ngains with personalized methods across different LLM architectures. These\nfindings underscore the importance of personalization for enhancing LLM\ncapabilities in subjective text perception tasks.\n","authors":["Stanisław Woźniak","Bartłomiej Koptyra","Arkadiusz Janz","Przemysław Kazienko","Jan Kocoń"],"pdf_url":"https://arxiv.org/pdf/2402.09269v2.pdf","comment":"Accepted to SENTIRE 2024 (ICDM Workshops):\n  https://sentic.net/sentire2024wozniak.pdf"},{"id":"http://arxiv.org/abs/2411.04847v1","updated":"2024-11-07T16:33:48Z","published":"2024-11-07T16:33:48Z","title":"Prompt-Guided Internal States for Hallucination Detection of Large\n  Language Models","summary":"  Large Language Models (LLMs) have demonstrated remarkable capabilities across\na variety of tasks in different domains. However, they sometimes generate\nresponses that are logically coherent but factually incorrect or misleading,\nwhich is known as LLM hallucinations. Data-driven supervised methods train\nhallucination detectors by leveraging the internal states of LLMs, but\ndetectors trained on specific domains often struggle to generalize well to\nother domains. In this paper, we aim to enhance the cross-domain performance of\nsupervised detectors with only in-domain data. We propose a novel framework,\nprompt-guided internal states for hallucination detection of LLMs, namely\nPRISM. By utilizing appropriate prompts to guide changes in the structure\nrelated to text truthfulness within the LLM's internal states, we make this\nstructure more salient and consistent across texts from different domains. We\nintegrated our framework with existing hallucination detection methods and\nconducted experiments on datasets from different domains. The experimental\nresults indicate that our framework significantly enhances the cross-domain\ngeneralization of existing hallucination detection methods.\n","authors":["Fujie Zhang","Peiqi Yu","Biao Yi","Baolei Zhang","Tong Li","Zheli Liu"],"pdf_url":"https://arxiv.org/pdf/2411.04847v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04825v1","updated":"2024-11-07T16:06:00Z","published":"2024-11-07T16:06:00Z","title":"VTechAGP: An Academic-to-General-Audience Text Paraphrase Dataset and\n  Benchmark Models","summary":"  Existing text simplification or paraphrase datasets mainly focus on\nsentence-level text generation in a general domain. These datasets are\ntypically developed without using domain knowledge. In this paper, we release a\nnovel dataset, VTechAGP, which is the first academic-to-general-audience text\nparaphrase dataset consisting of 4,938 document-level these and dissertation\nacademic and general-audience abstract pairs from 8 colleges authored over 25\nyears. We also propose a novel dynamic soft prompt generative language model,\nDSPT5. For training, we leverage a contrastive-generative loss function to\nlearn the keyword vectors in the dynamic prompt. For inference, we adopt a\ncrowd-sampling decoding strategy at both semantic and structural levels to\nfurther select the best output candidate. We evaluate DSPT5 and various\nstate-of-the-art large language models (LLMs) from multiple perspectives.\nResults demonstrate that the SOTA LLMs does not provide satisfactory outcomes,\nwhile the lightweight DSPT5 can achieve competitive results. To the best of our\nknowledge, we are the first to build a benchmark dataset and solutions for\nacademic-to-general-audience text paraphrase dataset.\n","authors":["Ming Cheng","Jiaying Gong","Chenhan Yuan","William A. Ingram","Edward Fox","Hoda Eldardiry"],"pdf_url":"https://arxiv.org/pdf/2411.04825v1.pdf","comment":"21 pages, 3 figures"},{"id":"http://arxiv.org/abs/2411.04822v1","updated":"2024-11-07T15:59:54Z","published":"2024-11-07T15:59:54Z","title":"When Does Classical Chinese Help? Quantifying Cross-Lingual Transfer in\n  Hanja and Kanbun","summary":"  Historical and linguistic connections within the Sinosphere have led\nresearchers to use Classical Chinese resources for cross-lingual transfer when\nprocessing historical documents from Korea and Japan. In this paper, we\nquestion the assumption of cross-lingual transferability from Classical Chinese\nto Hanja and Kanbun, the ancient written languages of Korea and Japan,\nrespectively. Our experiments across machine translation, named entity\nrecognition, and punctuation restoration tasks show minimal impact of Classical\nChinese datasets on language model performance for ancient Korean documents\nwritten in Hanja, with performance differences within $\\pm{}0.0068$ F1-score\nfor sequence labeling tasks and up to $+0.84$ BLEU score for translation. These\nlimitations persist consistently across various model sizes, architectures, and\ndomain-specific datasets. Our analysis reveals that the benefits of Classical\nChinese resources diminish rapidly as local language data increases for Hanja,\nwhile showing substantial improvements only in extremely low-resource scenarios\nfor both Korean and Japanese historical documents. These mixed results\nemphasize the need for careful empirical validation rather than assuming\nbenefits from indiscriminate cross-lingual transfer.\n","authors":["Seyoung Song","Haneul Yoo","Jiho Jin","Kyunghyun Cho","Alice Oh"],"pdf_url":"https://arxiv.org/pdf/2411.04822v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04813v1","updated":"2024-11-07T15:50:40Z","published":"2024-11-07T15:50:40Z","title":"LuxBank: The First Universal Dependency Treebank for Luxembourgish","summary":"  The Universal Dependencies (UD) project has significantly expanded linguistic\ncoverage across 161 languages, yet Luxembourgish, a West Germanic language\nspoken by approximately 400,000 people, has remained absent until now. In this\npaper, we introduce LuxBank, the first UD Treebank for Luxembourgish,\naddressing the gap in syntactic annotation and analysis for this `low-research'\nlanguage. We establish formal guidelines for Luxembourgish language annotation,\nproviding the foundation for the first large-scale quantitative analysis of its\nsyntax. LuxBank serves not only as a resource for linguists and language\nlearners but also as a tool for developing spell checkers and grammar checkers,\norganising existing text archives and even training large language models. By\nincorporating Luxembourgish into the UD framework, we aim to enhance the\nunderstanding of syntactic variation within West Germanic languages and offer a\nmodel for documenting smaller, semi-standardised languages. This work positions\nLuxembourgish as a valuable resource in the broader linguistic and NLP\ncommunities, contributing to the study of languages with limited research and\nresources.\n","authors":["Alistair Plum","Caroline Döhmer","Emilia Milano","Anne-Marie Lutgen","Christoph Purschke"],"pdf_url":"https://arxiv.org/pdf/2411.04813v1.pdf","comment":"Accepted at 22nd Workshop on Treebanks and Linguistic Theories (TLT\n  2024)"},{"id":"http://arxiv.org/abs/2408.16163v2","updated":"2024-11-07T15:48:11Z","published":"2024-08-28T22:51:29Z","title":"FRACTURED-SORRY-Bench: Framework for Revealing Attacks in Conversational\n  Turns Undermining Refusal Efficacy and Defenses over SORRY-Bench (Automated\n  Multi-shot Jailbreaks)","summary":"  This paper introduces FRACTURED-SORRY-Bench, a framework for evaluating the\nsafety of Large Language Models (LLMs) against multi-turn conversational\nattacks. Building upon the SORRY-Bench dataset, we propose a simple yet\neffective method for generating adversarial prompts by breaking down harmful\nqueries into seemingly innocuous sub-questions. Our approach achieves a maximum\nincrease of +46.22\\% in Attack Success Rates (ASRs) across GPT-4, GPT-4o,\nGPT-4o-mini, and GPT-3.5-Turbo models compared to baseline methods. We\ndemonstrate that this technique poses a challenge to current LLM safety\nmeasures and highlights the need for more robust defenses against subtle,\nmulti-turn attacks.\n","authors":["Aman Priyanshu","Supriti Vijay"],"pdf_url":"https://arxiv.org/pdf/2408.16163v2.pdf","comment":"4 pages, 2 tables"},{"id":"http://arxiv.org/abs/2302.12921v3","updated":"2024-11-07T15:44:43Z","published":"2023-02-24T22:38:54Z","title":"Pre-Finetuning for Few-Shot Emotional Speech Recognition","summary":"  Speech models have long been known to overfit individual speakers for many\nclassification tasks. This leads to poor generalization in settings where the\nspeakers are out-of-domain or out-of-distribution, as is common in production\nenvironments. We view speaker adaptation as a few-shot learning problem and\npropose investigating transfer learning approaches inspired by recent success\nwith pre-trained models in natural language tasks. We propose pre-finetuning\nspeech models on difficult tasks to distill knowledge into few-shot downstream\nclassification objectives. We pre-finetune Wav2Vec2.0 on every permutation of\nfour multiclass emotional speech recognition corpora and evaluate our\npre-finetuned models through 33,600 few-shot fine-tuning trials on the\nEmotional Speech Dataset.\n","authors":["Maximillian Chen","Zhou Yu"],"pdf_url":"https://arxiv.org/pdf/2302.12921v3.pdf","comment":"Published at INTERSPEECH 2023. 5 pages, 4 figures. Code available at\n  https://github.com/maxlchen/Speech-PreFinetuning"},{"id":"http://arxiv.org/abs/2403.00867v3","updated":"2024-11-07T15:41:38Z","published":"2024-03-01T03:29:54Z","title":"Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by\n  Exploring Refusal Loss Landscapes","summary":"  Large Language Models (LLMs) are becoming a prominent generative AI tool,\nwhere the user enters a query and the LLM generates an answer. To reduce harm\nand misuse, efforts have been made to align these LLMs to human values using\nadvanced training techniques such as Reinforcement Learning from Human Feedback\n(RLHF). However, recent studies have highlighted the vulnerability of LLMs to\nadversarial jailbreak attempts aiming at subverting the embedded safety\nguardrails. To address this challenge, this paper defines and investigates the\nRefusal Loss of LLMs and then proposes a method called Gradient Cuff to detect\njailbreak attempts. Gradient Cuff exploits the unique properties observed in\nthe refusal loss landscape, including functional values and its smoothness, to\ndesign an effective two-step detection strategy. Experimental results on two\naligned LLMs (LLaMA-2-7B-Chat and Vicuna-7B-V1.5) and six types of jailbreak\nattacks (GCG, AutoDAN, PAIR, TAP, Base64, and LRL) show that Gradient Cuff can\nsignificantly improve the LLM's rejection capability for malicious jailbreak\nqueries, while maintaining the model's performance for benign user queries by\nadjusting the detection threshold.\n","authors":["Xiaomeng Hu","Pin-Yu Chen","Tsung-Yi Ho"],"pdf_url":"https://arxiv.org/pdf/2403.00867v3.pdf","comment":"Accepted by NeurIPS 2024. Project page:\n  https://huggingface.co/spaces/TrustSafeAI/GradientCuff-Jailbreak-Defense"},{"id":"http://arxiv.org/abs/2411.04799v1","updated":"2024-11-07T15:38:25Z","published":"2024-11-07T15:38:25Z","title":"Kwai-STaR: Transform LLMs into State-Transition Reasoners","summary":"  Mathematical reasoning presents a significant challenge to the cognitive\ncapabilities of LLMs. Various methods have been proposed to enhance the\nmathematical ability of LLMs. However, few recognize the value of state\ntransition for LLM reasoning. In this work, we define mathematical\nproblem-solving as a process of transiting from an initial unsolved state to\nthe final resolved state, and propose Kwai-STaR framework, which transforms\nLLMs into State-Transition Reasoners to improve their intuitive reasoning\ncapabilities. Our approach comprises three main steps: (1) Define the state\nspace tailored to the mathematical reasoning. (2) Generate state-transition\ndata based on the state space. (3) Convert original LLMs into State-Transition\nReasoners via a curricular training strategy. Our experiments validate the\neffectiveness of Kwai-STaR in enhancing mathematical reasoning: After training\non the small-scale Kwai-STaR dataset, general LLMs, including Mistral-7B and\nLLaMA-3, achieve considerable performance gain on the GSM8K and GSM-Hard\ndataset. Additionally, the state transition-based design endows Kwai-STaR with\nremarkable training and inference efficiency. Further experiments are underway\nto establish the generality of Kwai-STaR.\n","authors":["Xingyu Lu","Yuhang Hu","Changyi Liu","Tianke Zhang","Zhenyu Yang","Zhixiang Ding","Shengsheng Qian","Meng Du","Ruiwen Kang","Kaiyu Tang","Fan Yang","Tingting Gao","Di Zhang","Hai-Tao Zheng","Bin Wen"],"pdf_url":"https://arxiv.org/pdf/2411.04799v1.pdf","comment":"6 pages, 2 figures"},{"id":"http://arxiv.org/abs/2411.04794v1","updated":"2024-11-07T15:36:05Z","published":"2024-11-07T15:36:05Z","title":"AlignXIE: Improving Multilingual Information Extraction by Cross-Lingual\n  Alignment","summary":"  Empirical evidence suggests that LLMs exhibit spontaneous cross-lingual\nalignment. Our findings suggest that although LLMs also demonstrate promising\ncross-lingual alignment in Information Extraction, there remains significant\nimbalance across languages, revealing an underlying deficiency in the IE\nalignment. To address this issue, we propose AlignXIE, a powerful code-based\nLLM that significantly enhances cross-lingual IE alignment through two\nstrategies. Firstly, AlignXIE formulates IE across different languages,\nespecially non-English ones, as code generation tasks, standardizing the\nrepresentation of various schemas using Python classes to ensure consistency of\nthe same ontology in different languages and align the schema. Secondly, it\nincorporates an IE cross-lingual alignment phase through a translated instance\nprediction task proposed in this paper to align the extraction process,\nutilizing ParallelNER, an IE bilingual parallel dataset with 257,190 samples,\ngenerated by our proposed LLM-based automatic pipeline for IE parallel data\nconstruction, with manual annotation to ensure quality. Ultimately, we obtain\nAlignXIE through multilingual IE instruction tuning. Although without training\nin 9 unseen languages, AlignXIE surpasses ChatGPT by $30.17\\%$ and SoTA by\n$20.03\\%$, thereby demonstrating superior cross-lingual IE capabilities.\nComprehensive evaluations on 63 IE benchmarks in Chinese and English under\nvarious settings, demonstrate that AlignXIE significantly enhances\ncross-lingual and multilingual IE through boosting the IE alignment.\n","authors":["Yuxin Zuo","Wenxuan Jiang","Wenxuan Liu","Zixuan Li","Long Bai","Hanbin Wang","Yutao Zeng","Xiaolong Jin","Jiafeng Guo","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2411.04794v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2411.04788v1","updated":"2024-11-07T15:28:20Z","published":"2024-11-07T15:28:20Z","title":"Enhancing Investment Analysis: Optimizing AI-Agent Collaboration in\n  Financial Research","summary":"  In recent years, the application of generative artificial intelligence\n(GenAI) in financial analysis and investment decision-making has gained\nsignificant attention. However, most existing approaches rely on single-agent\nsystems, which fail to fully utilize the collaborative potential of multiple AI\nagents. In this paper, we propose a novel multi-agent collaboration system\ndesigned to enhance decision-making in financial investment research. The\nsystem incorporates agent groups with both configurable group sizes and\ncollaboration structures to leverage the strengths of each agent group type. By\nutilizing a sub-optimal combination strategy, the system dynamically adapts to\nvarying market conditions and investment scenarios, optimizing performance\nacross different tasks. We focus on three sub-tasks: fundamentals, market\nsentiment, and risk analysis, by analyzing the 2023 SEC 10-K forms of 30\ncompanies listed on the Dow Jones Index. Our findings reveal significant\nperformance variations based on the configurations of AI agents for different\ntasks. The results demonstrate that our multi-agent collaboration system\noutperforms traditional single-agent models, offering improved accuracy,\nefficiency, and adaptability in complex financial environments. This study\nhighlights the potential of multi-agent systems in transforming financial\nanalysis and investment decision-making by integrating diverse analytical\nperspectives.\n","authors":["Xuewen Han","Neng Wang","Shangkun Che","Hongyang Yang","Kunpeng Zhang","Sean Xin Xu"],"pdf_url":"https://arxiv.org/pdf/2411.04788v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17437v2","updated":"2024-11-07T15:00:00Z","published":"2024-08-30T17:41:30Z","title":"SYNTHEVAL: Hybrid Behavioral Testing of NLP Models with Synthetic\n  CheckLists","summary":"  Traditional benchmarking in NLP typically involves using static held-out test\nsets. However, this approach often results in an overestimation of performance\nand lacks the ability to offer comprehensive, interpretable, and dynamic\nassessments of NLP models. Recently, works like DynaBench (Kiela et al., 2021)\nand CheckList (Ribeiro et al., 2020) have addressed these limitations through\nbehavioral testing of NLP models with test types generated by a multistep\nhuman-annotated pipeline. Unfortunately, manually creating a variety of test\ntypes requires much human labor, often at prohibitive cost. In this work, we\npropose SYNTHEVAL, a hybrid behavioral testing framework that leverages large\nlanguage models (LLMs) to generate a wide range of test types for a\ncomprehensive evaluation of NLP models. SYNTHEVAL first generates sentences via\nLLMs using controlled generation, and then identifies challenging examples by\ncomparing the predictions made by LLMs with task-specific NLP models. In the\nlast stage, human experts investigate the challenging examples, manually design\ntemplates, and identify the types of failures the taskspecific models\nconsistently exhibit. We apply SYNTHEVAL to two classification tasks, sentiment\nanalysis and toxic language detection, and show that our framework is effective\nin identifying weaknesses of strong models on these tasks. We share our code in\nhttps://github.com/Loreley99/SynthEval_CheckList.\n","authors":["Raoyuan Zhao","Abdullatif Köksal","Yihong Liu","Leonie Weissweiler","Anna Korhonen","Hinrich Schütze"],"pdf_url":"https://arxiv.org/pdf/2408.17437v2.pdf","comment":"EMNLP 2024 - Findings"},{"id":"http://arxiv.org/abs/2411.03883v2","updated":"2024-11-07T14:57:14Z","published":"2024-11-06T12:57:58Z","title":"MEG: Medical Knowledge-Augmented Large Language Models for Question\n  Answering","summary":"  Question answering is a natural language understanding task that involves\nreasoning over both explicit context and unstated, relevant domain knowledge.\nLarge language models (LLMs), which underpin most contemporary question\nanswering systems, struggle to induce how concepts relate in specialized\ndomains such as medicine. Existing medical LLMs are also costly to train. In\nthis work, we present MEG, a parameter-efficient approach for medical\nknowledge-augmented LLMs. MEG uses a lightweight mapping network to integrate\ngraph embeddings into the LLM, enabling it to leverage external knowledge in a\ncost-effective way. We evaluate our method on four popular medical\nmultiple-choice datasets and show that LLMs greatly benefit from the factual\ngrounding provided by knowledge graph embeddings. MEG attains an average of\n+10.2% accuracy over the Mistral-Instruct baseline, and +6.7% over specialized\nmodels like BioMistral. We also show results based on Llama-3. Finally, we show\nthat MEG's performance remains robust to the choice of graph encoder.\n","authors":["Laura Cabello","Carmen Martin-Turrero","Uchenna Akujuobi","Anders Søgaard","Carlos Bobed"],"pdf_url":"https://arxiv.org/pdf/2411.03883v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04756v1","updated":"2024-11-07T14:54:42Z","published":"2024-11-07T14:54:42Z","title":"A study of Vietnamese readability assessing through semantic and\n  statistical features","summary":"  Determining the difficulty of a text involves assessing various textual\nfeatures that may impact the reader's text comprehension, yet current research\nin Vietnamese has only focused on statistical features. This paper introduces a\nnew approach that integrates statistical and semantic approaches to assessing\ntext readability. Our research utilized three distinct datasets: the Vietnamese\nText Readability Dataset (ViRead), OneStopEnglish, and RACE, with the latter\ntwo translated into Vietnamese. Advanced semantic analysis methods were\nemployed for the semantic aspect using state-of-the-art language models such as\nPhoBERT, ViDeBERTa, and ViBERT. In addition, statistical methods were\nincorporated to extract syntactic and lexical features of the text. We\nconducted experiments using various machine learning models, including Support\nVector Machine (SVM), Random Forest, and Extra Trees and evaluated their\nperformance using accuracy and F1 score metrics. Our results indicate that a\njoint approach that combines semantic and statistical features significantly\nenhances the accuracy of readability classification compared to using each\nmethod in isolation. The current study emphasizes the importance of considering\nboth statistical and semantic aspects for a more accurate assessment of text\ndifficulty in Vietnamese. This contribution to the field provides insights into\nthe adaptability of advanced language models in the context of Vietnamese text\nreadability. It lays the groundwork for future research in this area.\n","authors":["Hung Tuan Le","Long Truong To","Manh Trong Nguyen","Quyen Nguyen","Trong-Hop Do"],"pdf_url":"https://arxiv.org/pdf/2411.04756v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04752v1","updated":"2024-11-07T14:41:01Z","published":"2024-11-07T14:41:01Z","title":"RetrieveGPT: Merging Prompts and Mathematical Models for Enhanced\n  Code-Mixed Information Retrieval","summary":"  Code-mixing, the integration of lexical and grammatical elements from\nmultiple languages within a single sentence, is a widespread linguistic\nphenomenon, particularly prevalent in multilingual societies. In India, social\nmedia users frequently engage in code-mixed conversations using the Roman\nscript, especially among migrant communities who form online groups to share\nrelevant local information. This paper focuses on the challenges of extracting\nrelevant information from code-mixed conversations, specifically within Roman\ntransliterated Bengali mixed with English. This study presents a novel approach\nto address these challenges by developing a mechanism to automatically identify\nthe most relevant answers from code-mixed conversations. We have experimented\nwith a dataset comprising of queries and documents from Facebook, and Query\nRelevance files (QRels) to aid in this task. Our results demonstrate the\neffectiveness of our approach in extracting pertinent information from complex,\ncode-mixed digital conversations, contributing to the broader field of natural\nlanguage processing in multilingual and informal text environments. We use\nGPT-3.5 Turbo via prompting alongwith using the sequential nature of relevant\ndocuments to frame a mathematical model which helps to detect relevant\ndocuments corresponding to a query.\n","authors":["Aniket Deroy","Subhankar Maity"],"pdf_url":"https://arxiv.org/pdf/2411.04752v1.pdf","comment":"Accepted at FIRE 2024 (Track: Code-Mixed Information Retrieval from\n  Social Media Data)"},{"id":"http://arxiv.org/abs/2310.09765v2","updated":"2024-11-07T13:56:58Z","published":"2023-10-15T07:49:56Z","title":"MILPaC: A Novel Benchmark for Evaluating Translation of Legal Text to\n  Indian Languages","summary":"  Most legal text in the Indian judiciary is written in complex English due to\nhistorical reasons. However, only a small fraction of the Indian population is\ncomfortable in reading English. Hence legal text needs to be made available in\nvarious Indian languages, possibly by translating the available legal text from\nEnglish. Though there has been a lot of research on translation to and between\nIndian languages, to our knowledge, there has not been much prior work on such\ntranslation in the legal domain. In this work, we construct the first\nhigh-quality legal parallel corpus containing aligned text units in English and\nnine Indian languages, that includes several low-resource languages. We also\nbenchmark the performance of a wide variety of Machine Translation (MT) systems\nover this corpus, including commercial MT systems, open-source MT systems and\nLarge Language Models. Through a comprehensive survey by Law practitioners, we\ncheck how satisfied they are with the translations by some of these MT systems,\nand how well automatic MT evaluation metrics agree with the opinions of Law\npractitioners.\n","authors":["Sayan Mahapatra","Debtanu Datta","Shubham Soni","Adrijit Goswami","Saptarshi Ghosh"],"pdf_url":"https://arxiv.org/pdf/2310.09765v2.pdf","comment":"To be published in ACM Transactions on Asian and Low-Resource\n  Language Information Processing (TALLIP)"},{"id":"http://arxiv.org/abs/2411.04699v1","updated":"2024-11-07T13:33:34Z","published":"2024-11-07T13:33:34Z","title":"BhasaAnuvaad: A Speech Translation Dataset for 14 Indian Languages","summary":"  Automatic Speech Translation (AST) datasets for Indian languages remain\ncritically scarce, with public resources covering fewer than 10 of the 22\nofficial languages. This scarcity has resulted in AST systems for Indian\nlanguages lagging far behind those available for high-resource languages like\nEnglish. In this paper, we first evaluate the performance of widely-used AST\nsystems on Indian languages, identifying notable performance gaps and\nchallenges. Our findings show that while these systems perform adequately on\nread speech, they struggle significantly with spontaneous speech, including\ndisfluencies like pauses and hesitations. Additionally, there is a striking\nabsence of systems capable of accurately translating colloquial and informal\nlanguage, a key aspect of everyday communication. To this end, we introduce\nBhasaAnuvaad, the largest publicly available dataset for AST involving 14\nscheduled Indian languages spanning over 44,400 hours and 17M text segments.\nBhasaAnuvaad contains data for English speech to Indic text, as well as Indic\nspeech to English text. This dataset comprises three key categories: (1)\nCurated datasets from existing resources, (2) Large-scale web mining, and (3)\nSynthetic data generation. By offering this diverse and expansive dataset, we\naim to bridge the resource gap and promote advancements in AST for low-resource\nIndian languages, especially in handling spontaneous and informal speech\npatterns.\n","authors":["Sparsh Jain","Ashwin Sankar","Devilal Choudhary","Dhairya Suman","Nikhil Narasimhan","Mohammed Safi Ur Rahman Khan","Anoop Kunchukuttan","Mitesh M Khapra","Raj Dabre"],"pdf_url":"https://arxiv.org/pdf/2411.04699v1.pdf","comment":"Work in Progress"},{"id":"http://arxiv.org/abs/2405.04304v5","updated":"2024-11-07T12:59:39Z","published":"2024-05-07T13:27:52Z","title":"Dynamic Speculation Lookahead Accelerates Speculative Decoding of Large\n  Language Models","summary":"  Speculative decoding is commonly used for reducing the inference latency of\nlarge language models. Its effectiveness depends highly on the speculation\nlookahead (SL)-the number of tokens generated by the draft model at each\niteration. In this work we show that the common practice of using the same SL\nfor all iterations (static SL) is suboptimal. We introduce DISCO (DynamIc\nSpeCulation lookahead Optimization), a novel method for dynamically selecting\nthe SL. Our experiments with four datasets show that DISCO reaches an average\nspeedup of 10% compared to the best static SL baseline, while generating the\nexact same text.\n","authors":["Jonathan Mamou","Oren Pereg","Daniel Korat","Moshe Berchansky","Nadav Timor","Moshe Wasserblat","Roy Schwartz"],"pdf_url":"https://arxiv.org/pdf/2405.04304v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02674v3","updated":"2024-11-07T12:38:41Z","published":"2024-11-04T23:21:12Z","title":"Wave Network: An Ultra-Small Language Model","summary":"  We propose an innovative token representation and update method in a new\nultra-small language model: the Wave network. Specifically, we use a complex\nvector to represent each token, encoding both global and local semantics of the\ninput text. A complex vector consists of two components: a magnitude vector\nrepresenting the global semantics of the input text, and a phase vector\ncapturing the relationships between individual tokens and global semantics.\nExperiments on the AG News text classification task demonstrate that, when\ngenerating complex vectors from randomly initialized token embeddings, our\nsingle-layer Wave Network achieves 90.91% accuracy with wave interference and\n91.66% with wave modulation - outperforming a single Transformer layer using\nBERT pre-trained embeddings by 19.23% and 19.98%, respectively, and approaching\nthe accuracy of the pre-trained and fine-tuned BERT base model (94.64%).\nAdditionally, compared to BERT base, the Wave Network reduces video memory\nusage and training time by 77.34% and 85.62% during wave modulation. In\nsummary, we used a 2.4-million-parameter small language model to achieve\naccuracy comparable to a 100-million-parameter BERT model in text\nclassification.\n","authors":["Xin Zhang","Victor S. Sheng"],"pdf_url":"https://arxiv.org/pdf/2411.02674v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04649v1","updated":"2024-11-07T12:12:44Z","published":"2024-11-07T12:12:44Z","title":"DISCO: DISCovering Overfittings as Causal Rules for Text Classification\n  Models","summary":"  With the rapid advancement of neural language models, the deployment of\nover-parameterized models has surged, increasing the need for interpretable\nexplanations comprehensible to human inspectors. Existing post-hoc\ninterpretability methods, which often focus on unigram features of single input\ntextual instances, fail to capture the models' decision-making process fully.\nAdditionally, many methods do not differentiate between decisions based on\nspurious correlations and those based on a holistic understanding of the input.\nOur paper introduces DISCO, a novel method for discovering global, rule-based\nexplanations by identifying causal n-gram associations with model predictions.\nThis method employs a scalable sequence mining technique to extract relevant\ntext spans from training data, associate them with model predictions, and\nconduct causality checks to distill robust rules that elucidate model behavior.\nThese rules expose potential overfitting and provide insights into misleading\nfeature combinations. We validate DISCO through extensive testing,\ndemonstrating its superiority over existing methods in offering comprehensive\ninsights into complex model behaviors. Our approach successfully identifies all\nshortcuts manually introduced into the training data (100% detection rate on\nthe MultiRC dataset), resulting in an 18.8% regression in model performance --\na capability unmatched by any other method. Furthermore, DISCO supports\ninteractive explanations, enabling human inspectors to distinguish spurious\ncauses in the rule-based output. This alleviates the burden of abundant\ninstance-wise explanations and helps assess the model's risk when encountering\nout-of-distribution (OOD) data.\n","authors":["Zijian Zhang","Vinay Setty","Yumeng Wang","Avishek Anand"],"pdf_url":"https://arxiv.org/pdf/2411.04649v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14125v3","updated":"2024-11-07T12:07:07Z","published":"2024-05-23T02:57:42Z","title":"ALI-Agent: Assessing LLMs' Alignment with Human Values via Agent-based\n  Evaluation","summary":"  Large Language Models (LLMs) can elicit unintended and even harmful content\nwhen misaligned with human values, posing severe risks to users and society. To\nmitigate these risks, current evaluation benchmarks predominantly employ\nexpert-designed contextual scenarios to assess how well LLMs align with human\nvalues. However, the labor-intensive nature of these benchmarks limits their\ntest scope, hindering their ability to generalize to the extensive variety of\nopen-world use cases and identify rare but crucial long-tail risks.\nAdditionally, these static tests fail to adapt to the rapid evolution of LLMs,\nmaking it hard to evaluate timely alignment issues. To address these\nchallenges, we propose ALI-Agent, an evaluation framework that leverages the\nautonomous abilities of LLM-powered agents to conduct in-depth and adaptive\nalignment assessments. ALI-Agent operates through two principal stages:\nEmulation and Refinement. During the Emulation stage, ALI-Agent automates the\ngeneration of realistic test scenarios. In the Refinement stage, it iteratively\nrefines the scenarios to probe long-tail risks. Specifically, ALI-Agent\nincorporates a memory module to guide test scenario generation, a tool-using\nmodule to reduce human labor in tasks such as evaluating feedback from target\nLLMs, and an action module to refine tests. Extensive experiments across three\naspects of human values--stereotypes, morality, and legality--demonstrate that\nALI-Agent, as a general evaluation framework, effectively identifies model\nmisalignment. Systematic analysis also validates that the generated test\nscenarios represent meaningful use cases, as well as integrate enhanced\nmeasures to probe long-tail risks. Our code is available at\nhttps://github.com/SophieZheng998/ALI-Agent.git\n","authors":["Jingnan Zheng","Han Wang","An Zhang","Tai D. Nguyen","Jun Sun","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2405.14125v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04637v1","updated":"2024-11-07T11:51:14Z","published":"2024-11-07T11:51:14Z","title":"Hands-On Tutorial: Labeling with LLM and Human-in-the-Loop","summary":"  Training and deploying machine learning models relies on a large amount of\nhuman-annotated data. As human labeling becomes increasingly expensive and\ntime-consuming, recent research has developed multiple strategies to speed up\nannotation and reduce costs and human workload: generating synthetic training\ndata, active learning, and hybrid labeling. This tutorial is oriented toward\npractical applications: we will present the basics of each strategy, highlight\ntheir benefits and limitations, and discuss in detail real-life case studies.\nAdditionally, we will walk through best practices for managing human annotators\nand controlling the quality of the final dataset. The tutorial includes a\nhands-on workshop, where attendees will be guided in implementing a hybrid\nannotation setup. This tutorial is designed for NLP practitioners from both\nresearch and industry backgrounds who are involved in or interested in\noptimizing data labeling projects.\n","authors":["Ekaterina Artemova","Akim Tsvigun","Dominik Schlechtweg","Natalia Fedorova","Sergei Tilga","Boris Obmoroshev"],"pdf_url":"https://arxiv.org/pdf/2411.04637v1.pdf","comment":"To be presented at COLING 2025"},{"id":"http://arxiv.org/abs/2411.04604v1","updated":"2024-11-07T10:39:10Z","published":"2024-11-07T10:39:10Z","title":"FASSILA: A Corpus for Algerian Dialect Fake News Detection and Sentiment\n  Analysis","summary":"  In the context of low-resource languages, the Algerian dialect (AD) faces\nchallenges due to the absence of annotated corpora, hindering its effective\nprocessing, notably in Machine Learning (ML) applications reliant on corpora\nfor training and assessment. This study outlines the development process of a\nspecialized corpus for Fake News (FN) detection and sentiment analysis (SA) in\nAD called FASSILA. This corpus comprises 10,087 sentences, encompassing over\n19,497 unique words in AD, and addresses the significant lack of linguistic\nresources in the language and covers seven distinct domains. We propose an\nannotation scheme for FN detection and SA, detailing the data collection,\ncleaning, and labelling process. Remarkable Inter-Annotator Agreement indicates\nthat the annotation scheme produces consistent annotations of high quality.\nSubsequent classification experiments using BERT-based models and ML models are\npresented, demonstrate promising results and highlight avenues for further\nresearch. The dataset is made freely available on GitHub\n(https://github.com/amincoding/FASSILA) to facilitate future advancements in\nthe field.\n","authors":["Amin Abdedaiem","Abdelhalim Hafedh Dahou","Mohamed Amine Cheragui","Brigitte Mathiak"],"pdf_url":"https://arxiv.org/pdf/2411.04604v1.pdf","comment":"16 pages, 6 Figuers"},{"id":"http://arxiv.org/abs/2411.04602v1","updated":"2024-11-07T10:31:31Z","published":"2024-11-07T10:31:31Z","title":"Self-Calibrated Listwise Reranking with Large Language Models","summary":"  Large language models (LLMs), with advanced linguistic capabilities, have\nbeen employed in reranking tasks through a sequence-to-sequence approach. In\nthis paradigm, multiple passages are reranked in a listwise manner and a\ntextual reranked permutation is generated. However, due to the limited context\nwindow of LLMs, this reranking paradigm requires a sliding window strategy to\niteratively handle larger candidate sets. This not only increases computational\ncosts but also restricts the LLM from fully capturing all the comparison\ninformation for all candidates. To address these challenges, we propose a novel\nself-calibrated listwise reranking method, which aims to leverage LLMs to\nproduce global relevance scores for ranking. To achieve it, we first propose\nthe relevance-aware listwise reranking framework, which incorporates explicit\nlist-view relevance scores to improve reranking efficiency and enable global\ncomparison across the entire candidate set. Second, to ensure the comparability\nof the computed scores, we propose self-calibrated training that uses\npoint-view relevance assessments generated internally by the LLM itself to\ncalibrate the list-view relevance assessments. Extensive experiments and\ncomprehensive analysis on the BEIR benchmark and TREC Deep Learning Tracks\ndemonstrate the effectiveness and efficiency of our proposed method.\n","authors":["Ruiyang Ren","Yuhao Wang","Kun Zhou","Wayne Xin Zhao","Wenjie Wang","Jing Liu","Ji-Rong Wen","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2411.04602v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04588v1","updated":"2024-11-07T10:17:40Z","published":"2024-11-07T10:17:40Z","title":"Tibyan Corpus: Balanced and Comprehensive Error Coverage Corpus Using\n  ChatGPT for Arabic Grammatical Error Correction","summary":"  Natural language processing (NLP) utilizes text data augmentation to overcome\nsample size constraints. Increasing the sample size is a natural and widely\nused strategy for alleviating these challenges. In this study, we chose Arabic\nto increase the sample size and correct grammatical errors. Arabic is\nconsidered one of the languages with limited resources for grammatical error\ncorrection (GEC). Furthermore, QALB-14 and QALB-15 are the only datasets used\nin most Arabic grammatical error correction research, with approximately 20,500\nparallel examples, which is considered low compared with other languages.\nTherefore, this study aims to develop an Arabic corpus called \"Tibyan\" for\ngrammatical error correction using ChatGPT. ChatGPT is used as a data augmenter\ntool based on a pair of Arabic sentences containing grammatical errors matched\nwith a sentence free of errors extracted from Arabic books, called guide\nsentences. Multiple steps were involved in establishing our corpus, including\nthe collection and pre-processing of a pair of Arabic texts from various\nsources, such as books and open-access corpora. We then used ChatGPT to\ngenerate a parallel corpus based on the text collected previously, as a guide\nfor generating sentences with multiple types of errors. By engaging linguistic\nexperts to review and validate the automatically generated sentences, we\nensured that they were correct and error-free. The corpus was validated and\nrefined iteratively based on feedback provided by linguistic experts to improve\nits accuracy. Finally, we used the Arabic Error Type Annotation tool (ARETA) to\nanalyze the types of errors in the Tibyan corpus. Our corpus contained 49 of\nerrors, including seven types: orthography, morphology, syntax, semantics,\npunctuation, merge, and split. The Tibyan corpus contains approximately 600 K\ntokens.\n","authors":["Ahlam Alrehili","Areej Alhothali"],"pdf_url":"https://arxiv.org/pdf/2411.04588v1.pdf","comment":"17 pages, 11 figures"},{"id":"http://arxiv.org/abs/2411.04585v1","updated":"2024-11-07T10:11:38Z","published":"2024-11-07T10:11:38Z","title":"The State and Fate of Summarization Datasets","summary":"  Automatic summarization has consistently attracted attention, due to its\nversatility and wide application in various downstream tasks. Despite its\npopularity, we find that annotation efforts have largely been disjointed, and\nhave lacked common terminology. Consequently, it is challenging to discover\nexisting resources or identify coherent research directions. To address this,\nwe survey a large body of work spanning 133 datasets in over 100 languages,\ncreating a novel ontology covering sample properties, collection methods and\ndistribution. With this ontology we make key observations, including the lack\nin accessible high-quality datasets for low-resource languages, and the field's\nover-reliance on the news domain and on automatically collected distant\nsupervision. Finally, we make available a web interface that allows users to\ninteract and explore our ontology and dataset collection, as well as a template\nfor a summarization data card, which can be used to streamline future research\ninto a more coherent body of work.\n","authors":["Noam Dahan","Gabriel Stanovsky"],"pdf_url":"https://arxiv.org/pdf/2411.04585v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04573v1","updated":"2024-11-07T09:57:57Z","published":"2024-11-07T09:57:57Z","title":"Multistage Fine-tuning Strategies for Automatic Speech Recognition in\n  Low-resource Languages","summary":"  This paper presents a novel multistage fine-tuning strategy designed to\nenhance automatic speech recognition (ASR) performance in low-resource\nlanguages using OpenAI's Whisper model. In this approach we aim to build ASR\nmodel for languages with limited digital resources by sequentially adapting the\nmodel across linguistically similar languages. We experimented this on the\nMalasar language, a Dravidian language spoken by approximately ten thousand\npeople in the Western Ghats of South India. Malasar language faces critical\nchallenges for technological intervention due to its lack of a native script\nand absence of digital or spoken data resources. Working in collaboration with\nWycliffe India and Malasar community members, we created a spoken Malasar\ncorpus paired with transcription in Tamil script, a closely related major\nlanguage. In our approach to build ASR model for Malasar, we first build an\nintermediate Tamil ASR, leveraging higher data availability for Tamil annotated\nspeech. This intermediate model is subsequently fine-tuned on Malasar data,\nallowing for more effective ASR adaptation despite limited resources. The\nmultistage fine-tuning strategy demonstrated significant improvements over\ndirect fine-tuning on Malasar data alone, achieving a word error rate (WER) of\n51.9%, which is 4.5% absolute reduction when compared to the direct fine-tuning\nmethod. Further a WER reduction to 47.3% was achieved through punctuation\nremoval in post-processing, which addresses formatting inconsistencies that\nimpact evaluation. Our results underscore the effectiveness of sequential\nmultistage fine-tuning combined with targeted post-processing as a scalable\nstrategy for ASR system development in low-resource languages, especially where\nlinguistic similarities can be leveraged to bridge gaps in training data.\n","authors":["Leena G Pillai","Kavya Manohar","Basil K Raju","Elizabeth Sherly"],"pdf_url":"https://arxiv.org/pdf/2411.04573v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12096v3","updated":"2024-11-07T09:29:32Z","published":"2024-04-18T11:29:23Z","title":"LongEmbed: Extending Embedding Models for Long Context Retrieval","summary":"  Embedding models play a pivot role in modern NLP applications such as IR and\nRAG. While the context limit of LLMs has been pushed beyond 1 million tokens,\nembedding models are still confined to a narrow context window not exceeding 8k\ntokens, refrained from application scenarios requiring long inputs such as\nlegal contracts. This paper explores context window extension of existing\nembedding models, pushing the limit to 32k without requiring additional\ntraining. First, we examine the performance of current embedding models for\nlong context retrieval on our newly constructed LongEmbed benchmark. LongEmbed\ncomprises two synthetic tasks and four carefully chosen real-world tasks,\nfeaturing documents of varying length and dispersed target information.\nBenchmarking results underscore huge room for improvement in these models.\nBased on this, comprehensive experiments show that training-free context window\nextension strategies like position interpolation can effectively extend the\ncontext window of existing embedding models by several folds, regardless of\ntheir original context being 512 or beyond 4k. Furthermore, for models\nemploying absolute position encoding (APE), we show the possibility of further\nfine-tuning to harvest notable performance gains while strictly preserving\noriginal behavior for short inputs. For models using rotary position embedding\n(RoPE), significant enhancements are observed when employing RoPE-specific\nmethods, such as NTK and SelfExtend, indicating RoPE's superiority over APE for\ncontext window extension. To facilitate future research, we release E5-Base-4k\nand E5-RoPE-Base, along with the LongEmbed benchmark.\n","authors":["Dawei Zhu","Liang Wang","Nan Yang","Yifan Song","Wenhao Wu","Furu Wei","Sujian Li"],"pdf_url":"https://arxiv.org/pdf/2404.12096v3.pdf","comment":"EMNLP 2024 Camera Ready"},{"id":"http://arxiv.org/abs/2411.04557v1","updated":"2024-11-07T09:28:38Z","published":"2024-11-07T09:28:38Z","title":"Pruning Literals for Highly Efficient Explainability at Word Level","summary":"  Designing an explainable model becomes crucial now for Natural Language\nProcessing(NLP) since most of the state-of-the-art machine learning models\nprovide a limited explanation for the prediction. In the spectrum of an\nexplainable model, Tsetlin Machine(TM) is promising because of its capability\nof providing word-level explanation using proposition logic. However, concern\nrises over the elaborated combination of literals (propositional logic) in the\nclause that makes the model difficult for humans to comprehend, despite having\na transparent learning process. In this paper, we design a post-hoc pruning of\nclauses that eliminate the randomly placed literals in the clause thereby\nmaking the model more efficiently interpretable than the vanilla TM.\nExperiments on the publicly available YELP-HAT Dataset demonstrate that the\nproposed pruned TM's attention map aligns more with the human attention map\nthan the vanilla TM's attention map. In addition, the pairwise similarity\nmeasure also surpasses the attention map-based neural network models. In terms\nof accuracy, the proposed pruning method does not degrade the accuracy\nsignificantly but rather enhances the performance up to 4% to 9% in some test\ndata.\n","authors":["Rohan Kumar Yadav","Bimal Bhattarai","Abhik Jana","Lei Jiao","Seid Muhie Yimam"],"pdf_url":"https://arxiv.org/pdf/2411.04557v1.pdf","comment":"8 pages, 3 figures"},{"id":"http://arxiv.org/abs/2411.04539v1","updated":"2024-11-07T08:54:46Z","published":"2024-11-07T08:54:46Z","title":"Best Practices for Distilling Large Language Models into BERT for Web\n  Search Ranking","summary":"  Recent studies have highlighted the significant potential of Large Language\nModels (LLMs) as zero-shot relevance rankers. These methods predominantly\nutilize prompt learning to assess the relevance between queries and documents\nby generating a ranked list of potential documents. Despite their promise, the\nsubstantial costs associated with LLMs pose a significant challenge for their\ndirect implementation in commercial search systems. To overcome this barrier\nand fully exploit the capabilities of LLMs for text ranking, we explore\ntechniques to transfer the ranking expertise of LLMs to a more compact model\nsimilar to BERT, using a ranking loss to enable the deployment of less\nresource-intensive models. Specifically, we enhance the training of LLMs\nthrough Continued Pre-Training, taking the query as input and the clicked title\nand summary as output. We then proceed with supervised fine-tuning of the LLM\nusing a rank loss, assigning the final token as a representative of the entire\nsentence. Given the inherent characteristics of autoregressive language models,\nonly the final token </s> can encapsulate all preceding tokens. Additionally,\nwe introduce a hybrid point-wise and margin MSE loss to transfer the ranking\nknowledge from LLMs to smaller models like BERT. This method creates a viable\nsolution for environments with strict resource constraints. Both offline and\nonline evaluations have confirmed the efficacy of our approach, and our model\nhas been successfully integrated into a commercial web search engine as of\nFebruary 2024.\n","authors":["Dezhi Ye","Junwei Hu","Jiabin Fan","Bowen Tian","Jie Liu","Haijin Liang","Jin Ma"],"pdf_url":"https://arxiv.org/pdf/2411.04539v1.pdf","comment":"Arxiv Version"},{"id":"http://arxiv.org/abs/2411.04535v1","updated":"2024-11-07T08:48:33Z","published":"2024-11-07T08:48:33Z","title":"Meta-Reasoning Improves Tool Use in Large Language Models","summary":"  External tools help large language models (LLMs) succeed at tasks where they\nwould otherwise typically fail. In existing frameworks, LLMs learn tool use\neither by in-context demonstrations or via full model fine-tuning on annotated\ndata. As these approaches do not easily scale, a recent trend is to abandon\nthem in favor of lightweight, parameter-efficient tuning paradigms. These\nmethods allow quickly alternating between the frozen LLM and its specialised\nfine-tuned version, by switching on or off a handful of additional custom\nparameters. Hence, we postulate that the generalization ability of the frozen\nmodel can be leveraged to improve tool selection. We present Tool selECTion via\nmeta-reasONing (TECTON), a two-phase system that first reasons over a task\nusing a custom fine-tuned LM head and outputs candidate tools. Then, with the\ncustom head disabled, it meta-reasons (i.e., it reasons over the previous\nreasoning process) to make a final choice. We show that TECTON results in\nsubstantial gains - both in-distribution and out-of-distribution - on a range\nof math reasoning datasets.\n","authors":["Lisa Alazraki","Marek Rei"],"pdf_url":"https://arxiv.org/pdf/2411.04535v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14974v2","updated":"2024-11-07T08:41:03Z","published":"2024-05-23T18:21:59Z","title":"LOVA3: Learning to Visual Question Answering, Asking and Assessment","summary":"  Question answering, asking, and assessment are three innate human traits\ncrucial for understanding the world and acquiring knowledge. By enhancing these\ncapabilities, humans can more effectively utilize data, leading to better\ncomprehension and learning outcomes. Current Multimodal Large Language Models\n(MLLMs) primarily focus on question answering, often neglecting the full\npotential of questioning and assessment skills. Inspired by the human learning\nmechanism, we introduce LOVA3, an innovative framework named \"Learning tO\nVisual question Answering, Asking and Assessment,\" designed to equip MLLMs with\nthese additional capabilities. Our approach involves the creation of two\nsupplementary training tasks GenQA and EvalQA, aiming at fostering the skills\nof asking and assessing questions in the context of images. To develop the\nquestioning ability, we compile a comprehensive set of multimodal foundational\ntasks. For assessment, we introduce a new benchmark called EvalQABench,\ncomprising 64,000 training samples (split evenly between positive and negative\nsamples) and 5,000 validation and testing samples. We posit that enhancing\nMLLMs with the capabilities to answer, ask, and assess questions will enhance\ntheir multimodal comprehension, ultimately improving overall performance. To\nvalidate this hypothesis, we train MLLMs using the LOVA3 framework and evaluate\nthem on a range of multimodal datasets and benchmarks. Our results demonstrate\nconsistent performance gains, underscoring the critical role of these\nadditional tasks in fostering comprehensive intelligence in MLLMs. The code is\navailable at https://github.com/showlab/LOVA3.\n","authors":["Henry Hengyuan Zhao","Pan Zhou","Difei Gao","Zechen Bai","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2405.14974v2.pdf","comment":"Accepted by NeurIPS 2024. The code is available at\n  https://github.com/showlab/LOVA3"},{"id":"http://arxiv.org/abs/2411.04530v1","updated":"2024-11-07T08:38:32Z","published":"2024-11-07T08:38:32Z","title":"Tomato, Tomahto, Tomate: Measuring the Role of Shared Semantics among\n  Subwords in Multilingual Language Models","summary":"  Human understanding of language is robust to different word choices as far as\nthey represent similar semantic concepts. To what extent does our human\nintuition transfer to language models, which represent all subwords as distinct\nembeddings? In this work, we take an initial step on measuring the role of\nshared semantics among subwords in the encoder-only multilingual language\nmodels (mLMs). To this end, we form \"semantic tokens\" by merging the\nsemantically similar subwords and their embeddings, and evaluate the updated\nmLMs on 5 heterogeneous multilingual downstream tasks. Results show that the\ngeneral shared semantics could get the models a long way in making the\npredictions on mLMs with different tokenizers and model sizes. Inspections on\nthe grouped subwords show that they exhibit a wide range of semantic\nsimilarities, including synonyms and translations across many languages and\nscripts. Lastly, we found the zero-shot results with semantic tokens are on par\nor even better than the original models on certain classification tasks,\nsuggesting that the shared subword-level semantics may serve as the anchors for\ncross-lingual transferring.\n","authors":["Xinyu Zhang","Jing Lu","Vinh Q. Tran","Tal Schuster","Donald Metzler","Jimmy Lin"],"pdf_url":"https://arxiv.org/pdf/2411.04530v1.pdf","comment":"8 pages, 9 figures"},{"id":"http://arxiv.org/abs/2405.17382v2","updated":"2024-11-07T08:34:27Z","published":"2024-05-27T17:38:33Z","title":"ReMoDetect: Reward Models Recognize Aligned LLM's Generations","summary":"  The remarkable capabilities and easy accessibility of large language models\n(LLMs) have significantly increased societal risks (e.g., fake news\ngeneration), necessitating the development of LLM-generated text (LGT)\ndetection methods for safe usage. However, detecting LGTs is challenging due to\nthe vast number of LLMs, making it impractical to account for each LLM\nindividually; hence, it is crucial to identify the common characteristics\nshared by these models. In this paper, we draw attention to a common feature of\nrecent powerful LLMs, namely the alignment training, i.e., training LLMs to\ngenerate human-preferable texts. Our key finding is that as these aligned LLMs\nare trained to maximize the human preferences, they generate texts with higher\nestimated preferences even than human-written texts; thus, such texts are\neasily detected by using the reward model (i.e., an LLM trained to model human\npreference distribution). Based on this finding, we propose two training\nschemes to further improve the detection ability of the reward model, namely\n(i) continual preference fine-tuning to make the reward model prefer aligned\nLGTs even further and (ii) reward modeling of Human/LLM mixed texts (a\nrephrased texts from human-written texts using aligned LLMs), which serves as a\nmedian preference text corpus between LGTs and human-written texts to learn the\ndecision boundary better. We provide an extensive evaluation by considering six\ntext domains across twelve aligned LLMs, where our method demonstrates\nstate-of-the-art results. Code is available at\nhttps://github.com/hyunseoklee-ai/ReMoDetect.\n","authors":["Hyunseok Lee","Jihoon Tack","Jinwoo Shin"],"pdf_url":"https://arxiv.org/pdf/2405.17382v2.pdf","comment":"Published as a conference proceeding for NeurIPS 2024"},{"id":"http://arxiv.org/abs/2308.13149v2","updated":"2024-11-07T08:20:46Z","published":"2023-08-25T03:05:33Z","title":"SciEval: A Multi-Level Large Language Model Evaluation Benchmark for\n  Scientific Research","summary":"  Recently, there has been growing interest in using Large Language Models\n(LLMs) for scientific research. Numerous benchmarks have been proposed to\nevaluate the ability of LLMs for scientific research. However, current\nbenchmarks are mostly based on pre-collected objective questions. This design\nsuffers from data leakage problem and lacks the evaluation of subjective Q/A\nability. In this paper, we propose SciEval, a comprehensive and\nmulti-disciplinary evaluation benchmark to address these issues. Based on\nBloom's taxonomy, SciEval covers four dimensions to systematically evaluate\nscientific research ability. In particular, we design a \"dynamic\" subset based\non scientific principles to prevent evaluation from potential data leakage.\nBoth objective and subjective questions are included in SciEval. These\ncharacteristics make SciEval a more effective benchmark for scientific research\nability evaluation of LLMs. Comprehensive experiments on most advanced LLMs\nshow that, although GPT-4 achieves SOTA performance compared to other LLMs,\nthere is still substantial room for improvement, especially for dynamic\nquestions. The codes and data are publicly available on\nhttps://github.com/OpenDFM/SciEval.\n","authors":["Liangtai Sun","Yang Han","Zihan Zhao","Da Ma","Zhennan Shen","Baocai Chen","Lu Chen","Kai Yu"],"pdf_url":"https://arxiv.org/pdf/2308.13149v2.pdf","comment":"12 pages, 17 figures, 12 tables. Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2411.04496v1","updated":"2024-11-07T07:46:06Z","published":"2024-11-07T07:46:06Z","title":"Thanos: Enhancing Conversational Agents with Skill-of-Mind-Infused Large\n  Language Model","summary":"  To increase social bonding with interlocutors, humans naturally acquire the\nability to respond appropriately in a given situation by considering which\nconversational skill is most suitable for the response - a process we call\nskill-of-mind. For large language model (LLM)-based conversational agents,\nplanning appropriate conversational skills, as humans do, is challenging due to\nthe complexity of social dialogue, especially in interactive scenarios. To\naddress this, we propose a skill-of-mind-annotated conversation dataset, named\nMultifaceted Skill-of-Mind, which includes multi-turn and multifaceted\nconversational skills across various interactive scenarios (e.g., long-term,\ncounseling, task-oriented), grounded in diverse social contexts (e.g.,\ndemographics, persona, rules of thumb). This dataset consists of roughly 100K\nconversations. Using this dataset, we introduce a new family of\nskill-of-mind-infused LLMs, named Thanos, with model sizes of 1B, 3B, and 8B\nparameters. With extensive experiments, these models successfully demonstrate\nthe skill-of-mind process and exhibit strong generalizability in inferring\nmultifaceted skills across a variety of domains. Moreover, we show that Thanos\nsignificantly enhances the quality of responses generated by LLM-based\nconversational agents and promotes prosocial behavior in human evaluations.\n","authors":["Young-Jun Lee","Dokyong Lee","Junyoung Youn","Kyeongjin Oh","Ho-Jin Choi"],"pdf_url":"https://arxiv.org/pdf/2411.04496v1.pdf","comment":"Code: https://github.com/passing2961/Thanos"},{"id":"http://arxiv.org/abs/2410.14979v3","updated":"2024-11-07T07:25:04Z","published":"2024-10-19T05:01:56Z","title":"Do Large Language Models Truly Grasp Mathematics? An Empirical\n  Exploration From A Psychological Perspective","summary":"  Despite their proficiency in math tasks, the mechanisms underlying LLMs'\nmathematical reasoning abilities remain a subject of debate. Recent studies\nsuggest that chain-of-thought (CoT) prompts can bolster mathematical reasoning\nby encouraging LLMs to employ human-like logical reasoning (System 2), enabling\nthem to excel on the Cognitive Reflection Test (CRT). To assess whether LLMs\ngenuinely possess System 2-like logical reasoning, we introduced targeted\nmodifications to CRT problems. Our findings reveal that, despite the use of CoT\nprompts, mainstream LLMs, including the latest o1-preview model, continue to\nexhibit a significant error rate. Further analysis indicates that they\npredominantly rely on System 1-like intuitive reasoning and pattern matching\nderived from training data, rather than demonstrating mastery of mathematical\nthinking. This discovery challenges the prevailing notion that LLMs possess\ngenuine logical reasoning abilities and that CoT can enhance them.\nConsequently, this work may temper overly optimistic projections regarding\nLLMs' advancement toward artificial general intelligence.\n","authors":["Wei Xie","Shuoyoucheng Ma","Zhenhua Wang","Enze Wang","Kai Chen","Xiaobing Sun","Baosheng Wang"],"pdf_url":"https://arxiv.org/pdf/2410.14979v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04090v2","updated":"2024-11-07T07:12:45Z","published":"2024-11-06T18:08:57Z","title":"A Collaborative Content Moderation Framework for Toxicity Detection\n  based on Conformalized Estimates of Annotation Disagreement","summary":"  Content moderation typically combines the efforts of human moderators and\nmachine learning models. However, these systems often rely on data where\nsignificant disagreement occurs during moderation, reflecting the subjective\nnature of toxicity perception. Rather than dismissing this disagreement as\nnoise, we interpret it as a valuable signal that highlights the inherent\nambiguity of the content,an insight missed when only the majority label is\nconsidered. In this work, we introduce a novel content moderation framework\nthat emphasizes the importance of capturing annotation disagreement. Our\napproach uses multitask learning, where toxicity classification serves as the\nprimary task and annotation disagreement is addressed as an auxiliary task.\nAdditionally, we leverage uncertainty estimation techniques, specifically\nConformal Prediction, to account for both the ambiguity in comment annotations\nand the model's inherent uncertainty in predicting toxicity and\ndisagreement.The framework also allows moderators to adjust thresholds for\nannotation disagreement, offering flexibility in determining when ambiguity\nshould trigger a review. We demonstrate that our joint approach enhances model\nperformance, calibration, and uncertainty estimation, while offering greater\nparameter efficiency and improving the review process in comparison to\nsingle-task methods.\n","authors":["Guillermo Villate-Castillo","Javier Del Ser","Borja Sanz"],"pdf_url":"https://arxiv.org/pdf/2411.04090v2.pdf","comment":"35 pages, 1 figure"},{"id":"http://arxiv.org/abs/2406.11709v4","updated":"2024-11-07T07:00:14Z","published":"2024-06-17T16:28:21Z","title":"Instruct, Not Assist: LLM-based Multi-Turn Planning and Hierarchical\n  Questioning for Socratic Code Debugging","summary":"  Socratic questioning is an effective teaching strategy, encouraging critical\nthinking and problem-solving. The conversational capabilities of large language\nmodels (LLMs) show great potential for providing scalable, real-time student\nguidance. However, current LLMs often give away solutions directly, making them\nineffective instructors. We tackle this issue in the code debugging domain with\nTreeInstruct, an Instructor agent guided by a novel state space-based planning\nalgorithm. TreeInstruct asks probing questions to help students independently\nidentify and resolve errors. It estimates a student's conceptual and\nsyntactical knowledge to dynamically construct a question tree based on their\nresponses and current knowledge state, effectively addressing both independent\nand dependent mistakes concurrently in a multi-turn interaction setting. In\naddition to using an existing single-bug debugging benchmark, we construct a\nmore challenging multi-bug dataset of 150 coding problems, incorrect solutions,\nand bug fixes -- all carefully constructed and annotated by experts. Extensive\nevaluation shows TreeInstruct's state-of-the-art performance on both datasets,\nproving it to be a more effective instructor than baselines. Furthermore, a\nreal-world case study with five students of varying skill levels further\ndemonstrates TreeInstruct's ability to guide students to debug their code\nefficiently with minimal turns and highly Socratic questioning.\n","authors":["Priyanka Kargupta","Ishika Agarwal","Dilek Hakkani-Tur","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2406.11709v4.pdf","comment":"Code available at: https://github.com/agarwalishika/TreeInstruct\n  Accepted at EMNLP'24 Findings"},{"id":"http://arxiv.org/abs/2411.04473v1","updated":"2024-11-07T06:51:24Z","published":"2024-11-07T06:51:24Z","title":"ML-Promise: A Multilingual Dataset for Corporate Promise Verification","summary":"  Promises made by politicians, corporate leaders, and public figures have a\nsignificant impact on public perception, trust, and institutional reputation.\nHowever, the complexity and volume of such commitments, coupled with\ndifficulties in verifying their fulfillment, necessitate innovative methods for\nassessing their credibility. This paper introduces the concept of Promise\nVerification, a systematic approach involving steps such as promise\nidentification, evidence assessment, and the evaluation of timing for\nverification. We propose the first multilingual dataset, ML-Promise, which\nincludes English, French, Chinese, Japanese, and Korean, aimed at facilitating\nin-depth verification of promises, particularly in the context of\nEnvironmental, Social, and Governance (ESG) reports. Given the growing emphasis\non corporate environmental contributions, this dataset addresses the challenge\nof evaluating corporate promises, especially in light of practices like\ngreenwashing. Our findings also explore textual and image-based baselines, with\npromising results from retrieval-augmented generation (RAG) approaches. This\nwork aims to foster further discourse on the accountability of public\ncommitments across multiple languages and domains.\n","authors":["Yohei Seki","Hakusen Shu","Anaïs Lhuissier","Hanwool Lee","Juyeon Kang","Min-Yuh Day","Chung-Chi Chen"],"pdf_url":"https://arxiv.org/pdf/2411.04473v1.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2410.04070v5","updated":"2024-11-07T06:21:14Z","published":"2024-10-05T08:00:55Z","title":"PAD: Personalized Alignment of LLMs at Decoding-Time","summary":"  Aligning with personalized preferences, which vary significantly across\ncultural, educational, and political differences, poses a significant challenge\ndue to the computational costs and data demands of traditional alignment\nmethods. In response, this paper presents Personalized Alignment at\nDecoding-time (PAD), a novel framework designed to align LLM outputs with\ndiverse personalized preferences during the inference phase, eliminating the\nneed for additional training. By introducing a unique personalized reward\nmodeling strategy, this framework decouples the text generation process from\npersonalized preferences, facilitating the generation of generalizable\ntoken-level personalized rewards. The PAD algorithm leverages these rewards to\nguide the decoding process, dynamically tailoring the base model's predictions\nto personalized preferences. Extensive experimental results demonstrate that\nPAD not only outperforms existing training-based alignment methods in terms of\naligning with diverse preferences but also shows significant generalizability\nto preferences unseen during training and scalability across different base\nmodels. This work advances the capability of LLMs to meet user needs in\nreal-time applications, presenting a substantial step forward in personalized\nLLM alignment.\n","authors":["Ruizhe Chen","Xiaotian Zhang","Meng Luo","Wenhao Chai","Zuozhu Liu"],"pdf_url":"https://arxiv.org/pdf/2410.04070v5.pdf","comment":"This paper presents Personalized Alignment at Decoding-time (PAD), a\n  novel framework designed to align LLM outputs with diverse personalized\n  preferences during the inference phase"},{"id":"http://arxiv.org/abs/2411.02887v2","updated":"2024-11-07T05:46:42Z","published":"2024-11-05T07:59:22Z","title":"The Translation of Circumlocution in Arabic Short Stories into English","summary":"  This study investigates the translation of circumlocution from Arabic to\nEnglish in a corpus of short stories by renowned Arabic authors. By analyzing\nthe source and target texts, the study aims to identify and categorize\ncircumlocution instances in Arabic and their corresponding renditions in\nEnglish. The study employs Nida's (1964) translation theory as a framework to\nassess the appropriateness of the translation strategies employed. It examines\nthe extent to which translators successfully rendered Arabic circumlocution\ninto English, identifying potential challenges and limitations in the\ntranslation process. The findings reveal significant similarities between\nArabic circumlocution categories and English metadiscourse categories,\nparticularly in terms of textual and interpersonal functions. However, the\nstudy also highlights instances where translators encountered difficulties in\naccurately conveying the nuances of circumlocution, often resorting to\nstrategies like addition, subtraction, and alteration.https://ntu.edu.iq/\n","authors":["Dalal Waadallah Shehab"],"pdf_url":"https://arxiv.org/pdf/2411.02887v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04448v1","updated":"2024-11-07T05:43:50Z","published":"2024-11-07T05:43:50Z","title":"Gradient Localization Improves Lifelong Pretraining of Language Models","summary":"  Large Language Models (LLMs) trained on web-scale text corpora have been\nshown to capture world knowledge in their parameters. However, the mechanism by\nwhich language models store different types of knowledge is poorly understood.\nIn this work, we examine two types of knowledge relating to temporally\nsensitive entities and demonstrate that each type is localized to different\nsets of parameters within the LLMs. We hypothesize that the lack of\nconsideration of the locality of knowledge in existing continual learning\nmethods contributes to both: the failed uptake of new information, and\ncatastrophic forgetting of previously learned information. We observe that\nsequences containing references to updated and newly mentioned entities exhibit\nlarger gradient norms in a subset of layers. We demonstrate that targeting\nparameter updates to these relevant layers can improve the performance of\ncontinually pretraining on language containing temporal drift.\n","authors":["Jared Fernandez","Yonatan Bisk","Emma Strubell"],"pdf_url":"https://arxiv.org/pdf/2411.04448v1.pdf","comment":"EMNLP Findings 2024"},{"id":"http://arxiv.org/abs/2409.18412v2","updated":"2024-11-07T05:38:31Z","published":"2024-09-27T03:00:29Z","title":"SciDFM: A Large Language Model with Mixture-of-Experts for Science","summary":"  Recently, there has been a significant upsurge of interest in leveraging\nlarge language models (LLMs) to assist scientific discovery. However, most LLMs\nonly focus on general science, while they lack domain-specific knowledge, such\nas chemical molecules and amino acid sequences. To bridge these gaps, we\nintroduce SciDFM, a mixture-of-experts LLM, which is trained from scratch and\nis able to conduct college-level scientific reasoning and understand molecules\nand amino acid sequences. We collect a large-scale training corpus containing\nnumerous scientific papers and books from different disciplines as well as data\nfrom domain-specific databases. We further fine-tune the pre-trained model on\nlots of instruction data to improve performances on downstream benchmarks. From\nexperiment results, we show that SciDFM achieves strong performance on general\nscientific benchmarks such as SciEval and SciQ, and it reaches a SOTA\nperformance on domain-specific benchmarks among models of similar size. We\nfurther analyze the expert layers and show that the results of expert selection\nvary with data from different disciplines. To benefit the broader research\ncommunity, we open-source SciDFM at\nhttps://huggingface.co/OpenDFM/SciDFM-MoE-A5.6B-v1.0.\n","authors":["Liangtai Sun","Danyu Luo","Da Ma","Zihan Zhao","Baocai Chen","Zhennan Shen","Su Zhu","Lu Chen","Xin Chen","Kai Yu"],"pdf_url":"https://arxiv.org/pdf/2409.18412v2.pdf","comment":"12 pages, 1 figure, 9 tables. Technical Report, accepted by NeurIPS\n  2024 Workshop FM4Science"},{"id":"http://arxiv.org/abs/2411.04443v1","updated":"2024-11-07T05:35:39Z","published":"2024-11-07T05:35:39Z","title":"ACCIO: Table Understanding Enhanced via Contrastive Learning with\n  Aggregations","summary":"  The attention to table understanding using recent natural language models has\nbeen growing. However, most related works tend to focus on learning the\nstructure of the table directly. Just as humans improve their understanding of\nsentences by comparing them, they can also enhance their understanding by\ncomparing tables. With this idea, in this paper, we introduce ACCIO, tAble\nunderstanding enhanCed via Contrastive learnIng with aggregatiOns, a novel\napproach to enhancing table understanding by contrasting original tables with\ntheir pivot summaries through contrastive learning. ACCIO trains an encoder to\nbring these table pairs closer together. Through validation via column type\nannotation, ACCIO achieves competitive performance with a macro F1 score of\n91.1 compared to state-of-the-art methods. This work represents the first\nattempt to utilize pairs of tables for table embedding, promising significant\nadvancements in table comprehension. Our code is available at\nhttps://github.com/whnhch/ACCIO/.\n","authors":["Whanhee Cho"],"pdf_url":"https://arxiv.org/pdf/2411.04443v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18032v2","updated":"2024-11-07T05:10:20Z","published":"2024-10-23T17:02:59Z","title":"GraphTeam: Facilitating Large Language Model-based Graph Analysis via\n  Multi-Agent Collaboration","summary":"  Graphs are widely used for modeling relational data in real-world scenarios,\nsuch as social networks and urban computing. Existing LLM-based graph analysis\napproaches either integrate graph neural networks (GNNs) for specific machine\nlearning tasks, limiting their transferability, or rely solely on LLMs'\ninternal reasoning ability, resulting in suboptimal performance. To address\nthese limitations, we take advantage of recent advances in LLM-based agents,\nwhich have shown capabilities of utilizing external knowledge or tools for\nproblem solving. By simulating human problem-solving strategies such as analogy\nand collaboration, we propose a multi-agent system based on LLMs named\nGraphTeam, for graph analysis. GraphTeam consists of five LLM-based agents from\nthree modules, and the agents with different specialities can collaborate with\neach other to address complex problems. Specifically, (1) input-output\nnormalization module: the question agent extracts and refines four key\narguments from the original question, facilitating the problem understanding,\nand the answer agent organizes the results to meet the output requirement; (2)\nexternal knowledge retrieval module: we first build a knowledge base consisting\nof relevant documentation and experience information, and then the search agent\nretrieves the most relevant entries for each question. (3) problem-solving\nmodule: given the retrieved information from search agent, the coding agent\nuses established algorithms via programming to generate solutions, and in case\nthe coding agent does not work, the reasoning agent will directly compute the\nresults without programming. Extensive experiments on six graph analysis\nbenchmarks demonstrate that GraphTeam achieves state-of-the-art performance\nwith an average 25.85% improvement over the best baseline in terms of accuracy.\nThe code and data are available at https://github.com/BUPT-GAMMA/GraphTeam.\n","authors":["Xin Li","Qizhi Chu","Yubin Chen","Yang Liu","Yaoqi Liu","Zekai Yu","Weize Chen","Chen Qian","Chuan Shi","Cheng Yang"],"pdf_url":"https://arxiv.org/pdf/2410.18032v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.02449v3","updated":"2024-11-07T04:55:29Z","published":"2024-09-04T05:08:23Z","title":"What is lost in Normalization? Exploring Pitfalls in Multilingual ASR\n  Model Evaluations","summary":"  This paper explores the pitfalls in evaluating multilingual automatic speech\nrecognition (ASR) models, with a particular focus on Indic language scripts. We\ninvestigate the text normalization routine employed by leading ASR models,\nincluding OpenAI Whisper, Meta's MMS, Seamless, and Assembly AI's Conformer,\nand their unintended consequences on performance metrics. Our research reveals\nthat current text normalization practices, while aiming to standardize ASR\noutputs for fair comparison, by removing inconsistencies such as variations in\nspelling, punctuation, and special characters, are fundamentally flawed when\napplied to Indic scripts. Through empirical analysis using text similarity\nscores and in-depth linguistic examination, we demonstrate that these flaws\nlead to artificially improved performance metrics for Indic languages. We\nconclude by proposing a shift towards developing text normalization routines\nthat leverage native linguistic expertise, ensuring more robust and accurate\nevaluations of multilingual ASR models.\n","authors":["Kavya Manohar","Leena G Pillai","Elizabeth Sherly"],"pdf_url":"https://arxiv.org/pdf/2409.02449v3.pdf","comment":"Accepted to EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2411.04427v1","updated":"2024-11-07T04:38:58Z","published":"2024-11-07T04:38:58Z","title":"One fish, two fish, but not the whole sea: Alignment reduces language\n  models' conceptual diversity","summary":"  Researchers in social science and psychology have recently proposed using\nlarge language models (LLMs) as replacements for humans in behavioral research.\nIn addition to arguments about whether LLMs accurately capture population-level\npatterns, this has raised questions about whether LLMs capture human-like\nconceptual diversity. Separately, it is debated whether post-training alignment\n(RLHF or RLAIF) affects models' internal diversity. Inspired by human studies,\nwe use a new way of measuring the conceptual diversity of\nsynthetically-generated LLM \"populations\" by relating the internal variability\nof simulated individuals to the population-level variability. We use this\napproach to evaluate non-aligned and aligned LLMs on two domains with rich\nhuman behavioral data. While no model reaches human-like diversity, aligned\nmodels generally display less diversity than their instruction fine-tuned\ncounterparts. Our findings highlight potential trade-offs between increasing\nmodels' value alignment and decreasing the diversity of their conceptual\nrepresentations.\n","authors":["Sonia K. Murthy","Tomer Ullman","Jennifer Hu"],"pdf_url":"https://arxiv.org/pdf/2411.04427v1.pdf","comment":"17 pages, 10 figures"},{"id":"http://arxiv.org/abs/2305.15265v4","updated":"2024-11-07T04:38:33Z","published":"2023-05-24T15:52:08Z","title":"Winner-Take-All Column Row Sampling for Memory Efficient Adaptation of\n  Language Model","summary":"  With the rapid growth in model size, fine-tuning the large pre-trained\nlanguage model has become increasingly difficult due to its extensive memory\nusage. Previous works usually focus on reducing the number of trainable\nparameters in the network. While the model parameters do contribute to memory\nusage, the primary memory bottleneck during training arises from storing\nfeature maps, also known as activations, as they are crucial for gradient\ncalculation. Notably, neural networks are usually trained using stochastic\ngradient descent. We argue that in stochastic optimization, models can handle\nnoisy gradients as long as the gradient estimator is unbiased with reasonable\nvariance. Following this motivation, we propose a new family of unbiased\nestimators called WTA-CRS, for matrix production with reduced variance, which\nonly requires storing the sub-sampled activations for calculating the gradient.\nOur work provides both theoretical and experimental evidence that, in the\ncontext of tuning transformers, our proposed estimators exhibit lower variance\ncompared to existing ones. By replacing the linear operation with our\napproximated one in transformers, we can achieve up to 2.7$\\times$ peak memory\nreduction with almost no accuracy drop and enables up to $6.4\\times$ larger\nbatch size. Under the same hardware, WTA-CRS enables better down-streaming task\nperformance by applying larger models and/or faster training speed with larger\nbatch sizes.\n","authors":["Zirui Liu","Guanchu Wang","Shaochen Zhong","Zhaozhuo Xu","Daochen Zha","Ruixiang Tang","Zhimeng Jiang","Kaixiong Zhou","Vipin Chaudhary","Shuai Xu","Xia Hu"],"pdf_url":"https://arxiv.org/pdf/2305.15265v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04425v1","updated":"2024-11-07T04:38:29Z","published":"2024-11-07T04:38:29Z","title":"DELIFT: Data Efficient Language model Instruction Fine Tuning","summary":"  Fine-tuning large language models (LLMs) is essential for enhancing their\nperformance on specific tasks but is often resource-intensive due to redundant\nor uninformative data. To address this inefficiency, we introduce DELIFT (Data\nEfficient Language model Instruction Fine-Tuning), a novel algorithm that\nsystematically optimizes data selection across the three key stages of\nfine-tuning: (1) instruction tuning, (2) task-specific fine-tuning (e.g.,\nreasoning, question-answering), and (3) continual fine-tuning (e.g.,\nincorporating new data versions). Unlike existing methods that focus on\nsingle-stage optimization or rely on computationally intensive gradient\ncalculations, DELIFT operates efficiently across all stages. Central to our\napproach is a pairwise utility metric that quantifies how beneficial a data\nsample is for improving the model's responses to other samples, effectively\nmeasuring the informational value relative to the model's current capabilities.\nBy leveraging different submodular functions applied to this metric, DELIFT\nselects diverse and optimal subsets that are useful across all stages of\nfine-tuning. Experiments across various tasks and model scales demonstrate that\nDELIFT can reduce the fine-tuning data size by up to 70% without compromising\nperformance, offering significant computational savings and outperforming\nexisting methods in both efficiency and efficacy.\n","authors":["Ishika Agarwal","Krishna Killamsetty","Lucian Popa","Marina Danilevksy"],"pdf_url":"https://arxiv.org/pdf/2411.04425v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04424v1","updated":"2024-11-07T04:32:40Z","published":"2024-11-07T04:32:40Z","title":"Bayesian Calibration of Win Rate Estimation with LLM Evaluators","summary":"  Recent advances in large language models (LLMs) show the potential of using\nLLMs as evaluators for assessing the quality of text generations from LLMs.\nHowever, applying LLM evaluators naively to compare or judge between different\nsystems can lead to unreliable results due to the intrinsic win rate estimation\nbias of LLM evaluators. In order to mitigate this problem, we propose two\ncalibration methods, Bayesian Win Rate Sampling (BWRS) and Bayesian\nDawid-Skene, both of which leverage Bayesian inference to more accurately infer\nthe true win rate of generative language models. We empirically validate our\nmethods on six datasets covering story generation, summarization, and\ninstruction following tasks. We show that both our methods are effective in\nimproving the accuracy of win rate estimation using LLMs as evaluators,\noffering a promising direction for reliable automatic text quality evaluation.\n","authors":["Yicheng Gao","Gonghan Xu","Zhe Wang","Arman Cohan"],"pdf_url":"https://arxiv.org/pdf/2411.04424v1.pdf","comment":"Accepted by EMNLP 2024"},{"id":"http://arxiv.org/abs/2411.04421v1","updated":"2024-11-07T04:17:30Z","published":"2024-11-07T04:17:30Z","title":"Variational Low-Rank Adaptation Using IVON","summary":"  We show that variational learning can significantly improve the accuracy and\ncalibration of Low-Rank Adaptation (LoRA) without a substantial increase in the\ncost. We replace AdamW by the Improved Variational Online Newton (IVON)\nalgorithm to finetune large language models. For Llama-2 with 7 billion\nparameters, IVON improves the accuracy over AdamW by 2.8% and expected\ncalibration error by 4.6%. The accuracy is also better than the other Bayesian\nalternatives, yet the cost is lower and the implementation is easier. Our work\nprovides additional evidence for the effectiveness of IVON for large language\nmodels. The code is available at\nhttps://github.com/team-approx-bayes/ivon-lora.\n","authors":["Bai Cong","Nico Daheim","Yuesong Shen","Daniel Cremers","Rio Yokota","Mohammad Emtiyaz Khan","Thomas Möllenhoff"],"pdf_url":"https://arxiv.org/pdf/2411.04421v1.pdf","comment":"Published at 38th Workshop on Fine-Tuning in Machine Learning\n  (NeurIPS 2024). Code available at\n  https://github.com/team-approx-bayes/ivon-lora"},{"id":"http://arxiv.org/abs/2406.18064v3","updated":"2024-11-07T04:03:04Z","published":"2024-06-26T04:49:41Z","title":"Evaluating Quality of Answers for Retrieval-Augmented Generation: A\n  Strong LLM Is All You Need","summary":"  We present a comprehensive study of answer quality evaluation in\nRetrieval-Augmented Generation (RAG) applications using vRAG-Eval, a novel\ngrading system that is designed to assess correctness, completeness, and\nhonesty. We further map the grading of quality aspects aforementioned into a\nbinary score, indicating an accept or reject decision, mirroring the intuitive\n\"thumbs-up\" or \"thumbs-down\" gesture commonly used in chat applications. This\napproach suits factual business contexts where a clear decision opinion is\nessential. Our assessment applies vRAG-Eval to two Large Language Models\n(LLMs), evaluating the quality of answers generated by a vanilla RAG\napplication. We compare these evaluations with human expert judgments and find\na substantial alignment between GPT-4's assessments and those of human experts,\nreaching 83% agreement on accept or reject decisions. This study highlights the\npotential of LLMs as reliable evaluators in closed-domain, closed-ended\nsettings, particularly when human evaluations require significant resources.\n","authors":["Yang Wang","Alberto Garcia Hernandez","Roman Kyslyi","Nicholas Kersting"],"pdf_url":"https://arxiv.org/pdf/2406.18064v3.pdf","comment":"13 pages, 8 figures, 12 tables"},{"id":"http://arxiv.org/abs/2411.04105v2","updated":"2024-11-07T03:50:19Z","published":"2024-11-06T18:35:32Z","title":"How Transformers Solve Propositional Logic Problems: A Mechanistic\n  Analysis","summary":"  Large language models (LLMs) have shown amazing performance on tasks that\nrequire planning and reasoning. Motivated by this, we investigate the internal\nmechanisms that underpin a network's ability to perform complex logical\nreasoning. We first construct a synthetic propositional logic problem that\nserves as a concrete test-bed for network training and evaluation. Crucially,\nthis problem demands nontrivial planning to solve, but we can train a small\ntransformer to achieve perfect accuracy. Building on our set-up, we then pursue\nan understanding of precisely how a three-layer transformer, trained from\nscratch, solves this problem. We are able to identify certain \"planning\" and\n\"reasoning\" circuits in the network that necessitate cooperation between the\nattention blocks to implement the desired logic. To expand our findings, we\nthen study a larger model, Mistral 7B. Using activation patching, we\ncharacterize internal components that are critical in solving our logic\nproblem. Overall, our work systemically uncovers novel aspects of small and\nlarge transformers, and continues the study of how they plan and reason.\n","authors":["Guan Zhe Hong","Nishanth Dikkala","Enming Luo","Cyrus Rashtchian","Xin Wang","Rina Panigrahy"],"pdf_url":"https://arxiv.org/pdf/2411.04105v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.07930v4","updated":"2024-11-07T03:37:51Z","published":"2024-08-15T04:57:55Z","title":"MAG-SQL: Multi-Agent Generative Approach with Soft Schema Linking and\n  Iterative Sub-SQL Refinement for Text-to-SQL","summary":"  Recent In-Context Learning based methods have achieved remarkable success in\nText-to-SQL task. However, there is still a large gap between the performance\nof these models and human performance on datasets with complex database schema\nand difficult questions, such as BIRD. Besides, existing work has neglected to\nsupervise intermediate steps when solving questions iteratively with question\ndecomposition methods, and the schema linking methods used in these works are\nvery rudimentary. To address these issues, we propose MAG-SQL, a multi-agent\ngenerative approach with soft schema linking and iterative Sub-SQL refinement.\nIn our framework, an entity-based method with tables' summary is used to select\nthe columns in database, and a novel targets-conditions decomposition method is\nintroduced to decompose those complex questions. Additionally, we build a\niterative generating module which includes a Sub-SQL Generator and Sub-SQL\nRefiner, introducing external oversight for each step of generation. Through a\nseries of ablation studies, the effectiveness of each agent in our framework\nhas been demonstrated. When evaluated on the BIRD benchmark with GPT-4, MAG-SQL\nachieves an execution accuracy of 61.08%, compared to the baseline accuracy of\n46.35% for vanilla GPT-4 and the baseline accuracy of 57.56% for MAC-SQL.\nBesides, our approach makes similar progress on Spider. The codes are available\nat https://github.com/LancelotXWX/MAG-SQL.\n","authors":["Wenxuan Xie","Gaochen Wu","Bowen Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.07930v4.pdf","comment":"22 pages, 14 figures"},{"id":"http://arxiv.org/abs/2407.15186v4","updated":"2024-11-07T03:26:58Z","published":"2024-07-21T14:48:23Z","title":"A Survey on Employing Large Language Models for Text-to-SQL Tasks","summary":"  The increasing volume of data in relational databases and the expertise\nneeded for writing SQL queries pose challenges for users to access and analyze\ndata. Text-to-SQL (Text2SQL) solves the issues by utilizing natural language\nprocessing (NLP) techniques to convert natural language into SQL queries. With\nthe development of Large Language Models (LLMs), a range of LLM-based Text2SQL\nmethods have emerged. This survey provides a comprehensive review of LLMs in\nText2SQL tasks. We review benchmark datasets, prompt engineering methods,\nfine-tuning methods, and base models in LLM-based Text2SQL methods. We provide\ninsights in each part and discuss future directions in this field.\n","authors":["Liang Shi","Zhengju Tang","Nan Zhang","Xiaotong Zhang","Zhi Yang"],"pdf_url":"https://arxiv.org/pdf/2407.15186v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11484v8","updated":"2024-11-07T03:23:16Z","published":"2024-07-16T08:20:39Z","title":"The Oscars of AI Theater: A Survey on Role-Playing with Language Models","summary":"  This survey explores the burgeoning field of role-playing with language\nmodels, focusing on their development from early persona-based models to\nadvanced character-driven simulations facilitated by Large Language Models\n(LLMs). Initially confined to simple persona consistency due to limited model\ncapabilities, role-playing tasks have now expanded to embrace complex character\nportrayals involving character consistency, behavioral alignment, and overall\nattractiveness. We provide a comprehensive taxonomy of the critical components\nin designing these systems, including data, models and alignment, agent\narchitecture and evaluation. This survey not only outlines the current\nmethodologies and challenges, such as managing dynamic personal profiles and\nachieving high-level persona consistency but also suggests avenues for future\nresearch in improving the depth and realism of role-playing applications. The\ngoal is to guide future research by offering a structured overview of current\nmethodologies and identifying potential areas for improvement. Related\nresources and papers are available at\nhttps://github.com/nuochenpku/Awesome-Role-Play-Papers.\n","authors":["Nuo Chen","Yan Wang","Yang Deng","Jia Li"],"pdf_url":"https://arxiv.org/pdf/2407.11484v8.pdf","comment":"28 pages"},{"id":"http://arxiv.org/abs/2411.02603v3","updated":"2024-11-07T03:17:42Z","published":"2024-11-04T20:53:04Z","title":"FactTest: Factuality Testing in Large Language Models with Finite-Sample\n  and Distribution-Free Guarantees","summary":"  The propensity of Large Language Models (LLMs) to generate hallucinations and\nnon-factual content undermines their reliability in high-stakes domains, where\nrigorous control over Type I errors (the conditional probability of incorrectly\nclassifying hallucinations as truthful content) is essential. Despite its\nimportance, formal verification of LLM factuality with such guarantees remains\nlargely unexplored. In this paper, we introduce FactTest, a novel framework\nthat statistically assesses whether a LLM can confidently provide correct\nanswers to given questions with high-probability correctness guarantees. We\nformulate factuality testing as hypothesis testing problem to enforce an upper\nbound of Type I errors at user-specified significance levels. Notably, we prove\nthat our framework also ensures strong Type II error control under mild\nconditions and can be extended to maintain its effectiveness when covariate\nshifts exist. Our approach is distribution-free and works for any number of\nhuman-annotated samples. It is model-agnostic and applies to any black-box or\nwhite-box LM. Extensive experiments on question-answering (QA) and\nmultiple-choice benchmarks demonstrate that FactTest effectively detects\nhallucinations and improves the model's ability to abstain from answering\nunknown questions, leading to an over 40% accuracy improvement.\n","authors":["Fan Nie","Xiaotian Hou","Shuhang Lin","James Zou","Huaxiu Yao","Linjun Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.02603v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17767v2","updated":"2024-11-07T03:16:02Z","published":"2024-05-28T02:46:11Z","title":"Linguistic Collapse: Neural Collapse in (Large) Language Models","summary":"  Neural collapse ($\\mathcal{NC}$) is a phenomenon observed in classification\ntasks where top-layer representations collapse into their class means, which\nbecome equinorm, equiangular and aligned with the classifiers. These behaviors\n-- associated with generalization and robustness -- would manifest under\nspecific conditions: models are trained towards zero loss, with noise-free\nlabels belonging to balanced classes, which do not outnumber the model's hidden\ndimension. Recent studies have explored $\\mathcal{NC}$ in the absence of one or\nmore of these conditions to extend and capitalize on the associated benefits of\nideal geometries. Language modeling presents a curious frontier, as\n\\textit{training by token prediction} constitutes a classification task where\nnone of the conditions exist: the vocabulary is imbalanced and exceeds the\nembedding dimension; different tokens might correspond to similar contextual\nembeddings; and large language models (LLMs) in particular are typically only\ntrained for a few epochs. This paper empirically investigates the impact of\nscaling the architectures and training of causal language models (CLMs) on\ntheir progression towards $\\mathcal{NC}$. We find that $\\mathcal{NC}$\nproperties that develop with scale (and regularization) are linked to\ngeneralization. Moreover, there is evidence of some relationship between\n$\\mathcal{NC}$ and generalization independent of scale. Our work thereby\nunderscores the generality of $\\mathcal{NC}$ as it extends to the novel and\nmore challenging setting of language modeling. Downstream, we seek to inspire\nfurther research on the phenomenon to deepen our understanding of LLMs -- and\nneural networks at large -- and improve existing architectures based on\n$\\mathcal{NC}$-related properties. Our code is hosted on GitHub at\nhttps://github.com/rhubarbwu/linguistic-collapse .\n","authors":["Robert Wu","Vardan Papyan"],"pdf_url":"https://arxiv.org/pdf/2405.17767v2.pdf","comment":"NeurIPS 2024; 36 pages; 30 figures"},{"id":"http://arxiv.org/abs/2403.18802v4","updated":"2024-11-07T03:14:38Z","published":"2024-03-27T17:48:55Z","title":"Long-form factuality in large language models","summary":"  Large language models (LLMs) often generate content that contains factual\nerrors when responding to fact-seeking prompts on open-ended topics. To\nbenchmark a model's long-form factuality in open domains, we first use GPT-4 to\ngenerate LongFact, a prompt set comprising thousands of questions spanning 38\ntopics. We then propose that LLM agents can be used as automated evaluators for\nlong-form factuality through a method which we call Search-Augmented Factuality\nEvaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into\na set of individual facts and to evaluate the accuracy of each fact using a\nmulti-step reasoning process comprising sending search queries to Google Search\nand determining whether a fact is supported by the search results. Furthermore,\nwe propose extending F1 score as an aggregated metric for long-form factuality.\nTo do so, we balance the percentage of supported facts in a response\n(precision) with the percentage of provided facts relative to a hyperparameter\nrepresenting a user's preferred response length (recall).\n  Empirically, we demonstrate that LLM agents can outperform crowdsourced human\nannotators - on a set of ~16k individual facts, SAFE agrees with crowdsourced\nhuman annotators 72% of the time, and on a random subset of 100 disagreement\ncases, SAFE wins 76% of the time. At the same time, SAFE is more than 20 times\ncheaper than human annotators. We also benchmark thirteen language models on\nLongFact across four model families (Gemini, GPT, Claude, and PaLM-2), finding\nthat larger language models generally achieve better long-form factuality.\nLongFact, SAFE, and all experimental code are available at\nhttps://github.com/google-deepmind/long-form-factuality.\n","authors":["Jerry Wei","Chengrun Yang","Xinying Song","Yifeng Lu","Nathan Hu","Jie Huang","Dustin Tran","Daiyi Peng","Ruibo Liu","Da Huang","Cosmo Du","Quoc V. Le"],"pdf_url":"https://arxiv.org/pdf/2403.18802v4.pdf","comment":"NeurIPS 2024; 72 pages, 18 figures, 30 tables. Code at\n  https://github.com/google-deepmind/long-form-factuality"},{"id":"http://arxiv.org/abs/2409.19487v3","updated":"2024-11-07T03:05:18Z","published":"2024-09-28T23:59:46Z","title":"HealthQ: Unveiling Questioning Capabilities of LLM Chains in Healthcare\n  Conversations","summary":"  In digital healthcare, large language models (LLMs) have primarily been\nutilized to enhance question-answering capabilities and improve patient\ninteractions. However, effective patient care necessitates LLM chains that can\nactively gather information by posing relevant questions. This paper presents\nHealthQ, a novel framework designed to evaluate the questioning capabilities of\nLLM healthcare chains. We implemented several LLM chains, including\nRetrieval-Augmented Generation (RAG), Chain of Thought (CoT), and reflective\nchains, and introduced an LLM judge to assess the relevance and informativeness\nof the generated questions. To validate HealthQ, we employed traditional\nNatural Language Processing (NLP) metrics such as Recall-Oriented Understudy\nfor Gisting Evaluation (ROUGE) and Named Entity Recognition (NER)-based set\ncomparison, and constructed two custom datasets from public medical note\ndatasets, ChatDoctor and MTS-Dialog. Our contributions are threefold: we\nprovide the first comprehensive study on the questioning capabilities of LLMs\nin healthcare conversations, develop a novel dataset generation pipeline, and\npropose a detailed evaluation methodology.\n","authors":["Ziyu Wang","Hao Li","Di Huang","Amir M. Rahmani"],"pdf_url":"https://arxiv.org/pdf/2409.19487v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03644v2","updated":"2024-11-07T02:44:34Z","published":"2024-11-06T03:48:41Z","title":"Deploying Multi-task Online Server with Large Language Model","summary":"  In the industry, numerous tasks are deployed online. Traditional approaches\noften tackle each task separately by its own network, which leads to excessive\ncosts for developing and scaling models, especially in the context of large\nlanguage models. Although multi-task methods can save costs through parameter\nsharing, they often struggle to outperform single-task methods in real-world\napplications. To tackle these challenges, we present a three-stage multi-task\nlearning framework for large language models. It involves task filtering,\nfollowed by fine-tuning on high-resource tasks, and finally fine-tuning on all\ntasks. We conducted comprehensive experiments in single-task and multi-task\nsettings. Our approach, exemplified on different benchmarks, demonstrates that\nit is able to achieve performance comparable to the single-task method while\nreducing up to 90.9\\% of its overhead.\n","authors":["Yincen Qu","Chao Ma","Xiangying Dai","Hui Zhou","Yiting Wu","Hengyue Liu"],"pdf_url":"https://arxiv.org/pdf/2411.03644v2.pdf","comment":"Accepted by COLING 2025 Industry Track"},{"id":"http://arxiv.org/abs/2411.04368v1","updated":"2024-11-07T01:58:42Z","published":"2024-11-07T01:58:42Z","title":"Measuring short-form factuality in large language models","summary":"  We present SimpleQA, a benchmark that evaluates the ability of language\nmodels to answer short, fact-seeking questions. We prioritized two properties\nin designing this eval. First, SimpleQA is challenging, as it is adversarially\ncollected against GPT-4 responses. Second, responses are easy to grade, because\nquestions are created such that there exists only a single, indisputable\nanswer. Each answer in SimpleQA is graded as either correct, incorrect, or not\nattempted. A model with ideal behavior would get as many questions correct as\npossible while not attempting the questions for which it is not confident it\nknows the correct answer. SimpleQA is a simple, targeted evaluation for whether\nmodels \"know what they know,\" and our hope is that this benchmark will remain\nrelevant for the next few generations of frontier models. SimpleQA can be found\nat https://github.com/openai/simple-evals.\n","authors":["Jason Wei","Nguyen Karina","Hyung Won Chung","Yunxin Joy Jiao","Spencer Papay","Amelia Glaese","John Schulman","William Fedus"],"pdf_url":"https://arxiv.org/pdf/2411.04368v1.pdf","comment":"Blog post: https://openai.com/index/introducing-simpleqa/"},{"id":"http://arxiv.org/abs/2411.04358v1","updated":"2024-11-07T01:31:48Z","published":"2024-11-07T01:31:48Z","title":"Robust and Efficient Fine-tuning of LLMs with Bayesian\n  Reparameterization of Low-Rank Adaptation","summary":"  Large Language Models (LLMs) are highly resource-intensive to fine-tune due\nto their enormous size. While low-rank adaptation is a prominent\nparameter-efficient fine-tuning approach, it suffers from sensitivity to\nhyperparameter choices, leading to instability in model performance on\nfine-tuning downstream tasks. This paper highlights the importance of effective\nparameterization in low-rank fine-tuning to reduce estimator variance and\nenhance the stability of final model outputs. We propose MonteCLoRA, an\nefficient fine-tuning technique, employing Monte Carlo estimation to learn an\nunbiased posterior estimation of low-rank parameters with low expected\nvariance, which stabilizes fine-tuned LLMs with only O(1) additional\nparameters. MonteCLoRA shows significant improvements in accuracy and\nrobustness, achieving up to 3.8% higher accuracy and 8.6% greater robustness\nthan existing efficient fine-tuning methods on natural language understanding\ntasks with pre-trained RoBERTa-base. Furthermore, in generative tasks with\npre-trained LLaMA-1-7B, MonteCLoRA demonstrates robust zero-shot performance\nwith 50% lower variance than the contemporary efficient fine-tuning methods.\nThe theoretical and empirical results presented in the paper underscore how\nparameterization and hyperpriors balance exploration-exploitation in the\nlow-rank parametric space, therefore leading to more optimal and robust\nparameter estimation during efficient fine-tuning.\n","authors":["Vaibhav Seth","Arinjay Pathak","Ayan Sengupta","Natraj Raman","Sriram Gopalakrishnan","Tanmoy Chakraborty"],"pdf_url":"https://arxiv.org/pdf/2411.04358v1.pdf","comment":"48 pages, 10 figures, 10 tables, Code:\n  https://github.com/LCS2-IIITD/MonteCLoRA"},{"id":"http://arxiv.org/abs/2411.01222v3","updated":"2024-11-07T01:26:43Z","published":"2024-11-02T12:01:44Z","title":"$B^4$: A Black-Box Scrubbing Attack on LLM Watermarks","summary":"  Watermarking has emerged as a prominent technique for LLM-generated content\ndetection by embedding imperceptible patterns. Despite supreme performance, its\nrobustness against adversarial attacks remains underexplored. Previous work\ntypically considers a grey-box attack setting, where the specific type of\nwatermark is already known. Some even necessitates knowledge about\nhyperparameters of the watermarking method. Such prerequisites are unattainable\nin real-world scenarios. Targeting at a more realistic black-box threat model\nwith fewer assumptions, we here propose $B^4$, a black-box scrubbing attack on\nwatermarks. Specifically, we formulate the watermark scrubbing attack as a\nconstrained optimization problem by capturing its objectives with two\ndistributions, a Watermark Distribution and a Fidelity Distribution. This\noptimization problem can be approximately solved using two proxy distributions.\nExperimental results across 12 different settings demonstrate the superior\nperformance of $B^4$ compared with other baselines.\n","authors":["Baizhou Huang","Xiao Pu","Xiaojun Wan"],"pdf_url":"https://arxiv.org/pdf/2411.01222v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06567v3","updated":"2024-11-07T00:54:30Z","published":"2024-07-09T05:52:26Z","title":"FinCon: A Synthesized LLM Multi-Agent System with Conceptual Verbal\n  Reinforcement for Enhanced Financial Decision Making","summary":"  Large language models (LLMs) have demonstrated notable potential in\nconducting complex tasks and are increasingly utilized in various financial\napplications. However, high-quality sequential financial investment\ndecision-making remains challenging. These tasks require multiple interactions\nwith a volatile environment for every decision, demanding sufficient\nintelligence to maximize returns and manage risks. Although LLMs have been used\nto develop agent systems that surpass human teams and yield impressive\ninvestment returns, opportunities to enhance multi-sourced information\nsynthesis and optimize decision-making outcomes through timely experience\nrefinement remain unexplored. Here, we introduce the FinCon, an LLM-based\nmulti-agent framework with CONceptual verbal reinforcement tailored for diverse\nFINancial tasks. Inspired by effective real-world investment firm\norganizational structures, FinCon utilizes a manager-analyst communication\nhierarchy. This structure allows for synchronized cross-functional agent\ncollaboration towards unified goals through natural language interactions and\nequips each agent with greater memory capacity than humans. Additionally, a\nrisk-control component in FinCon enhances decision quality by episodically\ninitiating a self-critiquing mechanism to update systematic investment beliefs.\nThe conceptualized beliefs serve as verbal reinforcement for the future agent's\nbehavior and can be selectively propagated to the appropriate node that\nrequires knowledge updates. This feature significantly improves performance\nwhile reducing unnecessary peer-to-peer communication costs. Moreover, FinCon\ndemonstrates strong generalization capabilities in various financial tasks,\nincluding single stock trading and portfolio management.\n","authors":["Yangyang Yu","Zhiyuan Yao","Haohang Li","Zhiyang Deng","Yupeng Cao","Zhi Chen","Jordan W. Suchow","Rong Liu","Zhenyu Cui","Zhaozhuo Xu","Denghui Zhang","Koduvayur Subbalakshmi","Guojun Xiong","Yueru He","Jimin Huang","Dong Li","Qianqian Xie"],"pdf_url":"https://arxiv.org/pdf/2407.06567v3.pdf","comment":"LLM Applications, LLM Agents, Financial Technology, Quantitative\n  Finance, Algorithmic Trading, Cognitive Science"},{"id":"http://arxiv.org/abs/2411.01030v3","updated":"2024-11-07T00:23:14Z","published":"2024-11-01T21:01:13Z","title":"Birdie: Advancing State Space Models with Reward-Driven Objectives and\n  Curricula","summary":"  Efficient state space models (SSMs), such as linear recurrent neural networks\nand linear attention variants, offer computational advantages over Transformers\nbut struggle with tasks requiring long-range in-context retrieval-like text\ncopying, associative recall, and question answering over long contexts.\nPrevious efforts to address these challenges have focused on architectural\nmodifications, often reintroducing computational inefficiencies. In this paper,\nwe propose a novel training procedure, Birdie, that significantly enhances the\nin-context retrieval capabilities of SSMs without altering their architecture.\nOur approach combines bidirectional input processing with dynamic mixtures of\nspecialized pre-training objectives, optimized via reinforcement learning. We\nintroduce a new bidirectional SSM architecture that seamlessly transitions from\nbidirectional context processing to causal generation. Experimental evaluations\ndemonstrate that Birdie markedly improves performance on retrieval-intensive\ntasks such as multi-number phone book lookup, long paragraph\nquestion-answering, and infilling. This narrows the performance gap with\nTransformers, while retaining computational efficiency. Our findings highlight\nthe importance of training procedures in leveraging the fixed-state capacity of\nSSMs, offering a new direction to advance their capabilities. All code and\npre-trained models are available at https://www.github.com/samblouir/birdie,\nwith support for JAX and PyTorch.\n","authors":["Sam Blouir","Jimmy T. H. Smith","Antonios Anastasopoulos","Amarda Shehu"],"pdf_url":"https://arxiv.org/pdf/2411.01030v3.pdf","comment":"Accepted to EMNLP 2024 (Main Conference)"},{"id":"http://arxiv.org/abs/2411.04330v1","updated":"2024-11-07T00:10:10Z","published":"2024-11-07T00:10:10Z","title":"Scaling Laws for Precision","summary":"  Low precision training and inference affect both the quality and cost of\nlanguage models, but current scaling laws do not account for this. In this\nwork, we devise \"precision-aware\" scaling laws for both training and inference.\nWe propose that training in lower precision reduces the model's \"effective\nparameter count,\" allowing us to predict the additional loss incurred from\ntraining in low precision and post-train quantization. For inference, we find\nthat the degradation introduced by post-training quantization increases as\nmodels are trained on more data, eventually making additional pretraining data\nactively harmful. For training, our scaling laws allow us to predict the loss\nof a model with different parts in different precisions, and suggest that\ntraining larger models in lower precision may be compute optimal. We unify the\nscaling laws for post and pretraining quantization to arrive at a single\nfunctional form that predicts degradation from training and inference in varied\nprecisions. We fit on over 465 pretraining runs and validate our predictions on\nmodel sizes up to 1.7B parameters trained on up to 26B tokens.\n","authors":["Tanishq Kumar","Zachary Ankner","Benjamin F. Spector","Blake Bordelon","Niklas Muennighoff","Mansheej Paul","Cengiz Pehlevan","Christopher Ré","Aditi Raghunathan"],"pdf_url":"https://arxiv.org/pdf/2411.04330v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04329v1","updated":"2024-11-07T00:09:54Z","published":"2024-11-07T00:09:54Z","title":"CodeTree: Agent-guided Tree Search for Code Generation with Large\n  Language Models","summary":"  Pre-trained on massive amounts of code and text data, large language models\n(LLMs) have demonstrated remarkable achievements in performing code generation\ntasks. With additional execution-based feedback, these models can act as agents\nwith capabilities to self-refine and improve generated code autonomously.\nHowever, on challenging coding tasks with extremely large search space, current\nagentic approaches still struggle with multi-stage planning, generating, and\ndebugging. To address this problem, we propose CodeTree, a framework for LLM\nagents to efficiently explore the search space in different stages of the code\ngeneration process. Specifically, we adopted a unified tree structure to\nexplicitly explore different coding strategies, generate corresponding coding\nsolutions, and subsequently refine the solutions. In each stage, critical\ndecision-making (ranking, termination, expanding) of the exploration process is\nguided by both the environmental execution-based feedback and\nLLM-agent-generated feedback. We comprehensively evaluated CodeTree on 7 code\ngeneration benchmarks and demonstrated the significant performance gains of\nCodeTree against strong baselines. Using GPT-4o as the base model, we\nconsistently achieved top results of 95.1 on HumanEval, 98.7 on MBPP, and 43.0\non CodeContests. On the challenging SWEBench benchmark, our approach led to\nsignificant performance gains.\n","authors":["Jierui Li","Hung Le","Yinbo Zhou","Caiming Xiong","Silvio Savarese","Doyen Sahoo"],"pdf_url":"https://arxiv.org/pdf/2411.04329v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04328v1","updated":"2024-11-07T00:09:18Z","published":"2024-11-07T00:09:18Z","title":"Balancing Transparency and Accuracy: A Comparative Analysis of\n  Rule-Based and Deep Learning Models in Political Bias Classification","summary":"  The unchecked spread of digital information, combined with increasing\npolitical polarization and the tendency of individuals to isolate themselves\nfrom opposing political viewpoints, has driven researchers to develop systems\nfor automatically detecting political bias in media. This trend has been\nfurther fueled by discussions on social media. We explore methods for\ncategorizing bias in US news articles, comparing rule-based and deep learning\napproaches. The study highlights the sensitivity of modern self-learning\nsystems to unconstrained data ingestion, while reconsidering the strengths of\ntraditional rule-based systems. Applying both models to left-leaning (CNN) and\nright-leaning (FOX) news articles, we assess their effectiveness on data beyond\nthe original training and test sets.This analysis highlights each model's\naccuracy, offers a framework for exploring deep-learning explainability, and\nsheds light on political bias in US news media. We contrast the opaque\narchitecture of a deep learning model with the transparency of a linguistically\ninformed rule-based model, showing that the rule-based model performs\nconsistently across different data conditions and offers greater transparency,\nwhereas the deep learning model is dependent on the training set and struggles\nwith unseen data.\n","authors":["Manuel Nunez Martinez","Sonja Schmer-Galunder","Zoey Liu","Sangpil Youm","Chathuri Jayaweera","Bonnie J. Dorr"],"pdf_url":"https://arxiv.org/pdf/2411.04328v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2411.05007v1","updated":"2024-11-07T18:59:58Z","published":"2024-11-07T18:59:58Z","title":"SVDQunat: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion\n  Models","summary":"  Diffusion models have been proven highly effective at generating high-quality\nimages. However, as these models grow larger, they require significantly more\nmemory and suffer from higher latency, posing substantial challenges for\ndeployment. In this work, we aim to accelerate diffusion models by quantizing\ntheir weights and activations to 4 bits. At such an aggressive level, both\nweights and activations are highly sensitive, where conventional post-training\nquantization methods for large language models like smoothing become\ninsufficient. To overcome this limitation, we propose SVDQuant, a new 4-bit\nquantization paradigm. Different from smoothing which redistributes outliers\nbetween weights and activations, our approach absorbs these outliers using a\nlow-rank branch. We first consolidate the outliers by shifting them from\nactivations to weights, then employ a high-precision low-rank branch to take in\nthe weight outliers with Singular Value Decomposition (SVD). This process eases\nthe quantization on both sides. However, na\\\"{\\i}vely running the low-rank\nbranch independently incurs significant overhead due to extra data movement of\nactivations, negating the quantization speedup. To address this, we co-design\nan inference engine Nunchaku that fuses the kernels of the low-rank branch into\nthose of the low-bit branch to cut off redundant memory access. It can also\nseamlessly support off-the-shelf low-rank adapters (LoRAs) without the need for\nre-quantization. Extensive experiments on SDXL, PixArt-$\\Sigma$, and FLUX.1\nvalidate the effectiveness of SVDQuant in preserving image quality. We reduce\nthe memory usage for the 12B FLUX.1 models by 3.5$\\times$, achieving\n3.0$\\times$ speedup over the 4-bit weight-only quantized baseline on the 16GB\nlaptop 4090 GPU, paving the way for more interactive applications on PCs. Our\nquantization library and inference engine are open-sourced.\n","authors":["Muyang Li","Yujun Lin","Zhekai Zhang","Tianle Cai","Xiuyu Li","Junxian Guo","Enze Xie","Chenlin Meng","Jun-Yan Zhu","Song Han"],"pdf_url":"https://arxiv.org/pdf/2411.05007v1.pdf","comment":"Quantization Library: https://github.com/mit-han-lab/deepcompressor\n  Inference Engine: https://github.com/mit-han-lab/nunchaku Website:\n  https://hanlab.mit.edu/projects/svdquant Demo: https://svdquant.mit.edu Blog:\n  https://hanlab.mit.edu/blog/svdquant"},{"id":"http://arxiv.org/abs/2411.05006v1","updated":"2024-11-07T18:59:54Z","published":"2024-11-07T18:59:54Z","title":"ProEdit: Simple Progression is All You Need for High-Quality 3D Scene\n  Editing","summary":"  This paper proposes ProEdit - a simple yet effective framework for\nhigh-quality 3D scene editing guided by diffusion distillation in a novel\nprogressive manner. Inspired by the crucial observation that multi-view\ninconsistency in scene editing is rooted in the diffusion model's large\nfeasible output space (FOS), our framework controls the size of FOS and reduces\ninconsistency by decomposing the overall editing task into several subtasks,\nwhich are then executed progressively on the scene. Within this framework, we\ndesign a difficulty-aware subtask decomposition scheduler and an adaptive 3D\nGaussian splatting (3DGS) training strategy, ensuring high quality and\nefficiency in performing each subtask. Extensive evaluation shows that our\nProEdit achieves state-of-the-art results in various scenes and challenging\nediting tasks, all through a simple framework without any expensive or\nsophisticated add-ons like distillation losses, components, or training\nprocedures. Notably, ProEdit also provides a new way to control, preview, and\nselect the \"aggressivity\" of editing operation during the editing process.\n","authors":["Jun-Kun Chen","Yu-Xiong Wang"],"pdf_url":"https://arxiv.org/pdf/2411.05006v1.pdf","comment":"NeurIPS 2024. Project Page: https://immortalco.github.io/ProEdit/"},{"id":"http://arxiv.org/abs/2411.05005v1","updated":"2024-11-07T18:59:53Z","published":"2024-11-07T18:59:53Z","title":"Diff-2-in-1: Bridging Generation and Dense Perception with Diffusion\n  Models","summary":"  Beyond high-fidelity image synthesis, diffusion models have recently\nexhibited promising results in dense visual perception tasks. However, most\nexisting work treats diffusion models as a standalone component for perception\ntasks, employing them either solely for off-the-shelf data augmentation or as\nmere feature extractors. In contrast to these isolated and thus sub-optimal\nefforts, we introduce a unified, versatile, diffusion-based framework,\nDiff-2-in-1, that can simultaneously handle both multi-modal data generation\nand dense visual perception, through a unique exploitation of the\ndiffusion-denoising process. Within this framework, we further enhance\ndiscriminative visual perception via multi-modal generation, by utilizing the\ndenoising network to create multi-modal data that mirror the distribution of\nthe original training set. Importantly, Diff-2-in-1 optimizes the utilization\nof the created diverse and faithful data by leveraging a novel self-improving\nlearning mechanism. Comprehensive experimental evaluations validate the\neffectiveness of our framework, showcasing consistent performance improvements\nacross various discriminative backbones and high-quality multi-modal data\ngeneration characterized by both realism and usefulness.\n","authors":["Shuhong Zheng","Zhipeng Bao","Ruoyu Zhao","Martial Hebert","Yu-Xiong Wang"],"pdf_url":"https://arxiv.org/pdf/2411.05005v1.pdf","comment":"26 pages, 14 figures"},{"id":"http://arxiv.org/abs/2411.05003v1","updated":"2024-11-07T18:59:45Z","published":"2024-11-07T18:59:45Z","title":"ReCapture: Generative Video Camera Controls for User-Provided Videos\n  using Masked Video Fine-Tuning","summary":"  Recently, breakthroughs in video modeling have allowed for controllable\ncamera trajectories in generated videos. However, these methods cannot be\ndirectly applied to user-provided videos that are not generated by a video\nmodel. In this paper, we present ReCapture, a method for generating new videos\nwith novel camera trajectories from a single user-provided video. Our method\nallows us to re-generate the reference video, with all its existing scene\nmotion, from vastly different angles and with cinematic camera motion. Notably,\nusing our method we can also plausibly hallucinate parts of the scene that were\nnot observable in the reference video. Our method works by (1) generating a\nnoisy anchor video with a new camera trajectory using multiview diffusion\nmodels or depth-based point cloud rendering and then (2) regenerating the\nanchor video into a clean and temporally consistent reangled video using our\nproposed masked video fine-tuning technique.\n","authors":["David Junhao Zhang","Roni Paiss","Shiran Zada","Nikhil Karnad","David E. Jacobs","Yael Pritch","Inbar Mosseri","Mike Zheng Shou","Neal Wadhwa","Nataniel Ruiz"],"pdf_url":"https://arxiv.org/pdf/2411.05003v1.pdf","comment":"project page: https://generative-video-camera-controls.github.io/"},{"id":"http://arxiv.org/abs/2411.05001v1","updated":"2024-11-07T18:59:28Z","published":"2024-11-07T18:59:28Z","title":"Analyzing The Language of Visual Tokens","summary":"  With the introduction of transformer-based models for vision and language\ntasks, such as LLaVA and Chameleon, there has been renewed interest in the\ndiscrete tokenized representation of images. These models often treat image\npatches as discrete tokens, analogous to words in natural language, learning\njoint alignments between visual and human languages. However, little is known\nabout the statistical behavior of these visual languages - whether they follow\nsimilar frequency distributions, grammatical structures, or topologies as\nnatural languages. In this paper, we take a natural-language-centric approach\nto analyzing discrete visual languages and uncover striking similarities and\nfundamental differences. We demonstrate that, although visual languages adhere\nto Zipfian distributions, higher token innovation drives greater entropy and\nlower compression, with tokens predominantly representing object parts,\nindicating intermediate granularity. We also show that visual languages lack\ncohesive grammatical structures, leading to higher perplexity and weaker\nhierarchical organization compared to natural languages. Finally, we\ndemonstrate that, while vision models align more closely with natural languages\nthan other models, this alignment remains significantly weaker than the\ncohesion found within natural languages. Through these experiments, we\ndemonstrate how understanding the statistical properties of discrete visual\nlanguages can inform the design of more effective computer vision models.\n","authors":["David M. Chan","Rodolfo Corona","Joonyong Park","Cheol Jun Cho","Yutong Bai","Trevor Darrell"],"pdf_url":"https://arxiv.org/pdf/2411.05001v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04997v1","updated":"2024-11-07T18:59:16Z","published":"2024-11-07T18:59:16Z","title":"LLM2CLIP: Powerful Language Model Unlock Richer Visual Representation","summary":"  CLIP is one of the most important multimodal foundational models today. What\npowers CLIP's capabilities? The rich supervision signals provided by natural\nlanguage, the carrier of human knowledge, shape a powerful cross-modal\nrepresentation space. However, with the rapid advancements in large language\nmodels LLMs like GPT-4 and LLaMA, the boundaries of language comprehension and\ngeneration are continually being pushed. This raises an intriguing question:\ncan the capabilities of LLMs be harnessed to further improve multimodal\nrepresentation learning? The potential benefits of incorporating LLMs into CLIP\nare clear. LLMs' strong textual understanding can fundamentally improve CLIP's\nability to handle image captions, drastically enhancing its ability to process\nlong and complex texts, a well-known limitation of vanilla CLIP. Moreover, LLMs\nare trained on a vast corpus of text, possessing open-world knowledge. This\nallows them to expand on caption information during training, increasing the\nefficiency of the learning process. In this paper, we propose LLM2CLIP, a novel\napproach that embraces the power of LLMs to unlock CLIP's potential. By\nfine-tuning the LLM in the caption space with contrastive learning, we extract\nits textual capabilities into the output embeddings, significantly improving\nthe output layer's textual discriminability. We then design an efficient\ntraining process where the fine-tuned LLM acts as a powerful teacher for CLIP's\nvisual encoder. Thanks to the LLM's presence, we can now incorporate longer and\nmore complex captions without being restricted by vanilla CLIP's text encoder's\ncontext window and ability limitations. Our experiments demonstrate that this\napproach brings substantial improvements in cross-modal tasks.\n","authors":["Weiquan Huang","Aoqi Wu","Yifan Yang","Xufang Luo","Yuqing Yang","Liang Hu","Qi Dai","Xiyang Dai","Dongdong Chen","Chong Luo","Lili Qiu"],"pdf_url":"https://arxiv.org/pdf/2411.04997v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04998v1","updated":"2024-11-07T18:59:16Z","published":"2024-11-07T18:59:16Z","title":"HourVideo: 1-Hour Video-Language Understanding","summary":"  We present HourVideo, a benchmark dataset for hour-long video-language\nunderstanding. Our dataset consists of a novel task suite comprising\nsummarization, perception (recall, tracking), visual reasoning (spatial,\ntemporal, predictive, causal, counterfactual), and navigation (room-to-room,\nobject retrieval) tasks. HourVideo includes 500 manually curated egocentric\nvideos from the Ego4D dataset, spanning durations of 20 to 120 minutes, and\nfeatures 12,976 high-quality, five-way multiple-choice questions. Benchmarking\nresults reveal that multimodal models, including GPT-4 and LLaVA-NeXT, achieve\nmarginal improvements over random chance. In stark contrast, human experts\nsignificantly outperform the state-of-the-art long-context multimodal model,\nGemini Pro 1.5 (85.0% vs. 37.3%), highlighting a substantial gap in multimodal\ncapabilities. Our benchmark, evaluation toolkit, prompts, and documentation are\navailable at https://hourvideo.stanford.edu\n","authors":["Keshigeyan Chandrasegaran","Agrim Gupta","Lea M. Hadzic","Taran Kota","Jimming He","Cristóbal Eyzaguirre","Zane Durante","Manling Li","Jiajun Wu","Li Fei-Fei"],"pdf_url":"https://arxiv.org/pdf/2411.04998v1.pdf","comment":"NeurIPS 2024 Datasets and Benchmarks Track; 28 pages"},{"id":"http://arxiv.org/abs/2411.04995v1","updated":"2024-11-07T18:58:57Z","published":"2024-11-07T18:58:57Z","title":"LoFi: Scalable Local Image Reconstruction with Implicit Neural\n  Representation","summary":"  Neural fields or implicit neural representations (INRs) have attracted\nsignificant attention in machine learning and signal processing due to their\nefficient continuous representation of images and 3D volumes. In this work, we\nbuild on INRs and introduce a coordinate-based local processing framework for\nsolving imaging inverse problems, termed LoFi (Local Field). Unlike\nconventional methods for image reconstruction, LoFi processes local information\nat each coordinate \\textit{separately} by multi-layer perceptrons (MLPs),\nrecovering the object at that specific coordinate. Similar to INRs, LoFi can\nrecover images at any continuous coordinate, enabling image reconstruction at\nmultiple resolutions. With comparable or better performance than standard CNNs\nfor image reconstruction, LoFi achieves excellent generalization to\nout-of-distribution data and memory usage almost independent of image\nresolution. Remarkably, training on $1024 \\times 1024$ images requires just 3GB\nof memory -- over 20 times less than the memory typically needed by standard\nCNNs. Additionally, LoFi's local design allows it to train on extremely small\ndatasets with less than 10 samples, without overfitting or the need for\nregularization or early stopping. Finally, we use LoFi as a denoising prior in\na plug-and-play framework for solving general inverse problems to benefit from\nits continuous image representation and strong generalization. Although trained\non low-resolution images, LoFi can be used as a low-dimensional prior to solve\ninverse problems at any resolution. We validate our framework across a variety\nof imaging modalities, from low-dose computed tomography to radio\ninterferometric imaging.\n","authors":["AmirEhsan Khorashadizadeh","Tobías I. Liaudat","Tianlin Liu","Jason D. McEwen","Ivan Dokmanić"],"pdf_url":"https://arxiv.org/pdf/2411.04995v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04989v1","updated":"2024-11-07T18:56:11Z","published":"2024-11-07T18:56:11Z","title":"SG-I2V: Self-Guided Trajectory Control in Image-to-Video Generation","summary":"  Methods for image-to-video generation have achieved impressive,\nphoto-realistic quality. However, adjusting specific elements in generated\nvideos, such as object motion or camera movement, is often a tedious process of\ntrial and error, e.g., involving re-generating videos with different random\nseeds. Recent techniques address this issue by fine-tuning a pre-trained model\nto follow conditioning signals, such as bounding boxes or point trajectories.\nYet, this fine-tuning procedure can be computationally expensive, and it\nrequires datasets with annotated object motion, which can be difficult to\nprocure. In this work, we introduce SG-I2V, a framework for controllable\nimage-to-video generation that is self-guided$\\unicode{x2013}$offering\nzero-shot control by relying solely on the knowledge present in a pre-trained\nimage-to-video diffusion model without the need for fine-tuning or external\nknowledge. Our zero-shot method outperforms unsupervised baselines while being\ncompetitive with supervised models in terms of visual quality and motion\nfidelity.\n","authors":["Koichi Namekata","Sherwin Bahmani","Ziyi Wu","Yash Kant","Igor Gilitschenski","David B. Lindell"],"pdf_url":"https://arxiv.org/pdf/2411.04989v1.pdf","comment":"Project page: https://kmcode1.github.io/Projects/SG-I2V/"},{"id":"http://arxiv.org/abs/2411.04984v1","updated":"2024-11-07T18:55:08Z","published":"2024-11-07T18:55:08Z","title":"Planar Reflection-Aware Neural Radiance Fields","summary":"  Neural Radiance Fields (NeRF) have demonstrated exceptional capabilities in\nreconstructing complex scenes with high fidelity. However, NeRF's view\ndependency can only handle low-frequency reflections. It falls short when\nhandling complex planar reflections, often interpreting them as erroneous scene\ngeometries and leading to duplicated and inaccurate scene representations. To\naddress this challenge, we introduce a reflection-aware NeRF that jointly\nmodels planar reflectors, such as windows, and explicitly casts reflected rays\nto capture the source of the high-frequency reflections. We query a single\nradiance field to render the primary color and the source of the reflection. We\npropose a sparse edge regularization to help utilize the true sources of\nreflections for rendering planar reflections rather than creating a duplicate\nalong the primary ray at the same depth. As a result, we obtain accurate scene\ngeometry. Rendering along the primary ray results in a clean, reflection-free\nview, while explicitly rendering along the reflected ray allows us to\nreconstruct highly detailed reflections. Our extensive quantitative and\nqualitative evaluations of real-world datasets demonstrate our method's\nenhanced performance in accurately handling reflections.\n","authors":["Chen Gao","Yipeng Wang","Changil Kim","Jia-Bin Huang","Johannes Kopf"],"pdf_url":"https://arxiv.org/pdf/2411.04984v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04967v1","updated":"2024-11-07T18:43:17Z","published":"2024-11-07T18:43:17Z","title":"AsCAN: Asymmetric Convolution-Attention Networks for Efficient\n  Recognition and Generation","summary":"  Neural network architecture design requires making many crucial decisions.\nThe common desiderata is that similar decisions, with little modifications, can\nbe reused in a variety of tasks and applications. To satisfy that,\narchitectures must provide promising latency and performance trade-offs,\nsupport a variety of tasks, scale efficiently with respect to the amounts of\ndata and compute, leverage available data from other tasks, and efficiently\nsupport various hardware. To this end, we introduce AsCAN -- a hybrid\narchitecture, combining both convolutional and transformer blocks. We revisit\nthe key design principles of hybrid architectures and propose a simple and\neffective \\emph{asymmetric} architecture, where the distribution of\nconvolutional and transformer blocks is \\emph{asymmetric}, containing more\nconvolutional blocks in the earlier stages, followed by more transformer blocks\nin later stages. AsCAN supports a variety of tasks: recognition, segmentation,\nclass-conditional image generation, and features a superior trade-off between\nperformance and latency. We then scale the same architecture to solve a\nlarge-scale text-to-image task and show state-of-the-art performance compared\nto the most recent public and commercial models. Notably, even without any\ncomputation optimization for transformer blocks, our models still yield faster\ninference speed than existing works featuring efficient attention mechanisms,\nhighlighting the advantages and the value of our approach.\n","authors":["Anil Kag","Huseyin Coskun","Jierun Chen","Junli Cao","Willi Menapace","Aliaksandr Siarohin","Sergey Tulyakov","Jian Ren"],"pdf_url":"https://arxiv.org/pdf/2411.04967v1.pdf","comment":"NeurIPS 2024. Project Page:\n  https://snap-research.github.io/snap_image/"},{"id":"http://arxiv.org/abs/2401.09980v2","updated":"2024-11-07T18:43:06Z","published":"2024-01-18T13:51:20Z","title":"A Comparative Analysis of U-Net-based models for Segmentation of Cardiac\n  MRI","summary":"  Medical imaging refers to the technologies and methods utilized to view the\nhuman body and its inside, in order to diagnose, monitor, or even treat medical\ndisorders. This paper aims to explore the application of deep learning\ntechniques in the semantic segmentation of Cardiac short-axis MRI (Magnetic\nResonance Imaging) images, aiming to enhance the diagnosis, monitoring, and\ntreatment of medical disorders related to the heart. The focus centers on\nimplementing various architectures that are derivatives of U-Net, to\neffectively isolate specific parts of the heart for comprehensive anatomical\nand functional analysis. Through a combination of images, graphs, and\nquantitative metrics, the efficacy of the models and their predictions are\nshowcased. Additionally, this paper addresses encountered challenges and\noutline strategies for future improvements. This abstract provides a concise\noverview of the efforts in utilizing deep learning for cardiac image\nsegmentation, emphasizing both the accomplishments and areas for further\nrefinement.\n","authors":["Ketan Suhaas Saichandran"],"pdf_url":"https://arxiv.org/pdf/2401.09980v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04963v1","updated":"2024-11-07T18:40:17Z","published":"2024-11-07T18:40:17Z","title":"VAIR: Visuo-Acoustic Implicit Representations for Low-Cost, Multi-Modal\n  Transparent Surface Reconstruction in Indoor Scenes","summary":"  Mobile robots operating indoors must be prepared to navigate challenging\nscenes that contain transparent surfaces. This paper proposes a novel method\nfor the fusion of acoustic and visual sensing modalities through implicit\nneural representations to enable dense reconstruction of transparent surfaces\nin indoor scenes. We propose a novel model that leverages generative latent\noptimization to learn an implicit representation of indoor scenes consisting of\ntransparent surfaces. We demonstrate that we can query the implicit\nrepresentation to enable volumetric rendering in image space or 3D geometry\nreconstruction (point clouds or mesh) with transparent surface prediction. We\nevaluate our method's effectiveness qualitatively and quantitatively on a new\ndataset collected using a custom, low-cost sensing platform featuring RGB-D\ncameras and ultrasonic sensors. Our method exhibits significant improvement\nover state-of-the-art for transparent surface reconstruction.\n","authors":["Advaith V. Sethuraman","Onur Bagoren","Harikrishnan Seetharaman","Dalton Richardson","Joseph Taylor","Katherine A. Skinner"],"pdf_url":"https://arxiv.org/pdf/2411.04963v1.pdf","comment":"https://umfieldrobotics.github.io/VAIR_site/"},{"id":"http://arxiv.org/abs/2411.04956v1","updated":"2024-11-07T18:32:00Z","published":"2024-11-07T18:32:00Z","title":"Uncovering Hidden Subspaces in Video Diffusion Models Using\n  Re-Identification","summary":"  Latent Video Diffusion Models can easily deceive casual observers and domain\nexperts alike thanks to the produced image quality and temporal consistency.\nBeyond entertainment, this creates opportunities around safe data sharing of\nfully synthetic datasets, which are crucial in healthcare, as well as other\ndomains relying on sensitive personal information. However, privacy concerns\nwith this approach have not fully been addressed yet, and models trained on\nsynthetic data for specific downstream tasks still perform worse than those\ntrained on real data. This discrepancy may be partly due to the sampling space\nbeing a subspace of the training videos, effectively reducing the training data\nsize for downstream models. Additionally, the reduced temporal consistency when\ngenerating long videos could be a contributing factor.\n  In this paper, we first show that training privacy-preserving models in\nlatent space is computationally more efficient and generalize better.\nFurthermore, to investigate downstream degradation factors, we propose to use a\nre-identification model, previously employed as a privacy preservation filter.\nWe demonstrate that it is sufficient to train this model on the latent space of\nthe video generator. Subsequently, we use these models to evaluate the subspace\ncovered by synthetic video datasets and thus introduce a new way to measure the\nfaithfulness of generative machine learning models. We focus on a specific\napplication in healthcare echocardiography to illustrate the effectiveness of\nour novel methods. Our findings indicate that only up to 30.8% of the training\nvideos are learned in latent video diffusion models, which could explain the\nlack of performance when training downstream tasks on synthetic data.\n","authors":["Mischa Dombrowski","Hadrien Reynaud","Bernhard Kainz"],"pdf_url":"https://arxiv.org/pdf/2411.04956v1.pdf","comment":"8 pages, 5 tables, 6 figures"},{"id":"http://arxiv.org/abs/2411.04954v1","updated":"2024-11-07T18:31:08Z","published":"2024-11-07T18:31:08Z","title":"CAD-MLLM: Unifying Multimodality-Conditioned CAD Generation With MLLM","summary":"  This paper aims to design a unified Computer-Aided Design (CAD) generation\nsystem that can easily generate CAD models based on the user's inputs in the\nform of textual description, images, point clouds, or even a combination of\nthem. Towards this goal, we introduce the CAD-MLLM, the first system capable of\ngenerating parametric CAD models conditioned on the multimodal input.\nSpecifically, within the CAD-MLLM framework, we leverage the command sequences\nof CAD models and then employ advanced large language models (LLMs) to align\nthe feature space across these diverse multi-modalities data and CAD models'\nvectorized representations. To facilitate the model training, we design a\ncomprehensive data construction and annotation pipeline that equips each CAD\nmodel with corresponding multimodal data. Our resulting dataset, named\nOmni-CAD, is the first multimodal CAD dataset that contains textual\ndescription, multi-view images, points, and command sequence for each CAD\nmodel. It contains approximately 450K instances and their CAD construction\nsequences. To thoroughly evaluate the quality of our generated CAD models, we\ngo beyond current evaluation metrics that focus on reconstruction quality by\nintroducing additional metrics that assess topology quality and surface\nenclosure extent. Extensive experimental results demonstrate that CAD-MLLM\nsignificantly outperforms existing conditional generative methods and remains\nhighly robust to noises and missing points. The project page and more\nvisualizations can be found at: https://cad-mllm.github.io/\n","authors":["Jingwei Xu","Chenyu Wang","Zibo Zhao","Wen Liu","Yi Ma","Shenghua Gao"],"pdf_url":"https://arxiv.org/pdf/2411.04954v1.pdf","comment":"Project page: https://cad-mllm.github.io/"},{"id":"http://arxiv.org/abs/2411.04952v1","updated":"2024-11-07T18:29:38Z","published":"2024-11-07T18:29:38Z","title":"M3DocRAG: Multi-modal Retrieval is What You Need for Multi-page\n  Multi-document Understanding","summary":"  Document visual question answering (DocVQA) pipelines that answer questions\nfrom documents have broad applications. Existing methods focus on handling\nsingle-page documents with multi-modal language models (MLMs), or rely on\ntext-based retrieval-augmented generation (RAG) that uses text extraction tools\nsuch as optical character recognition (OCR). However, there are difficulties in\napplying these methods in real-world scenarios: (a) questions often require\ninformation across different pages or documents, where MLMs cannot handle many\nlong documents; (b) documents often have important information in visual\nelements such as figures, but text extraction tools ignore them. We introduce\nM3DocRAG, a novel multi-modal RAG framework that flexibly accommodates various\ndocument contexts (closed-domain and open-domain), question hops (single-hop\nand multi-hop), and evidence modalities (text, chart, figure, etc.). M3DocRAG\nfinds relevant documents and answers questions using a multi-modal retriever\nand an MLM, so that it can efficiently handle single or many documents while\npreserving visual information. Since previous DocVQA datasets ask questions in\nthe context of a specific document, we also present M3DocVQA, a new benchmark\nfor evaluating open-domain DocVQA over 3,000+ PDF documents with 40,000+ pages.\nIn three benchmarks (M3DocVQA/MMLongBench-Doc/MP-DocVQA), empirical results\nshow that M3DocRAG with ColPali and Qwen2-VL 7B achieves superior performance\nthan many strong baselines, including state-of-the-art performance in\nMP-DocVQA. We provide comprehensive analyses of different indexing, MLMs, and\nretrieval models. Lastly, we qualitatively show that M3DocRAG can successfully\nhandle various scenarios, such as when relevant information exists across\nmultiple pages and when answer evidence only exists in images.\n","authors":["Jaemin Cho","Debanjan Mahata","Ozan Irsoy","Yujie He","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2411.04952v1.pdf","comment":"Project webpage: https://m3docrag.github.io"},{"id":"http://arxiv.org/abs/2401.08426v5","updated":"2024-11-07T18:22:41Z","published":"2024-01-16T15:11:29Z","title":"GD doesn't make the cut: Three ways that non-differentiability affects\n  neural network training","summary":"  This paper critically examines the fundamental distinctions between gradient\nmethods applied to non-differentiable functions (NGDMs) and classical gradient\ndescents (GDs) for differentiable functions, revealing significant gaps in\ncurrent deep learning optimization theory. We demonstrate that NGDMs exhibit\nmarkedly different convergence properties compared to GDs, strongly challenging\nthe applicability of extensive neural network convergence literature based on\n$L-smoothness$ to non-smooth neural networks. Our analysis reveals paradoxical\nbehavior of NDGM solutions for $L_{1}$-regularized problems, where increasing\nregularization counterintuitively leads to larger $L_{1}$ norms of optimal\nsolutions. This finding calls into question widely adopted $L_{1}$ penalization\ntechniques for network pruning. We further challenge the common assumption that\noptimization algorithms like RMSProp behave similarly in differentiable and\nnon-differentiable contexts. Expanding on the Edge of Stability phenomenon, we\ndemonstrate its occurrence in a broader class of functions, including Lipschitz\ncontinuous convex differentiable functions. This finding raises important\nquestions about its relevance and interpretation in non-convex,\nnon-differentiable neural networks, particularly those using ReLU activations.\nOur work identifies critical misunderstandings of NDGMs in influential\nliterature, stemming from an overreliance on strong smoothness assumptions.\nThese findings necessitate a reevaluation of optimization dynamics in deep\nlearning, emphasizing the crucial need for more nuanced theoretical foundations\nin analyzing these complex systems.\n","authors":["Siddharth Krishna Kumar"],"pdf_url":"https://arxiv.org/pdf/2401.08426v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04942v1","updated":"2024-11-07T18:20:28Z","published":"2024-11-07T18:20:28Z","title":"A Reinforcement Learning-Based Automatic Video Editing Method Using\n  Pre-trained Vision-Language Model","summary":"  In this era of videos, automatic video editing techniques attract more and\nmore attention from industry and academia since they can reduce workloads and\nlower the requirements for human editors. Existing automatic editing systems\nare mainly scene- or event-specific, e.g., soccer game broadcasting, yet the\nautomatic systems for general editing, e.g., movie or vlog editing which covers\nvarious scenes and events, were rarely studied before, and converting the\nevent-driven editing method to a general scene is nontrivial. In this paper, we\npropose a two-stage scheme for general editing. Firstly, unlike previous works\nthat extract scene-specific features, we leverage the pre-trained\nVision-Language Model (VLM) to extract the editing-relevant representations as\nediting context. Moreover, to close the gap between the professional-looking\nvideos and the automatic productions generated with simple guidelines, we\npropose a Reinforcement Learning (RL)-based editing framework to formulate the\nediting problem and train the virtual editor to make better sequential editing\ndecisions. Finally, we evaluate the proposed method on a more general editing\ntask with a real movie dataset. Experimental results demonstrate the\neffectiveness and benefits of the proposed context representation and the\nlearning ability of our RL-based editing framework.\n","authors":["Panwen Hu","Nan Xiao","Feifei Li","Yongquan Chen","Rui Huang"],"pdf_url":"https://arxiv.org/pdf/2411.04942v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04933v1","updated":"2024-11-07T18:12:49Z","published":"2024-11-07T18:12:49Z","title":"SaSR-Net: Source-Aware Semantic Representation Network for Enhancing\n  Audio-Visual Question Answering","summary":"  Audio-Visual Question Answering (AVQA) is a challenging task that involves\nanswering questions based on both auditory and visual information in videos. A\nsignificant challenge is interpreting complex multi-modal scenes, which include\nboth visual objects and sound sources, and connecting them to the given\nquestion. In this paper, we introduce the Source-aware Semantic Representation\nNetwork (SaSR-Net), a novel model designed for AVQA. SaSR-Net utilizes\nsource-wise learnable tokens to efficiently capture and align audio-visual\nelements with the corresponding question. It streamlines the fusion of audio\nand visual information using spatial and temporal attention mechanisms to\nidentify answers in multi-modal scenes. Extensive experiments on the Music-AVQA\nand AVQA-Yang datasets show that SaSR-Net outperforms state-of-the-art AVQA\nmethods.\n","authors":["ianyu Yang","Yiyang Nan","Lisen Dai","Zhenwen Liang","Yapeng Tian","Xiangliang Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.04933v1.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2411.04928v1","updated":"2024-11-07T18:07:31Z","published":"2024-11-07T18:07:31Z","title":"DimensionX: Create Any 3D and 4D Scenes from a Single Image with\n  Controllable Video Diffusion","summary":"  In this paper, we introduce \\textbf{DimensionX}, a framework designed to\ngenerate photorealistic 3D and 4D scenes from just a single image with video\ndiffusion. Our approach begins with the insight that both the spatial structure\nof a 3D scene and the temporal evolution of a 4D scene can be effectively\nrepresented through sequences of video frames. While recent video diffusion\nmodels have shown remarkable success in producing vivid visuals, they face\nlimitations in directly recovering 3D/4D scenes due to limited spatial and\ntemporal controllability during generation. To overcome this, we propose\nST-Director, which decouples spatial and temporal factors in video diffusion by\nlearning dimension-aware LoRAs from dimension-variant data. This controllable\nvideo diffusion approach enables precise manipulation of spatial structure and\ntemporal dynamics, allowing us to reconstruct both 3D and 4D representations\nfrom sequential frames with the combination of spatial and temporal dimensions.\nAdditionally, to bridge the gap between generated videos and real-world scenes,\nwe introduce a trajectory-aware mechanism for 3D generation and an\nidentity-preserving denoising strategy for 4D generation. Extensive experiments\non various real-world and synthetic datasets demonstrate that DimensionX\nachieves superior results in controllable video generation, as well as in 3D\nand 4D scene generation, compared with previous methods.\n","authors":["Wenqiang Sun","Shuo Chen","Fangfu Liu","Zilong Chen","Yueqi Duan","Jun Zhang","Yikai Wang"],"pdf_url":"https://arxiv.org/pdf/2411.04928v1.pdf","comment":"Project Page: https://chenshuo20.github.io/DimensionX/"},{"id":"http://arxiv.org/abs/2411.04925v1","updated":"2024-11-07T18:00:33Z","published":"2024-11-07T18:00:33Z","title":"StoryAgent: Customized Storytelling Video Generation via Multi-Agent\n  Collaboration","summary":"  The advent of AI-Generated Content (AIGC) has spurred research into automated\nvideo generation to streamline conventional processes. However, automating\nstorytelling video production, particularly for customized narratives, remains\nchallenging due to the complexity of maintaining subject consistency across\nshots. While existing approaches like Mora and AesopAgent integrate multiple\nagents for Story-to-Video (S2V) generation, they fall short in preserving\nprotagonist consistency and supporting Customized Storytelling Video Generation\n(CSVG). To address these limitations, we propose StoryAgent, a multi-agent\nframework designed for CSVG. StoryAgent decomposes CSVG into distinct subtasks\nassigned to specialized agents, mirroring the professional production process.\nNotably, our framework includes agents for story design, storyboard generation,\nvideo creation, agent coordination, and result evaluation. Leveraging the\nstrengths of different models, StoryAgent enhances control over the generation\nprocess, significantly improving character consistency. Specifically, we\nintroduce a customized Image-to-Video (I2V) method, LoRA-BE, to enhance\nintra-shot temporal consistency, while a novel storyboard generation pipeline\nis proposed to maintain subject consistency across shots. Extensive experiments\ndemonstrate the effectiveness of our approach in synthesizing highly consistent\nstorytelling videos, outperforming state-of-the-art methods. Our contributions\ninclude the introduction of StoryAgent, a versatile framework for video\ngeneration tasks, and novel techniques for preserving protagonist consistency.\n","authors":["Panwen Hu","Jin Jiang","Jianqi Chen","Mingfei Han","Shengcai Liao","Xiaojun Chang","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2411.04925v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04924v1","updated":"2024-11-07T17:59:31Z","published":"2024-11-07T17:59:31Z","title":"MVSplat360: Feed-Forward 360 Scene Synthesis from Sparse Views","summary":"  We introduce MVSplat360, a feed-forward approach for 360{\\deg} novel view\nsynthesis (NVS) of diverse real-world scenes, using only sparse observations.\nThis setting is inherently ill-posed due to minimal overlap among input views\nand insufficient visual information provided, making it challenging for\nconventional methods to achieve high-quality results. Our MVSplat360 addresses\nthis by effectively combining geometry-aware 3D reconstruction with temporally\nconsistent video generation. Specifically, it refactors a feed-forward 3D\nGaussian Splatting (3DGS) model to render features directly into the latent\nspace of a pre-trained Stable Video Diffusion (SVD) model, where these features\nthen act as pose and visual cues to guide the denoising process and produce\nphotorealistic 3D-consistent views. Our model is end-to-end trainable and\nsupports rendering arbitrary views with as few as 5 sparse input views. To\nevaluate MVSplat360's performance, we introduce a new benchmark using the\nchallenging DL3DV-10K dataset, where MVSplat360 achieves superior visual\nquality compared to state-of-the-art methods on wide-sweeping or even 360{\\deg}\nNVS tasks. Experiments on the existing benchmark RealEstate10K also confirm the\neffectiveness of our model. The video results are available on our project\npage: https://donydchen.github.io/mvsplat360.\n","authors":["Yuedong Chen","Chuanxia Zheng","Haofei Xu","Bohan Zhuang","Andrea Vedaldi","Tat-Jen Cham","Jianfei Cai"],"pdf_url":"https://arxiv.org/pdf/2411.04924v1.pdf","comment":"NeurIPS 2024, Project page: https://donydchen.github.io/mvsplat360,\n  Code: https://github.com/donydchen/mvsplat360"},{"id":"http://arxiv.org/abs/2411.04923v1","updated":"2024-11-07T17:59:27Z","published":"2024-11-07T17:59:27Z","title":"VideoGLaMM: A Large Multimodal Model for Pixel-Level Visual Grounding in\n  Videos","summary":"  Fine-grained alignment between videos and text is challenging due to complex\nspatial and temporal dynamics in videos. Existing video-based Large Multimodal\nModels (LMMs) handle basic conversations but struggle with precise pixel-level\ngrounding in videos. To address this, we introduce VideoGLaMM, a LMM designed\nfor fine-grained pixel-level grounding in videos based on user-provided textual\ninputs. Our design seamlessly connects three key components: a Large Language\nModel, a dual vision encoder that emphasizes both spatial and temporal details,\nand a spatio-temporal decoder for accurate mask generation. This connection is\nfacilitated via tunable V-L and L-V adapters that enable close Vision-Language\n(VL) alignment. The architecture is trained to synchronize both spatial and\ntemporal elements of video content with textual instructions. To enable\nfine-grained grounding, we curate a multimodal dataset featuring detailed\nvisually-grounded conversations using a semiautomatic annotation pipeline,\nresulting in a diverse set of 38k video-QA triplets along with 83k objects and\n671k masks. We evaluate VideoGLaMM on three challenging tasks: Grounded\nConversation Generation, Visual Grounding, and Referring Video Segmentation.\nExperimental results show that our model consistently outperforms existing\napproaches across all three tasks.\n","authors":["Shehan Munasinghe","Hanan Gani","Wenqi Zhu","Jiale Cao","Eric Xing","Fahad Shahbaz Khan","Salman Khan"],"pdf_url":"https://arxiv.org/pdf/2411.04923v1.pdf","comment":"Technical Report of VideoGLaMM"},{"id":"http://arxiv.org/abs/2411.04919v1","updated":"2024-11-07T17:56:16Z","published":"2024-11-07T17:56:16Z","title":"Stem-OB: Generalizable Visual Imitation Learning with Stem-Like\n  Convergent Observation through Diffusion Inversion","summary":"  Visual imitation learning methods demonstrate strong performance, yet they\nlack generalization when faced with visual input perturbations, including\nvariations in lighting and textures, impeding their real-world application. We\npropose Stem-OB that utilizes pretrained image diffusion models to suppress\nlow-level visual differences while maintaining high-level scene structures.\nThis image inversion process is akin to transforming the observation into a\nshared representation, from which other observations stem, with extraneous\ndetails removed. Stem-OB contrasts with data-augmentation approaches as it is\nrobust to various unspecified appearance changes without the need for\nadditional training. Our method is a simple yet highly effective plug-and-play\nsolution. Empirical results confirm the effectiveness of our approach in\nsimulated tasks and show an exceptionally significant improvement in real-world\napplications, with an average increase of 22.2% in success rates compared to\nthe best baseline. See https://hukz18.github.io/Stem-Ob/ for more info.\n","authors":["Kaizhe Hu","Zihang Rui","Yao He","Yuyao Liu","Pu Hua","Huazhe Xu"],"pdf_url":"https://arxiv.org/pdf/2411.04919v1.pdf","comment":"Arxiv preprint version"},{"id":"http://arxiv.org/abs/2411.04912v1","updated":"2024-11-07T17:51:28Z","published":"2024-11-07T17:51:28Z","title":"Robust Iris Centre Localisation for Assistive Eye-Gaze Tracking","summary":"  In this research work, we address the problem of robust iris centre\nlocalisation in unconstrained conditions as a core component of our eye-gaze\ntracking platform. We investigate the application of U-Net variants for\nsegmentation-based and regression-based approaches to improve our iris centre\nlocalisation, which was previously based on Bayes' classification. The achieved\nresults are comparable to or better than the state-of-the-art, offering a\ndrastic improvement over those achieved by the Bayes' classifier, and without\nsacrificing the real-time performance of our eye-gaze tracking platform.\n","authors":["Nipun Sandamal Ranasekara Pathiranage","Stefania Cristina","Kenneth P. Camilleri"],"pdf_url":"https://arxiv.org/pdf/2411.04912v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04892v1","updated":"2024-11-07T17:31:21Z","published":"2024-11-07T17:31:21Z","title":"In the Era of Prompt Learning with Vision-Language Models","summary":"  Large-scale foundation models like CLIP have shown strong zero-shot\ngeneralization but struggle with domain shifts, limiting their adaptability. In\nour work, we introduce \\textsc{StyLIP}, a novel domain-agnostic prompt learning\nstrategy for Domain Generalization (DG). StyLIP disentangles visual style and\ncontent in CLIP`s vision encoder by using style projectors to learn\ndomain-specific prompt tokens and combining them with content features. Trained\ncontrastively, this approach enables seamless adaptation across domains,\noutperforming state-of-the-art methods on multiple DG benchmarks. Additionally,\nwe propose AD-CLIP for unsupervised domain adaptation (DA), leveraging CLIP`s\nfrozen vision backbone to learn domain-invariant prompts through image style\nand content features. By aligning domains in embedding space with entropy\nminimization, AD-CLIP effectively handles domain shifts, even when only target\ndomain samples are available. Lastly, we outline future work on class discovery\nusing prompt learning for semantic segmentation in remote sensing, focusing on\nidentifying novel or rare classes in unstructured environments. This paves the\nway for more adaptive and generalizable models in complex, real-world\nscenarios.\n","authors":["Ankit Jha"],"pdf_url":"https://arxiv.org/pdf/2411.04892v1.pdf","comment":"ICVGIP 2024, Young Faculty Symposium"},{"id":"http://arxiv.org/abs/2410.03728v2","updated":"2024-11-07T17:19:26Z","published":"2024-09-30T10:50:12Z","title":"Exploring QUIC Dynamics: A Large-Scale Dataset for Encrypted Traffic\n  Analysis","summary":"  QUIC, a new and increasingly used transport protocol, addresses and resolves\nthe limitations of TCP by offering improved security, performance, and features\nsuch as stream multiplexing and connection migration. These features, however,\nalso present challenges for network operators who need to monitor and analyze\nweb traffic. In this paper, we introduce VisQUIC, a labeled dataset comprising\nover 100,000 QUIC traces from more than 44,000 websites (URLs), collected over\na four-month period. These traces provide the foundation for generating more\nthan seven million images, with configurable parameters of window length, pixel\nresolution, normalization, and labels. These images enable an observer looking\nat the interactions between a client and a server to analyze and gain insights\nabout QUIC encrypted connections. To illustrate the dataset's potential, we\noffer a use-case example of an observer estimating the number of HTTP/3\nresponses/requests pairs in a given QUIC, which can reveal server behavior,\nclient--server interactions, and the load imposed by an observed connection. We\nformulate the problem as a discrete regression problem, train a machine\nlearning (ML) model for it, and then evaluate it using the proposed dataset on\nan example use case.\n","authors":["Barak Gahtan","Robert J. Shahla","Alex M. Bronstein","Reuven Cohen"],"pdf_url":"https://arxiv.org/pdf/2410.03728v2.pdf","comment":"The dataset and the supplementary material can be provided upon\n  request"},{"id":"http://arxiv.org/abs/2407.16803v2","updated":"2024-11-07T17:10:15Z","published":"2024-07-23T19:06:44Z","title":"C3T: Cross-modal Transfer Through Time for Human Action Recognition","summary":"  In order to unlock the potential of diverse sensors, we investigate a method\nto transfer knowledge between modalities using the structure of a unified\nmultimodal representation space for Human Action Recognition (HAR). We\nformalize and explore an understudied cross-modal transfer setting we term\nUnsupervised Modality Adaptation (UMA), where the modality used in testing is\nnot used in supervised training, i.e. zero labeled instances of the test\nmodality are available during training. We develop three methods to perform\nUMA: Student-Teacher (ST), Contrastive Alignment (CA), and Cross-modal Transfer\nThrough Time (C3T). Our extensive experiments on various camera+IMU datasets\ncompare these methods to each other in the UMA setting, and to their empirical\nupper bound in the supervised setting. The results indicate C3T is the most\nrobust and highest performing by at least a margin of 8%, and nears the\nsupervised setting performance even in the presence of temporal noise. This\nmethod introduces a novel mechanism for aligning signals across time-varying\nlatent vectors, extracted from the receptive field of temporal convolutions.\nOur findings suggest that C3T has significant potential for developing\ngeneralizable models for time-series sensor data, opening new avenues for\nmulti-modal learning in various applications.\n","authors":["Abhi Kamboj","Anh Duy Nguyen","Minh Do"],"pdf_url":"https://arxiv.org/pdf/2407.16803v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02397v2","updated":"2024-11-07T17:06:32Z","published":"2024-11-04T18:59:44Z","title":"Adaptive Caching for Faster Video Generation with Diffusion Transformers","summary":"  Generating temporally-consistent high-fidelity videos can be computationally\nexpensive, especially over longer temporal spans. More-recent Diffusion\nTransformers (DiTs) -- despite making significant headway in this context --\nhave only heightened such challenges as they rely on larger models and heavier\nattention mechanisms, resulting in slower inference speeds. In this paper, we\nintroduce a training-free method to accelerate video DiTs, termed Adaptive\nCaching (AdaCache), which is motivated by the fact that \"not all videos are\ncreated equal\": meaning, some videos require fewer denoising steps to attain a\nreasonable quality than others. Building on this, we not only cache\ncomputations through the diffusion process, but also devise a caching schedule\ntailored to each video generation, maximizing the quality-latency trade-off. We\nfurther introduce a Motion Regularization (MoReg) scheme to utilize video\ninformation within AdaCache, essentially controlling the compute allocation\nbased on motion content. Altogether, our plug-and-play contributions grant\nsignificant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video\ngeneration) without sacrificing the generation quality, across multiple video\nDiT baselines.\n","authors":["Kumara Kahatapitiya","Haozhe Liu","Sen He","Ding Liu","Menglin Jia","Chenyang Zhang","Michael S. Ryoo","Tian Xie"],"pdf_url":"https://arxiv.org/pdf/2411.02397v2.pdf","comment":"Project-page is available at https://adacache-dit.github.io"},{"id":"http://arxiv.org/abs/2411.04865v1","updated":"2024-11-07T16:58:18Z","published":"2024-11-07T16:58:18Z","title":"ZAHA: Introducing the Level of Facade Generalization and the Large-Scale\n  Point Cloud Facade Semantic Segmentation Benchmark Dataset","summary":"  Facade semantic segmentation is a long-standing challenge in photogrammetry\nand computer vision. Although the last decades have witnessed the influx of\nfacade segmentation methods, there is a lack of comprehensive facade classes\nand data covering the architectural variability. In ZAHA, we introduce Level of\nFacade Generalization (LoFG), novel hierarchical facade classes designed based\non international urban modeling standards, ensuring compatibility with\nreal-world challenging classes and uniform methods' comparison. Realizing the\nLoFG, we present to date the largest semantic 3D facade segmentation dataset,\nproviding 601 million annotated points at five and 15 classes of LoFG2 and\nLoFG3, respectively. Moreover, we analyze the performance of baseline semantic\nsegmentation methods on our introduced LoFG classes and data, complementing it\nwith a discussion on the unresolved challenges for facade segmentation. We\nfirmly believe that ZAHA shall facilitate further development of 3D facade\nsemantic segmentation methods, enabling robust segmentation indispensable in\ncreating urban digital twins.\n","authors":["Olaf Wysocki","Yue Tan","Thomas Froech","Yan Xia","Magdalena Wysocki","Ludwig Hoegner","Daniel Cremers","Christoph Holst"],"pdf_url":"https://arxiv.org/pdf/2411.04865v1.pdf","comment":"Accepted to WACV 2025 (IEEE/CVF Winter Conference on Applications of\n  Computer Vision (WACV))"},{"id":"http://arxiv.org/abs/2411.04859v1","updated":"2024-11-07T16:49:25Z","published":"2024-11-07T16:49:25Z","title":"A multi-purpose automatic editing system based on lecture semantics for\n  remote education","summary":"  Remote teaching has become popular recently due to its convenience and\nsafety, especially under extreme circumstances like a pandemic. However, online\nstudents usually have a poor experience since the information acquired from the\nviews provided by the broadcast platforms is limited. One potential solution is\nto show more camera views simultaneously, but it is technically challenging and\ndistracting for the viewers. Therefore, an automatic multi-camera\ndirecting/editing system, which aims at selecting the most concerned view at\neach time instance to guide the attention of online students, is in urgent\ndemand. However, existing systems mostly make simple assumptions and focus on\ntracking the position of the speaker instead of the real lecture semantics, and\ntherefore have limited capacities to deliver optimal information flow. To this\nend, this paper proposes an automatic multi-purpose editing system based on the\nlecture semantics, which can both direct the multiple video streams for\nreal-time broadcasting and edit the optimal video offline for review purposes.\nOur system directs the views by semantically analyzing the class events while\nfollowing the professional directing rules, mimicking a human director to\ncapture the regions of interest from the viewpoint of the onsite students. We\nconduct both qualitative and quantitative analyses to verify the effectiveness\nof the proposed system and its components.\n","authors":["Panwen Hu","Rui Huang"],"pdf_url":"https://arxiv.org/pdf/2411.04859v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04844v1","updated":"2024-11-07T16:32:29Z","published":"2024-11-07T16:32:29Z","title":"Differentiable Gaussian Representation for Incomplete CT Reconstruction","summary":"  Incomplete Computed Tomography (CT) benefits patients by reducing radiation\nexposure. However, reconstructing high-fidelity images from limited views or\nangles remains challenging due to the ill-posed nature of the problem. Deep\nLearning Reconstruction (DLR) methods have shown promise in enhancing image\nquality, but the paradox between training data diversity and high\ngeneralization ability remains unsolved. In this paper, we propose a novel\nGaussian Representation for Incomplete CT Reconstruction (GRCT) without the\nusage of any neural networks or full-dose CT data. Specifically, we model the\n3D volume as a set of learnable Gaussians, which are optimized directly from\nthe incomplete sinogram. Our method can be applied to multiple views and angles\nwithout changing the architecture. Additionally, we propose a differentiable\nFast CT Reconstruction method for efficient clinical usage. Extensive\nexperiments on multiple datasets and settings demonstrate significant\nimprovements in reconstruction quality metrics and high efficiency. We plan to\nrelease our code as open-source.\n","authors":["Shaokai Wu","Yuxiang Lu","Wei Ji","Suizhi Huang","Fengyu Yang","Shalayiding Sirejiding","Qichen He","Jing Tong","Yanbiao Ji","Yue Ding","Hongtao Lu"],"pdf_url":"https://arxiv.org/pdf/2411.04844v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04826v1","updated":"2024-11-07T16:07:00Z","published":"2024-11-07T16:07:00Z","title":"D$^3$epth: Self-Supervised Depth Estimation with Dynamic Mask in Dynamic\n  Scenes","summary":"  Depth estimation is a crucial technology in robotics. Recently,\nself-supervised depth estimation methods have demonstrated great potential as\nthey can efficiently leverage large amounts of unlabelled real-world data.\nHowever, most existing methods are designed under the assumption of static\nscenes, which hinders their adaptability in dynamic environments. To address\nthis issue, we present D$^3$epth, a novel method for self-supervised depth\nestimation in dynamic scenes. It tackles the challenge of dynamic objects from\ntwo key perspectives. First, within the self-supervised framework, we design a\nreprojection constraint to identify regions likely to contain dynamic objects,\nallowing the construction of a dynamic mask that mitigates their impact at the\nloss level. Second, for multi-frame depth estimation, we introduce a cost\nvolume auto-masking strategy that leverages adjacent frames to identify regions\nassociated with dynamic objects and generate corresponding masks. This provides\nguidance for subsequent processes. Furthermore, we propose a spectral entropy\nuncertainty module that incorporates spectral entropy to guide uncertainty\nestimation during depth fusion, effectively addressing issues arising from cost\nvolume computation in dynamic environments. Extensive experiments on KITTI and\nCityscapes datasets demonstrate that the proposed method consistently\noutperforms existing self-supervised monocular depth estimation baselines. Code\nis available at \\url{https://github.com/Csyunling/D3epth}.\n","authors":["Siyu Chen","Hong Liu","Wenhao Li","Ying Zhu","Guoquan Wang","Jianbing Wu"],"pdf_url":"https://arxiv.org/pdf/2411.04826v1.pdf","comment":"Open sourced"},{"id":"http://arxiv.org/abs/2411.04821v1","updated":"2024-11-07T15:58:17Z","published":"2024-11-07T15:58:17Z","title":"End-to-end Inception-Unet based Generative Adversarial Networks for Snow\n  and Rain Removals","summary":"  The superior performance introduced by deep learning approaches in removing\natmospheric particles such as snow and rain from a single image; favors their\nusage over classical ones. However, deep learning-based approaches still suffer\nfrom challenges related to the particle appearance characteristics such as\nsize, type, and transparency. Furthermore, due to the unique characteristics of\nrain and snow particles, single network based deep learning approaches struggle\nin handling both degradation scenarios simultaneously. In this paper, a global\nframework that consists of two Generative Adversarial Networks (GANs) is\nproposed where each handles the removal of each particle individually. The\narchitectures of both desnowing and deraining GANs introduce the integration of\na feature extraction phase with the classical U-net generator network which in\nturn enhances the removal performance in the presence of severe variations in\nsize and appearance. Furthermore, a realistic dataset that contains pairs of\nsnowy images next to their groundtruth images estimated using a low-rank\napproximation approach; is presented. The experiments show that the proposed\ndesnowing and deraining approaches achieve significant improvements in\ncomparison to the state-of-the-art approaches when tested on both synthetic and\nrealistic datasets.\n","authors":["Ibrahim Kajo","Mohamed Kas","Yassine Ruichek"],"pdf_url":"https://arxiv.org/pdf/2411.04821v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.03794v2","updated":"2024-11-07T15:56:00Z","published":"2024-07-04T09:57:44Z","title":"CardioSpectrum: Comprehensive Myocardium Motion Analysis with 3D Deep\n  Learning and Geometric Insights","summary":"  The ability to map left ventricle (LV) myocardial motion using computed\ntomography angiography (CTA) is essential to diagnosing cardiovascular\nconditions and guiding interventional procedures. Due to their inherent\nlocality, conventional neural networks typically have difficulty predicting\nsubtle tangential movements, which considerably lessens the level of precision\nat which myocardium three-dimensional (3D) mapping can be performed. Using 3D\noptical flow techniques and Functional Maps (FMs), we present a comprehensive\napproach to address this problem. FMs are known for their capacity to capture\nglobal geometric features, thus providing a fuller understanding of 3D\ngeometry. As an alternative to traditional segmentation-based priors, we employ\nsurface-based two-dimensional (2D) constraints derived from spectral\ncorrespondence methods. Our 3D deep learning architecture, based on the ARFlow\nmodel, is optimized to handle complex 3D motion analysis tasks. By\nincorporating FMs, we can capture the subtle tangential movements of the\nmyocardium surface precisely, hence significantly improving the accuracy of 3D\nmapping of the myocardium. The experimental results confirm the effectiveness\nof this method in enhancing myocardium motion analysis. This approach can\ncontribute to improving cardiovascular diagnosis and treatment. Our code and\nadditional resources are available at:\nhttps://shaharzuler.github.io/CardioSpectrumPage\n","authors":["Shahar Zuler","Shai Tejman-Yarden","Dan Raviv"],"pdf_url":"https://arxiv.org/pdf/2407.03794v2.pdf","comment":"This paper has been early accepted to MICCAI 2024, LNCS 15005,\n  Springer, 2024"},{"id":"http://arxiv.org/abs/2411.04810v1","updated":"2024-11-07T15:47:07Z","published":"2024-11-07T15:47:07Z","title":"GANESH: Generalizable NeRF for Lensless Imaging","summary":"  Lensless imaging offers a significant opportunity to develop ultra-compact\ncameras by removing the conventional bulky lens system. However, without a\nfocusing element, the sensor's output is no longer a direct image but a complex\nmultiplexed scene representation. Traditional methods have attempted to address\nthis challenge by employing learnable inversions and refinement models, but\nthese methods are primarily designed for 2D reconstruction and do not\ngeneralize well to 3D reconstruction. We introduce GANESH, a novel framework\ndesigned to enable simultaneous refinement and novel view synthesis from\nmulti-view lensless images. Unlike existing methods that require scene-specific\ntraining, our approach supports on-the-fly inference without retraining on each\nscene. Moreover, our framework allows us to tune our model to specific scenes,\nenhancing the rendering and refinement quality. To facilitate research in this\narea, we also present the first multi-view lensless dataset, LenslessScenes.\nExtensive experiments demonstrate that our method outperforms current\napproaches in reconstruction accuracy and refinement quality. Code and video\nresults are available at https://rakesh-123-cryp.github.io/Rakesh.github.io/\n","authors":["Rakesh Raj Madavan","Akshat Kaimal","Badhrinarayanan K V","Vinayak Gupta","Rohit Choudhary","Chandrakala Shanmuganathan","Kaushik Mitra"],"pdf_url":"https://arxiv.org/pdf/2411.04810v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01031v2","updated":"2024-11-07T15:41:48Z","published":"2024-10-01T19:45:01Z","title":"Pediatric Wrist Fracture Detection Using Feature Context Excitation\n  Modules in X-ray Images","summary":"  Children often suffer wrist trauma in daily life, while they usually need\nradiologists to analyze and interpret X-ray images before surgical treatment by\nsurgeons. The development of deep learning has enabled neural networks to serve\nas computer-assisted diagnosis (CAD) tools to help doctors and experts in\nmedical image diagnostics. Since YOLOv8 model has obtained the satisfactory\nsuccess in object detection tasks, it has been applied to various fracture\ndetection. This work introduces four variants of Feature Contexts\nExcitation-YOLOv8 (FCE-YOLOv8) model, each incorporating a different FCE module\n(i.e., modules of Squeeze-and-Excitation (SE), Global Context (GC),\nGather-Excite (GE), and Gaussian Context Transformer (GCT)) to enhance the\nmodel performance. Experimental results on GRAZPEDWRI-DX dataset demonstrate\nthat our proposed YOLOv8+GC-M3 model improves the mAP@50 value from 65.78% to\n66.32%, outperforming the state-of-the-art (SOTA) model while reducing\ninference time. Furthermore, our proposed YOLOv8+SE-M3 model achieves the\nhighest mAP@50 value of 67.07%, exceeding the SOTA performance. The\nimplementation of this work is available at\nhttps://github.com/RuiyangJu/FCE-YOLOv8.\n","authors":["Rui-Yang Ju","Chun-Tse Chien","Enkaer Xieerke","Jen-Shiun Chiang"],"pdf_url":"https://arxiv.org/pdf/2410.01031v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2407.03163"},{"id":"http://arxiv.org/abs/2411.03225v2","updated":"2024-11-07T15:41:20Z","published":"2024-11-05T16:15:33Z","title":"Knowledge Graphs of Driving Scenes to Empower the Emerging Capabilities\n  of Neurosymbolic AI","summary":"  In the era of Generative AI, Neurosymbolic AI is emerging as a powerful\napproach for tasks spanning from perception to cognition. The use of\nNeurosymbolic AI has been shown to achieve enhanced capabilities, including\nimproved grounding, alignment, explainability, and reliability. However, due to\nits nascent stage, there is a lack of widely available real-world benchmark\ndatasets tailored to Neurosymbolic AI tasks. To address this gap and support\nthe evaluation of current and future methods, we introduce DSceneKG -- a suite\nof knowledge graphs of driving scenes built from real-world, high-quality\nscenes from multiple open autonomous driving datasets. In this article, we\ndetail the construction process of DSceneKG and highlight its application in\nseven different tasks. DSceneKG is publicly accessible at:\nhttps://github.com/ruwantw/DSceneKG\n","authors":["Ruwan Wickramarachchi","Cory Henson","Amit Sheth"],"pdf_url":"https://arxiv.org/pdf/2411.03225v2.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2310.13040v2","updated":"2024-11-07T15:40:09Z","published":"2023-10-19T17:59:12Z","title":"Interpreting CLIP: Insights on the Robustness to ImageNet Distribution\n  Shifts","summary":"  What distinguishes robust models from non-robust ones? While for ImageNet\ndistribution shifts it has been shown that such differences in robustness can\nbe traced back predominantly to differences in training data, so far it is not\nknown what that translates to in terms of what the model has learned. In this\nwork, we bridge this gap by probing the representation spaces of 16 robust\nzero-shot CLIP vision encoders with various backbones (ResNets and ViTs) and\npretraining sets (OpenAI, LAION-400M, LAION-2B, YFCC15M, CC12M and {DataComp}),\nand comparing them to the representation spaces of less robust models with\nidentical backbones, but different (pre)training sets or objectives (CLIP\npretraining on ImageNet-Captions, and supervised training or finetuning on\nImageNet).Through this analysis, we generate three novel insights. Firstly, we\ndetect the presence of outlier features in robust zero-shot CLIP vision\nencoders, which to the best of our knowledge is the first time these are\nobserved in non-language and non-transformer models. Secondly, we find the\nexistence of outlier features to be an indication of ImageNet shift robustness\nin models, since we only find them in robust models in our analysis. Lastly, we\nalso investigate the number of unique encoded concepts in the representation\nspace and find zero-shot CLIP models to encode a higher number of unique\nconcepts in their representation space. However, we do not find this to be an\nindicator of ImageNet shift robustness and hypothesize that it is rather\nrelated to the language supervision. Since the presence of outlier features can\nbe detected without access to any data from shifted datasets, we believe that\nthey could be a useful tool for practitioners to get a feeling for the\ndistribution shift robustness of a pretrained model during deployment.\n","authors":["Jonathan Crabbé","Pau Rodríguez","Vaishaal Shankar","Luca Zappella","Arno Blaas"],"pdf_url":"https://arxiv.org/pdf/2310.13040v2.pdf","comment":"Published in TMLR"},{"id":"http://arxiv.org/abs/2411.04796v1","updated":"2024-11-07T15:36:49Z","published":"2024-11-07T15:36:49Z","title":"MPVO: Motion-Prior based Visual Odometry for PointGoal Navigation","summary":"  Visual odometry (VO) is essential for enabling accurate point-goal navigation\nof embodied agents in indoor environments where GPS and compass sensors are\nunreliable and inaccurate. However, traditional VO methods face challenges in\nwide-baseline scenarios, where fast robot motions and low frames per second\n(FPS) during inference hinder their performance, leading to drift and\ncatastrophic failures in point-goal navigation. Recent deep-learned VO methods\nshow robust performance but suffer from sample inefficiency during training;\nhence, they require huge datasets and compute resources. So, we propose a\nrobust and sample-efficient VO pipeline based on motion priors available while\nan agent is navigating an environment. It consists of a training-free\naction-prior based geometric VO module that estimates a coarse relative pose\nwhich is further consumed as a motion prior by a deep-learned VO model, which\nfinally produces a fine relative pose to be used by the navigation policy. This\nstrategy helps our pipeline achieve up to 2x sample efficiency during training\nand demonstrates superior accuracy and robustness in point-goal navigation\ntasks compared to state-of-the-art VO method(s). Realistic indoor environments\nof the Gibson dataset is used in the AI-Habitat simulator to evaluate the\nproposed approach using navigation metrics (like success/SPL) and pose metrics\n(like RPE/ATE). We hope this method further opens a direction of work where\nmotion priors from various sources can be utilized to improve VO estimates and\nachieve better results in embodied navigation tasks.\n","authors":["Sayan Paul","Ruddra dev Roychoudhury","Brojeshwar Bhowmick"],"pdf_url":"https://arxiv.org/pdf/2411.04796v1.pdf","comment":"Accepted in 50SFM Workshop of the 18th European Conference on\n  Computer Vision (ECCV) 2024"},{"id":"http://arxiv.org/abs/2410.16261v3","updated":"2024-11-07T15:35:52Z","published":"2024-10-21T17:58:20Z","title":"Mini-InternVL: A Flexible-Transfer Pocket Multimodal Model with 5%\n  Parameters and 90% Performance","summary":"  Multimodal large language models (MLLMs) have demonstrated impressive\nperformance in vision-language tasks across a broad spectrum of domains.\nHowever, the large model scale and associated high computational costs pose\nsignificant challenges for training and deploying MLLMs on consumer-grade GPUs\nor edge devices, thereby hindering their widespread application. In this work,\nwe introduce Mini-InternVL, a series of MLLMs with parameters ranging from 1B\nto 4B, which achieves 90% of the performance with only 5% of the parameters.\nThis significant improvement in efficiency and effectiveness makes our models\nmore accessible and applicable in various real-world scenarios. To further\npromote the adoption of our models, we develop a unified adaptation framework\nfor Mini-InternVL, which enables our models to transfer and outperform\nspecialized models in downstream tasks, including autonomous driving, medical\nimages, and remote sensing. We believe that our study can provide valuable\ninsights and resources to advance the development of efficient and effective\nMLLMs. Code is available at https://github.com/OpenGVLab/InternVL.\n","authors":["Zhangwei Gao","Zhe Chen","Erfei Cui","Yiming Ren","Weiyun Wang","Jinguo Zhu","Hao Tian","Shenglong Ye","Junjun He","Xizhou Zhu","Lewei Lu","Tong Lu","Yu Qiao","Jifeng Dai","Wenhai Wang"],"pdf_url":"https://arxiv.org/pdf/2410.16261v3.pdf","comment":"Technical report"},{"id":"http://arxiv.org/abs/2406.10395v3","updated":"2024-11-07T15:28:50Z","published":"2024-06-14T19:49:45Z","title":"BrainSegFounder: Towards 3D Foundation Models for Neuroimage\n  Segmentation","summary":"  The burgeoning field of brain health research increasingly leverages\nartificial intelligence (AI) to interpret and analyze neurological data. This\nstudy introduces a novel approach towards the creation of medical foundation\nmodels by integrating a large-scale multi-modal magnetic resonance imaging\n(MRI) dataset derived from 41,400 participants in its own. Our method involves\na novel two-stage pretraining approach using vision transformers. The first\nstage is dedicated to encoding anatomical structures in generally healthy\nbrains, identifying key features such as shapes and sizes of different brain\nregions. The second stage concentrates on spatial information, encompassing\naspects like location and the relative positioning of brain structures. We\nrigorously evaluate our model, BrainFounder, using the Brain Tumor Segmentation\n(BraTS) challenge and Anatomical Tracings of Lesions After Stroke v2.0 (ATLAS\nv2.0) datasets. BrainFounder demonstrates a significant performance gain,\nsurpassing the achievements of the previous winning solutions using fully\nsupervised learning. Our findings underscore the impact of scaling up both the\ncomplexity of the model and the volume of unlabeled training data derived from\ngenerally healthy brains, which enhances the accuracy and predictive\ncapabilities of the model in complex neuroimaging tasks with MRI. The\nimplications of this research provide transformative insights and practical\napplications in healthcare and make substantial steps towards the creation of\nfoundation models for Medical AI. Our pretrained models and training code can\nbe found at https://github.com/lab-smile/GatorBrain.\n","authors":["Joseph Cox","Peng Liu","Skylar E. Stolte","Yunchao Yang","Kang Liu","Kyle B. See","Huiwen Ju","Ruogu Fang"],"pdf_url":"https://arxiv.org/pdf/2406.10395v3.pdf","comment":"19 pages, 5 figures, to be published in Medical Image Analysis"},{"id":"http://arxiv.org/abs/2411.04782v1","updated":"2024-11-07T15:22:32Z","published":"2024-11-07T15:22:32Z","title":"An Effective Pipeline for Whole-Slide Image Glomerulus Segmentation","summary":"  Whole-slide images (WSI) glomerulus segmentation is essential for accurately\ndiagnosing kidney diseases. In this work, we propose a practical pipeline for\nglomerulus segmentation that effectively enhances both patch-level and\nWSI-level segmentation tasks. Our approach leverages stitching on overlapping\npatches, increasing the detection coverage, especially when glomeruli are\nlocated near patch image borders. In addition, we conduct comprehensive\nevaluations from different segmentation models across two large and diverse\ndatasets with over 30K glomerulus annotations. Experimental results demonstrate\nthat models using our pipeline outperform the previous state-of-the-art method,\nachieving superior results across both datasets and setting a new benchmark for\nglomerulus segmentation in WSIs. The code and pre-trained models are available\nat https://github.com/huuquan1994/wsi_glomerulus_seg.\n","authors":["Quan Huu Cap"],"pdf_url":"https://arxiv.org/pdf/2411.04782v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03286v2","updated":"2024-11-07T15:07:59Z","published":"2024-11-05T17:35:41Z","title":"DiT4Edit: Diffusion Transformer for Image Editing","summary":"  Despite recent advances in UNet-based image editing, methods for shape-aware\nobject editing in high-resolution images are still lacking. Compared to UNet,\nDiffusion Transformers (DiT) demonstrate superior capabilities to effectively\ncapture the long-range dependencies among patches, leading to higher-quality\nimage generation. In this paper, we propose DiT4Edit, the first Diffusion\nTransformer-based image editing framework. Specifically, DiT4Edit uses the\nDPM-Solver inversion algorithm to obtain the inverted latents, reducing the\nnumber of steps compared to the DDIM inversion algorithm commonly used in\nUNet-based frameworks. Additionally, we design unified attention control and\npatches merging, tailored for transformer computation streams. This integration\nallows our framework to generate higher-quality edited images faster. Our\ndesign leverages the advantages of DiT, enabling it to surpass UNet structures\nin image editing, especially in high-resolution and arbitrary-size images.\nExtensive experiments demonstrate the strong performance of DiT4Edit across\nvarious editing scenarios, highlighting the potential of Diffusion Transformers\nin supporting image editing.\n","authors":["Kunyu Feng","Yue Ma","Bingyuan Wang","Chenyang Qi","Haozhe Chen","Qifeng Chen","Zeyu Wang"],"pdf_url":"https://arxiv.org/pdf/2411.03286v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08673v2","updated":"2024-11-07T14:49:28Z","published":"2024-10-11T09:59:21Z","title":"SpikeBottleNet: Spike-Driven Feature Compression Architecture for\n  Edge-Cloud Co-Inference","summary":"  Edge-cloud co-inference enables efficient deep neural network (DNN)\ndeployment by splitting the architecture between an edge device and cloud\nserver, crucial for resource-constraint edge devices. This approach requires\nbalancing on-device computations and communication costs, often achieved\nthrough compressed intermediate feature transmission. Conventional DNN\narchitectures require continuous data processing and floating point\nactivations, leading to considerable energy consumption and increased feature\nsizes, thus raising transmission costs. This challenge motivates exploring\nbinary, event-driven activations using spiking neural networks (SNNs), known\nfor their extreme energy efficiency. In this research, we propose\nSpikeBottleNet, a novel architecture for edge-cloud co-inference systems that\nintegrates a spiking neuron model to significantly reduce energy consumption on\nedge devices. A key innovation of our study is an intermediate feature\ncompression technique tailored for SNNs for efficient feature transmission.\nThis technique leverages a split computing approach to strategically place\nencoder-decoder bottleneck units within complex deep architectures like ResNet\nand MobileNet. Experimental results demonstrate that SpikeBottleNet achieves up\nto 256x bit compression in the final convolutional layer of ResNet, with\nminimal accuracy loss (0.16%). Additionally, our approach enhances edge device\nenergy efficiency by up to 144x compared to the baseline BottleNet, making it\nideal for resource-limited edge devices.\n","authors":["Maruf Hassan","Steven Davy"],"pdf_url":"https://arxiv.org/pdf/2410.08673v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04746v1","updated":"2024-11-07T14:29:02Z","published":"2024-11-07T14:29:02Z","title":"Taming Rectified Flow for Inversion and Editing","summary":"  Rectified-flow-based diffusion transformers, such as FLUX and OpenSora, have\ndemonstrated exceptional performance in the field of image and video\ngeneration. Despite their robust generative capabilities, these models often\nsuffer from inaccurate inversion, which could further limit their effectiveness\nin downstream tasks such as image and video editing. To address this issue, we\npropose RF-Solver, a novel training-free sampler that enhances inversion\nprecision by reducing errors in the process of solving rectified flow ODEs.\nSpecifically, we derive the exact formulation of the rectified flow ODE and\nperform a high-order Taylor expansion to estimate its nonlinear components,\nsignificantly decreasing the approximation error at each timestep. Building\nupon RF-Solver, we further design RF-Edit, which comprises specialized\nsub-modules for image and video editing. By sharing self-attention layer\nfeatures during the editing process, RF-Edit effectively preserves the\nstructural information of the source image or video while achieving\nhigh-quality editing results. Our approach is compatible with any pre-trained\nrectified-flow-based models for image and video tasks, requiring no additional\ntraining or optimization. Extensive experiments on text-to-image generation,\nimage & video inversion, and image & video editing demonstrate the robust\nperformance and adaptability of our methods. Code is available at\nhttps://github.com/wangjiangshan0725/RF-Solver-Edit.\n","authors":["Jiangshan Wang","Junfu Pu","Zhongang Qi","Jiayi Guo","Yue Ma","Nisha Huang","Yuxin Chen","Xiu Li","Ying Shan"],"pdf_url":"https://arxiv.org/pdf/2411.04746v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04732v1","updated":"2024-11-07T14:12:00Z","published":"2024-11-07T14:12:00Z","title":"Convolutional Differentiable Logic Gate Networks","summary":"  With the increasing inference cost of machine learning models, there is a\ngrowing interest in models with fast and efficient inference. Recently, an\napproach for learning logic gate networks directly via a differentiable\nrelaxation was proposed. Logic gate networks are faster than conventional\nneural network approaches because their inference only requires logic gate\noperators such as NAND, OR, and XOR, which are the underlying building blocks\nof current hardware and can be efficiently executed. We build on this idea,\nextending it by deep logic gate tree convolutions, logical OR pooling, and\nresidual initializations. This allows scaling logic gate networks up by over\none order of magnitude and utilizing the paradigm of convolution. On CIFAR-10,\nwe achieve an accuracy of 86.29% using only 61 million logic gates, which\nimproves over the SOTA while being 29x smaller.\n","authors":["Felix Petersen","Hilde Kuehne","Christian Borgelt","Julian Welzel","Stefano Ermon"],"pdf_url":"https://arxiv.org/pdf/2411.04732v1.pdf","comment":"Published at NeurIPS 2024 (Oral)"},{"id":"http://arxiv.org/abs/2411.04724v1","updated":"2024-11-07T14:02:41Z","published":"2024-11-07T14:02:41Z","title":"Controlling Human Shape and Pose in Text-to-Image Diffusion Models via\n  Domain Adaptation","summary":"  We present a methodology for conditional control of human shape and pose in\npretrained text-to-image diffusion models using a 3D human parametric model\n(SMPL). Fine-tuning these diffusion models to adhere to new conditions requires\nlarge datasets and high-quality annotations, which can be more cost-effectively\nacquired through synthetic data generation rather than real-world data.\nHowever, the domain gap and low scene diversity of synthetic data can\ncompromise the pretrained model's visual fidelity. We propose a\ndomain-adaptation technique that maintains image quality by isolating\nsynthetically trained conditional information in the classifier-free guidance\nvector and composing it with another control network to adapt the generated\nimages to the input domain. To achieve SMPL control, we fine-tune a\nControlNet-based architecture on the synthetic SURREAL dataset of rendered\nhumans and apply our domain adaptation at generation time. Experiments\ndemonstrate that our model achieves greater shape and pose diversity than the\n2d pose-based ControlNet, while maintaining the visual fidelity and improving\nstability, proving its usefulness for downstream tasks such as human animation.\n","authors":["Benito Buchheim","Max Reimann","Jürgen Döllner"],"pdf_url":"https://arxiv.org/pdf/2411.04724v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.02340v5","updated":"2024-11-07T14:00:08Z","published":"2023-09-05T15:57:23Z","title":"Local Padding in Patch-Based GANs for Seamless Infinite-Sized Texture\n  Synthesis","summary":"  Texture models based on Generative Adversarial Networks (GANs) use\nzero-padding to implicitly encode positional information of the image features.\nHowever, when extending the spatial input to generate images at large sizes,\nzero-padding can often lead to degradation in image quality due to the\nincorrect positional information at the center of the image. Moreover,\nzero-padding can limit the diversity within the generated large images. In this\npaper, we propose a novel approach for generating stochastic texture images at\nlarge arbitrary sizes using GANs based on patch-by-patch generation. Instead of\nzero-padding, the model uses \\textit{local padding} in the generator that\nshares border features between the generated patches; providing positional\ncontext and ensuring consistency at the boundaries. The proposed models are\ntrainable on a single texture image and have a constant GPU scalability with\nrespect to the output image size, and hence can generate images of infinite\nsizes. We show in the experiments that our method has a significant advancement\nbeyond existing GANs-based texture models in terms of the quality and diversity\nof the generated textures. Furthermore, the implementation of local padding in\nthe state-of-the-art super-resolution models effectively eliminates tiling\nartifacts enabling large-scale super-resolution. Our code is available at\n\\url{https://github.com/ai4netzero/Infinite_Texture_GANs}.\n","authors":["Alhasan Abdellatif","Ahmed H. Elsheikh","Hannah P. Menke"],"pdf_url":"https://arxiv.org/pdf/2309.02340v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04717v1","updated":"2024-11-07T13:57:53Z","published":"2024-11-07T13:57:53Z","title":"Subspace-Constrained Quadratic Matrix Factorization: Algorithm and\n  Applications","summary":"  Matrix Factorization has emerged as a widely adopted framework for modeling\ndata exhibiting low-rank structures. To address challenges in manifold\nlearning, this paper presents a subspace-constrained quadratic matrix\nfactorization model. The model is designed to jointly learn key low-dimensional\nstructures, including the tangent space, the normal subspace, and the quadratic\nform that links the tangent space to a low-dimensional representation. We solve\nthe proposed factorization model using an alternating minimization method,\ninvolving an in-depth investigation of nonlinear regression and projection\nsubproblems. Theoretical properties of the quadratic projection problem and\nconvergence characteristics of the alternating strategy are also investigated.\nTo validate our approach, we conduct numerical experiments on synthetic and\nreal-world datasets. Results demonstrate that our model outperforms existing\nmethods, highlighting its robustness and efficacy in capturing core\nlow-dimensional structures.\n","authors":["Zheng Zhai","Xiaohui Li"],"pdf_url":"https://arxiv.org/pdf/2411.04717v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04715v1","updated":"2024-11-07T13:56:13Z","published":"2024-11-07T13:56:13Z","title":"NeuroFly: A framework for whole-brain single neuron reconstruction","summary":"  Neurons, with their elongated, tree-like dendritic and axonal structures,\nenable efficient signal integration and long-range communication across brain\nregions. By reconstructing individual neurons' morphology, we can gain valuable\ninsights into brain connectivity, revealing the structure basis of cognition,\nmovement, and perception. Despite the accumulation of extensive 3D microscopic\nimaging data, progress has been considerably hindered by the absence of\nautomated tools to streamline this process. Here we introduce NeuroFly, a\nvalidated framework for large-scale automatic single neuron reconstruction.\nThis framework breaks down the process into three distinct stages:\nsegmentation, connection, and proofreading. In the segmentation stage, we\nperform automatic segmentation followed by skeletonization to generate\nover-segmented neuronal fragments without branches. During the connection\nstage, we use a 3D image-based path following approach to extend each fragment\nand connect it with other fragments of the same neuron. Finally, human\nannotators are required only to proofread the few unresolved positions. The\nfirst two stages of our process are clearly defined computer vision problems,\nand we have trained robust baseline models to solve them. We validated\nNeuroFly's efficiency using in-house datasets that include a variety of\nchallenging scenarios, such as dense arborizations, weak axons, images with\ncontamination. We will release the datasets along with a suite of visualization\nand annotation tools for better reproducibility. Our goal is to foster\ncollaboration among researchers to address the neuron reconstruction challenge,\nultimately accelerating advancements in neuroscience research. The dataset and\ncode are available at https://github.com/beanli161514/neurofly\n","authors":["Rubin Zhao","Yang Liu","Shiqi Zhang","Zijian Yi","Yanyang Xiao","Fang Xu","Yi Yang","Pencheng Zhou"],"pdf_url":"https://arxiv.org/pdf/2411.04715v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04711v1","updated":"2024-11-07T13:53:13Z","published":"2024-11-07T13:53:13Z","title":"Progressive Multi-Level Alignments for Semi-Supervised Domain Adaptation\n  SAR Target Recognition Using Simulated Data","summary":"  Recently, an intriguing research trend for automatic target recognition (ATR)\nfrom synthetic aperture radar (SAR) imagery has arisen: using simulated data to\ntrain ATR models is a feasible solution to the issue of inadequate measured\ndata. To close the domain gap that exists between the real and simulated data,\nthe unsupervised domain adaptation (UDA) techniques are frequently exploited to\nconstruct ATR models. However, for UDA, the target domain lacks labeled data to\ndirect the model training, posing a great challenge to ATR performance. To\naddress the above problem, a semi-supervised domain adaptation (SSDA) framework\nhas been proposed adopting progressive multi-level alignments for simulated\ndata-aided SAR ATR. First, a progressive wavelet transform data augmentation\n(PWTDA) is presented by analyzing the discrepancies of wavelet decomposition\nsub-bands of two domain images, obtaining the domain-level alignment.\nSpecifically, the domain gap is narrowed by mixing the wavelet transform\nhigh-frequency sub-band components. Second, we develop an asymptotic\ninstance-prototype alignment (AIPA) strategy to push the source domain\ninstances close to the corresponding target prototypes, aiming to achieve\ncategory-level alignment. Moreover, the consistency alignment is implemented by\nexcavating the strong-weak augmentation consistency of both individual samples\nand the multi-sample relationship, enhancing the generalization capability of\nthe model. Extensive experiments on the Synthetic and Measured Paired Labeled\nExperiment (SAMPLE) dataset, indicate that our approach obtains recognition\naccuracies of 99.63% and 98.91% in two common experimental settings with only\none labeled sample per class of the target domain, outperforming the most\nadvanced SSDA techniques.\n","authors":["Xinzheng Zhang","Hui Zhu","Hongqian Zhuang"],"pdf_url":"https://arxiv.org/pdf/2411.04711v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04707v1","updated":"2024-11-07T13:45:23Z","published":"2024-11-07T13:45:23Z","title":"From CNN to ConvRNN: Adapting Visualization Techniques for Time-Series\n  Anomaly Detection","summary":"  Nowadays, neural networks are commonly used to solve various problems.\nUnfortunately, despite their effectiveness, they are often perceived as black\nboxes capable of providing answers without explaining their decisions, which\nraises numerous ethical and legal concerns. Fortunately, the field of\nexplainability helps users understand these results. This aspect of machine\nlearning allows users to grasp the decision-making process of a model and\nverify the relevance of its outcomes. In this article, we focus on the learning\nprocess carried out by a ``time distributed`` convRNN, which performs anomaly\ndetection from video data.\n","authors":["Fabien Poirier"],"pdf_url":"https://arxiv.org/pdf/2411.04707v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04706v1","updated":"2024-11-07T13:45:04Z","published":"2024-11-07T13:45:04Z","title":"ESC-MISR: Enhancing Spatial Correlations for Multi-Image\n  Super-Resolution in Remote Sensing","summary":"  Multi-Image Super-Resolution (MISR) is a crucial yet challenging research\ntask in the remote sensing community. In this paper, we address the challenging\ntask of Multi-Image Super-Resolution in Remote Sensing (MISR-RS), aiming to\ngenerate a High-Resolution (HR) image from multiple Low-Resolution (LR) images\nobtained by satellites. Recently, the weak temporal correlations among LR\nimages have attracted increasing attention in the MISR-RS task. However,\nexisting MISR methods treat the LR images as sequences with strong temporal\ncorrelations, overlooking spatial correlations and imposing temporal\ndependencies. To address this problem, we propose a novel end-to-end framework\nnamed Enhancing Spatial Correlations in MISR (ESC-MISR), which fully exploits\nthe spatial-temporal relations of multiple images for HR image reconstruction.\nSpecifically, we first introduce a novel fusion module named Multi-Image\nSpatial Transformer (MIST), which emphasizes parts with clearer global spatial\nfeatures and enhances the spatial correlations between LR images. Besides, we\nperform a random shuffle strategy for the sequential inputs of LR images to\nattenuate temporal dependencies and capture weak temporal correlations in the\ntraining stage. Compared with the state-of-the-art methods, our ESC-MISR\nachieves 0.70dB and 0.76dB cPSNR improvements on the two bands of the PROBA-V\ndataset respectively, demonstrating the superiority of our method.\n","authors":["Zhihui Zhang","Jinhui Pang","Jianan Li","Xiaoshuai Hao"],"pdf_url":"https://arxiv.org/pdf/2411.04706v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17822v3","updated":"2024-11-07T13:34:24Z","published":"2024-03-26T16:00:31Z","title":"DN-Splatter: Depth and Normal Priors for Gaussian Splatting and Meshing","summary":"  High-fidelity 3D reconstruction of common indoor scenes is crucial for VR and\nAR applications. 3D Gaussian splatting, a novel differentiable rendering\ntechnique, has achieved state-of-the-art novel view synthesis results with high\nrendering speeds and relatively low training times. However, its performance on\nscenes commonly seen in indoor datasets is poor due to the lack of geometric\nconstraints during optimization. In this work, we explore the use of readily\naccessible geometric cues to enhance Gaussian splatting optimization in\nchallenging, ill-posed, and textureless scenes. We extend 3D Gaussian splatting\nwith depth and normal cues to tackle challenging indoor datasets and showcase\ntechniques for efficient mesh extraction. Specifically, we regularize the\noptimization procedure with depth information, enforce local smoothness of\nnearby Gaussians, and use off-the-shelf monocular networks to achieve better\nalignment with the true scene geometry. We propose an adaptive depth loss based\non the gradient of color images, improving depth estimation and novel view\nsynthesis results over various baselines. Our simple yet effective\nregularization technique enables direct mesh extraction from the Gaussian\nrepresentation, yielding more physically accurate reconstructions of indoor\nscenes.\n","authors":["Matias Turkulainen","Xuqian Ren","Iaroslav Melekhov","Otto Seiskari","Esa Rahtu","Juho Kannala"],"pdf_url":"https://arxiv.org/pdf/2403.17822v3.pdf","comment":"To be published in 2025 IEEE/CVF Winter Conference on Applications of\n  Computer Vision (WACV)"},{"id":"http://arxiv.org/abs/2411.04697v1","updated":"2024-11-07T13:31:07Z","published":"2024-11-07T13:31:07Z","title":"Dynamic Brightness Adaptation for Robust Multi-modal Image Fusion","summary":"  Infrared and visible image fusion aim to integrate modality strengths for\nvisually enhanced, informative images. Visible imaging in real-world scenarios\nis susceptible to dynamic environmental brightness fluctuations, leading to\ntexture degradation. Existing fusion methods lack robustness against such\nbrightness perturbations, significantly compromising the visual fidelity of the\nfused imagery. To address this challenge, we propose the Brightness Adaptive\nmultimodal dynamic fusion framework (BA-Fusion), which achieves robust image\nfusion despite dynamic brightness fluctuations. Specifically, we introduce a\nBrightness Adaptive Gate (BAG) module, which is designed to dynamically select\nfeatures from brightness-related channels for normalization, while preserving\nbrightness-independent structural information within the source images.\nFurthermore, we propose a brightness consistency loss function to optimize the\nBAG module. The entire framework is tuned via alternating training strategies.\nExtensive experiments validate that our method surpasses state-of-the-art\nmethods in preserving multi-modal image information and visual fidelity, while\nexhibiting remarkable robustness across varying brightness levels. Our code is\navailable: https://github.com/SunYM2020/BA-Fusion.\n","authors":["Yiming Sun","Bing Cao","Pengfei Zhu","Qinghua Hu"],"pdf_url":"https://arxiv.org/pdf/2411.04697v1.pdf","comment":"Accepted by IJCAI 2024"},{"id":"http://arxiv.org/abs/2411.04693v1","updated":"2024-11-07T13:26:20Z","published":"2024-11-07T13:26:20Z","title":"Reciprocal Point Learning Network with Large Electromagnetic Kernel for\n  SAR Open-Set Recognition","summary":"  The limitations of existing Synthetic Aperture Radar (SAR) Automatic Target\nRecognition (ATR) methods lie in their confinement by the closed-environment\nassumption, hindering their effective and robust handling of unknown target\ncategories in open environments. Open Set Recognition (OSR), a pivotal facet\nfor algorithmic practicality, intends to categorize known classes while\ndenoting unknown ones as \"unknown.\" The chief challenge in OSR involves\nconcurrently mitigating risks associated with generalizing features from a\nrestricted set of known classes to numerous unknown samples and the open space\nexposure to potential unknown data. To enhance open-set SAR classification, a\nmethod called scattering kernel with reciprocal learning network is proposed.\nInitially, a feature learning framework is constructed based on reciprocal\npoint learning (RPL), establishing a bounded space for potential unknown\nclasses. This approach indirectly introduces unknown information into a learner\nconfined to known classes, thereby acquiring more concise and discriminative\nrepresentations. Subsequently, considering the variability in the imaging of\ntargets at different angles and the discreteness of components in SAR images, a\nproposal is made to design convolutional kernels based on large-sized attribute\nscattering center models. This enhances the ability to extract intrinsic\nnon-linear features and specific scattering characteristics in SAR images,\nthereby improving the discriminative features of the model and mitigating the\nimpact of imaging variations on classification performance. Experiments on the\nMSTAR datasets substantiate the superior performance of the proposed approach\ncalled ASC-RPL over mainstream methods.\n","authors":["Xiayang Xiao","Zhuoxuan Li","Ruyi Zhang","Jiacheng Chen","Haipeng Wang"],"pdf_url":"https://arxiv.org/pdf/2411.04693v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04692v1","updated":"2024-11-07T13:25:52Z","published":"2024-11-07T13:25:52Z","title":"Personalized Federated Learning for Cross-view Geo-localization","summary":"  In this paper we propose a methodology combining Federated Learning (FL) with\nCross-view Image Geo-localization (CVGL) techniques. We address the challenges\nof data privacy and heterogeneity in autonomous vehicle environments by\nproposing a personalized Federated Learning scenario that allows selective\nsharing of model parameters. Our method implements a coarse-to-fine approach,\nwhere clients share only the coarse feature extractors while keeping\nfine-grained features specific to local environments. We evaluate our approach\nagainst traditional centralized and single-client training schemes using the\nKITTI dataset combined with satellite imagery. Results demonstrate that our\nfederated CVGL method achieves performance close to centralized training while\nmaintaining data privacy. The proposed partial model sharing strategy shows\ncomparable or slightly better performance than classical FL, offering\nsignificant reduced communication overhead without sacrificing accuracy. Our\nwork contributes to more robust and privacy-preserving localization systems for\nautonomous vehicles operating in diverse environments\n","authors":["Christos Anagnostopoulos","Alexandros Gkillas","Nikos Piperigkos","Aris S. Lalos"],"pdf_url":"https://arxiv.org/pdf/2411.04692v1.pdf","comment":"6 pages, 2 figures, Preprint submitted to the IEEE 26th International\n  Workshop on Multimedia Signal Processing (MMSP)"},{"id":"http://arxiv.org/abs/2411.04682v1","updated":"2024-11-07T13:13:23Z","published":"2024-11-07T13:13:23Z","title":"DNN-based 3D Cloud Retrieval for Variable Solar Illumination and\n  Multiview Spaceborne Imaging","summary":"  Climate studies often rely on remotely sensed images to retrieve\ntwo-dimensional maps of cloud properties. To advance volumetric analysis, we\nfocus on recovering the three-dimensional (3D) heterogeneous extinction\ncoefficient field of shallow clouds using multiview remote sensing data.\nClimate research requires large-scale worldwide statistics. To enable scalable\ndata processing, previous deep neural networks (DNNs) can infer at spaceborne\nremote sensing downlink rates. However, prior methods are limited to a fixed\nsolar illumination direction. In this work, we introduce the first scalable\nDNN-based system for 3D cloud retrieval that accommodates varying camera poses\nand solar directions. By integrating multiview cloud intensity images with\ncamera poses and solar direction data, we achieve greater flexibility in\nrecovery. Training of the DNN is performed by a novel two-stage scheme to\naddress the high number of degrees of freedom in this problem. Our approach\nshows substantial improvements over previous state-of-the-art, particularly in\nhandling variations in the sun's zenith angle.\n","authors":["Tamar Klein","Tom Aizenberg","Roi Ronen"],"pdf_url":"https://arxiv.org/pdf/2411.04682v1.pdf","comment":"4 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.10012v2","updated":"2024-11-07T13:11:02Z","published":"2024-03-15T04:35:25Z","title":"Representing Domain-Mixing Optical Degradation for Real-World\n  Computational Aberration Correction via Vector Quantization","summary":"  Relying on paired synthetic data, existing learning-based Computational\nAberration Correction (CAC) methods are confronted with the intricate and\nmultifaceted synthetic-to-real domain gap, which leads to suboptimal\nperformance in real-world applications. In this paper, in contrast to improving\nthe simulation pipeline, we deliver a novel insight into real-world CAC from\nthe perspective of Unsupervised Domain Adaptation (UDA). By incorporating\nreadily accessible unpaired real-world data into training, we formalize the\nDomain Adaptive CAC (DACAC) task, and then introduce a comprehensive Real-world\naberrated images (Realab) dataset to benchmark it. The setup task presents a\nformidable challenge due to the intricacy of understanding the target optical\ndegradation domain. To this intent, we propose a novel Quantized Domain-Mixing\nRepresentation (QDMR) framework as a potent solution to the issue. Centering\naround representing and quantizing the optical degradation which is consistent\nacross different images, QDMR adapts the CAC model to the target domain from\nthree key aspects: (1) reconstructing aberrated images of both domains by a\nVQGAN to learn a Domain-Mixing Codebook (DMC) characterizing the optical\ndegradation; (2) modulating the deep features in CAC model with DMC to transfer\nthe target domain knowledge; and (3) leveraging the trained VQGAN to generate\npseudo target aberrated images from the source ones for convincing target\ndomain supervision. Extensive experiments on both synthetic and real-world\nbenchmarks reveal that the models with QDMR consistently surpass the\ncompetitive methods in mitigating the synthetic-to-real gap, which produces\nvisually pleasant real-world CAC results with fewer artifacts. Codes and\ndatasets are made publicly available at https://github.com/zju-jiangqi/QDMR.\n","authors":["Qi Jiang","Zhonghua Yi","Shaohua Gao","Yao Gao","Xiaolong Qian","Hao Shi","Lei Sun","JinXing Niu","Kaiwei Wang","Kailun Yang","Jian Bai"],"pdf_url":"https://arxiv.org/pdf/2403.10012v2.pdf","comment":"Accepted to Optics & Laser Technology. Codes and datasets are made\n  publicly available at https://github.com/zju-jiangqi/QDMR"},{"id":"http://arxiv.org/abs/2411.04679v1","updated":"2024-11-07T13:08:04Z","published":"2024-11-07T13:08:04Z","title":"CaPo: Cooperative Plan Optimization for Efficient Embodied Multi-Agent\n  Cooperation","summary":"  In this work, we address the cooperation problem among large language model\n(LLM) based embodied agents, where agents must cooperate to achieve a common\ngoal. Previous methods often execute actions extemporaneously and incoherently,\nwithout long-term strategic and cooperative planning, leading to redundant\nsteps, failures, and even serious repercussions in complex tasks like\nsearch-and-rescue missions where discussion and cooperative plan are crucial.\nTo solve this issue, we propose Cooperative Plan Optimization (CaPo) to enhance\nthe cooperation efficiency of LLM-based embodied agents. Inspired by human\ncooperation schemes, CaPo improves cooperation efficiency with two phases: 1)\nmeta-plan generation, and 2) progress-adaptive meta-plan and execution. In the\nfirst phase, all agents analyze the task, discuss, and cooperatively create a\nmeta-plan that decomposes the task into subtasks with detailed steps, ensuring\na long-term strategic and coherent plan for efficient coordination. In the\nsecond phase, agents execute tasks according to the meta-plan and dynamically\nadjust it based on their latest progress (e.g., discovering a target object)\nthrough multi-turn discussions. This progress-based adaptation eliminates\nredundant actions, improving the overall cooperation efficiency of agents.\nExperimental results on the ThreeDworld Multi-Agent Transport and Communicative\nWatch-And-Help tasks demonstrate that CaPo achieves much higher task completion\nrate and efficiency compared with state-of-the-arts.\n","authors":["Jie Liu","Pan Zhou","Yingjun Du","Ah-Hwee Tan","Cees G. M. Snoek","Jan-Jakob Sonke","Efstratios Gavves"],"pdf_url":"https://arxiv.org/pdf/2411.04679v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2411.04663v1","updated":"2024-11-07T12:48:39Z","published":"2024-11-07T12:48:39Z","title":"Explainable Search and Discovery of Visual Cultural Heritage Collections\n  with Multimodal Large Language Models","summary":"  Many cultural institutions have made large digitized visual collections\navailable online, often under permissible re-use licences. Creating interfaces\nfor exploring and searching these collections is difficult, particularly in the\nabsence of granular metadata. In this paper, we introduce a method for using\nstate-of-the-art multimodal large language models (LLMs) to enable an\nopen-ended, explainable search and discovery interface for visual collections.\nWe show how our approach can create novel clustering and recommendation systems\nthat avoid common pitfalls of methods based directly on visual embeddings. Of\nparticular interest is the ability to offer concrete textual explanations of\neach recommendation without the need to preselect the features of interest.\nTogether, these features can create a digital interface that is more open-ended\nand flexible while also being better suited to addressing privacy and ethical\nconcerns. Through a case study using a collection of documentary photographs,\nwe provide several metrics showing the efficacy and possibilities of our\napproach.\n","authors":["Taylor Arnold","Lauren Tilton"],"pdf_url":"https://arxiv.org/pdf/2411.04663v1.pdf","comment":"16 pages, CHR 2024: Computational Humanities Research Conference,\n  December 4 - 6, 2024, Aarhus University, Denmark"},{"id":"http://arxiv.org/abs/2411.04659v1","updated":"2024-11-07T12:42:48Z","published":"2024-11-07T12:42:48Z","title":"Automated Image Color Mapping for a Historic Photographic Collection","summary":"  In the 1970s, the United States Environmental Protection Agency sponsored\nDocumerica, a large-scale photography initiative to document environmental\nsubjects nation-wide. While over 15,000 digitized public-domain photographs\nfrom the collection are available online, most of the images were scanned from\ndamaged copies of the original prints. We present and evaluate a modified\nhistogram matching technique based on the underlying chemistry of the prints\nfor correcting the damaged images by using training data collected from a small\nset of undamaged prints. The entire set of color-adjusted Documerica images is\nmade available in an open repository.\n","authors":["Taylor Arnold","Lauren Tilton"],"pdf_url":"https://arxiv.org/pdf/2411.04659v1.pdf","comment":"11 pages, CHR 2024: Computational Humanities Research Conference,\n  December 4 - 6, 2024, Aarhus University, Denmark"},{"id":"http://arxiv.org/abs/2411.04656v1","updated":"2024-11-07T12:34:25Z","published":"2024-11-07T12:34:25Z","title":"ICH-SCNet: Intracerebral Hemorrhage Segmentation and Prognosis\n  Classification Network Using CLIP-guided SAM mechanism","summary":"  Intracerebral hemorrhage (ICH) is the most fatal subtype of stroke and is\ncharacterized by a high incidence of disability. Accurate segmentation of the\nICH region and prognosis prediction are critically important for developing and\nrefining treatment plans for post-ICH patients. However, existing approaches\naddress these two tasks independently and predominantly focus on imaging data\nalone, thereby neglecting the intrinsic correlation between the tasks and\nmodalities. This paper introduces a multi-task network, ICH-SCNet, designed for\nboth ICH segmentation and prognosis classification. Specifically, we integrate\na SAM-CLIP cross-modal interaction mechanism that combines medical text and\nsegmentation auxiliary information with neuroimaging data to enhance\ncross-modal feature recognition. Additionally, we develop an effective feature\nfusion module and a multi-task loss function to improve performance further.\nExtensive experiments on an ICH dataset reveal that our approach surpasses\nother state-of-the-art methods. It excels in the overall performance of\nclassification tasks and outperforms competing models in all segmentation task\nmetrics.\n","authors":["Xinlei Yu","Ahmed Elazab","Ruiquan Ge","Hui Jin","Xinchen Jiang","Gangyong Jia","Qing Wu","Qinglei Shi","Changmiao Wang"],"pdf_url":"https://arxiv.org/pdf/2411.04656v1.pdf","comment":"6 pages, 2 figures, 3 tables, published to BIBM 2024"},{"id":"http://arxiv.org/abs/2411.04646v1","updated":"2024-11-07T12:11:11Z","published":"2024-11-07T12:11:11Z","title":"DanceFusion: A Spatio-Temporal Skeleton Diffusion Transformer for\n  Audio-Driven Dance Motion Reconstruction","summary":"  This paper introduces DanceFusion, a novel framework for reconstructing and\ngenerating dance movements synchronized to music, utilizing a Spatio-Temporal\nSkeleton Diffusion Transformer. The framework adeptly handles incomplete and\nnoisy skeletal data common in short-form dance videos on social media platforms\nlike TikTok. DanceFusion incorporates a hierarchical Transformer-based\nVariational Autoencoder (VAE) integrated with a diffusion model, significantly\nenhancing motion realism and accuracy. Our approach introduces sophisticated\nmasking techniques and a unique iterative diffusion process that refines the\nmotion sequences, ensuring high fidelity in both motion generation and\nsynchronization with accompanying audio cues. Comprehensive evaluations\ndemonstrate that DanceFusion surpasses existing methods, providing\nstate-of-the-art performance in generating dynamic, realistic, and\nstylistically diverse dance motions. Potential applications of this framework\nextend to content creation, virtual reality, and interactive entertainment,\npromising substantial advancements in automated dance generation. Visit our\nproject page at https://th-mlab.github.io/DanceFusion/.\n","authors":["Li Zhao","Zhengmin Lu"],"pdf_url":"https://arxiv.org/pdf/2411.04646v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04642v1","updated":"2024-11-07T11:54:01Z","published":"2024-11-07T11:54:01Z","title":"TAP-VL: Text Layout-Aware Pre-training for Enriched Vision-Language\n  Models","summary":"  Vision-Language (VL) models have garnered considerable research interest;\nhowever, they still face challenges in effectively handling text within images.\nTo address this limitation, researchers have developed two approaches. The\nfirst method involves utilizing external Optical Character Recognition (OCR)\ntools to extract textual information from images, which is then prepended to\nother textual inputs. The second strategy focuses on employing extremely\nhigh-resolution images to improve text recognition capabilities. In this paper,\nwe focus on enhancing the first strategy by introducing a novel method, named\nTAP-VL, which treats OCR information as a distinct modality and seamlessly\nintegrates it into any VL model. TAP-VL employs a lightweight transformer-based\nOCR module to receive OCR with layout information, compressing it into a short\nfixed-length sequence for input into the LLM. Initially, we conduct\nmodel-agnostic pretraining of the OCR module on unlabeled documents, followed\nby its integration into any VL architecture through brief fine-tuning.\nExtensive experiments demonstrate consistent performance improvements when\napplying TAP-VL to top-performing VL models, across scene-text and\ndocument-based VL benchmarks.\n","authors":["Jonathan Fhima","Elad Ben Avraham","Oren Nuriel","Yair Kittenplon","Roy Ganz","Aviad Aberdam","Ron Litman"],"pdf_url":"https://arxiv.org/pdf/2411.04642v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04632v1","updated":"2024-11-07T11:35:31Z","published":"2024-11-07T11:35:31Z","title":"Improved Multi-Task Brain Tumour Segmentation with Synthetic Data\n  Augmentation","summary":"  This paper presents the winning solution of task 1 and the third-placed\nsolution of task 3 of the BraTS challenge. The use of automated tools in\nclinical practice has increased due to the development of more and more\nsophisticated and reliable algorithms. However, achieving clinical standards\nand developing tools for real-life scenarios is a major challenge. To this end,\nBraTS has organised tasks to find the most advanced solutions for specific\npurposes. In this paper, we propose the use of synthetic data to train\nstate-of-the-art frameworks in order to improve the segmentation of adult\ngliomas in a post-treatment scenario, and the segmentation of meningioma for\nradiotherapy planning. Our results suggest that the use of synthetic data leads\nto more robust algorithms, although the synthetic data generation pipeline is\nnot directly suited to the meningioma task. The code for these tasks is\navailable at https://github.com/ShadowTwin41/BraTS_2023_2024_solutions.\n","authors":["André Ferreira","Tiago Jesus","Behrus Puladi","Jens Kleesiek","Victor Alves","Jan Egger"],"pdf_url":"https://arxiv.org/pdf/2411.04632v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04630v1","updated":"2024-11-07T11:29:55Z","published":"2024-11-07T11:29:55Z","title":"Brain Tumour Removing and Missing Modality Generation using 3D WDM","summary":"  This paper presents the second-placed solution for task 8 and the\nparticipation solution for task 7 of BraTS 2024. The adoption of automated\nbrain analysis algorithms to support clinical practice is increasing. However,\nmany of these algorithms struggle with the presence of brain lesions or the\nabsence of certain MRI modalities. The alterations in the brain's morphology\nleads to high variability and thus poor performance of predictive models that\nwere trained only on healthy brains. The lack of information that is usually\nprovided by some of the missing MRI modalities also reduces the reliability of\nthe prediction models trained with all modalities. In order to improve the\nperformance of these models, we propose the use of conditional 3D wavelet\ndiffusion models. The wavelet transform enabled full-resolution image training\nand prediction on a GPU with 48 GB VRAM, without patching or downsampling,\npreserving all information for prediction. For the inpainting task of BraTS\n2024, the use of a large and variable number of healthy masks and the stability\nand efficiency of the 3D wavelet diffusion model resulted in 0.007, 22.61 and\n0.842 in the validation set and 0.07 , 22.8 and 0.91 in the testing set (MSE,\nPSNR and SSIM respectively). The code for these tasks is available at\nhttps://github.com/ShadowTwin41/BraTS_2023_2024_solutions.\n","authors":["André Ferreira","Gijs Luijten","Behrus Puladi","Jens Kleesiek","Victor Alves","Jan Egger"],"pdf_url":"https://arxiv.org/pdf/2411.04630v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.09377v2","updated":"2024-11-07T11:21:56Z","published":"2023-06-15T08:18:29Z","title":"Evaluating alignment between humans and neural network representations\n  in image-based learning tasks","summary":"  Humans represent scenes and objects in rich feature spaces, carrying\ninformation that allows us to generalise about category memberships and\nabstract functions with few examples. What determines whether a neural network\nmodel generalises like a human? We tested how well the representations of $86$\npretrained neural network models mapped to human learning trajectories across\ntwo tasks where humans had to learn continuous relationships and categories of\nnatural images. In these tasks, both human participants and neural networks\nsuccessfully identified the relevant stimulus features within a few trials,\ndemonstrating effective generalisation. We found that while training dataset\nsize was a core determinant of alignment with human choices, contrastive\ntraining with multi-modal data (text and imagery) was a common feature of\ncurrently publicly available models that predicted human generalisation.\nIntrinsic dimensionality of representations had different effects on alignment\nfor different model types. Lastly, we tested three sets of human-aligned\nrepresentations and found no consistent improvements in predictive accuracy\ncompared to the baselines. In conclusion, pretrained neural networks can serve\nto extract representations for cognitive models, as they appear to capture some\nfundamental aspects of cognition that are transferable across tasks. Both our\nparadigms and modelling approach offer a novel way to quantify alignment\nbetween neural networks and humans and extend cognitive science into more\nnaturalistic domains.\n","authors":["Can Demircan","Tankred Saanum","Leonardo Pettini","Marcel Binz","Blazej M Baczkowski","Christian F Doeller","Mona M Garvert","Eric Schulz"],"pdf_url":"https://arxiv.org/pdf/2306.09377v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04620v1","updated":"2024-11-07T11:09:29Z","published":"2024-11-07T11:09:29Z","title":"Multi-temporal crack segmentation in concrete structure using deep\n  learning approaches","summary":"  Cracks are among the earliest indicators of deterioration in concrete\nstructures. Early automatic detection of these cracks can significantly extend\nthe lifespan of critical infrastructures, such as bridges, buildings, and\ntunnels, while simultaneously reducing maintenance costs and facilitating\nefficient structural health monitoring. This study investigates whether\nleveraging multi-temporal data for crack segmentation can enhance segmentation\nquality. Therefore, we compare a Swin UNETR trained on multi-temporal data with\na U-Net trained on mono-temporal data to assess the effect of temporal\ninformation compared with conventional single-epoch approaches. To this end, a\nmulti-temporal dataset comprising 1356 images, each with 32 sequential crack\npropagation images, was created. After training the models, experiments were\nconducted to analyze their generalization ability, temporal consistency, and\nsegmentation quality. The multi-temporal approach consistently outperformed its\nmono-temporal counterpart, achieving an IoU of $82.72\\%$ and a F1-score of\n$90.54\\%$, representing a significant improvement over the mono-temporal\nmodel's IoU of $76.69\\%$ and F1-score of $86.18\\%$, despite requiring only half\nof the trainable parameters. The multi-temporal model also displayed a more\nconsistent segmentation quality, with reduced noise and fewer errors. These\nresults suggest that temporal information significantly enhances the\nperformance of segmentation models, offering a promising solution for improved\ncrack detection and the long-term monitoring of concrete structures, even with\nlimited sequential data.\n","authors":["Said Harb","Pedro Achanccaray","Mehdi Maboudi","Markus Gerke"],"pdf_url":"https://arxiv.org/pdf/2411.04620v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02099v2","updated":"2024-11-07T10:53:14Z","published":"2024-11-04T14:08:26Z","title":"Differentially Private Integrated Decision Gradients (IDG-DP) for\n  Radar-based Human Activity Recognition","summary":"  Human motion analysis offers significant potential for healthcare monitoring\nand early detection of diseases. The advent of radar-based sensing systems has\ncaptured the spotlight for they are able to operate without physical contact\nand they can integrate with pre-existing Wi-Fi networks. They are also seen as\nless privacy-invasive compared to camera-based systems. However, recent\nresearch has shown high accuracy in recognizing subjects or gender from radar\ngait patterns, raising privacy concerns. This study addresses these issues by\ninvestigating privacy vulnerabilities in radar-based Human Activity Recognition\n(HAR) systems and proposing a novel method for privacy preservation using\nDifferential Privacy (DP) driven by attributions derived with Integrated\nDecision Gradient (IDG) algorithm. We investigate Black-box Membership\nInference Attack (MIA) Models in HAR settings across various levels of\nattacker-accessible information. We extensively evaluated the effectiveness of\nthe proposed IDG-DP method by designing a CNN-based HAR model and rigorously\nassessing its resilience against MIAs. Experimental results demonstrate the\npotential of IDG-DP in mitigating privacy attacks while maintaining utility\nacross all settings, particularly excelling against label-only and shadow model\nblack-box MIA attacks. This work represents a crucial step towards balancing\nthe need for effective radar-based HAR with robust privacy protection in\nhealthcare environments.\n","authors":["Idris Zakariyya","Linda Tran","Kaushik Bhargav Sivangi","Paul Henderson","Fani Deligianni"],"pdf_url":"https://arxiv.org/pdf/2411.02099v2.pdf","comment":"Accepted at WACV 2025. 12 pages, 7 figures"},{"id":"http://arxiv.org/abs/2411.04612v1","updated":"2024-11-07T10:52:57Z","published":"2024-11-07T10:52:57Z","title":"Population estimation using 3D city modelling and Carto2S datasets -- A\n  case study","summary":"  With the launch of Carto2S series of satellites, high resolution images\n(0.6-1.0 meters) are acquired and available for use. High resolution Digital\nElevation Model (DEM) with better accuracies can be generated using C2S\nmulti-view and multi date datasets. DEMs are further used as an input to derive\nDigital terrain models (DTMs) and to extract accurate heights of the objects\n(building and tree) over the surface of the Earth. Extracted building heights\nare validated with ground control points and can be used for generation of city\nmodelling and resource estimation like population estimation, health planning,\nwater and transport resource estimations. In this study, an attempt is made to\nassess the population of a township using high-resolution Indian remote sensing\nsatellite datasets. We used Carto 2S multi-view data and generated a precise\nDEM and DTM over a city area. Using DEM and DTM datasets, accurate heights of\nthe buildings are extracted which are further validated with ground data.\nAccurate building heights and high resolution imagery are used for generating\naccurate virtual 3D city model and assessing the number of floor and carpet\narea of the houses/ flats/ apartments. Population estimation of the area is\nmade using derived information of no of houses/ flats/ apartments from the\nsatellite datasets. Further, information about number of hospital and schools\naround the residential area is extracted from open street maps (OSM).\nPopulation estimation using satellite data and derived information from OSM\ndatasets can prove to be very good tool for local administrator and decision\nmakers.\n","authors":["Jai G Singla"],"pdf_url":"https://arxiv.org/pdf/2411.04612v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04610v1","updated":"2024-11-07T10:50:39Z","published":"2024-11-07T10:50:39Z","title":"Solar potential analysis over Indian cities using high-resolution\n  satellite imagery and DEM","summary":"  Most of the research work in the solar potential analysis is performed\nutilizing aerial imagery, LiDAR data, and satellite imagery. However, in the\nexisting studies using satellite data, parameters such as trees/ vegetation\nshadow, adjacent higher architectural structures, and eccentric roof structures\nin urban areas were not considered, and relatively coarser-resolution datasets\nwere used for analysis. In this work, we have implemented a novel approach to\nestimate rooftop solar potential using inputs of high-resolution satellite\nimagery (0.5 cm), a digital elevation model (1m), along with ground station\nradiation data. Solar radiation analysis is performed using the diffusion\nproportion and transmissivity ratio derived from the ground station data hosted\nby IMD. It was observed that due to seasonal variations, environmental effects\nand technical reasons such as solar panel structure etc., there can be a\nsignificant loss of electricity generation up to 50%. Based on the results, it\nis also understood that using 1m DEM and 50cm satellite imagery, more authentic\nresults are produced over the urban areas.\n","authors":["Jai Singla"],"pdf_url":"https://arxiv.org/pdf/2411.04610v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04607v1","updated":"2024-11-07T10:46:01Z","published":"2024-11-07T10:46:01Z","title":"Cross- and Intra-image Prototypical Learning for Multi-label Disease\n  Diagnosis and Interpretation","summary":"  Recent advances in prototypical learning have shown remarkable potential to\nprovide useful decision interpretations associating activation maps and\npredictions with class-specific training prototypes. Such prototypical learning\nhas been well-studied for various single-label diseases, but for quite relevant\nand more challenging multi-label diagnosis, where multiple diseases are often\nconcurrent within an image, existing prototypical learning models struggle to\nobtain meaningful activation maps and effective class prototypes due to the\nentanglement of the multiple diseases. In this paper, we present a novel Cross-\nand Intra-image Prototypical Learning (CIPL) framework, for accurate\nmulti-label disease diagnosis and interpretation from medical images. CIPL\ntakes advantage of common cross-image semantics to disentangle the multiple\ndiseases when learning the prototypes, allowing a comprehensive understanding\nof complicated pathological lesions. Furthermore, we propose a new two-level\nalignment-based regularisation strategy that effectively leverages consistent\nintra-image information to enhance interpretation robustness and predictive\nperformance. Extensive experiments show that our CIPL attains the\nstate-of-the-art (SOTA) classification accuracy in two public multi-label\nbenchmarks of disease diagnosis: thoracic radiography and fundus images.\nQuantitative interpretability results show that CIPL also has superiority in\nweakly-supervised thoracic disease localisation over other leading saliency-\nand prototype-based explanation methods.\n","authors":["Chong Wang","Fengbei Liu","Yuanhong Chen","Helen Frazer","Gustavo Carneiro"],"pdf_url":"https://arxiv.org/pdf/2411.04607v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04598v1","updated":"2024-11-07T10:28:49Z","published":"2024-11-07T10:28:49Z","title":"Social EgoMesh Estimation","summary":"  Accurately estimating the 3D pose of the camera wearer in egocentric video\nsequences is crucial to modeling human behavior in virtual and augmented\nreality applications. The task presents unique challenges due to the limited\nvisibility of the user's body caused by the front-facing camera mounted on\ntheir head. Recent research has explored the utilization of the scene and\nego-motion, but it has overlooked humans' interactive nature. We propose a\nnovel framework for Social Egocentric Estimation of body MEshes (SEE-ME). Our\napproach is the first to estimate the wearer's mesh using only a latent\nprobabilistic diffusion model, which we condition on the scene and, for the\nfirst time, on the social wearer-interactee interactions. Our in-depth study\nsheds light on when social interaction matters most for ego-mesh estimation; it\nquantifies the impact of interpersonal distance and gaze direction. Overall,\nSEE-ME surpasses the current best technique, reducing the pose estimation error\n(MPJPE) by 53%. The code is available at https://github.com/L-Scofano/SEEME.\n","authors":["Luca Scofano","Alessio Sampieri","Edoardo De Matteis","Indro Spinelli","Fabio Galasso"],"pdf_url":"https://arxiv.org/pdf/2411.04598v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04596v1","updated":"2024-11-07T10:28:11Z","published":"2024-11-07T10:28:11Z","title":"The Impact of Semi-Supervised Learning on Line Segment Detection","summary":"  In this paper we present a method for line segment detection in images, based\non a semi-supervised framework. Leveraging the use of a consistency loss based\non differently augmented and perturbed unlabeled images with a small amount of\nlabeled data, we show comparable results to fully supervised methods. This\nopens up application scenarios where annotation is difficult or expensive, and\nfor domain specific adaptation of models. We are specifically interested in\nreal-time and online applications, and investigate small and efficient learning\nbackbones. Our method is to our knowledge the first to target line detection\nusing modern state-of-the-art methodologies for semi-supervised learning. We\ntest the method on both standard benchmarks and domain specific scenarios for\nforestry applications, showing the tractability of the proposed method.\n","authors":["Johanna Engman","Karl Åström","Magnus Oskarsson"],"pdf_url":"https://arxiv.org/pdf/2411.04596v1.pdf","comment":"9 pages, 6 figures, 7 tables"},{"id":"http://arxiv.org/abs/2411.04595v1","updated":"2024-11-07T10:26:38Z","published":"2024-11-07T10:26:38Z","title":"TexLiverNet: Leveraging Medical Knowledge and Spatial-Frequency\n  Perception for Enhanced Liver Tumor Segmentation","summary":"  Integrating textual data with imaging in liver tumor segmentation is\nessential for enhancing diagnostic accuracy. However, current multi-modal\nmedical datasets offer only general text annotations, lacking lesion-specific\ndetails critical for extracting nuanced features, especially for fine-grained\nsegmentation of tumor boundaries and small lesions. To address these\nlimitations, we developed datasets with lesion-specific text annotations for\nliver tumors and introduced the TexLiverNet model. TexLiverNet employs an\nagent-based cross-attention module that integrates text features efficiently\nwith visual features, significantly reducing computational costs. Additionally,\nenhanced spatial and adaptive frequency domain perception is proposed to\nprecisely delineate lesion boundaries, reduce background interference, and\nrecover fine details in small lesions. Comprehensive evaluations on public and\nprivate datasets demonstrate that TexLiverNet achieves superior performance\ncompared to current state-of-the-art methods.\n","authors":["Xiaoyan Jiang","Zhi Zhou","Hailing Wang","Guozhong Wang","Zhijun Fang"],"pdf_url":"https://arxiv.org/pdf/2411.04595v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04594v1","updated":"2024-11-07T10:25:20Z","published":"2024-11-07T10:25:20Z","title":"Verification of Neural Networks against Convolutional Perturbations via\n  Parameterised Kernels","summary":"  We develop a method for the efficient verification of neural networks against\nconvolutional perturbations such as blurring or sharpening. To define input\nperturbations we use well-known camera shake, box blur and sharpen kernels. We\ndemonstrate that these kernels can be linearly parameterised in a way that\nallows for a variation of the perturbation strength while preserving desired\nkernel properties. To facilitate their use in neural network verification, we\ndevelop an efficient way of convolving a given input with these parameterised\nkernels. The result of this convolution can be used to encode the perturbation\nin a verification setting by prepending a linear layer to a given network. This\nleads to tight bounds and a high effectiveness in the resulting verification\nstep. We add further precision by employing input splitting as a branch and\nbound strategy. We demonstrate that we are able to verify robustness on a\nnumber of standard benchmarks where the baseline is unable to provide any\nsafety certificates. To the best of our knowledge, this is the first solution\nfor verifying robustness against specific convolutional perturbations such as\ncamera shake.\n","authors":["Benedikt Brückner","Alessio Lomuscio"],"pdf_url":"https://arxiv.org/pdf/2411.04594v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04586v1","updated":"2024-11-07T10:15:25Z","published":"2024-11-07T10:15:25Z","title":"On the Inherent Robustness of One-Stage Object Detection against\n  Out-of-Distribution Data","summary":"  Robustness is a fundamental aspect for developing safe and trustworthy\nmodels, particularly when they are deployed in the open world. In this work we\nanalyze the inherent capability of one-stage object detectors to robustly\noperate in the presence of out-of-distribution (OoD) data. Specifically, we\npropose a novel detection algorithm for detecting unknown objects in image\ndata, which leverages the features extracted by the model from each sample.\nDifferently from other recent approaches in the literature, our proposal does\nnot require retraining the object detector, thereby allowing for the use of\npretrained models. Our proposed OoD detector exploits the application of\nsupervised dimensionality reduction techniques to mitigate the effects of the\ncurse of dimensionality on the features extracted by the model. Furthermore, it\nutilizes high-resolution feature maps to identify potential unknown objects in\nan unsupervised fashion. Our experiments analyze the Pareto trade-off between\nthe performance detecting known and unknown objects resulting from different\nalgorithmic configurations and inference confidence thresholds. We also compare\nthe performance of our proposed algorithm to that of logits-based post-hoc OoD\nmethods, as well as possible fusion strategies. Finally, we discuss on the\ncompetitiveness of all tested methods against state-of-the-art OoD approaches\nfor object detection models over the recently published Unknown Object\nDetection benchmark. The obtained results verify that the performance of\navant-garde post-hoc OoD detectors can be further improved when combined with\nour proposed algorithm.\n","authors":["Aitor Martinez-Seras","Javier Del Ser","Alain Andres","Pablo Garcia-Bringas"],"pdf_url":"https://arxiv.org/pdf/2411.04586v1.pdf","comment":"12 figures, 4 tables, under review"},{"id":"http://arxiv.org/abs/2411.04584v1","updated":"2024-11-07T10:11:37Z","published":"2024-11-07T10:11:37Z","title":"PASSION for Dermatology: Bridging the Diversity Gap with Pigmented Skin\n  Images from Sub-Saharan Africa","summary":"  Africa faces a huge shortage of dermatologists, with less than one per\nmillion people. This is in stark contrast to the high demand for dermatologic\ncare, with 80% of the paediatric population suffering from largely untreated\nskin conditions. The integration of AI into healthcare sparks significant hope\nfor treatment accessibility, especially through the development of AI-supported\nteledermatology. Current AI models are predominantly trained on white-skinned\npatients and do not generalize well enough to pigmented patients. The PASSION\nproject aims to address this issue by collecting images of skin diseases in\nSub-Saharan countries with the aim of open-sourcing this data. This dataset is\nthe first of its kind, consisting of 1,653 patients for a total of 4,901\nimages. The images are representative of telemedicine settings and encompass\nthe most common paediatric conditions: eczema, fungals, scabies, and impetigo.\nWe also provide a baseline machine learning model trained on the dataset and a\ndetailed performance analysis for the subpopulations represented in the\ndataset. The project website can be found at https://passionderm.github.io/.\n","authors":["Philippe Gottfrois","Fabian Gröger","Faly Herizo Andriambololoniaina","Ludovic Amruthalingam","Alvaro Gonzalez-Jimenez","Christophe Hsu","Agnes Kessy","Simone Lionetti","Daudi Mavura","Wingston Ng'ambi","Dingase Faith Ngongonda","Marc Pouly","Mendrika Fifaliana Rakotoarisaona","Fahafahantsoa Rapelanoro Rabenja","Ibrahima Traoré","Alexander A. Navarini"],"pdf_url":"https://arxiv.org/pdf/2411.04584v1.pdf","comment":"MICCAI 2024"},{"id":"http://arxiv.org/abs/2405.09032v4","updated":"2024-11-07T10:06:18Z","published":"2024-05-15T02:03:44Z","title":"ICAL: Implicit Character-Aided Learning for Enhanced Handwritten\n  Mathematical Expression Recognition","summary":"  Significant progress has been made in the field of handwritten mathematical\nexpression recognition, while existing encoder-decoder methods are usually\ndifficult to model global information in $LaTeX$. Therefore, this paper\nintroduces a novel approach, Implicit Character-Aided Learning (ICAL), to mine\nthe global expression information and enhance handwritten mathematical\nexpression recognition. Specifically, we propose the Implicit Character\nConstruction Module (ICCM) to predict implicit character sequences and use a\nFusion Module to merge the outputs of the ICCM and the decoder, thereby\nproducing corrected predictions. By modeling and utilizing implicit character\ninformation, ICAL achieves a more accurate and context-aware interpretation of\nhandwritten mathematical expressions. Experimental results demonstrate that\nICAL notably surpasses the state-of-the-art(SOTA) models, improving the\nexpression recognition rate (ExpRate) by 2.25\\%/1.81\\%/1.39\\% on the CROHME\n2014/2016/2019 datasets respectively, and achieves a remarkable 69.06\\% on the\nchallenging HME100k test set. We make our code available on the GitHub:\nhttps://github.com/qingzhenduyu/ICAL\n","authors":["Jianhua Zhu","Liangcai Gao","Wenqi Zhao"],"pdf_url":"https://arxiv.org/pdf/2405.09032v4.pdf","comment":"ICDAR 2024 Oral Paper"},{"id":"http://arxiv.org/abs/2411.04571v1","updated":"2024-11-07T09:55:36Z","published":"2024-11-07T09:55:36Z","title":"DomainGallery: Few-shot Domain-driven Image Generation by\n  Attribute-centric Finetuning","summary":"  The recent progress in text-to-image models pretrained on large-scale\ndatasets has enabled us to generate various images as long as we provide a text\nprompt describing what we want. Nevertheless, the availability of these models\nis still limited when we expect to generate images that fall into a specific\ndomain either hard to describe or just unseen to the models. In this work, we\npropose DomainGallery, a few-shot domain-driven image generation method which\naims at finetuning pretrained Stable Diffusion on few-shot target datasets in\nan attribute-centric manner. Specifically, DomainGallery features prior\nattribute erasure, attribute disentanglement, regularization and enhancement.\nThese techniques are tailored to few-shot domain-driven generation in order to\nsolve key issues that previous works have failed to settle. Extensive\nexperiments are given to validate the superior performance of DomainGallery on\na variety of domain-driven generation scenarios. Codes are available at\nhttps://github.com/Ldhlwh/DomainGallery.\n","authors":["Yuxuan Duan","Yan Hong","Bo Zhang","Jun Lan","Huijia Zhu","Weiqiang Wang","Jianfu Zhang","Li Niu","Liqing Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.04571v1.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2405.16591v2","updated":"2024-11-07T09:33:40Z","published":"2024-05-26T14:50:40Z","title":"CapS-Adapter: Caption-based MultiModal Adapter in Zero-Shot\n  Classification","summary":"  Recent advances in vision-language foundational models, such as CLIP, have\ndemonstrated significant strides in zero-shot classification. However, the\nextensive parameterization of models like CLIP necessitates a\nresource-intensive fine-tuning process. In response, TIP-Adapter and SuS-X have\nintroduced training-free methods aimed at bolstering the efficacy of downstream\ntasks. While these approaches incorporate support sets to maintain data\ndistribution consistency between knowledge cache and test sets, they often fall\nshort in terms of generalization on the test set, particularly when faced with\ntest data exhibiting substantial distributional variations. In this work, we\npresent CapS-Adapter, an innovative method that employs a caption-based support\nset, effectively harnessing both image and caption features to exceed existing\nstate-of-the-art techniques in training-free scenarios. CapS-Adapter adeptly\nconstructs support sets that closely mirror target distributions, utilizing\ninstance-level distribution features extracted from multimodal large models. By\nleveraging CLIP's single and cross-modal strengths, CapS-Adapter enhances\npredictive accuracy through the use of multimodal support sets. Our method\nachieves outstanding zero-shot classification results across 19 benchmark\ndatasets, improving accuracy by 2.19\\% over the previous leading method. Our\ncontributions are substantiated through extensive validation on multiple\nbenchmark datasets, demonstrating superior performance and robust\ngeneralization capabilities. Our code is made publicly available at\nhttps://github.com/WLuLi/CapS-Adapter.\n","authors":["Qijie Wang","Guandu Liu","Bin Wang"],"pdf_url":"https://arxiv.org/pdf/2405.16591v2.pdf","comment":"ACM Multimedia 2024 Poster"},{"id":"http://arxiv.org/abs/2406.01494v2","updated":"2024-11-07T09:23:34Z","published":"2024-06-03T16:21:29Z","title":"Robust Classification by Coupling Data Mollification with Label\n  Smoothing","summary":"  Introducing training-time augmentations is a key technique to enhance\ngeneralization and prepare deep neural networks against test-time corruptions.\nInspired by the success of generative diffusion models, we propose a novel\napproach of coupling data mollification, in the form of image noising and\nblurring, with label smoothing to align predicted label confidences with image\ndegradation. The method is simple to implement, introduces negligible\noverheads, and can be combined with existing augmentations. We demonstrate\nimproved robustness and uncertainty quantification on the corrupted image\nbenchmarks of the CIFAR and TinyImageNet datasets.\n","authors":["Markus Heinonen","Ba-Hien Tran","Michael Kampffmeyer","Maurizio Filippone"],"pdf_url":"https://arxiv.org/pdf/2406.01494v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2411.04533v1","updated":"2024-11-07T08:43:42Z","published":"2024-11-07T08:43:42Z","title":"Neural Fingerprints for Adversarial Attack Detection","summary":"  Deep learning models for image classification have become standard tools in\nrecent years. A well known vulnerability of these models is their\nsusceptibility to adversarial examples. These are generated by slightly\naltering an image of a certain class in a way that is imperceptible to humans\nbut causes the model to classify it wrongly as another class. Many algorithms\nhave been proposed to address this problem, falling generally into one of two\ncategories: (i) building robust classifiers (ii) directly detecting attacked\nimages. Despite the good performance of these detectors, we argue that in a\nwhite-box setting, where the attacker knows the configuration and weights of\nthe network and the detector, they can overcome the detector by running many\nexamples on a local copy, and sending only those that were not detected to the\nactual model. This problem is common in security applications where even a very\ngood model is not sufficient to ensure safety. In this paper we propose to\novercome this inherent limitation of any static defence with randomization. To\ndo so, one must generate a very large family of detectors with consistent\nperformance, and select one or more of them randomly for each input. For the\nindividual detectors, we suggest the method of neural fingerprints. In the\ntraining phase, for each class we repeatedly sample a tiny random subset of\nneurons from certain layers of the network, and if their average is\nsufficiently different between clean and attacked images of the focal class\nthey are considered a fingerprint and added to the detector bank. During test\ntime, we sample fingerprints from the bank associated with the label predicted\nby the model, and detect attacks using a likelihood ratio test. We evaluate our\ndetectors on ImageNet with different attack methods and model architectures,\nand show near-perfect detection with low rates of false detection.\n","authors":["Haim Fisher","Moni Shahar","Yehezkel S. Resheff"],"pdf_url":"https://arxiv.org/pdf/2411.04533v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2405.14974v2","updated":"2024-11-07T08:41:03Z","published":"2024-05-23T18:21:59Z","title":"LOVA3: Learning to Visual Question Answering, Asking and Assessment","summary":"  Question answering, asking, and assessment are three innate human traits\ncrucial for understanding the world and acquiring knowledge. By enhancing these\ncapabilities, humans can more effectively utilize data, leading to better\ncomprehension and learning outcomes. Current Multimodal Large Language Models\n(MLLMs) primarily focus on question answering, often neglecting the full\npotential of questioning and assessment skills. Inspired by the human learning\nmechanism, we introduce LOVA3, an innovative framework named \"Learning tO\nVisual question Answering, Asking and Assessment,\" designed to equip MLLMs with\nthese additional capabilities. Our approach involves the creation of two\nsupplementary training tasks GenQA and EvalQA, aiming at fostering the skills\nof asking and assessing questions in the context of images. To develop the\nquestioning ability, we compile a comprehensive set of multimodal foundational\ntasks. For assessment, we introduce a new benchmark called EvalQABench,\ncomprising 64,000 training samples (split evenly between positive and negative\nsamples) and 5,000 validation and testing samples. We posit that enhancing\nMLLMs with the capabilities to answer, ask, and assess questions will enhance\ntheir multimodal comprehension, ultimately improving overall performance. To\nvalidate this hypothesis, we train MLLMs using the LOVA3 framework and evaluate\nthem on a range of multimodal datasets and benchmarks. Our results demonstrate\nconsistent performance gains, underscoring the critical role of these\nadditional tasks in fostering comprehensive intelligence in MLLMs. The code is\navailable at https://github.com/showlab/LOVA3.\n","authors":["Henry Hengyuan Zhao","Pan Zhou","Difei Gao","Zechen Bai","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2405.14974v2.pdf","comment":"Accepted by NeurIPS 2024. The code is available at\n  https://github.com/showlab/LOVA3"},{"id":"http://arxiv.org/abs/2409.18694v2","updated":"2024-11-07T08:27:16Z","published":"2024-09-27T12:28:47Z","title":"Learning from Pattern Completion: Self-supervised Controllable\n  Generation","summary":"  The human brain exhibits a strong ability to spontaneously associate\ndifferent visual attributes of the same or similar visual scene, such as\nassociating sketches and graffiti with real-world visual objects, usually\nwithout supervising information. In contrast, in the field of artificial\nintelligence, controllable generation methods like ControlNet heavily rely on\nannotated training datasets such as depth maps, semantic segmentation maps, and\nposes, which limits the method's scalability. Inspired by the neural mechanisms\nthat may contribute to the brain's associative power, specifically the cortical\nmodularization and hippocampal pattern completion, here we propose a\nself-supervised controllable generation (SCG) framework. Firstly, we introduce\nan equivariant constraint to promote inter-module independence and intra-module\ncorrelation in a modular autoencoder network, thereby achieving functional\nspecialization. Subsequently, based on these specialized modules, we employ a\nself-supervised pattern completion approach for controllable generation\ntraining. Experimental results demonstrate that the proposed modular\nautoencoder effectively achieves functional specialization, including the\nmodular processing of color, brightness, and edge detection, and exhibits\nbrain-like features including orientation selectivity, color antagonism, and\ncenter-surround receptive fields. Through self-supervised training, associative\ngeneration capabilities spontaneously emerge in SCG, demonstrating excellent\ngeneralization ability to various tasks such as associative generation on\npainting, sketches, and ancient graffiti. Compared to the previous\nrepresentative method ControlNet, our proposed approach not only demonstrates\nsuperior robustness in more challenging high-noise scenarios but also possesses\nmore promising scalability potential due to its self-supervised manner.Codes\nare released on Github and Gitee.\n","authors":["Zhiqiang Chen","Guofan Fan","Jinying Gao","Lei Ma","Bo Lei","Tiejun Huang","Shan Yu"],"pdf_url":"https://arxiv.org/pdf/2409.18694v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04519v1","updated":"2024-11-07T08:20:29Z","published":"2024-11-07T08:20:29Z","title":"l0-Regularized Sparse Coding-based Interpretable Network for Multi-Modal\n  Image Fusion","summary":"  Multi-modal image fusion (MMIF) enhances the information content of the fused\nimage by combining the unique as well as common features obtained from\ndifferent modality sensor images, improving visualization, object detection,\nand many more tasks. In this work, we introduce an interpretable network for\nthe MMIF task, named FNet, based on an l0-regularized multi-modal convolutional\nsparse coding (MCSC) model. Specifically, for solving the l0-regularized CSC\nproblem, we develop an algorithm unrolling-based l0-regularized sparse coding\n(LZSC) block. Given different modality source images, FNet first separates the\nunique and common features from them using the LZSC block and then these\nfeatures are combined to generate the final fused image. Additionally, we\npropose an l0-regularized MCSC model for the inverse fusion process. Based on\nthis model, we introduce an interpretable inverse fusion network named IFNet,\nwhich is utilized during FNet's training. Extensive experiments show that FNet\nachieves high-quality fusion results across five different MMIF tasks.\nFurthermore, we show that FNet enhances downstream object detection in\nvisible-thermal image pairs. We have also visualized the intermediate results\nof FNet, which demonstrates the good interpretability of our network.\n","authors":["Gargi Panda","Soumitra Kundu","Saumik Bhattacharya","Aurobinda Routray"],"pdf_url":"https://arxiv.org/pdf/2411.04519v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04517v1","updated":"2024-11-07T08:19:39Z","published":"2024-11-07T08:19:39Z","title":"Continuous Sign Language Recognition System using Deep Learning with\n  MediaPipe Holistic","summary":"  Sign languages are the language of hearing-impaired people who use visuals\nlike the hand, facial, and body movements for communication. There are\ndifferent signs and gestures representing alphabets, words, and phrases.\nNowadays approximately 300 sign languages are being practiced worldwide such as\nAmerican Sign Language (ASL), Chinese Sign Language (CSL), Indian Sign Language\n(ISL), and many more. Sign languages are dependent on the vocal language of a\nplace. Unlike vocal or spoken languages, there are no helping words in sign\nlanguage like is, am, are, was, were, will, be, etc. As only a limited\npopulation is well-versed in sign language, this lack of familiarity of sign\nlanguage hinders hearing-impaired people from communicating freely and easily\nwith everyone. This issue can be addressed by a sign language recognition (SLR)\nsystem which has the capability to translate the sign language into vocal\nlanguage. In this paper, a continuous SLR system is proposed using a deep\nlearning model employing Long Short-Term Memory (LSTM), trained and tested on\nan ISL primary dataset. This dataset is created using MediaPipe Holistic\npipeline for tracking face, hand, and body movements and collecting landmarks.\nThe system recognizes the signs and gestures in real-time with 88.23% accuracy.\n","authors":["Sharvani Srivastava","Sudhakar Singh"," Pooja","Shiv Prakash"],"pdf_url":"https://arxiv.org/pdf/2411.04517v1.pdf","comment":"14 pages, 4 figures, Wireless Pers Commun"},{"id":"http://arxiv.org/abs/2411.04509v1","updated":"2024-11-07T08:02:58Z","published":"2024-11-07T08:02:58Z","title":"FedDP: Privacy-preserving method based on federated learning for\n  histopathology image segmentation","summary":"  Hematoxylin and Eosin (H&E) staining of whole slide images (WSIs) is\nconsidered the gold standard for pathologists and medical practitioners for\ntumor diagnosis, surgical planning, and post-operative assessment. With the\nrapid advancement of deep learning technologies, the development of numerous\nmodels based on convolutional neural networks and transformer-based models has\nbeen applied to the precise segmentation of WSIs. However, due to privacy\nregulations and the need to protect patient confidentiality, centralized\nstorage and processing of image data are impractical. Training a centralized\nmodel directly is challenging to implement in medical settings due to these\nprivacy concerns.This paper addresses the dispersed nature and privacy\nsensitivity of medical image data by employing a federated learning framework,\nallowing medical institutions to collaboratively learn while protecting patient\nprivacy. Additionally, to address the issue of original data reconstruction\nthrough gradient inversion during the federated learning training process,\ndifferential privacy introduces noise into the model updates, preventing\nattackers from inferring the contributions of individual samples, thereby\nprotecting the privacy of the training data.Experimental results show that the\nproposed method, FedDP, minimally impacts model accuracy while effectively\nsafeguarding the privacy of cancer pathology image data, with only a slight\ndecrease in Dice, Jaccard, and Acc indices by 0.55%, 0.63%, and 0.42%,\nrespectively. This approach facilitates cross-institutional collaboration and\nknowledge sharing while protecting sensitive data privacy, providing a viable\nsolution for further research and application in the medical field.\n","authors":["Liangrui Pan","Mao Huang","Lian Wang","Pinle Qin","Shaoliang Peng"],"pdf_url":"https://arxiv.org/pdf/2411.04509v1.pdf","comment":"Accepted in BIBM2024"},{"id":"http://arxiv.org/abs/2401.15613v7","updated":"2024-11-07T07:58:50Z","published":"2024-01-28T10:00:45Z","title":"An efficient dual-branch framework via implicit self-texture enhancement\n  for arbitrary-scale histopathology image super-resolution","summary":"  High-quality whole-slide scanning is expensive, complex, and time-consuming,\nthus limiting the acquisition and utilization of high-resolution histopathology\nimages in daily clinical work. Deep learning-based single-image\nsuper-resolution (SISR) techniques provide an effective way to solve this\nproblem. However, the existing SISR models applied in histopathology images can\nonly work in fixed integer scaling factors, decreasing their applicability.\nThough methods based on implicit neural representation (INR) have shown\npromising results in arbitrary-scale super-resolution (SR) of natural images,\napplying them directly to histopathology images is inadequate because they have\nunique fine-grained image textures different from natural images. Thus, we\npropose an Implicit Self-Texture Enhancement-based dual-branch framework (ISTE)\nfor arbitrary-scale SR of histopathology images to address this challenge. The\nproposed ISTE contains a feature aggregation branch and a texture learning\nbranch. We employ the feature aggregation branch to enhance the learning of the\nlocal details for SR images while utilizing the texture learning branch to\nenhance the learning of high-frequency texture details. Then, we design a\ntwo-stage texture enhancement strategy to fuse the features from the two\nbranches to obtain the SR images. Experiments on publicly available datasets,\nincluding TMA, HistoSR, and the TCGA lung cancer datasets, demonstrate that\nISTE outperforms existing fixed-scale and arbitrary-scale SR algorithms across\nvarious scaling factors. Additionally, extensive experiments have shown that\nthe histopathology images reconstructed by the proposed ISTE are applicable to\ndownstream pathology image analysis tasks.\n","authors":["Minghong Duan","Linhao Qu","Zhiwei Yang","Manning Wang","Chenxi Zhang","Zhijian Song"],"pdf_url":"https://arxiv.org/pdf/2401.15613v7.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07269v2","updated":"2024-11-07T07:52:59Z","published":"2024-10-09T04:07:38Z","title":"Deep Learning for Surgical Instrument Recognition and Segmentation in\n  Robotic-Assisted Surgeries: A Systematic Review","summary":"  Applying deep learning (DL) for annotating surgical instruments in\nrobot-assisted minimally invasive surgeries (MIS) represents a significant\nadvancement in surgical technology. This systematic review examines 48 studies\nthat and advanced DL methods and architectures. These sophisticated DL models\nhave shown notable improvements in the precision and efficiency of detecting\nand segmenting surgical tools. The enhanced capabilities of these models\nsupport various clinical applications, including real-time intraoperative\nguidance, comprehensive postoperative evaluations, and objective assessments of\nsurgical skills. By accurately identifying and segmenting surgical instruments\nin video data, DL models provide detailed feedback to surgeons, thereby\nimproving surgical outcomes and reducing complication risks. Furthermore, the\napplication of DL in surgical education is transformative. The review\nunderscores the significant impact of DL on improving the accuracy of skill\nassessments and the overall quality of surgical training programs. However,\nimplementing DL in surgical tool detection and segmentation faces challenges,\nsuch as the need for large, accurately annotated datasets to train these models\neffectively. The manual annotation process is labor-intensive and\ntime-consuming, posing a significant bottleneck. Future research should focus\non automating the detection and segmentation process and enhancing the\nrobustness of DL models against environmental variations. Expanding the\napplication of DL models across various surgical specialties will be essential\nto fully realize this technology's potential. Integrating DL with other\nemerging technologies, such as augmented reality (AR), also offers promising\nopportunities to further enhance the precision and efficacy of surgical\nprocedures.\n","authors":["Fatimaelzahraa Ali Ahmed","Mahmoud Yousef","Mariam Ali Ahmed","Hasan Omar Ali","Anns Mahboob","Hazrat Ali","Zubair Shah","Omar Aboumarzouk","Abdulla Al Ansari","Shidin Balakrishnan"],"pdf_url":"https://arxiv.org/pdf/2410.07269v2.pdf","comment":"57 pages, 9 figures, Published in Artificial Intelligence Reviews\n  journal <https://link.springer.com/journal/10462>"},{"id":"http://arxiv.org/abs/2411.04501v1","updated":"2024-11-07T07:50:58Z","published":"2024-11-07T07:50:58Z","title":"Pose2Trajectory: Using Transformers on Body Pose to Predict Tennis\n  Player's Trajectory","summary":"  Tracking the trajectory of tennis players can help camera operators in\nproduction. Predicting future movement enables cameras to automatically track\nand predict a player's future trajectory without human intervention. Predicting\nfuture human movement in the context of complex physical tasks is also\nintellectually satisfying. Swift advancements in sports analytics and the wide\navailability of videos for tennis have inspired us to propose a novel method\ncalled Pose2Trajectory, which predicts a tennis player's future trajectory as a\nsequence derived from their body joints' data and ball position. Demonstrating\nimpressive accuracy, our approach capitalizes on body joint information to\nprovide a comprehensive understanding of the human body's geometry and motion,\nthereby enhancing the prediction of the player's trajectory. We use\nencoder-decoder Transformer architecture trained on the joints and trajectory\ninformation of the players with ball positions. The predicted sequence can\nprovide information to help close-up cameras to keep tracking the tennis\nplayer, following centroid coordinates. We generate a high-quality dataset from\nmultiple videos to assist tennis player movement prediction using object\ndetection and human pose estimation methods. It contains bounding boxes and\njoint information for tennis players and ball positions in singles tennis\ngames. Our method shows promising results in predicting the tennis player's\nmovement trajectory with different sequence prediction lengths using the joints\nand trajectory information with the ball position.\n","authors":["Ali K. AlShami","Terrance Boult","Jugal Kalita"],"pdf_url":"https://arxiv.org/pdf/2411.04501v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.05797v2","updated":"2024-11-07T07:47:17Z","published":"2024-02-08T16:37:04Z","title":"TaE: Task-aware Expandable Representation for Long Tail Class\n  Incremental Learning","summary":"  Class-incremental learning is dedicated to the development of deep learning\nmodels that are capable of acquiring new knowledge while retaining previously\nlearned information. Most methods focus on balanced data distribution for each\ntask, overlooking real-world long-tailed distributions. Therefore, Long-Tailed\nClass-Incremental Learning has been introduced, which trains on data where head\nclasses have more samples than tail classes. Existing methods mainly focus on\npreserving representative samples from previous classes to combat catastrophic\nforgetting. Recently, dynamic network algorithms freeze old network structures\nand expand new ones, achieving significant performance. However, with the\nintroduction of the long-tail problem, merely extending Determined blocks can\nlead to miscalibrated predictions, while expanding the entire backbone results\nin an explosion of memory size. To address these issues, we introduce a novel\nTask-aware Expandable (TaE) framework, dynamically allocating and updating\ntask-specific trainable parameters to learn diverse representations from each\nincremental task while resisting forgetting through the majority of frozen\nmodel parameters. To further encourage the class-specific feature\nrepresentation, we develop a Centroid-Enhanced (CEd) method to guide the update\nof these task-aware parameters. This approach is designed to adaptively\nallocate feature space for every class by adjusting the distance between intra-\nand inter-class features, which can extend to all \"training from sketch\"\nalgorithms. Extensive experiments demonstrate that TaE achieves\nstate-of-the-art performance.\n","authors":["Linjie Li","Zhenyu Wu","Jiaming Liu","Yang Ji"],"pdf_url":"https://arxiv.org/pdf/2402.05797v2.pdf","comment":"Accepted to ACCV2024"},{"id":"http://arxiv.org/abs/2411.04493v1","updated":"2024-11-07T07:41:04Z","published":"2024-11-07T07:41:04Z","title":"Synergy-Guided Regional Supervision of Pseudo Labels for Semi-Supervised\n  Medical Image Segmentation","summary":"  Semi-supervised learning has received considerable attention for its\npotential to leverage abundant unlabeled data to enhance model robustness.\nPseudo labeling is a widely used strategy in semi supervised learning. However,\nexisting methods often suffer from noise contamination, which can undermine\nmodel performance. To tackle this challenge, we introduce a novel\nSynergy-Guided Regional Supervision of Pseudo Labels (SGRS-Net) framework.\nBuilt upon the mean teacher network, we employ a Mix Augmentation module to\nenhance the unlabeled data. By evaluating the synergy before and after\naugmentation, we strategically partition the pseudo labels into distinct\nregions. Additionally, we introduce a Region Loss Evaluation module to assess\nthe loss across each delineated area. Extensive experiments conducted on the LA\ndataset have demonstrated superior performance over state-of-the-art\ntechniques, underscoring the efficiency and practicality of our framework.\n","authors":["Tao Wang","Xinlin Zhang","Yuanbin Chen","Yuanbo Zhou","Longxuan Zhao","Tao Tan","Tong Tong"],"pdf_url":"https://arxiv.org/pdf/2411.04493v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03807v2","updated":"2024-11-07T07:32:33Z","published":"2024-11-06T10:07:46Z","title":"GS2Pose: Two-stage 6D Object Pose Estimation Guided by Gaussian\n  Splatting","summary":"  This paper proposes a new method for accurate and robust 6D pose estimation\nof novel objects, named GS2Pose. By introducing 3D Gaussian splatting, GS2Pose\ncan utilize the reconstruction results without requiring a high-quality CAD\nmodel, which means it only requires segmented RGBD images as input.\nSpecifically, GS2Pose employs a two-stage structure consisting of coarse\nestimation followed by refined estimation. In the coarse stage, a lightweight\nU-Net network with a polarization attention mechanism, called Pose-Net, is\ndesigned. By using the 3DGS model for supervised training, Pose-Net can\ngenerate NOCS images to compute a coarse pose. In the refinement stage, GS2Pose\nformulates a pose regression algorithm following the idea of reprojection or\nBundle Adjustment (BA), referred to as GS-Refiner. By leveraging Lie algebra to\nextend 3DGS, GS-Refiner obtains a pose-differentiable rendering pipeline that\nrefines the coarse pose by comparing the input images with the rendered images.\nGS-Refiner also selectively updates parameters in the 3DGS model to achieve\nenvironmental adaptation, thereby enhancing the algorithm's robustness and\nflexibility to illuminative variation, occlusion, and other challenging\ndisruptive factors. GS2Pose was evaluated through experiments conducted on the\nLineMod dataset, where it was compared with similar algorithms, yielding highly\ncompetitive results. The code for GS2Pose will soon be released on GitHub.\n","authors":["Jilan Mei","Junbo Li","Cai Meng"],"pdf_url":"https://arxiv.org/pdf/2411.03807v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04480v1","updated":"2024-11-07T07:19:28Z","published":"2024-11-07T07:19:28Z","title":"CFPNet: Improving Lightweight ToF Depth Completion via Cross-zone\n  Feature Propagation","summary":"  Depth completion using lightweight time-of-flight (ToF) depth sensors is\nattractive due to their low cost. However, lightweight ToF sensors usually have\na limited field of view (FOV) compared with cameras. Thus, only pixels in the\nzone area of the image can be associated with depth signals. Previous methods\nfail to propagate depth features from the zone area to the outside-zone area\neffectively, thus suffering from degraded depth completion performance outside\nthe zone. To this end, this paper proposes the CFPNet to achieve cross-zone\nfeature propagation from the zone area to the outside-zone area with two novel\nmodules. The first is a direct-attention-based propagation module (DAPM), which\nenforces direct cross-zone feature acquisition. The second is a\nlarge-kernel-based propagation module (LKPM), which realizes cross-zone feature\npropagation by utilizing convolution layers with kernel sizes up to 31. CFPNet\nachieves state-of-the-art (SOTA) depth completion performance by combining\nthese two modules properly, as verified by extensive experimental results on\nthe ZJU-L5 dataset. The code will be made public.\n","authors":["Laiyan Ding","Hualie Jiang","Rui Xu","Rui Huang"],"pdf_url":"https://arxiv.org/pdf/2411.04480v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04475v1","updated":"2024-11-07T07:03:40Z","published":"2024-11-07T07:03:40Z","title":"Deep Learning Models for UAV-Assisted Bridge Inspection: A YOLO\n  Benchmark Analysis","summary":"  Visual inspections of bridges are critical to ensure their safety and\nidentify potential failures early. This inspection process can be rapidly and\naccurately automated by using unmanned aerial vehicles (UAVs) integrated with\ndeep learning models. However, choosing an appropriate model that is\nlightweight enough to integrate into the UAV and fulfills the strict\nrequirements for inference time and accuracy is challenging. Therefore, our\nwork contributes to the advancement of this model selection process by\nconducting a benchmark of 23 models belonging to the four newest YOLO variants\n(YOLOv5, YOLOv6, YOLOv7, YOLOv8) on COCO-Bridge-2021+, a dataset for bridge\ndetails detection. Through comprehensive benchmarking, we identify YOLOv8n,\nYOLOv7tiny, YOLOv6m, and YOLOv6m6 as the models offering an optimal balance\nbetween accuracy and processing speed, with mAP@50 scores of 0.803, 0.837,\n0.853, and 0.872, and inference times of 5.3ms, 7.5ms, 14.06ms, and 39.33ms,\nrespectively. Our findings accelerate the model selection process for UAVs,\nenabling more efficient and reliable bridge inspections.\n","authors":["Trong-Nhan Phan","Hoang-Hai Nguyen","Thi-Thu-Hien Ha","Huy-Tan Thai","Kim-Hung Le"],"pdf_url":"https://arxiv.org/pdf/2411.04475v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04918v4","updated":"2024-11-07T06:58:04Z","published":"2024-09-07T21:52:58Z","title":"Training-free Zero-shot Composed Image Retrieval via Weighted Modality\n  Fusion and Similarity","summary":"  Composed image retrieval (CIR), which formulates the query as a combination\nof a reference image and modified text, has emerged as a new form of image\nsearch due to its enhanced ability to capture user intent. However, training a\nCIR model in a supervised manner typically requires labor-intensive collection\nof (reference image, text modifier, target image) triplets. While existing\nzero-shot CIR (ZS-CIR) methods eliminate the need for training on specific\ndownstream datasets, they still require additional pretraining on large-scale\nimage datasets. In this paper, we introduce a training-free approach for\nZS-CIR. Our approach, Weighted Modality fusion and similarity for CIR\n(WeiMoCIR), operates under the assumption that image and text modalities can be\neffectively combined using a simple weighted average. This allows the query\nrepresentation to be constructed directly from the reference image and text\nmodifier. To further enhance retrieval performance, we employ multimodal large\nlanguage models (MLLMs) to generate image captions for the database images and\nincorporate these textual captions into the similarity computation by combining\nthem with image information using a weighted average. Our approach is simple,\neasy to implement, and its effectiveness is validated through experiments on\nthe FashionIQ and CIRR datasets. Code is available at\nhttps://github.com/whats2000/WeiMoCIR.\n","authors":["Ren-Di Wu","Yu-Yen Lin","Huei-Fang Yang"],"pdf_url":"https://arxiv.org/pdf/2409.04918v4.pdf","comment":"14 pages, 6 figures, International Conference on Technologies and\n  Applications of Artificial Intelligence (TAAI) Camera Ready"},{"id":"http://arxiv.org/abs/2411.04469v1","updated":"2024-11-07T06:39:50Z","published":"2024-11-07T06:39:50Z","title":"FreeCap: Hybrid Calibration-Free Motion Capture in Open Environments","summary":"  We propose a novel hybrid calibration-free method FreeCap to accurately\ncapture global multi-person motions in open environments. Our system combines a\nsingle LiDAR with expandable moving cameras, allowing for flexible and precise\nmotion estimation in a unified world coordinate. In particular, We introduce a\nlocal-to-global pose-aware cross-sensor human-matching module that predicts the\nalignment among each sensor, even in the absence of calibration. Additionally,\nour coarse-to-fine sensor-expandable pose optimizer further optimizes the 3D\nhuman key points and the alignments, it is also capable of incorporating\nadditional cameras to enhance accuracy. Extensive experiments on Human-M3 and\nFreeMotion datasets demonstrate that our method significantly outperforms\nstate-of-the-art single-modal methods, offering an expandable and efficient\nsolution for multi-person motion capture across various applications.\n","authors":["Aoru Xue","Yiming Ren","Zining Song","Mao Ye","Xinge Zhu","Yuexin Ma"],"pdf_url":"https://arxiv.org/pdf/2411.04469v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.01723v7","updated":"2024-11-07T06:25:25Z","published":"2023-11-03T05:41:25Z","title":"Towards Calibrated Robust Fine-Tuning of Vision-Language Models","summary":"  Improving out-of-distribution (OOD) generalization during in-distribution\n(ID) adaptation is a primary goal of robust fine-tuning of zero-shot models\nbeyond naive fine-tuning. However, despite decent OOD generalization\nperformance from recent robust fine-tuning methods, confidence calibration for\nreliable model output has not been fully addressed. This work proposes a robust\nfine-tuning method that improves both OOD accuracy and confidence calibration\nsimultaneously in vision language models. Firstly, we show that both OOD\nclassification and OOD calibration errors have a shared upper bound consisting\nof two terms of ID data: 1) ID calibration error and 2) the smallest singular\nvalue of the ID input covariance matrix. Based on this insight, we design a\nnovel framework that conducts fine-tuning with a constrained multimodal\ncontrastive loss enforcing a larger smallest singular value, which is further\nguided by the self-distillation of a moving-averaged model to achieve\ncalibrated prediction as well. Starting from empirical evidence supporting our\ntheoretical statements, we provide extensive experimental results on ImageNet\ndistribution shift benchmarks that demonstrate the effectiveness of our theorem\nand its practical implementation.\n","authors":["Changdae Oh","Hyesu Lim","Mijoo Kim","Dongyoon Han","Sangdoo Yun","Jaegul Choo","Alexander Hauptmann","Zhi-Qi Cheng","Kyungwoo Song"],"pdf_url":"https://arxiv.org/pdf/2311.01723v7.pdf","comment":"NeurIPS 2024 (a short version was presented at the NeurIPS 2023\n  Workshop on Distribution Shifts); Major modification of (v7): Fixing the\n  x-axis of Figure 3 and Pearson correlation, accordingly"},{"id":"http://arxiv.org/abs/2411.04457v1","updated":"2024-11-07T06:04:57Z","published":"2024-11-07T06:04:57Z","title":"Efficient single image non-uniformity correction algorithm","summary":"  This paper introduces a new way to correct the non-uniformity (NU) in\nuncooled infrared-type images. The main defect of these uncooled images is the\nlack of a column (resp. line) time-dependent cross-calibration, resulting in a\nstrong column (resp. line) and time dependent noise. This problem can be\nconsidered as a 1D flicker of the columns inside each frame. Thus, classic\nmovie deflickering algorithms can be adapted, to equalize the columns (resp.\nthe lines). The proposed method therefore applies to the series formed by the\ncolumns of an infrared image a movie deflickering algorithm. The obtained\nsingle image method works on static images, and therefore requires no\nregistration, no camera motion compensation, and no closed aperture sensor\nequalization. Thus, the method has only one camera dependent parameter, and is\nlandscape independent. This simple method will be compared to a state of the\nart total variation single image correction on raw real and simulated images.\nThe method is real time, requiring only two operations per pixel. It involves\nno test-pattern calibration and produces no \"ghost artifacts\".\n","authors":["Yohann Tendero","Jerome Gilles","Stephane Landeau","Jean-Michel Morel"],"pdf_url":"https://arxiv.org/pdf/2411.04457v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2411.03615"},{"id":"http://arxiv.org/abs/2411.04456v1","updated":"2024-11-07T06:04:43Z","published":"2024-11-07T06:04:43Z","title":"Properties of BV-G structures + textures decomposition models.\n  Application to road detection in satellite images","summary":"  In this paper we present some theoretical results about a structures-textures\nimage decomposition model which was proposed by the second author. We prove a\ntheorem which gives the behavior of this model in different cases. Finally, as\na consequence of the theorem we derive an algorithm for the detection of long\nand thin objects applied to a road networks detection application in aerial or\nsatellite images.\n","authors":["Jerome Gilles","Yves Meyer"],"pdf_url":"https://arxiv.org/pdf/2411.04456v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.18057v2","updated":"2024-11-07T04:49:24Z","published":"2024-09-26T17:00:02Z","title":"LightAvatar: Efficient Head Avatar as Dynamic Neural Light Field","summary":"  Recent works have shown that neural radiance fields (NeRFs) on top of\nparametric models have reached SOTA quality to build photorealistic head\navatars from a monocular video. However, one major limitation of the NeRF-based\navatars is the slow rendering speed due to the dense point sampling of NeRF,\npreventing them from broader utility on resource-constrained devices. We\nintroduce LightAvatar, the first head avatar model based on neural light fields\n(NeLFs). LightAvatar renders an image from 3DMM parameters and a camera pose\nvia a single network forward pass, without using mesh or volume rendering. The\nproposed approach, while being conceptually appealing, poses a significant\nchallenge towards real-time efficiency and training stability. To resolve them,\nwe introduce dedicated network designs to obtain proper representations for the\nNeLF model and maintain a low FLOPs budget. Meanwhile, we tap into a\ndistillation-based training strategy that uses a pretrained avatar model as\nteacher to synthesize abundant pseudo data for training. A warping field\nnetwork is introduced to correct the fitting error in the real data so that the\nmodel can learn better. Extensive experiments suggest that our method can\nachieve new SOTA image quality quantitatively or qualitatively, while being\nsignificantly faster than the counterparts, reporting 174.1 FPS (512x512\nresolution) on a consumer-grade GPU (RTX3090) with no customized optimization.\n","authors":["Huan Wang","Feitong Tan","Ziqian Bai","Yinda Zhang","Shichen Liu","Qiangeng Xu","Menglei Chai","Anish Prabhu","Rohit Pandey","Sean Fanello","Zeng Huang","Yun Fu"],"pdf_url":"https://arxiv.org/pdf/2409.18057v2.pdf","comment":"ECCV'24 CADL Workshop. Code:\n  https://github.com/MingSun-Tse/LightAvatar-TensorFlow. V2: Corrected speed\n  benchmark with GaussianAvatar"},{"id":"http://arxiv.org/abs/2312.02548v3","updated":"2024-11-07T04:44:38Z","published":"2023-12-05T07:34:30Z","title":"GeNIe: Generative Hard Negative Images Through Diffusion","summary":"  Data augmentation is crucial in training deep models, preventing them from\noverfitting to limited data. Recent advances in generative AI, e.g., diffusion\nmodels, have enabled more sophisticated augmentation techniques that produce\ndata resembling natural images. We introduce GeNIe a novel augmentation method\nwhich leverages a latent diffusion model conditioned on a text prompt to\ncombine two contrasting data points (an image from the source category and a\ntext prompt from the target category) to generate challenging augmentations. To\nachieve this, we adjust the noise level (equivalently, number of diffusion\niterations) to ensure the generated image retains low-level and background\nfeatures from the source image while representing the target category,\nresulting in a hard negative sample for the source category. We further\nautomate and enhance GeNIe by adaptively adjusting the noise level selection on\na per image basis (coined as GeNIe-Ada), leading to further performance\nimprovements. Our extensive experiments, in both few-shot and long-tail\ndistribution settings, demonstrate the effectiveness of our novel augmentation\nmethod and its superior performance over the prior art. Our code is available\nat: https://github.com/UCDvision/GeNIe\n","authors":["Soroush Abbasi Koohpayegani","Anuj Singh","K L Navaneet","Hamed Pirsiavash","Hadi Jamali-Rad"],"pdf_url":"https://arxiv.org/pdf/2312.02548v3.pdf","comment":"Our code is available https://github.com/UCDvision/GeNIe"},{"id":"http://arxiv.org/abs/2411.00172v2","updated":"2024-11-07T04:41:32Z","published":"2024-10-31T19:37:47Z","title":"SeafloorAI: A Large-scale Vision-Language Dataset for Seafloor\n  Geological Survey","summary":"  A major obstacle to the advancements of machine learning models in marine\nscience, particularly in sonar imagery analysis, is the scarcity of AI-ready\ndatasets. While there have been efforts to make AI-ready sonar image dataset\npublicly available, they suffer from limitations in terms of environment\nsetting and scale. To bridge this gap, we introduce SeafloorAI, the first\nextensive AI-ready datasets for seafloor mapping across 5 geological layers\nthat is curated in collaboration with marine scientists. We further extend the\ndataset to SeafloorGenAI by incorporating the language component in order to\nfacilitate the development of both vision- and language-capable machine\nlearning models for sonar imagery. The dataset consists of 62 geo-distributed\ndata surveys spanning 17,300 square kilometers, with 696K sonar images, 827K\nannotated segmentation masks, 696K detailed language descriptions and\napproximately 7M question-answer pairs. By making our data processing source\ncode publicly available, we aim to engage the marine science community to\nenrich the data pool and inspire the machine learning community to develop more\nrobust models. This collaborative approach will enhance the capabilities and\napplications of our datasets within both fields.\n","authors":["Kien X. Nguyen","Fengchun Qiao","Arthur Trembanis","Xi Peng"],"pdf_url":"https://arxiv.org/pdf/2411.00172v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04420v1","updated":"2024-11-07T04:16:15Z","published":"2024-11-07T04:16:15Z","title":"BendVLM: Test-Time Debiasing of Vision-Language Embeddings","summary":"  Vision-language model (VLM) embeddings have been shown to encode biases\npresent in their training data, such as societal biases that prescribe negative\ncharacteristics to members of various racial and gender identities. VLMs are\nbeing quickly adopted for a variety of tasks ranging from few-shot\nclassification to text-guided image generation, making debiasing VLM embeddings\ncrucial. Debiasing approaches that fine-tune the VLM often suffer from\ncatastrophic forgetting. On the other hand, fine-tuning-free methods typically\nutilize a \"one-size-fits-all\" approach that assumes that correlation with the\nspurious attribute can be explained using a single linear direction across all\npossible inputs. In this work, we propose Bend-VLM, a nonlinear,\nfine-tuning-free approach for VLM embedding debiasing that tailors the\ndebiasing operation to each unique input. This allows for a more flexible\ndebiasing approach. Additionally, we do not require knowledge of the set of\ninputs a priori to inference time, making our method more appropriate for\nonline, open-set tasks such as retrieval and text guided image generation.\n","authors":["Walter Gerych","Haoran Zhang","Kimia Hamidieh","Eileen Pan","Maanas Sharma","Thomas Hartvigsen","Marzyeh Ghassemi"],"pdf_url":"https://arxiv.org/pdf/2411.04420v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01981v2","updated":"2024-11-07T04:10:10Z","published":"2024-11-04T11:09:47Z","title":"Typicalness-Aware Learning for Failure Detection","summary":"  Deep neural networks (DNNs) often suffer from the overconfidence issue, where\nincorrect predictions are made with high confidence scores, hindering the\napplications in critical systems. In this paper, we propose a novel approach\ncalled Typicalness-Aware Learning (TAL) to address this issue and improve\nfailure detection performance. We observe that, with the cross-entropy loss,\nmodel predictions are optimized to align with the corresponding labels via\nincreasing logit magnitude or refining logit direction. However, regarding\natypical samples, the image content and their labels may exhibit disparities.\nThis discrepancy can lead to overfitting on atypical samples, ultimately\nresulting in the overconfidence issue that we aim to address. To tackle the\nproblem, we have devised a metric that quantifies the typicalness of each\nsample, enabling the dynamic adjustment of the logit magnitude during the\ntraining process. By allowing atypical samples to be adequately fitted while\npreserving reliable logit direction, the problem of overconfidence can be\nmitigated. TAL has been extensively evaluated on benchmark datasets, and the\nresults demonstrate its superiority over existing failure detection methods.\nSpecifically, TAL achieves a more than 5% improvement on CIFAR100 in terms of\nthe Area Under the Risk-Coverage Curve (AURC) compared to the state-of-the-art.\nCode is available at https://github.com/liuyijungoon/TAL.\n","authors":["Yijun Liu","Jiequan Cui","Zhuotao Tian","Senqiao Yang","Qingdong He","Xiaoling Wang","Jingyong Su"],"pdf_url":"https://arxiv.org/pdf/2411.01981v2.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.03348v2","updated":"2024-11-07T04:05:58Z","published":"2024-11-03T18:44:28Z","title":"Undermining Image and Text Classification Algorithms Using Adversarial\n  Attacks","summary":"  Machine learning models are prone to adversarial attacks, where inputs can be\nmanipulated in order to cause misclassifications. While previous research has\nfocused on techniques like Generative Adversarial Networks (GANs), there's\nlimited exploration of GANs and Synthetic Minority Oversampling Technique\n(SMOTE) in text and image classification models to perform adversarial attacks.\nOur study addresses this gap by training various machine learning models and\nusing GANs and SMOTE to generate additional data points aimed at attacking text\nclassification models. Furthermore, we extend our investigation to face\nrecognition models, training a Convolutional Neural Network(CNN) and subjecting\nit to adversarial attacks with fast gradient sign perturbations on key features\nidentified by GradCAM, a technique used to highlight key image characteristics\nCNNs use in classification. Our experiments reveal a significant vulnerability\nin classification models. Specifically, we observe a 20 % decrease in accuracy\nfor the top-performing text classification models post-attack, along with a 30\n% decrease in facial recognition accuracy. This highlights the susceptibility\nof these models to manipulation of input data. Adversarial attacks not only\ncompromise the security but also undermine the reliability of machine learning\nsystems. By showcasing the impact of adversarial attacks on both text\nclassification and face recognition models, our study underscores the urgent\nneed for develop robust defenses against such vulnerabilities.\n","authors":["Langalibalele Lunga","Suhas Sreehari"],"pdf_url":"https://arxiv.org/pdf/2411.03348v2.pdf","comment":"Accepted for presentation at Electronic Imaging Conference 2025"},{"id":"http://arxiv.org/abs/2411.04406v1","updated":"2024-11-07T03:55:23Z","published":"2024-11-07T03:55:23Z","title":"Image Understanding Makes for A Good Tokenizer for Image Generation","summary":"  Abstract Modern image generation (IG) models have been shown to capture rich\nsemantics valuable for image understanding (IU) tasks. However, the potential\nof IU models to improve IG performance remains uncharted. We address this issue\nusing a token-based IG framework, which relies on effective tokenizers to\nproject images into token sequences. Currently, pixel reconstruction (e.g.,\nVQGAN) dominates the training objective for image tokenizers. In contrast, our\napproach adopts the feature reconstruction objective, where tokenizers are\ntrained by distilling knowledge from pretrained IU encoders. Comprehensive\ncomparisons indicate that tokenizers with strong IU capabilities achieve\nsuperior IG performance across a variety of metrics, datasets, tasks, and\nproposal networks. Notably, VQ-KD CLIP achieves $4.10$ FID on ImageNet-1k\n(IN-1k). Visualization suggests that the superiority of VQ-KD can be partly\nattributed to the rich semantics within the VQ-KD codebook. We further\nintroduce a straightforward pipeline to directly transform IU encoders into\ntokenizers, demonstrating exceptional effectiveness for IG tasks. These\ndiscoveries may energize further exploration into image tokenizer research and\ninspire the community to reassess the relationship between IU and IG. The code\nis released at https://github.com/magic-research/vector_quantization.\n","authors":["Luting Wang","Yang Zhao","Zijian Zhang","Jiashi Feng","Si Liu","Bingyi Kang"],"pdf_url":"https://arxiv.org/pdf/2411.04406v1.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2310.03739v5","updated":"2024-11-07T03:54:22Z","published":"2023-10-05T17:59:18Z","title":"Aligning Text-to-Image Diffusion Models with Reward Backpropagation","summary":"  Text-to-image diffusion models have recently emerged at the forefront of\nimage generation, powered by very large-scale unsupervised or weakly supervised\ntext-to-image training datasets. Due to their unsupervised training,\ncontrolling their behavior in downstream tasks, such as maximizing\nhuman-perceived image quality, image-text alignment, or ethical image\ngeneration, is difficult. Recent works finetune diffusion models to downstream\nreward functions using vanilla reinforcement learning, notorious for the high\nvariance of the gradient estimators. In this paper, we propose AlignProp, a\nmethod that aligns diffusion models to downstream reward functions using\nend-to-end backpropagation of the reward gradient through the denoising\nprocess. While naive implementation of such backpropagation would require\nprohibitive memory resources for storing the partial derivatives of modern\ntext-to-image models, AlignProp finetunes low-rank adapter weight modules and\nuses gradient checkpointing, to render its memory usage viable. We test\nAlignProp in finetuning diffusion models to various objectives, such as\nimage-text semantic alignment, aesthetics, compressibility and controllability\nof the number of objects present, as well as their combinations. We show\nAlignProp achieves higher rewards in fewer training steps than alternatives,\nwhile being conceptually simpler, making it a straightforward choice for\noptimizing diffusion models for differentiable reward functions of interest.\nCode and Visualization results are available at https://align-prop.github.io/.\n","authors":["Mihir Prabhudesai","Anirudh Goyal","Deepak Pathak","Katerina Fragkiadaki"],"pdf_url":"https://arxiv.org/pdf/2310.03739v5.pdf","comment":"This paper is subsumed by a later paper of ours: arXiv:2407.08737"},{"id":"http://arxiv.org/abs/2411.04404v1","updated":"2024-11-07T03:48:35Z","published":"2024-11-07T03:48:35Z","title":"Enhancing Bronchoscopy Depth Estimation through Synthetic-to-Real Domain\n  Adaptation","summary":"  Monocular depth estimation has shown promise in general imaging tasks, aiding\nin localization and 3D reconstruction. While effective in various domains, its\napplication to bronchoscopic images is hindered by the lack of labeled data,\nchallenging the use of supervised learning methods. In this work, we propose a\ntransfer learning framework that leverages synthetic data with depth labels for\ntraining and adapts domain knowledge for accurate depth estimation in real\nbronchoscope data. Our network demonstrates improved depth prediction on real\nfootage using domain adaptation compared to training solely on synthetic data,\nvalidating our approach.\n","authors":["Qingyao Tian","Huai Liao","Xinyan Huang","Lujie Li","Hongbin Liu"],"pdf_url":"https://arxiv.org/pdf/2411.04404v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04399v1","updated":"2024-11-07T03:28:24Z","published":"2024-11-07T03:28:24Z","title":"ProGraph: Temporally-alignable Probability Guided Graph Topological\n  Modeling for 3D Human Reconstruction","summary":"  Current 3D human motion reconstruction methods from monocular videos rely on\nfeatures within the current reconstruction window, leading to distortion and\ndeformations in the human structure under local occlusions or blurriness in\nvideo frames. To estimate realistic 3D human mesh sequences based on incomplete\nfeatures, we propose Temporally-alignable Probability Guided Graph Topological\nModeling for 3D Human Reconstruction (ProGraph). For missing parts recovery, we\nexploit the explicit topological-aware probability distribution across the\nentire motion sequence. To restore the complete human, Graph Topological\nModeling (GTM) learns the underlying topological structure, focusing on the\nrelationships inherent in the individual parts. Next, to generate blurred\nmotion parts, Temporal-alignable Probability Distribution (TPDist) utilizes the\nGTM to predict features based on distribution. This interactive mechanism\nfacilitates motion consistency, allowing the restoration of human parts.\nFurthermore, Hierarchical Human Loss (HHLoss) constrains the probability\ndistribution errors of inter-frame features during topological structure\nvariation. Our Method achieves superior results than other SOTA methods in\naddressing occlusions and blurriness on 3DPW.\n","authors":["Hongsheng Wang","Zehui Feng","Tong Xiao","Genfan Yang","Shengyu Zhang","Fei Wu","Feng Lin"],"pdf_url":"https://arxiv.org/pdf/2411.04399v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.00132v2","updated":"2024-11-07T03:22:56Z","published":"2024-10-31T18:33:39Z","title":"Beyond Accuracy: Ensuring Correct Predictions With Correct Rationales","summary":"  Large pretrained foundation models demonstrate exceptional performance and,\nin some high-stakes applications, even surpass human experts. However, most of\nthese models are currently evaluated primarily on prediction accuracy,\noverlooking the validity of the rationales behind their accurate predictions.\nFor the safe deployment of foundation models, there is a pressing need to\nensure double-correct predictions, i.e., correct prediction backed by correct\nrationales. To achieve this, we propose a two-phase scheme: First, we curate a\nnew dataset that offers structured rationales for visual recognition tasks.\nSecond, we propose a rationale-informed optimization method to guide the model\nin disentangling and localizing visual evidence for each rationale, without\nrequiring manual annotations. Extensive experiments and ablation studies\ndemonstrate that our model outperforms state-of-the-art models by up to 10.1%\nin prediction accuracy across a wide range of tasks. Furthermore, our method\nsignificantly improves the model's rationale correctness, improving\nlocalization by 7.5% and disentanglement by 36.5%. Our dataset, source code,\nand pretrained weights: https://github.com/deep-real/DCP\n","authors":["Tang Li","Mengmeng Ma","Xi Peng"],"pdf_url":"https://arxiv.org/pdf/2411.00132v2.pdf","comment":"In Proceedings of the 38th Conference on Neural Information\n  Processing Systems (NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2410.23247v3","updated":"2024-11-07T02:52:47Z","published":"2024-10-30T17:30:35Z","title":"bit2bit: 1-bit quanta video reconstruction via self-supervised photon\n  prediction","summary":"  Quanta image sensors, such as SPAD arrays, are an emerging sensor technology,\nproducing 1-bit arrays representing photon detection events over exposures as\nshort as a few nanoseconds. In practice, raw data are post-processed using\nheavy spatiotemporal binning to create more useful and interpretable images at\nthe cost of degrading spatiotemporal resolution. In this work, we propose\nbit2bit, a new method for reconstructing high-quality image stacks at the\noriginal spatiotemporal resolution from sparse binary quanta image data.\nInspired by recent work on Poisson denoising, we developed an algorithm that\ncreates a dense image sequence from sparse binary photon data by predicting the\nphoton arrival location probability distribution. However, due to the binary\nnature of the data, we show that the assumption of a Poisson distribution is\ninadequate. Instead, we model the process with a Bernoulli lattice process from\nthe truncated Poisson. This leads to the proposal of a novel self-supervised\nsolution based on a masked loss function. We evaluate our method using both\nsimulated and real data. On simulated data from a conventional video, we\nachieve 34.35 mean PSNR with extremely photon-sparse binary input (<0.06\nphotons per pixel per frame). We also present a novel dataset containing a wide\nrange of real SPAD high-speed videos under various challenging imaging\nconditions. The scenes cover strong/weak ambient light, strong motion,\nultra-fast events, etc., which will be made available to the community, on\nwhich we demonstrate the promise of our approach. Both reconstruction quality\nand throughput substantially surpass the state-of-the-art methods (e.g., Quanta\nBurst Photography (QBP)). Our approach significantly enhances the visualization\nand usability of the data, enabling the application of existing analysis\ntechniques.\n","authors":["Yehe Liu","Alexander Krull","Hector Basevi","Ales Leonardis","Michael W. Jenkins"],"pdf_url":"https://arxiv.org/pdf/2410.23247v3.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2408.14789v3","updated":"2024-11-07T02:43:03Z","published":"2024-08-27T05:31:30Z","title":"Revisiting Surgical Instrument Segmentation Without Human Intervention:\n  A Graph Partitioning View","summary":"  Surgical instrument segmentation (SIS) on endoscopic images stands as a\nlong-standing and essential task in the context of computer-assisted\ninterventions for boosting minimally invasive surgery. Given the recent surge\nof deep learning methodologies and their data-hungry nature, training a neural\npredictive model based on massive expert-curated annotations has been\ndominating and served as an off-the-shelf approach in the field, which could,\nhowever, impose prohibitive burden to clinicians for preparing fine-grained\npixel-wise labels corresponding to the collected surgical video frames. In this\nwork, we propose an unsupervised method by reframing the video frame\nsegmentation as a graph partitioning problem and regarding image pixels as\ngraph nodes, which is significantly different from the previous efforts. A\nself-supervised pre-trained model is firstly leveraged as a feature extractor\nto capture high-level semantic features. Then, Laplacian matrixs are computed\nfrom the features and are eigendecomposed for graph partitioning. On the \"deep\"\neigenvectors, a surgical video frame is meaningfully segmented into different\nmodules such as tools and tissues, providing distinguishable semantic\ninformation like locations, classes, and relations. The segmentation problem\ncan then be naturally tackled by applying clustering or threshold on the\neigenvectors. Extensive experiments are conducted on various datasets (e.g.,\nEndoVis2017, EndoVis2018, UCL, etc.) for different clinical endpoints. Across\nall the challenging scenarios, our method demonstrates outstanding performance\nand robustness higher than unsupervised state-of-the-art (SOTA) methods. The\ncode is released at https://github.com/MingyuShengSMY/GraphClusteringSIS.git.\n","authors":["Mingyu Sheng","Jianan Fan","Dongnan Liu","Ron Kikinis","Weidong Cai"],"pdf_url":"https://arxiv.org/pdf/2408.14789v3.pdf","comment":"Accepted by The 32nd ACM International Conference on Multimedia (ACM\n  MM 2024) Workshop on Multimedia Computing for Health and Medicine (MCHM)"},{"id":"http://arxiv.org/abs/2411.03695v2","updated":"2024-11-07T02:40:12Z","published":"2024-11-06T06:33:55Z","title":"AMNCutter: Affinity-Attention-Guided Multi-View Normalized Cutter for\n  Unsupervised Surgical Instrument Segmentation","summary":"  Surgical instrument segmentation (SIS) is pivotal for robotic-assisted\nminimally invasive surgery, assisting surgeons by identifying surgical\ninstruments in endoscopic video frames. Recent unsupervised surgical instrument\nsegmentation (USIS) methods primarily rely on pseudo-labels derived from\nlow-level features such as color and optical flow, but these methods show\nlimited effectiveness and generalizability in complex and unseen endoscopic\nscenarios. In this work, we propose a label-free unsupervised model featuring a\nnovel module named Multi-View Normalized Cutter (m-NCutter). Different from\nprevious USIS works, our model is trained using a graph-cutting loss function\nthat leverages patch affinities for supervision, eliminating the need for\npseudo-labels. The framework adaptively determines which affinities from which\nlevels should be prioritized. Therefore, the low- and high-level features and\ntheir affinities are effectively integrated to train a label-free unsupervised\nmodel, showing superior effectiveness and generalization ability. We conduct\ncomprehensive experiments across multiple SIS datasets to validate our\napproach's state-of-the-art (SOTA) performance, robustness, and exceptional\npotential as a pre-trained model. Our code is released at\nhttps://github.com/MingyuShengSMY/AMNCutter.\n","authors":["Mingyu Sheng","Jianan Fan","Dongnan Liu","Ron Kikinis","Weidong Cai"],"pdf_url":"https://arxiv.org/pdf/2411.03695v2.pdf","comment":"Accepted by the 2025 IEEE/CVF Winter Conference on Applications of\n  Computer Vision (WACV 2025)"},{"id":"http://arxiv.org/abs/2406.02880v2","updated":"2024-11-07T02:26:49Z","published":"2024-06-05T02:54:46Z","title":"Controllable Talking Face Generation by Implicit Facial Keypoints\n  Editing","summary":"  Audio-driven talking face generation has garnered significant interest within\nthe domain of digital human research. Existing methods are encumbered by\nintricate model architectures that are intricately dependent on each other,\ncomplicating the process of re-editing image or video inputs. In this work, we\npresent ControlTalk, a talking face generation method to control face\nexpression deformation based on driven audio, which can construct the head pose\nand facial expression including lip motion for both single image or sequential\nvideo inputs in a unified manner. By utilizing a pre-trained video synthesis\nrenderer and proposing the lightweight adaptation, ControlTalk achieves precise\nand naturalistic lip synchronization while enabling quantitative control over\nmouth opening shape. Our experiments show that our method is superior to\nstate-of-the-art performance on widely used benchmarks, including HDTF and\nMEAD. The parameterized adaptation demonstrates remarkable generalization\ncapabilities, effectively handling expression deformation across same-ID and\ncross-ID scenarios, and extending its utility to out-of-domain portraits,\nregardless of languages. Code is available at\nhttps://github.com/NetEase-Media/ControlTalk.\n","authors":["Dong Zhao","Jiaying Shi","Wenjun Li","Shudong Wang","Shenghui Xu","Zhaoming Pan"],"pdf_url":"https://arxiv.org/pdf/2406.02880v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.18782v2","updated":"2024-11-07T01:31:00Z","published":"2024-05-29T05:42:25Z","title":"Principled Probabilistic Imaging using Diffusion Models as Plug-and-Play\n  Priors","summary":"  Diffusion models (DMs) have recently shown outstanding capabilities in\nmodeling complex image distributions, making them expressive image priors for\nsolving Bayesian inverse problems. However, most existing DM-based methods rely\non approximations in the generative process to be generic to different inverse\nproblems, leading to inaccurate sample distributions that deviate from the\ntarget posterior defined within the Bayesian framework. To harness the\ngenerative power of DMs while avoiding such approximations, we propose a Markov\nchain Monte Carlo algorithm that performs posterior sampling for general\ninverse problems by reducing it to sampling the posterior of a Gaussian\ndenoising problem. Crucially, we leverage a general DM formulation as a unified\ninterface that allows for rigorously solving the denoising problem with a range\nof state-of-the-art DMs. We demonstrate the effectiveness of the proposed\nmethod on six inverse problems (three linear and three nonlinear), including a\nreal-world black hole imaging problem. Experimental results indicate that our\nproposed method offers more accurate reconstructions and posterior estimation\ncompared to existing DM-based imaging inverse methods.\n","authors":["Zihui Wu","Yu Sun","Yifan Chen","Bingliang Zhang","Yisong Yue","Katherine L. Bouman"],"pdf_url":"https://arxiv.org/pdf/2405.18782v2.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.04357v1","updated":"2024-11-07T01:30:30Z","published":"2024-11-07T01:30:30Z","title":"MegaPortrait: Revisiting Diffusion Control for High-fidelity Portrait\n  Generation","summary":"  We propose MegaPortrait. It's an innovative system for creating personalized\nportrait images in computer vision. It has three modules: Identity Net, Shading\nNet, and Harmonization Net. Identity Net generates learned identity using a\ncustomized model fine-tuned with source images. Shading Net re-renders\nportraits using extracted representations. Harmonization Net fuses pasted faces\nand the reference image's body for coherent results. Our approach with\noff-the-shelf Controlnets is better than state-of-the-art AI portrait products\nin identity preservation and image fidelity. MegaPortrait has a simple but\neffective design and we compare it with other methods and products to show its\nsuperiority.\n","authors":["Han Yang","Sotiris Anagnostidis","Enis Simsar","Thomas Hofmann"],"pdf_url":"https://arxiv.org/pdf/2411.04357v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2406.05768v6","updated":"2024-11-07T01:29:26Z","published":"2024-06-09T12:55:50Z","title":"TLCM: Training-efficient Latent Consistency Model for Image Generation\n  with 2-8 Steps","summary":"  Distilling latent diffusion models (LDMs) into ones that are fast to sample\nfrom is attracting growing research interest. However, the majority of existing\nmethods face two critical challenges: (1) They hinge on long training using a\nhuge volume of real data. (2) They routinely lead to quality degradation for\ngeneration, especially in text-image alignment. This paper proposes a novel\ntraining-efficient Latent Consistency Model (TLCM) to overcome these\nchallenges. Our method first accelerates LDMs via data-free multistep latent\nconsistency distillation (MLCD), and then data-free latent consistency\ndistillation is proposed to efficiently guarantee the inter-segment consistency\nin MLCD. Furthermore, we introduce bags of techniques, e.g., distribution\nmatching, adversarial learning, and preference learning, to enhance TLCM's\nperformance at few-step inference without any real data. TLCM demonstrates a\nhigh level of flexibility by enabling adjustment of sampling steps within the\nrange of 2 to 8 while still producing competitive outputs compared to full-step\napproaches. Notably, TLCM enjoys the data-free merit by employing synthetic\ndata from the teacher for distillation. With just 70 training hours on an A100\nGPU, a 3-step TLCM distilled from SDXL achieves an impressive CLIP Score of\n33.68 and an Aesthetic Score of 5.97 on the MSCOCO-2017 5K benchmark,\nsurpassing various accelerated models and even outperforming the teacher model\nin human preference metrics. We also demonstrate the versatility of TLCMs in\napplications including image style transfer, controllable generation, and\nChinese-to-image generation.\n","authors":["Qingsong Xie","Zhenyi Liao","Zhijie Deng","Chen chen","Haonan Lu"],"pdf_url":"https://arxiv.org/pdf/2406.05768v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06633v2","updated":"2024-11-07T01:14:29Z","published":"2024-07-09T07:59:34Z","title":"Variational Zero-shot Multispectral Pansharpening","summary":"  Pansharpening aims to generate a high spatial resolution multispectral image\n(HRMS) by fusing a low spatial resolution multispectral image (LRMS) and a\npanchromatic image (PAN). The most challenging issue for this task is that only\nthe to-be-fused LRMS and PAN are available, and the existing deep\nlearning-based methods are unsuitable since they rely on many training pairs.\nTraditional variational optimization (VO) based methods are well-suited for\naddressing such a problem. They focus on carefully designing explicit fusion\nrules as well as regularizations for an optimization problem, which are based\non the researcher's discovery of the image relationships and image structures.\nUnlike previous VO-based methods, in this work, we explore such complex\nrelationships by a parameterized term rather than a manually designed one.\nSpecifically, we propose a zero-shot pansharpening method by introducing a\nneural network into the optimization objective. This network estimates a\nrepresentation component of HRMS, which mainly describes the relationship\nbetween HRMS and PAN. In this way, the network achieves a similar goal to the\nso-called deep image prior because it implicitly regulates the relationship\nbetween the HRMS and PAN images through its inherent structure. We directly\nminimize this optimization objective via network parameters and the expected\nHRMS image through iterative updating. Extensive experiments on various\nbenchmark datasets demonstrate that our proposed method can achieve better\nperformance compared with other state-of-the-art methods. The codes are\navailable at https://github.com/xyrui/PSDip.\n","authors":["Xiangyu Rui","Xiangyong Cao","Yining Li","Deyu Meng"],"pdf_url":"https://arxiv.org/pdf/2407.06633v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04351v1","updated":"2024-11-07T01:12:01Z","published":"2024-11-07T01:12:01Z","title":"LidaRefer: Outdoor 3D Visual Grounding for Autonomous Driving with\n  Transformers","summary":"  3D visual grounding (VG) aims to locate relevant objects or regions within 3D\nscenes based on natural language descriptions. Although recent methods for\nindoor 3D VG have successfully transformer-based architectures to capture\nglobal contextual information and enable fine-grained cross-modal fusion, they\nare unsuitable for outdoor environments due to differences in the distribution\nof point clouds between indoor and outdoor settings. Specifically, first,\nextensive LiDAR point clouds demand unacceptable computational and memory\nresources within transformers due to the high-dimensional visual features.\nSecond, dominant background points and empty spaces in sparse LiDAR point\nclouds complicate cross-modal fusion owing to their irrelevant visual\ninformation. To address these challenges, we propose LidaRefer, a\ntransformer-based 3D VG framework designed for large-scale outdoor scenes.\nMoreover, during training, we introduce a simple and effective localization\nmethod, which supervises the decoder's queries to localize not only a target\nobject but also ambiguous objects that might be confused as the target due to\nthe exhibition of similar attributes in a scene or the incorrect understanding\nof a language description. This supervision enhances the model's ability to\ndistinguish ambiguous objects from a target by learning the differences in\ntheir spatial relationships and attributes. LidaRefer achieves state-of-the-art\nperformance on Talk2Car-3D, a 3D VG dataset for autonomous driving, with\nsignificant improvements under various evaluation settings.\n","authors":["Yeong-Seung Baek","Heung-Seon Oh"],"pdf_url":"https://arxiv.org/pdf/2411.04351v1.pdf","comment":"16 pages, 5 figures"},{"id":"http://arxiv.org/abs/2411.04348v1","updated":"2024-11-07T01:10:05Z","published":"2024-11-07T01:10:05Z","title":"UEVAVD: A Dataset for Developing UAV's Eye View Active Object Detection","summary":"  Occlusion is a longstanding difficulty that challenges the UAV-based object\ndetection. Many works address this problem by adapting the detection model.\nHowever, few of them exploit that the UAV could fundamentally improve detection\nperformance by changing its viewpoint. Active Object Detection (AOD) offers an\neffective way to achieve this purpose. Through Deep Reinforcement Learning\n(DRL), AOD endows the UAV with the ability of autonomous path planning to\nsearch for the observation that is more conducive to target identification.\nUnfortunately, there exists no available dataset for developing the UAV AOD\nmethod. To fill this gap, we released a UAV's eye view active vision dataset\nnamed UEVAVD and hope it can facilitate research on the UAV AOD problem.\nAdditionally, we improve the existing DRL-based AOD method by incorporating the\ninductive bias when learning the state representation. First, due to the\npartial observability, we use the gated recurrent unit to extract state\nrepresentations from the observation sequence instead of the single-view\nobservation. Second, we pre-decompose the scene with the Segment Anything Model\n(SAM) and filter out the irrelevant information with the derived masks. With\nthese practices, the agent could learn an active viewing policy with better\ngeneralization capability. The effectiveness of our innovations is validated by\nthe experiments on the UEVAVD dataset. Our dataset will soon be available at\nhttps://github.com/Leo000ooo/UEVAVD_dataset.\n","authors":["Xinhua Jiang","Tianpeng Liu","Li Liu","Zhen Liu","Yongxiang Liu"],"pdf_url":"https://arxiv.org/pdf/2411.04348v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21739v2","updated":"2024-11-07T00:37:50Z","published":"2024-10-29T04:54:45Z","title":"SS3DM: Benchmarking Street-View Surface Reconstruction with a Synthetic\n  3D Mesh Dataset","summary":"  Reconstructing accurate 3D surfaces for street-view scenarios is crucial for\napplications such as digital entertainment and autonomous driving simulation.\nHowever, existing street-view datasets, including KITTI, Waymo, and nuScenes,\nonly offer noisy LiDAR points as ground-truth data for geometric evaluation of\nreconstructed surfaces. These geometric ground-truths often lack the necessary\nprecision to evaluate surface positions and do not provide data for assessing\nsurface normals. To overcome these challenges, we introduce the SS3DM dataset,\ncomprising precise \\textbf{S}ynthetic \\textbf{S}treet-view \\textbf{3D}\n\\textbf{M}esh models exported from the CARLA simulator. These mesh models\nfacilitate accurate position evaluation and include normal vectors for\nevaluating surface normal. To simulate the input data in realistic driving\nscenarios for 3D reconstruction, we virtually drive a vehicle equipped with six\nRGB cameras and five LiDAR sensors in diverse outdoor scenes. Leveraging this\ndataset, we establish a benchmark for state-of-the-art surface reconstruction\nmethods, providing a comprehensive evaluation of the associated challenges.\n  For more information, visit our homepage at https://ss3dm.top.\n","authors":["Yubin Hu","Kairui Wen","Heng Zhou","Xiaoyang Guo","Yong-Jin Liu"],"pdf_url":"https://arxiv.org/pdf/2410.21739v2.pdf","comment":"NeurIPS 2024, Track on Datasets and Benchmarks"},{"id":"http://arxiv.org/abs/2411.04335v1","updated":"2024-11-07T00:22:38Z","published":"2024-11-07T00:22:38Z","title":"GazeGen: Gaze-Driven User Interaction for Visual Content Generation","summary":"  We present GazeGen, a user interaction system that generates visual content\n(images and videos) for locations indicated by the user's eye gaze. GazeGen\nallows intuitive manipulation of visual content by targeting regions of\ninterest with gaze. Using advanced techniques in object detection and\ngenerative AI, GazeGen performs gaze-controlled image adding/deleting,\nrepositioning, and surface material changes of image objects, and converts\nstatic images into videos. Central to GazeGen is the DFT Gaze (Distilled and\nFine-Tuned Gaze) agent, an ultra-lightweight model with only 281K parameters,\nperforming accurate real-time gaze predictions tailored to individual users'\neyes on small edge devices. GazeGen is the first system to combine visual\ncontent generation with real-time gaze estimation, made possible exclusively by\nDFT Gaze. This real-time gaze estimation enables various visual content\ngeneration tasks, all controlled by the user's gaze. The input for DFT Gaze is\nthe user's eye images, while the inputs for visual content generation are the\nuser's view and the predicted gaze point from DFT Gaze. To achieve efficient\ngaze predictions, we derive the small model from a large model (10x larger) via\nnovel knowledge distillation and personal adaptation techniques. We integrate\nknowledge distillation with a masked autoencoder, developing a compact yet\npowerful gaze estimation model. This model is further fine-tuned with Adapters,\nenabling highly accurate and personalized gaze predictions with minimal user\ninput. DFT Gaze ensures low-latency and precise gaze tracking, supporting a\nwide range of gaze-driven tasks. We validate the performance of DFT Gaze on AEA\nand OpenEDS2020 benchmarks, demonstrating low angular gaze error and low\nlatency on the edge device (Raspberry Pi 4). Furthermore, we describe\napplications of GazeGen, illustrating its versatility and effectiveness in\nvarious usage scenarios.\n","authors":["He-Yen Hsieh","Ziyun Li","Sai Qian Zhang","Wei-Te Mark Ting","Kao-Den Chang","Barbara De Salvo","Chiao Liu","H. T. Kung"],"pdf_url":"https://arxiv.org/pdf/2411.04335v1.pdf","comment":"13 pages, 10 figures"},{"id":"http://arxiv.org/abs/2411.04332v1","updated":"2024-11-07T00:14:39Z","published":"2024-11-07T00:14:39Z","title":"HandCraft: Anatomically Correct Restoration of Malformed Hands in\n  Diffusion Generated Images","summary":"  Generative text-to-image models, such as Stable Diffusion, have demonstrated\na remarkable ability to generate diverse, high-quality images. However, they\nare surprisingly inept when it comes to rendering human hands, which are often\nanatomically incorrect or reside in the \"uncanny valley\". In this paper, we\npropose a method HandCraft for restoring such malformed hands. This is achieved\nby automatically constructing masks and depth images for hands as conditioning\nsignals using a parametric model, allowing a diffusion-based image editor to\nfix the hand's anatomy and adjust its pose while seamlessly integrating the\nchanges into the original image, preserving pose, color, and style. Our\nplug-and-play hand restoration solution is compatible with existing pretrained\ndiffusion models, and the restoration process facilitates adoption by eschewing\nany fine-tuning or training requirements for the diffusion models. We also\ncontribute MalHand datasets that contain generated images with a wide variety\nof malformed hands in several styles for hand detector training and hand\nrestoration benchmarking, and demonstrate through qualitative and quantitative\nevaluation that HandCraft not only restores anatomical correctness but also\nmaintains the integrity of the overall image.\n","authors":["Zhenyue Qin","Yiqun Zhang","Yang Liu","Dylan Campbell"],"pdf_url":"https://arxiv.org/pdf/2411.04332v1.pdf","comment":"Accepted by WACV 2025"}]}}