<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2024-11-07T00:00:00Z">2024-11-07</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">89</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analyzing The Language of Visual Tokens 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05001v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05001v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David M. Chan, Rodolfo Corona, Joonyong Park, Cheol Jun Cho, Yutong Bai, Trevor Darrell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the introduction of transformer-based models for vision and language
tasks, such as LLaVA and Chameleon, there has been renewed interest in the
discrete tokenized representation of images. These models often treat image
patches as discrete tokens, analogous to words in natural language, learning
joint alignments between visual and human languages. However, little is known
about the statistical behavior of these visual languages - whether they follow
similar frequency distributions, grammatical structures, or topologies as
natural languages. In this paper, we take a natural-language-centric approach
to analyzing discrete visual languages and uncover striking similarities and
fundamental differences. We demonstrate that, although visual languages adhere
to Zipfian distributions, higher token innovation drives greater entropy and
lower compression, with tokens predominantly representing object parts,
indicating intermediate granularity. We also show that visual languages lack
cohesive grammatical structures, leading to higher perplexity and weaker
hierarchical organization compared to natural languages. Finally, we
demonstrate that, while vision models align more closely with natural languages
than other models, this alignment remains significantly weaker than the
cohesion found within natural languages. Through these experiments, we
demonstrate how understanding the statistical properties of discrete visual
languages can inform the design of more effective computer vision models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Needle Threading: Can LLMs Follow Threads through Near-Million-Scale
  Haystacks? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05000v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05000v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Roberts, Kai Han, Samuel Albanie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the context limits of Large Language Models (LLMs) increase, the range of
possible applications and downstream functions broadens. In many real-world
tasks, decisions depend on details scattered across collections of often
disparate documents containing mostly irrelevant information. Long-context LLMs
appear well-suited to this form of complex information retrieval and reasoning,
which has traditionally proven costly and time-consuming. However, although the
development of longer context models has seen rapid gains in recent years, our
understanding of how effectively LLMs use their context has not kept pace. To
address this, we conduct a set of retrieval experiments designed to evaluate
the capabilities of 17 leading LLMs, such as their ability to follow threads of
information through the context window. Strikingly, we find that many models
are remarkably threadsafe: capable of simultaneously following multiple threads
without significant loss in performance. Still, for many models, we find the
effective context limit is significantly shorter than the supported context
length, with accuracy decreasing as the context window grows. Our study also
highlights the important point that token counts from different tokenizers
should not be directly compared -- they often correspond to substantially
different numbers of written characters. We release our code and long-context
experimental data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM2<span class="highlight-title">CLIP</span>: Powerful Language Model Unlock Richer Visual Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04997v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04997v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiquan Huang, Aoqi Wu, Yifan Yang, Xufang Luo, Yuqing Yang, Liang Hu, Qi Dai, Xiyang Dai, Dongdong Chen, Chong Luo, Lili Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  CLIP is one of the most important multimodal foundational models today. What
powers CLIP's capabilities? The rich supervision signals provided by natural
language, the carrier of human knowledge, shape a powerful cross-modal
representation space. However, with the rapid advancements in large language
models LLMs like GPT-4 and LLaMA, the boundaries of language comprehension and
generation are continually being pushed. This raises an intriguing question:
can the capabilities of LLMs be harnessed to further improve multimodal
representation learning? The potential benefits of incorporating LLMs into CLIP
are clear. LLMs' strong textual understanding can fundamentally improve CLIP's
ability to handle image captions, drastically enhancing its ability to process
long and complex texts, a well-known limitation of vanilla CLIP. Moreover, LLMs
are trained on a vast corpus of text, possessing open-world knowledge. This
allows them to expand on caption information during training, increasing the
efficiency of the learning process. In this paper, we propose LLM2CLIP, a novel
approach that embraces the power of LLMs to unlock CLIP's potential. By
fine-tuning the LLM in the caption space with contrastive learning, we extract
its textual capabilities into the output embeddings, significantly improving
the output layer's textual discriminability. We then design an efficient
training process where the fine-tuned LLM acts as a powerful teacher for CLIP's
visual encoder. Thanks to the LLM's presence, we can now incorporate longer and
more complex captions without being restricted by vanilla CLIP's text encoder's
context window and ability limitations. Our experiments demonstrate that this
approach brings substantial improvements in cross-modal tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mixture-of-Transformers: A Sparse and Scalable Architecture for
  <span class="highlight-title">Multi-Modal</span> Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04996v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04996v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weixin Liang, Lili Yu, Liang Luo, Srinivasan Iyer, Ning Dong, Chunting Zhou, Gargi Ghosh, Mike Lewis, Wen-tau Yih, Luke Zettlemoyer, Xi Victoria Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of large language models (LLMs) has expanded to multi-modal
systems capable of processing text, images, and speech within a unified
framework. Training these models demands significantly larger datasets and
computational resources compared to text-only LLMs. To address the scaling
challenges, we introduce Mixture-of-Transformers (MoT), a sparse multi-modal
transformer architecture that significantly reduces pretraining computational
costs. MoT decouples non-embedding parameters of the model by modality --
including feed-forward networks, attention matrices, and layer normalization --
enabling modality-specific processing with global self-attention over the full
input sequence. We evaluate MoT across multiple settings and model scales. In
the Chameleon 7B setting (autoregressive text-and-image generation), MoT
matches the dense baseline's performance using only 55.8\% of the FLOPs. When
extended to include speech, MoT reaches speech performance comparable to the
dense baseline with only 37.2\% of the FLOPs. In the Transfusion setting, where
text and image are trained with different objectives, a 7B MoT model matches
the image modality performance of the dense baseline with one third of the
FLOPs, and a 760M MoT model outperforms a 1.4B dense baseline across key image
generation metrics. System profiling further highlights MoT's practical
benefits, achieving dense baseline image quality in 47.2\% of the wall-clock
time and text quality in 75.6\% of the wall-clock time (measured on AWS
p4de.24xlarge instances with NVIDIA A100 GPUs).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Semantic Hub Hypothesis: Language Models Share Semantic
  Representations Across Languages and Modalities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04986v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04986v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaofeng Wu, Xinyan Velocity Yu, Dani Yogatama, Jiasen Lu, Yoon Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern language models can process inputs across diverse languages and
modalities. We hypothesize that models acquire this capability through learning
a shared representation space across heterogeneous data types (e.g., different
languages and modalities), which places semantically similar inputs near one
another, even if they are from different modalities/languages. We term this the
semantic hub hypothesis, following the hub-and-spoke model from neuroscience
(Patterson et al., 2007) which posits that semantic knowledge in the human
brain is organized through a transmodal semantic "hub" which integrates
information from various modality-specific "spokes" regions. We first show that
model representations for semantically equivalent inputs in different languages
are similar in the intermediate layers, and that this space can be interpreted
using the model's dominant pretraining language via the logit lens. This
tendency extends to other data types, including arithmetic expressions, code,
and visual/audio inputs. Interventions in the shared representation space in
one data type also predictably affect model outputs in other data types,
suggesting that this shared representations space is not simply a vestigial
byproduct of large-scale training on broad data, but something that is actively
utilized by the model during input processing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SuffixDecoding: A Model-Free Approach to Speeding Up Large Language
  Model Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04975v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04975v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriele Oliaro, Zhihao Jia, Daniel Campos, Aurick Qiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present SuffixDecoding, a novel model-free approach to accelerating large
language model (LLM) inference through speculative decoding. Unlike existing
methods that rely on draft models or specialized decoding heads, SuffixDecoding
leverages suffix trees built from previously generated outputs to efficiently
predict candidate token sequences. Our approach enables flexible
tree-structured speculation without the overhead of maintaining and
orchestrating additional models. SuffixDecoding builds and dynamically updates
suffix trees to capture patterns in the generated text, using them to construct
speculation trees through a principled scoring mechanism based on empirical
token frequencies. SuffixDecoding requires only CPU memory which is plentiful
and underutilized on typical LLM serving nodes. We demonstrate that
SuffixDecoding achieves competitive speedups compared to model-based approaches
across diverse workloads including open-domain chat, code generation, and
text-to-SQL tasks. For open-ended chat and code generation tasks,
SuffixDecoding achieves up to $1.4\times$ higher output throughput than
SpecInfer and up to $1.1\times$ lower time-per-token (TPOT) latency. For a
proprietary multi-LLM text-to-SQL application, SuffixDecoding achieves up to
$2.9\times$ higher output throughput and $3\times$ lower latency than
speculative decoding. Our evaluation shows that SuffixDecoding maintains high
acceptance rates even with small reference corpora of 256 examples, while
continuing to improve performance as more historical outputs are incorporated.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BitNet a4.8: 4-bit Activations for 1-bit LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04965v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04965v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyu Wang, Shuming Ma, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research on the 1-bit Large Language Models (LLMs), such as BitNet
b1.58, presents a promising direction for reducing the inference cost of LLMs
while maintaining their performance. In this work, we introduce BitNet a4.8,
enabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid
quantization and sparsification strategy to mitigate the quantization errors
introduced by the outlier channels. Specifically, we utilize 4-bit activations
for inputs to the attention and feed-forward network layers, while sparsifying
intermediate states followed with 8-bit quantization. Extensive experiments
demonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58
with equivalent training costs, while being faster in inference with enabling
4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of
parameters and supports 3-bit KV cache, further enhancing the efficiency of
large-scale LLM deployment and inference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Position Paper On Diagnostic Uncertainty Estimation from Large Language
  Models: Next-Word Probability Is Not Pre-test Probability <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04962v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04962v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanjun Gao, Skatje Myers, Shan Chen, Dmitriy Dligach, Timothy A Miller, Danielle Bitterman, Guanhua Chen, Anoop Mayampurath, Matthew Churpek, Majid Afshar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are being explored for diagnostic decision
support, yet their ability to estimate pre-test probabilities, vital for
clinical decision-making, remains limited. This study evaluates two LLMs,
Mistral-7B and Llama3-70B, using structured electronic health record data on
three diagnosis tasks. We examined three current methods of extracting LLM
probability estimations and revealed their limitations. We aim to highlight the
need for improved techniques in LLM confidence estimation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to GenAI4Health Workshop at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ M3DocRAG: <span class="highlight-title">Multi-modal</span> Retrieval is What You Need for Multi-page
  Multi-document Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04952v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04952v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaemin Cho, Debanjan Mahata, Ozan Irsoy, Yujie He, Mohit Bansal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Document visual question answering (DocVQA) pipelines that answer questions
from documents have broad applications. Existing methods focus on handling
single-page documents with multi-modal language models (MLMs), or rely on
text-based retrieval-augmented generation (RAG) that uses text extraction tools
such as optical character recognition (OCR). However, there are difficulties in
applying these methods in real-world scenarios: (a) questions often require
information across different pages or documents, where MLMs cannot handle many
long documents; (b) documents often have important information in visual
elements such as figures, but text extraction tools ignore them. We introduce
M3DocRAG, a novel multi-modal RAG framework that flexibly accommodates various
document contexts (closed-domain and open-domain), question hops (single-hop
and multi-hop), and evidence modalities (text, chart, figure, etc.). M3DocRAG
finds relevant documents and answers questions using a multi-modal retriever
and an MLM, so that it can efficiently handle single or many documents while
preserving visual information. Since previous DocVQA datasets ask questions in
the context of a specific document, we also present M3DocVQA, a new benchmark
for evaluating open-domain DocVQA over 3,000+ PDF documents with 40,000+ pages.
In three benchmarks (M3DocVQA/MMLongBench-Doc/MP-DocVQA), empirical results
show that M3DocRAG with ColPali and Qwen2-VL 7B achieves superior performance
than many strong baselines, including state-of-the-art performance in
MP-DocVQA. We provide comprehensive analyses of different indexing, MLMs, and
retrieval models. Lastly, we qualitatively show that M3DocRAG can successfully
handle various scenarios, such as when relevant information exists across
multiple pages and when answer evidence only exists in images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project webpage: https://m3docrag.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Estimating the Influence of Sequentially Correlated Literary Properties
  in Textual Classification: A Data-Centric Hypothesis-Testing Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04950v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04950v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gideon Yoffe, Nachum Dershowitz, Ariel Vishne, Barak Sober
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stylometry aims to distinguish authors by analyzing literary traits assumed
to reflect semi-conscious choices distinct from elements like genre or theme.
However, these components often overlap, complicating text classification based
solely on feature distributions. While some literary properties, such as
thematic content, are likely to manifest as correlations between adjacent text
units, others, like authorial style, may be independent thereof. We introduce a
hypothesis-testing approach to evaluate the influence of sequentially
correlated literary properties on text classification, aiming to determine when
these correlations drive classification. Using a multivariate binary
distribution, our method models sequential correlations between text units as a
stochastic process, assessing the likelihood of clustering across varying
adjacency scales. This enables us to examine whether classification is
dominated by sequentially correlated properties or remains independent. In
experiments on a diverse English prose corpus, our analysis integrates
traditional and neural embeddings within supervised and unsupervised
frameworks. Results demonstrate that our approach effectively identifies when
textual classification is not primarily influenced by sequentially correlated
literary properties, particularly in cases where texts differ in authorial
style or genre rather than by a single author within a similar genre.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GPTKB: Building Very Large Knowledge Bases from Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04920v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04920v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujia Hu, Shrestha Ghosh, Tuan-Phong Nugyen, Simon Razniewski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  General-domain knowledge bases (KB), in particular the "big three" --
Wikidata, Yago and DBpedia -- are the backbone of many intelligent
applications. While these three have seen steady development, comprehensive KB
construction at large has seen few fresh attempts. In this work, we propose to
build a large general-domain KB entirely from a large language model (LLM). We
demonstrate the feasibility of large-scale KB construction from LLMs, while
highlighting specific challenges arising around entity recognition, entity and
property canonicalization, and taxonomy construction. As a prototype, we use
GPT-4o-mini to construct GPTKB, which contains 105 million triples for more
than 2.9 million entities, at a cost 100x less than previous KBC projects. Our
work is a landmark for two fields: For NLP, for the first time, it provides
\textit{constructive} insights into the knowledge (or beliefs) of LLMs. For the
Semantic Web, it shows novel ways forward for the long-standing challenge of
general-domain KB construction. GPTKB is accessible at https://gptkb.org.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GASE: Generatively Augmented Sentence Encoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04914v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04914v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manuel Frank, Haithem Afli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose an approach to enhance sentence embeddings by applying generative
text models for data augmentation at inference time. Unlike conventional data
augmentation that utilises synthetic training data, our approach does not
require access to model parameters or the computational resources typically
required for fine-tuning state-of-the-art models. Generatively Augmented
Sentence Encoding uses diverse linguistic synthetic variants of input texts
generated by paraphrasing, summarising, or extracting keywords, followed by
pooling the original and synthetic embeddings. Experimental results on the
Massive Text Embedding Benchmark for Semantic Textual Similarity (STS)
demonstrate performance improvements across a range of embedding models using
different generative models for augmentation. We find that generative
augmentation leads to larger performance improvements for embedding models with
lower baseline performance. These findings suggest that integrating generative
augmentation at inference time adds semantic diversity and can enhance the
robustness and generalizability of sentence embeddings for embedding models.
Our results show that the degree to which generative augmentation can improve
STS performance depends not only on the embedding model but also on the
dataset. From a broader perspective, the approach allows trading training for
inference compute.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04905v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04905v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siming Huang, Tianhao Cheng, Jason Klein Liu, Jiaran Hao, Liuyihan Song, Yang Xu, J. Yang, J. H. Liu, Chenchen Zhang, Linzheng Chai, Ruifeng Yuan, Zhaoxiang Zhang, Jie Fu, Qian Liu, Ge Zhang, Zili Wang, Yuan Qi, Yinghui Xu, Wei Chu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) for code have become indispensable in various
domains, including code generation, reasoning tasks and agent systems.While
open-access code LLMs are increasingly approaching the performance levels of
proprietary models, high-quality code LLMs suitable for rigorous scientific
investigation, particularly those with reproducible data processing pipelines
and transparent training protocols, remain limited. The scarcity is due to
various challenges, including resource constraints, ethical considerations, and
the competitive advantages of keeping models advanced. To address the gap, we
introduce OpenCoder, a top-tier code LLM that not only achieves performance
comparable to leading models but also serves as an ``open cookbook'' for the
research community. Unlike most prior efforts, we release not only model
weights and inference code, but also the reproducible training data, complete
data processing pipeline, rigorous experimental ablation results, and detailed
training protocols for open scientific research. Through this comprehensive
release, we identify the key ingredients for building a top-tier code LLM: (1)
code optimized heuristic rules for data cleaning and methods for data
deduplication, (2) recall of text corpus related to code and (3) high-quality
synthetic data in both annealing and supervised fine-tuning stages. By offering
this level of openness, we aim to broaden access to all aspects of a top-tier
code LLM, with OpenCoder serving as both a powerful model and an open
foundation to accelerate research, and enable reproducible advancements in code
AI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sentiment Analysis of Spanish Political Party Tweets Using Pre-trained
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04862v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04862v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuqiao Song, Shunzhang Chen, Xinyi Cai, Hao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Title: Sentiment Analysis of Spanish Political Party Communications on
Twitter Using Pre-trained Language Models
  Authors: Chuqiao Song, Shunzhang Chen, Xinyi Cai, Hao Chen
  Comments: 21 pages, 6 figures
  Abstract: This study investigates sentiment patterns within Spanish political
party communications on Twitter by leveraging BETO and RoBERTuito, two
pre-trained language models optimized for Spanish text. Using a dataset of
tweets from major Spanish political parties: PSOE, PP, Vox, Podemos, and
Ciudadanos, spanning 2019 to 2024, this research analyzes sentiment
distributions and explores the relationship between sentiment expression and
party ideology. The findings indicate that both models consistently identify a
predominant Neutral sentiment across all parties, with significant variations
in Negative and Positive sentiments that align with ideological distinctions.
Specifically, Vox exhibits higher levels of Negative sentiment, while PSOE
demonstrates relatively high Positive sentiment, supporting the hypothesis that
emotional appeals in political messaging reflect ideological stances. This
study underscores the potential of pre-trained language models for non-English
sentiment analysis on social media, providing insights into sentiment dynamics
that shape public discourse within Spain's multi-party political system.
  Keywords: Spanish politics, sentiment analysis, pre-trained language models,
Twitter, BETO, RoBERTuito, political ideology, multi-party system
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Prompt</span>-Guided Internal States for Hallucination Detection of Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04847v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04847v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fujie Zhang, Peiqi Yu, Biao Yi, Baolei Zhang, Tong Li, Zheli Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated remarkable capabilities across
a variety of tasks in different domains. However, they sometimes generate
responses that are logically coherent but factually incorrect or misleading,
which is known as LLM hallucinations. Data-driven supervised methods train
hallucination detectors by leveraging the internal states of LLMs, but
detectors trained on specific domains often struggle to generalize well to
other domains. In this paper, we aim to enhance the cross-domain performance of
supervised detectors with only in-domain data. We propose a novel framework,
prompt-guided internal states for hallucination detection of LLMs, namely
PRISM. By utilizing appropriate prompts to guide changes in the structure
related to text truthfulness within the LLM's internal states, we make this
structure more salient and consistent across texts from different domains. We
integrated our framework with existing hallucination detection methods and
conducted experiments on datasets from different domains. The experimental
results indicate that our framework significantly enhances the cross-domain
generalization of existing hallucination detection methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VTechAGP: An Academic-to-General-Audience Text Paraphrase Dataset and
  Benchmark Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04825v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04825v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ming Cheng, Jiaying Gong, Chenhan Yuan, William A. Ingram, Edward Fox, Hoda Eldardiry
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing text simplification or paraphrase datasets mainly focus on
sentence-level text generation in a general domain. These datasets are
typically developed without using domain knowledge. In this paper, we release a
novel dataset, VTechAGP, which is the first academic-to-general-audience text
paraphrase dataset consisting of 4,938 document-level these and dissertation
academic and general-audience abstract pairs from 8 colleges authored over 25
years. We also propose a novel dynamic soft prompt generative language model,
DSPT5. For training, we leverage a contrastive-generative loss function to
learn the keyword vectors in the dynamic prompt. For inference, we adopt a
crowd-sampling decoding strategy at both semantic and structural levels to
further select the best output candidate. We evaluate DSPT5 and various
state-of-the-art large language models (LLMs) from multiple perspectives.
Results demonstrate that the SOTA LLMs does not provide satisfactory outcomes,
while the lightweight DSPT5 can achieve competitive results. To the best of our
knowledge, we are the first to build a benchmark dataset and solutions for
academic-to-general-audience text paraphrase dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When Does Classical Chinese Help? Quantifying Cross-Lingual Transfer in
  Hanja and Kanbun 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04822v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04822v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seyoung Song, Haneul Yoo, Jiho Jin, Kyunghyun Cho, Alice Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Historical and linguistic connections within the Sinosphere have led
researchers to use Classical Chinese resources for cross-lingual transfer when
processing historical documents from Korea and Japan. In this paper, we
question the assumption of cross-lingual transferability from Classical Chinese
to Hanja and Kanbun, the ancient written languages of Korea and Japan,
respectively. Our experiments across machine translation, named entity
recognition, and punctuation restoration tasks show minimal impact of Classical
Chinese datasets on language model performance for ancient Korean documents
written in Hanja, with performance differences within $\pm{}0.0068$ F1-score
for sequence labeling tasks and up to $+0.84$ BLEU score for translation. These
limitations persist consistently across various model sizes, architectures, and
domain-specific datasets. Our analysis reveals that the benefits of Classical
Chinese resources diminish rapidly as local language data increases for Hanja,
while showing substantial improvements only in extremely low-resource scenarios
for both Korean and Japanese historical documents. These mixed results
emphasize the need for careful empirical validation rather than assuming
benefits from indiscriminate cross-lingual transfer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LuxBank: The First Universal Dependency Treebank for Luxembourgish 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04813v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04813v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alistair Plum, Caroline Döhmer, Emilia Milano, Anne-Marie Lutgen, Christoph Purschke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Universal Dependencies (UD) project has significantly expanded linguistic
coverage across 161 languages, yet Luxembourgish, a West Germanic language
spoken by approximately 400,000 people, has remained absent until now. In this
paper, we introduce LuxBank, the first UD Treebank for Luxembourgish,
addressing the gap in syntactic annotation and analysis for this `low-research'
language. We establish formal guidelines for Luxembourgish language annotation,
providing the foundation for the first large-scale quantitative analysis of its
syntax. LuxBank serves not only as a resource for linguists and language
learners but also as a tool for developing spell checkers and grammar checkers,
organising existing text archives and even training large language models. By
incorporating Luxembourgish into the UD framework, we aim to enhance the
understanding of syntactic variation within West Germanic languages and offer a
model for documenting smaller, semi-standardised languages. This work positions
Luxembourgish as a valuable resource in the broader linguistic and NLP
communities, contributing to the study of languages with limited research and
resources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 22nd Workshop on Treebanks and Linguistic Theories (TLT
  2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Kwai-STaR: Transform LLMs into State-Transition Reasoners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04799v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04799v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyu Lu, Yuhang Hu, Changyi Liu, Tianke Zhang, Zhenyu Yang, Zhixiang Ding, Shengsheng Qian, Meng Du, Ruiwen Kang, Kaiyu Tang, Fan Yang, Tingting Gao, Di Zhang, Hai-Tao Zheng, Bin Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mathematical reasoning presents a significant challenge to the cognitive
capabilities of LLMs. Various methods have been proposed to enhance the
mathematical ability of LLMs. However, few recognize the value of state
transition for LLM reasoning. In this work, we define mathematical
problem-solving as a process of transiting from an initial unsolved state to
the final resolved state, and propose Kwai-STaR framework, which transforms
LLMs into State-Transition Reasoners to improve their intuitive reasoning
capabilities. Our approach comprises three main steps: (1) Define the state
space tailored to the mathematical reasoning. (2) Generate state-transition
data based on the state space. (3) Convert original LLMs into State-Transition
Reasoners via a curricular training strategy. Our experiments validate the
effectiveness of Kwai-STaR in enhancing mathematical reasoning: After training
on the small-scale Kwai-STaR dataset, general LLMs, including Mistral-7B and
LLaMA-3, achieve considerable performance gain on the GSM8K and GSM-Hard
dataset. Additionally, the state transition-based design endows Kwai-STaR with
remarkable training and inference efficiency. Further experiments are underway
to establish the generality of Kwai-STaR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AlignXIE: Improving Multilingual Information Extraction by Cross-Lingual
  Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04794v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04794v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Zuo, Wenxuan Jiang, Wenxuan Liu, Zixuan Li, Long Bai, Hanbin Wang, Yutao Zeng, Xiaolong Jin, Jiafeng Guo, Xueqi Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Empirical evidence suggests that LLMs exhibit spontaneous cross-lingual
alignment. Our findings suggest that although LLMs also demonstrate promising
cross-lingual alignment in Information Extraction, there remains significant
imbalance across languages, revealing an underlying deficiency in the IE
alignment. To address this issue, we propose AlignXIE, a powerful code-based
LLM that significantly enhances cross-lingual IE alignment through two
strategies. Firstly, AlignXIE formulates IE across different languages,
especially non-English ones, as code generation tasks, standardizing the
representation of various schemas using Python classes to ensure consistency of
the same ontology in different languages and align the schema. Secondly, it
incorporates an IE cross-lingual alignment phase through a translated instance
prediction task proposed in this paper to align the extraction process,
utilizing ParallelNER, an IE bilingual parallel dataset with 257,190 samples,
generated by our proposed LLM-based automatic pipeline for IE parallel data
construction, with manual annotation to ensure quality. Ultimately, we obtain
AlignXIE through multilingual IE instruction tuning. Although without training
in 9 unseen languages, AlignXIE surpasses ChatGPT by $30.17\%$ and SoTA by
$20.03\%$, thereby demonstrating superior cross-lingual IE capabilities.
Comprehensive evaluations on 63 IE benchmarks in Chinese and English under
various settings, demonstrate that AlignXIE significantly enhances
cross-lingual and multilingual IE through boosting the IE alignment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Investment Analysis: Optimizing AI-Agent Collaboration in
  Financial Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04788v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04788v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuewen Han, Neng Wang, Shangkun Che, Hongyang Yang, Kunpeng Zhang, Sean Xin Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, the application of generative artificial intelligence
(GenAI) in financial analysis and investment decision-making has gained
significant attention. However, most existing approaches rely on single-agent
systems, which fail to fully utilize the collaborative potential of multiple AI
agents. In this paper, we propose a novel multi-agent collaboration system
designed to enhance decision-making in financial investment research. The
system incorporates agent groups with both configurable group sizes and
collaboration structures to leverage the strengths of each agent group type. By
utilizing a sub-optimal combination strategy, the system dynamically adapts to
varying market conditions and investment scenarios, optimizing performance
across different tasks. We focus on three sub-tasks: fundamentals, market
sentiment, and risk analysis, by analyzing the 2023 SEC 10-K forms of 30
companies listed on the Dow Jones Index. Our findings reveal significant
performance variations based on the configurations of AI agents for different
tasks. The results demonstrate that our multi-agent collaboration system
outperforms traditional single-agent models, offering improved accuracy,
efficiency, and adaptability in complex financial environments. This study
highlights the potential of multi-agent systems in transforming financial
analysis and investment decision-making by integrating diverse analytical
perspectives.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A study of Vietnamese readability assessing through semantic and
  statistical features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04756v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04756v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hung Tuan Le, Long Truong To, Manh Trong Nguyen, Quyen Nguyen, Trong-Hop Do
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Determining the difficulty of a text involves assessing various textual
features that may impact the reader's text comprehension, yet current research
in Vietnamese has only focused on statistical features. This paper introduces a
new approach that integrates statistical and semantic approaches to assessing
text readability. Our research utilized three distinct datasets: the Vietnamese
Text Readability Dataset (ViRead), OneStopEnglish, and RACE, with the latter
two translated into Vietnamese. Advanced semantic analysis methods were
employed for the semantic aspect using state-of-the-art language models such as
PhoBERT, ViDeBERTa, and ViBERT. In addition, statistical methods were
incorporated to extract syntactic and lexical features of the text. We
conducted experiments using various machine learning models, including Support
Vector Machine (SVM), Random Forest, and Extra Trees and evaluated their
performance using accuracy and F1 score metrics. Our results indicate that a
joint approach that combines semantic and statistical features significantly
enhances the accuracy of readability classification compared to using each
method in isolation. The current study emphasizes the importance of considering
both statistical and semantic aspects for a more accurate assessment of text
difficulty in Vietnamese. This contribution to the field provides insights into
the adaptability of advanced language models in the context of Vietnamese text
readability. It lays the groundwork for future research in this area.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RetrieveGPT: Merging <span class="highlight-title">Prompt</span>s and Mathematical Models for Enhanced
  Code-Mixed Information Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04752v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04752v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aniket Deroy, Subhankar Maity
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Code-mixing, the integration of lexical and grammatical elements from
multiple languages within a single sentence, is a widespread linguistic
phenomenon, particularly prevalent in multilingual societies. In India, social
media users frequently engage in code-mixed conversations using the Roman
script, especially among migrant communities who form online groups to share
relevant local information. This paper focuses on the challenges of extracting
relevant information from code-mixed conversations, specifically within Roman
transliterated Bengali mixed with English. This study presents a novel approach
to address these challenges by developing a mechanism to automatically identify
the most relevant answers from code-mixed conversations. We have experimented
with a dataset comprising of queries and documents from Facebook, and Query
Relevance files (QRels) to aid in this task. Our results demonstrate the
effectiveness of our approach in extracting pertinent information from complex,
code-mixed digital conversations, contributing to the broader field of natural
language processing in multilingual and informal text environments. We use
GPT-3.5 Turbo via prompting alongwith using the sequential nature of relevant
documents to frame a mathematical model which helps to detect relevant
documents corresponding to a query.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at FIRE 2024 (Track: Code-Mixed Information Retrieval from
  Social Media Data)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BhasaAnuvaad: A Speech Translation Dataset for 14 Indian Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04699v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04699v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sparsh Jain, Ashwin Sankar, Devilal Choudhary, Dhairya Suman, Nikhil Narasimhan, Mohammed Safi Ur Rahman Khan, Anoop Kunchukuttan, Mitesh M Khapra, Raj Dabre
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic Speech Translation (AST) datasets for Indian languages remain
critically scarce, with public resources covering fewer than 10 of the 22
official languages. This scarcity has resulted in AST systems for Indian
languages lagging far behind those available for high-resource languages like
English. In this paper, we first evaluate the performance of widely-used AST
systems on Indian languages, identifying notable performance gaps and
challenges. Our findings show that while these systems perform adequately on
read speech, they struggle significantly with spontaneous speech, including
disfluencies like pauses and hesitations. Additionally, there is a striking
absence of systems capable of accurately translating colloquial and informal
language, a key aspect of everyday communication. To this end, we introduce
BhasaAnuvaad, the largest publicly available dataset for AST involving 14
scheduled Indian languages spanning over 44,400 hours and 17M text segments.
BhasaAnuvaad contains data for English speech to Indic text, as well as Indic
speech to English text. This dataset comprises three key categories: (1)
Curated datasets from existing resources, (2) Large-scale web mining, and (3)
Synthetic data generation. By offering this diverse and expansive dataset, we
aim to bridge the resource gap and promote advancements in AST for low-resource
Indian languages, especially in handling spontaneous and informal speech
patterns.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in Progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DISCO: DISCovering Overfittings as Causal Rules for Text Classification
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04649v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04649v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijian Zhang, Vinay Setty, Yumeng Wang, Avishek Anand
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid advancement of neural language models, the deployment of
over-parameterized models has surged, increasing the need for interpretable
explanations comprehensible to human inspectors. Existing post-hoc
interpretability methods, which often focus on unigram features of single input
textual instances, fail to capture the models' decision-making process fully.
Additionally, many methods do not differentiate between decisions based on
spurious correlations and those based on a holistic understanding of the input.
Our paper introduces DISCO, a novel method for discovering global, rule-based
explanations by identifying causal n-gram associations with model predictions.
This method employs a scalable sequence mining technique to extract relevant
text spans from training data, associate them with model predictions, and
conduct causality checks to distill robust rules that elucidate model behavior.
These rules expose potential overfitting and provide insights into misleading
feature combinations. We validate DISCO through extensive testing,
demonstrating its superiority over existing methods in offering comprehensive
insights into complex model behaviors. Our approach successfully identifies all
shortcuts manually introduced into the training data (100% detection rate on
the MultiRC dataset), resulting in an 18.8% regression in model performance --
a capability unmatched by any other method. Furthermore, DISCO supports
interactive explanations, enabling human inspectors to distinguish spurious
causes in the rule-based output. This alleviates the burden of abundant
instance-wise explanations and helps assess the model's risk when encountering
out-of-distribution (OOD) data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hands-On Tutorial: Labeling with LLM and Human-in-the-Loop <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04637v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04637v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ekaterina Artemova, Akim Tsvigun, Dominik Schlechtweg, Natalia Fedorova, Sergei Tilga, Boris Obmoroshev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training and deploying machine learning models relies on a large amount of
human-annotated data. As human labeling becomes increasingly expensive and
time-consuming, recent research has developed multiple strategies to speed up
annotation and reduce costs and human workload: generating synthetic training
data, active learning, and hybrid labeling. This tutorial is oriented toward
practical applications: we will present the basics of each strategy, highlight
their benefits and limitations, and discuss in detail real-life case studies.
Additionally, we will walk through best practices for managing human annotators
and controlling the quality of the final dataset. The tutorial includes a
hands-on workshop, where attendees will be guided in implementing a hybrid
annotation setup. This tutorial is designed for NLP practitioners from both
research and industry backgrounds who are involved in or interested in
optimizing data labeling projects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be presented at COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FASSILA: A Corpus for Algerian Dialect Fake News Detection and Sentiment
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04604v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04604v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amin Abdedaiem, Abdelhalim Hafedh Dahou, Mohamed Amine Cheragui, Brigitte Mathiak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the context of low-resource languages, the Algerian dialect (AD) faces
challenges due to the absence of annotated corpora, hindering its effective
processing, notably in Machine Learning (ML) applications reliant on corpora
for training and assessment. This study outlines the development process of a
specialized corpus for Fake News (FN) detection and sentiment analysis (SA) in
AD called FASSILA. This corpus comprises 10,087 sentences, encompassing over
19,497 unique words in AD, and addresses the significant lack of linguistic
resources in the language and covers seven distinct domains. We propose an
annotation scheme for FN detection and SA, detailing the data collection,
cleaning, and labelling process. Remarkable Inter-Annotator Agreement indicates
that the annotation scheme produces consistent annotations of high quality.
Subsequent classification experiments using BERT-based models and ML models are
presented, demonstrate promising results and highlight avenues for further
research. The dataset is made freely available on GitHub
(https://github.com/amincoding/FASSILA) to facilitate future advancements in
the field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 6 Figuers</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Calibrated Listwise Reranking with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04602v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04602v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruiyang Ren, Yuhao Wang, Kun Zhou, Wayne Xin Zhao, Wenjie Wang, Jing Liu, Ji-Rong Wen, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs), with advanced linguistic capabilities, have
been employed in reranking tasks through a sequence-to-sequence approach. In
this paradigm, multiple passages are reranked in a listwise manner and a
textual reranked permutation is generated. However, due to the limited context
window of LLMs, this reranking paradigm requires a sliding window strategy to
iteratively handle larger candidate sets. This not only increases computational
costs but also restricts the LLM from fully capturing all the comparison
information for all candidates. To address these challenges, we propose a novel
self-calibrated listwise reranking method, which aims to leverage LLMs to
produce global relevance scores for ranking. To achieve it, we first propose
the relevance-aware listwise reranking framework, which incorporates explicit
list-view relevance scores to improve reranking efficiency and enable global
comparison across the entire candidate set. Second, to ensure the comparability
of the computed scores, we propose self-calibrated training that uses
point-view relevance assessments generated internally by the LLM itself to
calibrate the list-view relevance assessments. Extensive experiments and
comprehensive analysis on the BEIR benchmark and TREC Deep Learning Tracks
demonstrate the effectiveness and efficiency of our proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tibyan Corpus: Balanced and Comprehensive Error Coverage Corpus Using
  ChatGPT for Arabic Grammatical Error Correction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04588v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04588v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahlam Alrehili, Areej Alhothali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural language processing (NLP) utilizes text data augmentation to overcome
sample size constraints. Increasing the sample size is a natural and widely
used strategy for alleviating these challenges. In this study, we chose Arabic
to increase the sample size and correct grammatical errors. Arabic is
considered one of the languages with limited resources for grammatical error
correction (GEC). Furthermore, QALB-14 and QALB-15 are the only datasets used
in most Arabic grammatical error correction research, with approximately 20,500
parallel examples, which is considered low compared with other languages.
Therefore, this study aims to develop an Arabic corpus called "Tibyan" for
grammatical error correction using ChatGPT. ChatGPT is used as a data augmenter
tool based on a pair of Arabic sentences containing grammatical errors matched
with a sentence free of errors extracted from Arabic books, called guide
sentences. Multiple steps were involved in establishing our corpus, including
the collection and pre-processing of a pair of Arabic texts from various
sources, such as books and open-access corpora. We then used ChatGPT to
generate a parallel corpus based on the text collected previously, as a guide
for generating sentences with multiple types of errors. By engaging linguistic
experts to review and validate the automatically generated sentences, we
ensured that they were correct and error-free. The corpus was validated and
refined iteratively based on feedback provided by linguistic experts to improve
its accuracy. Finally, we used the Arabic Error Type Annotation tool (ARETA) to
analyze the types of errors in the Tibyan corpus. Our corpus contained 49 of
errors, including seven types: orthography, morphology, syntax, semantics,
punctuation, merge, and split. The Tibyan corpus contains approximately 600 K
tokens.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The State and Fate of Summarization Datasets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04585v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04585v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noam Dahan, Gabriel Stanovsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic summarization has consistently attracted attention, due to its
versatility and wide application in various downstream tasks. Despite its
popularity, we find that annotation efforts have largely been disjointed, and
have lacked common terminology. Consequently, it is challenging to discover
existing resources or identify coherent research directions. To address this,
we survey a large body of work spanning 133 datasets in over 100 languages,
creating a novel ontology covering sample properties, collection methods and
distribution. With this ontology we make key observations, including the lack
in accessible high-quality datasets for low-resource languages, and the field's
over-reliance on the news domain and on automatically collected distant
supervision. Finally, we make available a web interface that allows users to
interact and explore our ontology and dataset collection, as well as a template
for a summarization data card, which can be used to streamline future research
into a more coherent body of work.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multistage <span class="highlight-title">Fine-tuning</span> Strategies for Automatic Speech Recognition in
  Low-resource Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04573v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04573v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leena G Pillai, Kavya Manohar, Basil K Raju, Elizabeth Sherly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel multistage fine-tuning strategy designed to
enhance automatic speech recognition (ASR) performance in low-resource
languages using OpenAI's Whisper model. In this approach we aim to build ASR
model for languages with limited digital resources by sequentially adapting the
model across linguistically similar languages. We experimented this on the
Malasar language, a Dravidian language spoken by approximately ten thousand
people in the Western Ghats of South India. Malasar language faces critical
challenges for technological intervention due to its lack of a native script
and absence of digital or spoken data resources. Working in collaboration with
Wycliffe India and Malasar community members, we created a spoken Malasar
corpus paired with transcription in Tamil script, a closely related major
language. In our approach to build ASR model for Malasar, we first build an
intermediate Tamil ASR, leveraging higher data availability for Tamil annotated
speech. This intermediate model is subsequently fine-tuned on Malasar data,
allowing for more effective ASR adaptation despite limited resources. The
multistage fine-tuning strategy demonstrated significant improvements over
direct fine-tuning on Malasar data alone, achieving a word error rate (WER) of
51.9%, which is 4.5% absolute reduction when compared to the direct fine-tuning
method. Further a WER reduction to 47.3% was achieved through punctuation
removal in post-processing, which addresses formatting inconsistencies that
impact evaluation. Our results underscore the effectiveness of sequential
multistage fine-tuning combined with targeted post-processing as a scalable
strategy for ASR system development in low-resource languages, especially where
linguistic similarities can be leveraged to bridge gaps in training data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pruning Literals for Highly Efficient Explainability at Word Level 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04557v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04557v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohan Kumar Yadav, Bimal Bhattarai, Abhik Jana, Lei Jiao, Seid Muhie Yimam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designing an explainable model becomes crucial now for Natural Language
Processing(NLP) since most of the state-of-the-art machine learning models
provide a limited explanation for the prediction. In the spectrum of an
explainable model, Tsetlin Machine(TM) is promising because of its capability
of providing word-level explanation using proposition logic. However, concern
rises over the elaborated combination of literals (propositional logic) in the
clause that makes the model difficult for humans to comprehend, despite having
a transparent learning process. In this paper, we design a post-hoc pruning of
clauses that eliminate the randomly placed literals in the clause thereby
making the model more efficiently interpretable than the vanilla TM.
Experiments on the publicly available YELP-HAT Dataset demonstrate that the
proposed pruned TM's attention map aligns more with the human attention map
than the vanilla TM's attention map. In addition, the pairwise similarity
measure also surpasses the attention map-based neural network models. In terms
of accuracy, the proposed pruning method does not degrade the accuracy
significantly but rather enhances the performance up to 4% to 9% in some test
data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Best Practices for Distilling Large Language Models into BERT for Web
  Search Ranking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04539v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04539v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dezhi Ye, Junwei Hu, Jiabin Fan, Bowen Tian, Jie Liu, Haijin Liang, Jin Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have highlighted the significant potential of Large Language
Models (LLMs) as zero-shot relevance rankers. These methods predominantly
utilize prompt learning to assess the relevance between queries and documents
by generating a ranked list of potential documents. Despite their promise, the
substantial costs associated with LLMs pose a significant challenge for their
direct implementation in commercial search systems. To overcome this barrier
and fully exploit the capabilities of LLMs for text ranking, we explore
techniques to transfer the ranking expertise of LLMs to a more compact model
similar to BERT, using a ranking loss to enable the deployment of less
resource-intensive models. Specifically, we enhance the training of LLMs
through Continued Pre-Training, taking the query as input and the clicked title
and summary as output. We then proceed with supervised fine-tuning of the LLM
using a rank loss, assigning the final token as a representative of the entire
sentence. Given the inherent characteristics of autoregressive language models,
only the final token </s> can encapsulate all preceding tokens. Additionally,
we introduce a hybrid point-wise and margin MSE loss to transfer the ranking
knowledge from LLMs to smaller models like BERT. This method creates a viable
solution for environments with strict resource constraints. Both offline and
online evaluations have confirmed the efficacy of our approach, and our model
has been successfully integrated into a commercial web search engine as of
February 2024.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Arxiv Version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Meta-Reasoning Improves Tool Use in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04535v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04535v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lisa Alazraki, Marek Rei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  External tools help large language models (LLMs) succeed at tasks where they
would otherwise typically fail. In existing frameworks, LLMs learn tool use
either by in-context demonstrations or via full model fine-tuning on annotated
data. As these approaches do not easily scale, a recent trend is to abandon
them in favor of lightweight, parameter-efficient tuning paradigms. These
methods allow quickly alternating between the frozen LLM and its specialised
fine-tuned version, by switching on or off a handful of additional custom
parameters. Hence, we postulate that the generalization ability of the frozen
model can be leveraged to improve tool selection. We present Tool selECTion via
meta-reasONing (TECTON), a two-phase system that first reasons over a task
using a custom fine-tuned LM head and outputs candidate tools. Then, with the
custom head disabled, it meta-reasons (i.e., it reasons over the previous
reasoning process) to make a final choice. We show that TECTON results in
substantial gains - both in-distribution and out-of-distribution - on a range
of math reasoning datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tomato, Tomahto, Tomate: Measuring the Role of Shared Semantics among
  Subwords in Multilingual Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04530v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04530v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Zhang, Jing Lu, Vinh Q. Tran, Tal Schuster, Donald Metzler, Jimmy Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human understanding of language is robust to different word choices as far as
they represent similar semantic concepts. To what extent does our human
intuition transfer to language models, which represent all subwords as distinct
embeddings? In this work, we take an initial step on measuring the role of
shared semantics among subwords in the encoder-only multilingual language
models (mLMs). To this end, we form "semantic tokens" by merging the
semantically similar subwords and their embeddings, and evaluate the updated
mLMs on 5 heterogeneous multilingual downstream tasks. Results show that the
general shared semantics could get the models a long way in making the
predictions on mLMs with different tokenizers and model sizes. Inspections on
the grouped subwords show that they exhibit a wide range of semantic
similarities, including synonyms and translations across many languages and
scripts. Lastly, we found the zero-shot results with semantic tokens are on par
or even better than the original models on certain classification tasks,
suggesting that the shared subword-level semantics may serve as the anchors for
cross-lingual transferring.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Thanos: Enhancing Conversational Agents with Skill-of-Mind-Infused Large
  Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04496v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04496v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Young-Jun Lee, Dokyong Lee, Junyoung Youn, Kyeongjin Oh, Ho-Jin Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To increase social bonding with interlocutors, humans naturally acquire the
ability to respond appropriately in a given situation by considering which
conversational skill is most suitable for the response - a process we call
skill-of-mind. For large language model (LLM)-based conversational agents,
planning appropriate conversational skills, as humans do, is challenging due to
the complexity of social dialogue, especially in interactive scenarios. To
address this, we propose a skill-of-mind-annotated conversation dataset, named
Multifaceted Skill-of-Mind, which includes multi-turn and multifaceted
conversational skills across various interactive scenarios (e.g., long-term,
counseling, task-oriented), grounded in diverse social contexts (e.g.,
demographics, persona, rules of thumb). This dataset consists of roughly 100K
conversations. Using this dataset, we introduce a new family of
skill-of-mind-infused LLMs, named Thanos, with model sizes of 1B, 3B, and 8B
parameters. With extensive experiments, these models successfully demonstrate
the skill-of-mind process and exhibit strong generalizability in inferring
multifaceted skills across a variety of domains. Moreover, we show that Thanos
significantly enhances the quality of responses generated by LLM-based
conversational agents and promotes prosocial behavior in human evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/passing2961/Thanos</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ML-Promise: A Multilingual Dataset for Corporate Promise Verification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04473v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04473v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yohei Seki, Hakusen Shu, Anaïs Lhuissier, Hanwool Lee, Juyeon Kang, Min-Yuh Day, Chung-Chi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Promises made by politicians, corporate leaders, and public figures have a
significant impact on public perception, trust, and institutional reputation.
However, the complexity and volume of such commitments, coupled with
difficulties in verifying their fulfillment, necessitate innovative methods for
assessing their credibility. This paper introduces the concept of Promise
Verification, a systematic approach involving steps such as promise
identification, evidence assessment, and the evaluation of timing for
verification. We propose the first multilingual dataset, ML-Promise, which
includes English, French, Chinese, Japanese, and Korean, aimed at facilitating
in-depth verification of promises, particularly in the context of
Environmental, Social, and Governance (ESG) reports. Given the growing emphasis
on corporate environmental contributions, this dataset addresses the challenge
of evaluating corporate promises, especially in light of practices like
greenwashing. Our findings also explore textual and image-based baselines, with
promising results from retrieval-augmented generation (RAG) approaches. This
work aims to foster further discourse on the accountability of public
commitments across multiple languages and domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gradient Localization Improves Lifelong Pretraining of Language Models <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04448v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04448v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jared Fernandez, Yonatan Bisk, Emma Strubell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) trained on web-scale text corpora have been
shown to capture world knowledge in their parameters. However, the mechanism by
which language models store different types of knowledge is poorly understood.
In this work, we examine two types of knowledge relating to temporally
sensitive entities and demonstrate that each type is localized to different
sets of parameters within the LLMs. We hypothesize that the lack of
consideration of the locality of knowledge in existing continual learning
methods contributes to both: the failed uptake of new information, and
catastrophic forgetting of previously learned information. We observe that
sequences containing references to updated and newly mentioned entities exhibit
larger gradient norms in a subset of layers. We demonstrate that targeting
parameter updates to these relevant layers can improve the performance of
continually pretraining on language containing temporal drift.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP Findings 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ACCIO: Table Understanding Enhanced via Contrastive Learning with
  Aggregations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04443v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04443v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Whanhee Cho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The attention to table understanding using recent natural language models has
been growing. However, most related works tend to focus on learning the
structure of the table directly. Just as humans improve their understanding of
sentences by comparing them, they can also enhance their understanding by
comparing tables. With this idea, in this paper, we introduce ACCIO, tAble
understanding enhanCed via Contrastive learnIng with aggregatiOns, a novel
approach to enhancing table understanding by contrasting original tables with
their pivot summaries through contrastive learning. ACCIO trains an encoder to
bring these table pairs closer together. Through validation via column type
annotation, ACCIO achieves competitive performance with a macro F1 score of
91.1 compared to state-of-the-art methods. This work represents the first
attempt to utilize pairs of tables for table embedding, promising significant
advancements in table comprehension. Our code is available at
https://github.com/whnhch/ACCIO/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ One fish, two fish, but not the whole sea: Alignment reduces language
  models' conceptual diversity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04427v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04427v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sonia K. Murthy, Tomer Ullman, Jennifer Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Researchers in social science and psychology have recently proposed using
large language models (LLMs) as replacements for humans in behavioral research.
In addition to arguments about whether LLMs accurately capture population-level
patterns, this has raised questions about whether LLMs capture human-like
conceptual diversity. Separately, it is debated whether post-training alignment
(RLHF or RLAIF) affects models' internal diversity. Inspired by human studies,
we use a new way of measuring the conceptual diversity of
synthetically-generated LLM "populations" by relating the internal variability
of simulated individuals to the population-level variability. We use this
approach to evaluate non-aligned and aligned LLMs on two domains with rich
human behavioral data. While no model reaches human-like diversity, aligned
models generally display less diversity than their instruction fine-tuned
counterparts. Our findings highlight potential trade-offs between increasing
models' value alignment and decreasing the diversity of their conceptual
representations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DELIFT: Data Efficient Language model Instruction Fine <span class="highlight-title">Tuning</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ishika Agarwal, Krishna Killamsetty, Lucian Popa, Marina Danilevksy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning large language models (LLMs) is essential for enhancing their
performance on specific tasks but is often resource-intensive due to redundant
or uninformative data. To address this inefficiency, we introduce DELIFT (Data
Efficient Language model Instruction Fine-Tuning), a novel algorithm that
systematically optimizes data selection across the three key stages of
fine-tuning: (1) instruction tuning, (2) task-specific fine-tuning (e.g.,
reasoning, question-answering), and (3) continual fine-tuning (e.g.,
incorporating new data versions). Unlike existing methods that focus on
single-stage optimization or rely on computationally intensive gradient
calculations, DELIFT operates efficiently across all stages. Central to our
approach is a pairwise utility metric that quantifies how beneficial a data
sample is for improving the model's responses to other samples, effectively
measuring the informational value relative to the model's current capabilities.
By leveraging different submodular functions applied to this metric, DELIFT
selects diverse and optimal subsets that are useful across all stages of
fine-tuning. Experiments across various tasks and model scales demonstrate that
DELIFT can reduce the fine-tuning data size by up to 70% without compromising
performance, offering significant computational savings and outperforming
existing methods in both efficiency and efficacy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bayesian Calibration of Win Rate Estimation with LLM Evaluators <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04424v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04424v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yicheng Gao, Gonghan Xu, Zhe Wang, Arman Cohan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in large language models (LLMs) show the potential of using
LLMs as evaluators for assessing the quality of text generations from LLMs.
However, applying LLM evaluators naively to compare or judge between different
systems can lead to unreliable results due to the intrinsic win rate estimation
bias of LLM evaluators. In order to mitigate this problem, we propose two
calibration methods, Bayesian Win Rate Sampling (BWRS) and Bayesian
Dawid-Skene, both of which leverage Bayesian inference to more accurately infer
the true win rate of generative language models. We empirically validate our
methods on six datasets covering story generation, summarization, and
instruction following tasks. We show that both our methods are effective in
improving the accuracy of win rate estimation using LLMs as evaluators,
offering a promising direction for reliable automatic text quality evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Variational Low-Rank Adaptation Using IVON <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04421v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04421v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bai Cong, Nico Daheim, Yuesong Shen, Daniel Cremers, Rio Yokota, Mohammad Emtiyaz Khan, Thomas Möllenhoff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We show that variational learning can significantly improve the accuracy and
calibration of Low-Rank Adaptation (LoRA) without a substantial increase in the
cost. We replace AdamW by the Improved Variational Online Newton (IVON)
algorithm to finetune large language models. For Llama-2 with 7 billion
parameters, IVON improves the accuracy over AdamW by 2.8% and expected
calibration error by 4.6%. The accuracy is also better than the other Bayesian
alternatives, yet the cost is lower and the implementation is easier. Our work
provides additional evidence for the effectiveness of IVON for large language
models. The code is available at
https://github.com/team-approx-bayes/ivon-lora.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at 38th Workshop on Fine-Tuning in Machine Learning
  (NeurIPS 2024). Code available at
  https://github.com/team-approx-bayes/ivon-lora</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Measuring short-form factuality in large language models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04368v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04368v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jason Wei, Nguyen Karina, Hyung Won Chung, Yunxin Joy Jiao, Spencer Papay, Amelia Glaese, John Schulman, William Fedus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present SimpleQA, a benchmark that evaluates the ability of language
models to answer short, fact-seeking questions. We prioritized two properties
in designing this eval. First, SimpleQA is challenging, as it is adversarially
collected against GPT-4 responses. Second, responses are easy to grade, because
questions are created such that there exists only a single, indisputable
answer. Each answer in SimpleQA is graded as either correct, incorrect, or not
attempted. A model with ideal behavior would get as many questions correct as
possible while not attempting the questions for which it is not confident it
knows the correct answer. SimpleQA is a simple, targeted evaluation for whether
models "know what they know," and our hope is that this benchmark will remain
relevant for the next few generations of frontier models. SimpleQA can be found
at https://github.com/openai/simple-evals.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Blog post: https://openai.com/index/introducing-simpleqa/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust and Efficient <span class="highlight-title">Fine-tuning</span> of LLMs with Bayesian
  Reparameterization of Low-Rank Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04358v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04358v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vaibhav Seth, Arinjay Pathak, Ayan Sengupta, Natraj Raman, Sriram Gopalakrishnan, Tanmoy Chakraborty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are highly resource-intensive to fine-tune due
to their enormous size. While low-rank adaptation is a prominent
parameter-efficient fine-tuning approach, it suffers from sensitivity to
hyperparameter choices, leading to instability in model performance on
fine-tuning downstream tasks. This paper highlights the importance of effective
parameterization in low-rank fine-tuning to reduce estimator variance and
enhance the stability of final model outputs. We propose MonteCLoRA, an
efficient fine-tuning technique, employing Monte Carlo estimation to learn an
unbiased posterior estimation of low-rank parameters with low expected
variance, which stabilizes fine-tuned LLMs with only O(1) additional
parameters. MonteCLoRA shows significant improvements in accuracy and
robustness, achieving up to 3.8% higher accuracy and 8.6% greater robustness
than existing efficient fine-tuning methods on natural language understanding
tasks with pre-trained RoBERTa-base. Furthermore, in generative tasks with
pre-trained LLaMA-1-7B, MonteCLoRA demonstrates robust zero-shot performance
with 50% lower variance than the contemporary efficient fine-tuning methods.
The theoretical and empirical results presented in the paper underscore how
parameterization and hyperpriors balance exploration-exploitation in the
low-rank parametric space, therefore leading to more optimal and robust
parameter estimation during efficient fine-tuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>48 pages, 10 figures, 10 tables, Code:
  https://github.com/LCS2-IIITD/MonteCLoRA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Laws for Precision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04330v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04330v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tanishq Kumar, Zachary Ankner, Benjamin F. Spector, Blake Bordelon, Niklas Muennighoff, Mansheej Paul, Cengiz Pehlevan, Christopher Ré, Aditi Raghunathan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low precision training and inference affect both the quality and cost of
language models, but current scaling laws do not account for this. In this
work, we devise "precision-aware" scaling laws for both training and inference.
We propose that training in lower precision reduces the model's "effective
parameter count," allowing us to predict the additional loss incurred from
training in low precision and post-train quantization. For inference, we find
that the degradation introduced by post-training quantization increases as
models are trained on more data, eventually making additional pretraining data
actively harmful. For training, our scaling laws allow us to predict the loss
of a model with different parts in different precisions, and suggest that
training larger models in lower precision may be compute optimal. We unify the
scaling laws for post and pretraining quantization to arrive at a single
functional form that predicts degradation from training and inference in varied
precisions. We fit on over 465 pretraining runs and validate our predictions on
model sizes up to 1.7B parameters trained on up to 26B tokens.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CodeTree: Agent-guided Tree Search for Code Generation with Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04329v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04329v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jierui Li, Hung Le, Yinbo Zhou, Caiming Xiong, Silvio Savarese, Doyen Sahoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained on massive amounts of code and text data, large language models
(LLMs) have demonstrated remarkable achievements in performing code generation
tasks. With additional execution-based feedback, these models can act as agents
with capabilities to self-refine and improve generated code autonomously.
However, on challenging coding tasks with extremely large search space, current
agentic approaches still struggle with multi-stage planning, generating, and
debugging. To address this problem, we propose CodeTree, a framework for LLM
agents to efficiently explore the search space in different stages of the code
generation process. Specifically, we adopted a unified tree structure to
explicitly explore different coding strategies, generate corresponding coding
solutions, and subsequently refine the solutions. In each stage, critical
decision-making (ranking, termination, expanding) of the exploration process is
guided by both the environmental execution-based feedback and
LLM-agent-generated feedback. We comprehensively evaluated CodeTree on 7 code
generation benchmarks and demonstrated the significant performance gains of
CodeTree against strong baselines. Using GPT-4o as the base model, we
consistently achieved top results of 95.1 on HumanEval, 98.7 on MBPP, and 43.0
on CodeContests. On the challenging SWEBench benchmark, our approach led to
significant performance gains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Balancing Transparency and Accuracy: A Comparative Analysis of
  Rule-Based and Deep Learning Models in Political Bias Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04328v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04328v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manuel Nunez Martinez, Sonja Schmer-Galunder, Zoey Liu, Sangpil Youm, Chathuri Jayaweera, Bonnie J. Dorr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The unchecked spread of digital information, combined with increasing
political polarization and the tendency of individuals to isolate themselves
from opposing political viewpoints, has driven researchers to develop systems
for automatically detecting political bias in media. This trend has been
further fueled by discussions on social media. We explore methods for
categorizing bias in US news articles, comparing rule-based and deep learning
approaches. The study highlights the sensitivity of modern self-learning
systems to unconstrained data ingestion, while reconsidering the strengths of
traditional rule-based systems. Applying both models to left-leaning (CNN) and
right-leaning (FOX) news articles, we assess their effectiveness on data beyond
the original training and test sets.This analysis highlights each model's
accuracy, offers a framework for exploring deep-learning explainability, and
sheds light on political bias in US news media. We contrast the opaque
architecture of a deep learning model with the transparency of a linguistically
informed rule-based model, showing that the rule-based model performs
consistently across different data conditions and offers greater transparency,
whereas the deep learning model is dependent on the training set and struggles
with unseen data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MediQ: Question-Asking LLMs and a Benchmark for Reliable Interactive
  Clinical Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.00922v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.00922v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuyue Stella Li, Vidhisha Balachandran, Shangbin Feng, Jonathan S. Ilgen, Emma Pierson, Pang Wei Koh, Yulia Tsvetkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Users typically engage with LLMs interactively, yet most existing benchmarks
evaluate them in a static, single-turn format, posing reliability concerns in
interactive scenarios. We identify a key obstacle towards reliability: LLMs are
trained to answer any question, even with incomplete context or insufficient
knowledge. In this paper, we propose to change the static paradigm to an
interactive one, develop systems that proactively ask questions to gather more
information and respond reliably, and introduce an benchmark - MediQ - to
evaluate question-asking ability in LLMs. MediQ simulates clinical interactions
consisting of a Patient System and an adaptive Expert System; with potentially
incomplete initial information, the Expert refrains from making diagnostic
decisions when unconfident, and instead elicits missing details via follow-up
questions. We provide a pipeline to convert single-turn medical benchmarks into
an interactive format. Our results show that directly prompting
state-of-the-art LLMs to ask questions degrades performance, indicating that
adapting LLMs to proactive information-seeking settings is nontrivial. We
experiment with abstention strategies to better estimate model confidence and
decide when to ask questions, improving diagnostic accuracy by 22.3%; however,
performance still lags compared to an (unrealistic in practice) upper bound
with complete information upfront. Further analyses show improved interactive
performance with filtering irrelevant contexts and reformatting conversations.
Overall, we introduce a novel problem towards LLM reliability, an interactive
MediQ benchmark and a novel question-asking system, and highlight directions to
extend LLMs' information-seeking abilities in critical domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Talking the Talk Does Not Entail Walking the Walk: On the Limits of
  Large Language Models in Lexical Entailment Recognition <span class="chip">EMNLP-2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14894v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14894v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Candida M. Greco, Lucio La Cava, Andrea Tagarelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Verbs form the backbone of language, providing the structure and meaning to
sentences. Yet, their intricate semantic nuances pose a longstanding challenge.
Understanding verb relations through the concept of lexical entailment is
crucial for comprehending sentence meanings and grasping verb dynamics. This
work investigates the capabilities of eight Large Language Models in
recognizing lexical entailment relations among verbs through differently
devised prompting strategies and zero-/few-shot settings over verb pairs from
two lexical databases, namely WordNet and HyperLex. Our findings unveil that
the models can tackle the lexical entailment recognition task with moderately
good performance, although at varying degree of effectiveness and under
different conditions. Also, utilizing few-shot prompting can enhance the
models' performance. However, perfectly solving the task arises as an unmet
challenge for all examined LLMs, which raises an emergence for further research
developments on this topic.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at The 2024 Conference on Empirical Methods
  in Natural Language Processing (EMNLP-2024) - Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TinyStyler: Efficient Few-Shot Text Style Transfer with Authorship
  Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15586v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15586v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zachary Horvitz, Ajay Patel, Kanishk Singh, Chris Callison-Burch, Kathleen McKeown, Zhou Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of text style transfer is to transform the style of texts while
preserving their original meaning, often with only a few examples of the target
style. Existing style transfer methods generally rely on the few-shot
capabilities of large language models or on complex controllable text
generation approaches that are inefficient and underperform on fluency metrics.
We introduce TinyStyler, a lightweight but effective approach, which leverages
a small language model (800M params) and pre-trained authorship embeddings to
perform efficient, few-shot text style transfer. We evaluate on the challenging
task of authorship style transfer and find TinyStyler outperforms strong
approaches such as GPT-4. We also evaluate TinyStyler's ability to perform text
attribute style transfer (formal $\leftrightarrow$ informal) with automatic and
human evaluations and find that the approach outperforms recent controllable
text generation methods. Our model has been made publicly available at
https://huggingface.co/tinystyler/tinystyler .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Perceptions of Linguistic Uncertainty by Language Models and Humans <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15814v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15814v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Catarina G Belem, Markelle Kelly, Mark Steyvers, Sameer Singh, Padhraic Smyth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  _Uncertainty expressions_ such as "probably" or "highly unlikely" are
pervasive in human language. While prior work has established that there is
population-level agreement in terms of how humans quantitatively interpret
these expressions, there has been little inquiry into the abilities of language
models in the same context. In this paper, we investigate how language models
map linguistic expressions of uncertainty to numerical responses. Our approach
assesses whether language models can employ theory of mind in this setting:
understanding the uncertainty of another agent about a particular statement,
independently of the model's own certainty about that statement. We find that 7
out of 10 models are able to map uncertainty expressions to probabilistic
responses in a human-like manner. However, we observe systematically different
behavior depending on whether a statement is actually true or false. This
sensitivity indicates that language models are substantially more susceptible
to bias based on their prior knowledge (as compared to humans). These findings
raise important questions and have broad implications for human-AI and AI-AI
communication.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP 2024 (Main)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Rigour of Scientific Writing: Criteria, Analysis, and Insights <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04981v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04981v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joseph James, Chenghao Xiao, Yucheng Li, Chenghua Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rigour is crucial for scientific research as it ensures the reproducibility
and validity of results and findings. Despite its importance, little work
exists on modelling rigour computationally, and there is a lack of analysis on
whether these criteria can effectively signal or measure the rigour of
scientific papers in practice. In this paper, we introduce a bottom-up,
data-driven framework to automatically identify and define rigour criteria and
assess their relevance in scientific writing. Our framework includes rigour
keyword extraction, detailed rigour definition generation, and salient criteria
identification. Furthermore, our framework is domain-agnostic and can be
tailored to the evaluation of scientific rigour for different areas,
accommodating the distinct salient criteria across fields. We conducted
comprehensive experiments based on datasets collected from two high impact
venues for Machine Learning and NLP (i.e., ICLR and ACL) to demonstrate the
effectiveness of our framework in modelling rigour. In addition, we analyse
linguistic patterns of rigour, revealing that framing certainty is crucial for
enhancing the perception of scientific rigour, while suggestion certainty and
probability uncertainty diminish it.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted Findings at EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Personalized Large Language Models <span class="chip">ICDM</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09269v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09269v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stanisław Woźniak, Bartłomiej Koptyra, Arkadiusz Janz, Przemysław Kazienko, Jan Kocoń
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have significantly advanced Natural Language
Processing (NLP) tasks in recent years. However, their universal nature poses
limitations in scenarios requiring personalized responses, such as
recommendation systems and chatbots. This paper investigates methods to
personalize LLMs, comparing fine-tuning and zero-shot reasoning approaches on
subjective tasks. Results demonstrate that personalized fine-tuning improves
model reasoning compared to non-personalized models. Experiments on datasets
for emotion recognition and hate speech detection show consistent performance
gains with personalized methods across different LLM architectures. These
findings underscore the importance of personalization for enhancing LLM
capabilities in subjective text perception tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to SENTIRE 2024 (ICDM Workshops):
  https://sentic.net/sentire2024wozniak.pdf</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FRACTURED-SORRY-Bench: Framework for Revealing Attacks in Conversational
  Turns Undermining Refusal Efficacy and Defenses over SORRY-Bench (Automated
  Multi-shot Jailbreaks) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.16163v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.16163v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aman Priyanshu, Supriti Vijay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces FRACTURED-SORRY-Bench, a framework for evaluating the
safety of Large Language Models (LLMs) against multi-turn conversational
attacks. Building upon the SORRY-Bench dataset, we propose a simple yet
effective method for generating adversarial prompts by breaking down harmful
queries into seemingly innocuous sub-questions. Our approach achieves a maximum
increase of +46.22\% in Attack Success Rates (ASRs) across GPT-4, GPT-4o,
GPT-4o-mini, and GPT-3.5-Turbo models compared to baseline methods. We
demonstrate that this technique poses a challenge to current LLM safety
measures and highlights the need for more robust defenses against subtle,
multi-turn attacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pre-Fine<span class="highlight-title">tuning</span> for Few-Shot Emotional Speech Recognition <span class="chip">INTERSPEECH 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.12921v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.12921v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maximillian Chen, Zhou Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech models have long been known to overfit individual speakers for many
classification tasks. This leads to poor generalization in settings where the
speakers are out-of-domain or out-of-distribution, as is common in production
environments. We view speaker adaptation as a few-shot learning problem and
propose investigating transfer learning approaches inspired by recent success
with pre-trained models in natural language tasks. We propose pre-finetuning
speech models on difficult tasks to distill knowledge into few-shot downstream
classification objectives. We pre-finetune Wav2Vec2.0 on every permutation of
four multiclass emotional speech recognition corpora and evaluate our
pre-finetuned models through 33,600 few-shot fine-tuning trials on the
Emotional Speech Dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at INTERSPEECH 2023. 5 pages, 4 figures. Code available at
  https://github.com/maxlchen/Speech-PreFinetuning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by
  Exploring Refusal Loss Landscapes <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00867v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00867v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaomeng Hu, Pin-Yu Chen, Tsung-Yi Ho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are becoming a prominent generative AI tool,
where the user enters a query and the LLM generates an answer. To reduce harm
and misuse, efforts have been made to align these LLMs to human values using
advanced training techniques such as Reinforcement Learning from Human Feedback
(RLHF). However, recent studies have highlighted the vulnerability of LLMs to
adversarial jailbreak attempts aiming at subverting the embedded safety
guardrails. To address this challenge, this paper defines and investigates the
Refusal Loss of LLMs and then proposes a method called Gradient Cuff to detect
jailbreak attempts. Gradient Cuff exploits the unique properties observed in
the refusal loss landscape, including functional values and its smoothness, to
design an effective two-step detection strategy. Experimental results on two
aligned LLMs (LLaMA-2-7B-Chat and Vicuna-7B-V1.5) and six types of jailbreak
attacks (GCG, AutoDAN, PAIR, TAP, Base64, and LRL) show that Gradient Cuff can
significantly improve the LLM's rejection capability for malicious jailbreak
queries, while maintaining the model's performance for benign user queries by
adjusting the detection threshold.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024. Project page:
  https://huggingface.co/spaces/TrustSafeAI/GradientCuff-Jailbreak-Defense</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SYNTHEVAL: Hybrid Behavioral Testing of NLP Models with Synthetic
  CheckLists <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.17437v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.17437v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raoyuan Zhao, Abdullatif Köksal, Yihong Liu, Leonie Weissweiler, Anna Korhonen, Hinrich Schütze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional benchmarking in NLP typically involves using static held-out test
sets. However, this approach often results in an overestimation of performance
and lacks the ability to offer comprehensive, interpretable, and dynamic
assessments of NLP models. Recently, works like DynaBench (Kiela et al., 2021)
and CheckList (Ribeiro et al., 2020) have addressed these limitations through
behavioral testing of NLP models with test types generated by a multistep
human-annotated pipeline. Unfortunately, manually creating a variety of test
types requires much human labor, often at prohibitive cost. In this work, we
propose SYNTHEVAL, a hybrid behavioral testing framework that leverages large
language models (LLMs) to generate a wide range of test types for a
comprehensive evaluation of NLP models. SYNTHEVAL first generates sentences via
LLMs using controlled generation, and then identifies challenging examples by
comparing the predictions made by LLMs with task-specific NLP models. In the
last stage, human experts investigate the challenging examples, manually design
templates, and identify the types of failures the taskspecific models
consistently exhibit. We apply SYNTHEVAL to two classification tasks, sentiment
analysis and toxic language detection, and show that our framework is effective
in identifying weaknesses of strong models on these tasks. We share our code in
https://github.com/Loreley99/SynthEval_CheckList.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 - Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MEG: Medical Knowledge-Augmented Large Language Models for Question
  Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03883v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03883v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laura Cabello, Carmen Martin-Turrero, Uchenna Akujuobi, Anders Søgaard, Carlos Bobed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Question answering is a natural language understanding task that involves
reasoning over both explicit context and unstated, relevant domain knowledge.
Large language models (LLMs), which underpin most contemporary question
answering systems, struggle to induce how concepts relate in specialized
domains such as medicine. Existing medical LLMs are also costly to train. In
this work, we present MEG, a parameter-efficient approach for medical
knowledge-augmented LLMs. MEG uses a lightweight mapping network to integrate
graph embeddings into the LLM, enabling it to leverage external knowledge in a
cost-effective way. We evaluate our method on four popular medical
multiple-choice datasets and show that LLMs greatly benefit from the factual
grounding provided by knowledge graph embeddings. MEG attains an average of
+10.2% accuracy over the Mistral-Instruct baseline, and +6.7% over specialized
models like BioMistral. We also show results based on Llama-3. Finally, we show
that MEG's performance remains robust to the choice of graph encoder.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MILPaC: A Novel Benchmark for Evaluating Translation of Legal Text to
  Indian Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.09765v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.09765v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sayan Mahapatra, Debtanu Datta, Shubham Soni, Adrijit Goswami, Saptarshi Ghosh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most legal text in the Indian judiciary is written in complex English due to
historical reasons. However, only a small fraction of the Indian population is
comfortable in reading English. Hence legal text needs to be made available in
various Indian languages, possibly by translating the available legal text from
English. Though there has been a lot of research on translation to and between
Indian languages, to our knowledge, there has not been much prior work on such
translation in the legal domain. In this work, we construct the first
high-quality legal parallel corpus containing aligned text units in English and
nine Indian languages, that includes several low-resource languages. We also
benchmark the performance of a wide variety of Machine Translation (MT) systems
over this corpus, including commercial MT systems, open-source MT systems and
Large Language Models. Through a comprehensive survey by Law practitioners, we
check how satisfied they are with the translations by some of these MT systems,
and how well automatic MT evaluation metrics agree with the opinions of Law
practitioners.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in ACM Transactions on Asian and Low-Resource
  Language Information Processing (TALLIP)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic Speculation Lookahead Accelerates Speculative Decoding of Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.04304v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.04304v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Mamou, Oren Pereg, Daniel Korat, Moshe Berchansky, Nadav Timor, Moshe Wasserblat, Roy Schwartz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speculative decoding is commonly used for reducing the inference latency of
large language models. Its effectiveness depends highly on the speculation
lookahead (SL)-the number of tokens generated by the draft model at each
iteration. In this work we show that the common practice of using the same SL
for all iterations (static SL) is suboptimal. We introduce DISCO (DynamIc
SpeCulation lookahead Optimization), a novel method for dynamically selecting
the SL. Our experiments with four datasets show that DISCO reaches an average
speedup of 10% compared to the best static SL baseline, while generating the
exact same text.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Wave Network: An Ultra-Small Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02674v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02674v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Zhang, Victor S. Sheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose an innovative token representation and update method in a new
ultra-small language model: the Wave network. Specifically, we use a complex
vector to represent each token, encoding both global and local semantics of the
input text. A complex vector consists of two components: a magnitude vector
representing the global semantics of the input text, and a phase vector
capturing the relationships between individual tokens and global semantics.
Experiments on the AG News text classification task demonstrate that, when
generating complex vectors from randomly initialized token embeddings, our
single-layer Wave Network achieves 90.91% accuracy with wave interference and
91.66% with wave modulation - outperforming a single Transformer layer using
BERT pre-trained embeddings by 19.23% and 19.98%, respectively, and approaching
the accuracy of the pre-trained and fine-tuned BERT base model (94.64%).
Additionally, compared to BERT base, the Wave Network reduces video memory
usage and training time by 77.34% and 85.62% during wave modulation. In
summary, we used a 2.4-million-parameter small language model to achieve
accuracy comparable to a 100-million-parameter BERT model in text
classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ALI-Agent: Assessing LLMs' Alignment with Human Values via Agent-based
  Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14125v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14125v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingnan Zheng, Han Wang, An Zhang, Tai D. Nguyen, Jun Sun, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) can elicit unintended and even harmful content
when misaligned with human values, posing severe risks to users and society. To
mitigate these risks, current evaluation benchmarks predominantly employ
expert-designed contextual scenarios to assess how well LLMs align with human
values. However, the labor-intensive nature of these benchmarks limits their
test scope, hindering their ability to generalize to the extensive variety of
open-world use cases and identify rare but crucial long-tail risks.
Additionally, these static tests fail to adapt to the rapid evolution of LLMs,
making it hard to evaluate timely alignment issues. To address these
challenges, we propose ALI-Agent, an evaluation framework that leverages the
autonomous abilities of LLM-powered agents to conduct in-depth and adaptive
alignment assessments. ALI-Agent operates through two principal stages:
Emulation and Refinement. During the Emulation stage, ALI-Agent automates the
generation of realistic test scenarios. In the Refinement stage, it iteratively
refines the scenarios to probe long-tail risks. Specifically, ALI-Agent
incorporates a memory module to guide test scenario generation, a tool-using
module to reduce human labor in tasks such as evaluating feedback from target
LLMs, and an action module to refine tests. Extensive experiments across three
aspects of human values--stereotypes, morality, and legality--demonstrate that
ALI-Agent, as a general evaluation framework, effectively identifies model
misalignment. Systematic analysis also validates that the generated test
scenarios represent meaningful use cases, as well as integrate enhanced
measures to probe long-tail risks. Our code is available at
https://github.com/SophieZheng998/ALI-Agent.git
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LongEmbed: Extending Embedding Models for Long Context Retrieval <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.12096v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.12096v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dawei Zhu, Liang Wang, Nan Yang, Yifan Song, Wenhao Wu, Furu Wei, Sujian Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Embedding models play a pivot role in modern NLP applications such as IR and
RAG. While the context limit of LLMs has been pushed beyond 1 million tokens,
embedding models are still confined to a narrow context window not exceeding 8k
tokens, refrained from application scenarios requiring long inputs such as
legal contracts. This paper explores context window extension of existing
embedding models, pushing the limit to 32k without requiring additional
training. First, we examine the performance of current embedding models for
long context retrieval on our newly constructed LongEmbed benchmark. LongEmbed
comprises two synthetic tasks and four carefully chosen real-world tasks,
featuring documents of varying length and dispersed target information.
Benchmarking results underscore huge room for improvement in these models.
Based on this, comprehensive experiments show that training-free context window
extension strategies like position interpolation can effectively extend the
context window of existing embedding models by several folds, regardless of
their original context being 512 or beyond 4k. Furthermore, for models
employing absolute position encoding (APE), we show the possibility of further
fine-tuning to harvest notable performance gains while strictly preserving
original behavior for short inputs. For models using rotary position embedding
(RoPE), significant enhancements are observed when employing RoPE-specific
methods, such as NTK and SelfExtend, indicating RoPE's superiority over APE for
context window extension. To facilitate future research, we release E5-Base-4k
and E5-RoPE-Base, along with the LongEmbed benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Camera Ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LOVA3: Learning to Visual Question Answering, Asking and Assessment <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14974v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14974v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Henry Hengyuan Zhao, Pan Zhou, Difei Gao, Zechen Bai, Mike Zheng Shou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Question answering, asking, and assessment are three innate human traits
crucial for understanding the world and acquiring knowledge. By enhancing these
capabilities, humans can more effectively utilize data, leading to better
comprehension and learning outcomes. Current Multimodal Large Language Models
(MLLMs) primarily focus on question answering, often neglecting the full
potential of questioning and assessment skills. Inspired by the human learning
mechanism, we introduce LOVA3, an innovative framework named "Learning tO
Visual question Answering, Asking and Assessment," designed to equip MLLMs with
these additional capabilities. Our approach involves the creation of two
supplementary training tasks GenQA and EvalQA, aiming at fostering the skills
of asking and assessing questions in the context of images. To develop the
questioning ability, we compile a comprehensive set of multimodal foundational
tasks. For assessment, we introduce a new benchmark called EvalQABench,
comprising 64,000 training samples (split evenly between positive and negative
samples) and 5,000 validation and testing samples. We posit that enhancing
MLLMs with the capabilities to answer, ask, and assess questions will enhance
their multimodal comprehension, ultimately improving overall performance. To
validate this hypothesis, we train MLLMs using the LOVA3 framework and evaluate
them on a range of multimodal datasets and benchmarks. Our results demonstrate
consistent performance gains, underscoring the critical role of these
additional tasks in fostering comprehensive intelligence in MLLMs. The code is
available at https://github.com/showlab/LOVA3.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024. The code is available at
  https://github.com/showlab/LOVA3</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ReMoDetect: Reward Models Recognize Aligned LLM's Generations <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17382v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17382v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyunseok Lee, Jihoon Tack, Jinwoo Shin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The remarkable capabilities and easy accessibility of large language models
(LLMs) have significantly increased societal risks (e.g., fake news
generation), necessitating the development of LLM-generated text (LGT)
detection methods for safe usage. However, detecting LGTs is challenging due to
the vast number of LLMs, making it impractical to account for each LLM
individually; hence, it is crucial to identify the common characteristics
shared by these models. In this paper, we draw attention to a common feature of
recent powerful LLMs, namely the alignment training, i.e., training LLMs to
generate human-preferable texts. Our key finding is that as these aligned LLMs
are trained to maximize the human preferences, they generate texts with higher
estimated preferences even than human-written texts; thus, such texts are
easily detected by using the reward model (i.e., an LLM trained to model human
preference distribution). Based on this finding, we propose two training
schemes to further improve the detection ability of the reward model, namely
(i) continual preference fine-tuning to make the reward model prefer aligned
LGTs even further and (ii) reward modeling of Human/LLM mixed texts (a
rephrased texts from human-written texts using aligned LLMs), which serves as a
median preference text corpus between LGTs and human-written texts to learn the
decision boundary better. We provide an extensive evaluation by considering six
text domains across twelve aligned LLMs, where our method demonstrates
state-of-the-art results. Code is available at
https://github.com/hyunseoklee-ai/ReMoDetect.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference proceeding for NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SciEval: A Multi-Level Large Language Model Evaluation Benchmark for
  Scientific Research <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.13149v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.13149v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liangtai Sun, Yang Han, Zihan Zhao, Da Ma, Zhennan Shen, Baocai Chen, Lu Chen, Kai Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, there has been growing interest in using Large Language Models
(LLMs) for scientific research. Numerous benchmarks have been proposed to
evaluate the ability of LLMs for scientific research. However, current
benchmarks are mostly based on pre-collected objective questions. This design
suffers from data leakage problem and lacks the evaluation of subjective Q/A
ability. In this paper, we propose SciEval, a comprehensive and
multi-disciplinary evaluation benchmark to address these issues. Based on
Bloom's taxonomy, SciEval covers four dimensions to systematically evaluate
scientific research ability. In particular, we design a "dynamic" subset based
on scientific principles to prevent evaluation from potential data leakage.
Both objective and subjective questions are included in SciEval. These
characteristics make SciEval a more effective benchmark for scientific research
ability evaluation of LLMs. Comprehensive experiments on most advanced LLMs
show that, although GPT-4 achieves SOTA performance compared to other LLMs,
there is still substantial room for improvement, especially for dynamic
questions. The codes and data are publicly available on
https://github.com/OpenDFM/SciEval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 17 figures, 12 tables. Accepted by AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do Large Language Models Truly Grasp Mathematics? An Empirical
  Exploration From A Psychological Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14979v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14979v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Xie, Shuoyoucheng Ma, Zhenhua Wang, Enze Wang, Kai Chen, Xiaobing Sun, Baosheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite their proficiency in math tasks, the mechanisms underlying LLMs'
mathematical reasoning abilities remain a subject of debate. Recent studies
suggest that chain-of-thought (CoT) prompts can bolster mathematical reasoning
by encouraging LLMs to employ human-like logical reasoning (System 2), enabling
them to excel on the Cognitive Reflection Test (CRT). To assess whether LLMs
genuinely possess System 2-like logical reasoning, we introduced targeted
modifications to CRT problems. Our findings reveal that, despite the use of CoT
prompts, mainstream LLMs, including the latest o1-preview model, continue to
exhibit a significant error rate. Further analysis indicates that they
predominantly rely on System 1-like intuitive reasoning and pattern matching
derived from training data, rather than demonstrating mastery of mathematical
thinking. This discovery challenges the prevailing notion that LLMs possess
genuine logical reasoning abilities and that CoT can enhance them.
Consequently, this work may temper overly optimistic projections regarding
LLMs' advancement toward artificial general intelligence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Collaborative Content Moderation Framework for Toxicity Detection
  based on Conformalized Estimates of Annotation Disagreement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04090v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04090v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guillermo Villate-Castillo, Javier Del Ser, Borja Sanz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Content moderation typically combines the efforts of human moderators and
machine learning models. However, these systems often rely on data where
significant disagreement occurs during moderation, reflecting the subjective
nature of toxicity perception. Rather than dismissing this disagreement as
noise, we interpret it as a valuable signal that highlights the inherent
ambiguity of the content,an insight missed when only the majority label is
considered. In this work, we introduce a novel content moderation framework
that emphasizes the importance of capturing annotation disagreement. Our
approach uses multitask learning, where toxicity classification serves as the
primary task and annotation disagreement is addressed as an auxiliary task.
Additionally, we leverage uncertainty estimation techniques, specifically
Conformal Prediction, to account for both the ambiguity in comment annotations
and the model's inherent uncertainty in predicting toxicity and
disagreement.The framework also allows moderators to adjust thresholds for
annotation disagreement, offering flexibility in determining when ambiguity
should trigger a review. We demonstrate that our joint approach enhances model
performance, calibration, and uncertainty estimation, while offering greater
parameter efficiency and improving the review process in comparison to
single-task methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Instruct, Not Assist: LLM-based Multi-Turn Planning and Hierarchical
  Questioning for Socratic Code Debugging <span class="chip">EMNLP'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11709v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11709v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Priyanka Kargupta, Ishika Agarwal, Dilek Hakkani-Tur, Jiawei Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Socratic questioning is an effective teaching strategy, encouraging critical
thinking and problem-solving. The conversational capabilities of large language
models (LLMs) show great potential for providing scalable, real-time student
guidance. However, current LLMs often give away solutions directly, making them
ineffective instructors. We tackle this issue in the code debugging domain with
TreeInstruct, an Instructor agent guided by a novel state space-based planning
algorithm. TreeInstruct asks probing questions to help students independently
identify and resolve errors. It estimates a student's conceptual and
syntactical knowledge to dynamically construct a question tree based on their
responses and current knowledge state, effectively addressing both independent
and dependent mistakes concurrently in a multi-turn interaction setting. In
addition to using an existing single-bug debugging benchmark, we construct a
more challenging multi-bug dataset of 150 coding problems, incorrect solutions,
and bug fixes -- all carefully constructed and annotated by experts. Extensive
evaluation shows TreeInstruct's state-of-the-art performance on both datasets,
proving it to be a more effective instructor than baselines. Furthermore, a
real-world case study with five students of varying skill levels further
demonstrates TreeInstruct's ability to guide students to debug their code
efficiently with minimal turns and highly Socratic questioning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code available at: https://github.com/agarwalishika/TreeInstruct
  Accepted at EMNLP'24 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PAD: Personalized Alignment of LLMs at Decoding-Time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04070v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04070v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruizhe Chen, Xiaotian Zhang, Meng Luo, Wenhao Chai, Zuozhu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aligning with personalized preferences, which vary significantly across
cultural, educational, and political differences, poses a significant challenge
due to the computational costs and data demands of traditional alignment
methods. In response, this paper presents Personalized Alignment at
Decoding-time (PAD), a novel framework designed to align LLM outputs with
diverse personalized preferences during the inference phase, eliminating the
need for additional training. By introducing a unique personalized reward
modeling strategy, this framework decouples the text generation process from
personalized preferences, facilitating the generation of generalizable
token-level personalized rewards. The PAD algorithm leverages these rewards to
guide the decoding process, dynamically tailoring the base model's predictions
to personalized preferences. Extensive experimental results demonstrate that
PAD not only outperforms existing training-based alignment methods in terms of
aligning with diverse preferences but also shows significant generalizability
to preferences unseen during training and scalability across different base
models. This work advances the capability of LLMs to meet user needs in
real-time applications, presenting a substantial step forward in personalized
LLM alignment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper presents Personalized Alignment at Decoding-time (PAD), a
  novel framework designed to align LLM outputs with diverse personalized
  preferences during the inference phase</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Translation of Circumlocution in Arabic Short Stories into English 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02887v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02887v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dalal Waadallah Shehab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study investigates the translation of circumlocution from Arabic to
English in a corpus of short stories by renowned Arabic authors. By analyzing
the source and target texts, the study aims to identify and categorize
circumlocution instances in Arabic and their corresponding renditions in
English. The study employs Nida's (1964) translation theory as a framework to
assess the appropriateness of the translation strategies employed. It examines
the extent to which translators successfully rendered Arabic circumlocution
into English, identifying potential challenges and limitations in the
translation process. The findings reveal significant similarities between
Arabic circumlocution categories and English metadiscourse categories,
particularly in terms of textual and interpersonal functions. However, the
study also highlights instances where translators encountered difficulties in
accurately conveying the nuances of circumlocution, often resorting to
strategies like addition, subtraction, and alteration.https://ntu.edu.iq/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SciDFM: A Large Language Model with Mixture-of-Experts for Science <span class="chip">NeurIPS
  2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18412v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18412v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liangtai Sun, Danyu Luo, Da Ma, Zihan Zhao, Baocai Chen, Zhennan Shen, Su Zhu, Lu Chen, Xin Chen, Kai Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, there has been a significant upsurge of interest in leveraging
large language models (LLMs) to assist scientific discovery. However, most LLMs
only focus on general science, while they lack domain-specific knowledge, such
as chemical molecules and amino acid sequences. To bridge these gaps, we
introduce SciDFM, a mixture-of-experts LLM, which is trained from scratch and
is able to conduct college-level scientific reasoning and understand molecules
and amino acid sequences. We collect a large-scale training corpus containing
numerous scientific papers and books from different disciplines as well as data
from domain-specific databases. We further fine-tune the pre-trained model on
lots of instruction data to improve performances on downstream benchmarks. From
experiment results, we show that SciDFM achieves strong performance on general
scientific benchmarks such as SciEval and SciQ, and it reaches a SOTA
performance on domain-specific benchmarks among models of similar size. We
further analyze the expert layers and show that the results of expert selection
vary with data from different disciplines. To benefit the broader research
community, we open-source SciDFM at
https://huggingface.co/OpenDFM/SciDFM-MoE-A5.6B-v1.0.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 1 figure, 9 tables. Technical Report, accepted by NeurIPS
  2024 Workshop FM4Science</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GraphTeam: Facilitating Large Language Model-based Graph Analysis via
  Multi-Agent Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18032v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18032v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Li, Qizhi Chu, Yubin Chen, Yang Liu, Yaoqi Liu, Zekai Yu, Weize Chen, Chen Qian, Chuan Shi, Cheng Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graphs are widely used for modeling relational data in real-world scenarios,
such as social networks and urban computing. Existing LLM-based graph analysis
approaches either integrate graph neural networks (GNNs) for specific machine
learning tasks, limiting their transferability, or rely solely on LLMs'
internal reasoning ability, resulting in suboptimal performance. To address
these limitations, we take advantage of recent advances in LLM-based agents,
which have shown capabilities of utilizing external knowledge or tools for
problem solving. By simulating human problem-solving strategies such as analogy
and collaboration, we propose a multi-agent system based on LLMs named
GraphTeam, for graph analysis. GraphTeam consists of five LLM-based agents from
three modules, and the agents with different specialities can collaborate with
each other to address complex problems. Specifically, (1) input-output
normalization module: the question agent extracts and refines four key
arguments from the original question, facilitating the problem understanding,
and the answer agent organizes the results to meet the output requirement; (2)
external knowledge retrieval module: we first build a knowledge base consisting
of relevant documentation and experience information, and then the search agent
retrieves the most relevant entries for each question. (3) problem-solving
module: given the retrieved information from search agent, the coding agent
uses established algorithms via programming to generate solutions, and in case
the coding agent does not work, the reasoning agent will directly compute the
results without programming. Extensive experiments on six graph analysis
benchmarks demonstrate that GraphTeam achieves state-of-the-art performance
with an average 25.85% improvement over the best baseline in terms of accuracy.
The code and data are available at https://github.com/BUPT-GAMMA/GraphTeam.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What is lost in Normalization? Exploring Pitfalls in Multilingual ASR
  Model Evaluations <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02449v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02449v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kavya Manohar, Leena G Pillai, Elizabeth Sherly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the pitfalls in evaluating multilingual automatic speech
recognition (ASR) models, with a particular focus on Indic language scripts. We
investigate the text normalization routine employed by leading ASR models,
including OpenAI Whisper, Meta's MMS, Seamless, and Assembly AI's Conformer,
and their unintended consequences on performance metrics. Our research reveals
that current text normalization practices, while aiming to standardize ASR
outputs for fair comparison, by removing inconsistencies such as variations in
spelling, punctuation, and special characters, are fundamentally flawed when
applied to Indic scripts. Through empirical analysis using text similarity
scores and in-depth linguistic examination, we demonstrate that these flaws
lead to artificially improved performance metrics for Indic languages. We
conclude by proposing a shift towards developing text normalization routines
that leverage native linguistic expertise, ensuring more robust and accurate
evaluations of multilingual ASR models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Winner-Take-All Column Row Sampling for Memory Efficient Adaptation of
  Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.15265v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.15265v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zirui Liu, Guanchu Wang, Shaochen Zhong, Zhaozhuo Xu, Daochen Zha, Ruixiang Tang, Zhimeng Jiang, Kaixiong Zhou, Vipin Chaudhary, Shuai Xu, Xia Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid growth in model size, fine-tuning the large pre-trained
language model has become increasingly difficult due to its extensive memory
usage. Previous works usually focus on reducing the number of trainable
parameters in the network. While the model parameters do contribute to memory
usage, the primary memory bottleneck during training arises from storing
feature maps, also known as activations, as they are crucial for gradient
calculation. Notably, neural networks are usually trained using stochastic
gradient descent. We argue that in stochastic optimization, models can handle
noisy gradients as long as the gradient estimator is unbiased with reasonable
variance. Following this motivation, we propose a new family of unbiased
estimators called WTA-CRS, for matrix production with reduced variance, which
only requires storing the sub-sampled activations for calculating the gradient.
Our work provides both theoretical and experimental evidence that, in the
context of tuning transformers, our proposed estimators exhibit lower variance
compared to existing ones. By replacing the linear operation with our
approximated one in transformers, we can achieve up to 2.7$\times$ peak memory
reduction with almost no accuracy drop and enables up to $6.4\times$ larger
batch size. Under the same hardware, WTA-CRS enables better down-streaming task
performance by applying larger models and/or faster training speed with larger
batch sizes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Quality of Answers for Retrieval-Augmented Generation: A
  Strong LLM Is All You Need 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18064v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18064v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Wang, Alberto Garcia Hernandez, Roman Kyslyi, Nicholas Kersting
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a comprehensive study of answer quality evaluation in
Retrieval-Augmented Generation (RAG) applications using vRAG-Eval, a novel
grading system that is designed to assess correctness, completeness, and
honesty. We further map the grading of quality aspects aforementioned into a
binary score, indicating an accept or reject decision, mirroring the intuitive
"thumbs-up" or "thumbs-down" gesture commonly used in chat applications. This
approach suits factual business contexts where a clear decision opinion is
essential. Our assessment applies vRAG-Eval to two Large Language Models
(LLMs), evaluating the quality of answers generated by a vanilla RAG
application. We compare these evaluations with human expert judgments and find
a substantial alignment between GPT-4's assessments and those of human experts,
reaching 83% agreement on accept or reject decisions. This study highlights the
potential of LLMs as reliable evaluators in closed-domain, closed-ended
settings, particularly when human evaluations require significant resources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 8 figures, 12 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Transformers Solve Propositional Logic Problems: A Mechanistic
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04105v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04105v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guan Zhe Hong, Nishanth Dikkala, Enming Luo, Cyrus Rashtchian, Xin Wang, Rina Panigrahy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown amazing performance on tasks that
require planning and reasoning. Motivated by this, we investigate the internal
mechanisms that underpin a network's ability to perform complex logical
reasoning. We first construct a synthetic propositional logic problem that
serves as a concrete test-bed for network training and evaluation. Crucially,
this problem demands nontrivial planning to solve, but we can train a small
transformer to achieve perfect accuracy. Building on our set-up, we then pursue
an understanding of precisely how a three-layer transformer, trained from
scratch, solves this problem. We are able to identify certain "planning" and
"reasoning" circuits in the network that necessitate cooperation between the
attention blocks to implement the desired logic. To expand our findings, we
then study a larger model, Mistral 7B. Using activation patching, we
characterize internal components that are critical in solving our logic
problem. Overall, our work systemically uncovers novel aspects of small and
large transformers, and continues the study of how they plan and reason.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MAG-SQL: Multi-Agent Generative Approach with Soft Schema Linking and
  Iterative Sub-SQL Refinement for Text-to-SQL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.07930v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.07930v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxuan Xie, Gaochen Wu, Bowen Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent In-Context Learning based methods have achieved remarkable success in
Text-to-SQL task. However, there is still a large gap between the performance
of these models and human performance on datasets with complex database schema
and difficult questions, such as BIRD. Besides, existing work has neglected to
supervise intermediate steps when solving questions iteratively with question
decomposition methods, and the schema linking methods used in these works are
very rudimentary. To address these issues, we propose MAG-SQL, a multi-agent
generative approach with soft schema linking and iterative Sub-SQL refinement.
In our framework, an entity-based method with tables' summary is used to select
the columns in database, and a novel targets-conditions decomposition method is
introduced to decompose those complex questions. Additionally, we build a
iterative generating module which includes a Sub-SQL Generator and Sub-SQL
Refiner, introducing external oversight for each step of generation. Through a
series of ablation studies, the effectiveness of each agent in our framework
has been demonstrated. When evaluated on the BIRD benchmark with GPT-4, MAG-SQL
achieves an execution accuracy of 61.08%, compared to the baseline accuracy of
46.35% for vanilla GPT-4 and the baseline accuracy of 57.56% for MAC-SQL.
Besides, our approach makes similar progress on Spider. The codes are available
at https://github.com/LancelotXWX/MAG-SQL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Survey on Employing Large Language Models for Text-to-SQL Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15186v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15186v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liang Shi, Zhengju Tang, Nan Zhang, Xiaotong Zhang, Zhi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing volume of data in relational databases and the expertise
needed for writing SQL queries pose challenges for users to access and analyze
data. Text-to-SQL (Text2SQL) solves the issues by utilizing natural language
processing (NLP) techniques to convert natural language into SQL queries. With
the development of Large Language Models (LLMs), a range of LLM-based Text2SQL
methods have emerged. This survey provides a comprehensive review of LLMs in
Text2SQL tasks. We review benchmark datasets, prompt engineering methods,
fine-tuning methods, and base models in LLM-based Text2SQL methods. We provide
insights in each part and discuss future directions in this field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Oscars of AI Theater: A Survey on Role-Playing with Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11484v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11484v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nuo Chen, Yan Wang, Yang Deng, Jia Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This survey explores the burgeoning field of role-playing with language
models, focusing on their development from early persona-based models to
advanced character-driven simulations facilitated by Large Language Models
(LLMs). Initially confined to simple persona consistency due to limited model
capabilities, role-playing tasks have now expanded to embrace complex character
portrayals involving character consistency, behavioral alignment, and overall
attractiveness. We provide a comprehensive taxonomy of the critical components
in designing these systems, including data, models and alignment, agent
architecture and evaluation. This survey not only outlines the current
methodologies and challenges, such as managing dynamic personal profiles and
achieving high-level persona consistency but also suggests avenues for future
research in improving the depth and realism of role-playing applications. The
goal is to guide future research by offering a structured overview of current
methodologies and identifying potential areas for improvement. Related
resources and papers are available at
https://github.com/nuochenpku/Awesome-Role-Play-Papers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FactTest: Factuality Testing in Large Language Models with Finite-Sample
  and Distribution-Free Guarantees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02603v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02603v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Nie, Xiaotian Hou, Shuhang Lin, James Zou, Huaxiu Yao, Linjun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The propensity of Large Language Models (LLMs) to generate hallucinations and
non-factual content undermines their reliability in high-stakes domains, where
rigorous control over Type I errors (the conditional probability of incorrectly
classifying hallucinations as truthful content) is essential. Despite its
importance, formal verification of LLM factuality with such guarantees remains
largely unexplored. In this paper, we introduce FactTest, a novel framework
that statistically assesses whether a LLM can confidently provide correct
answers to given questions with high-probability correctness guarantees. We
formulate factuality testing as hypothesis testing problem to enforce an upper
bound of Type I errors at user-specified significance levels. Notably, we prove
that our framework also ensures strong Type II error control under mild
conditions and can be extended to maintain its effectiveness when covariate
shifts exist. Our approach is distribution-free and works for any number of
human-annotated samples. It is model-agnostic and applies to any black-box or
white-box LM. Extensive experiments on question-answering (QA) and
multiple-choice benchmarks demonstrate that FactTest effectively detects
hallucinations and improves the model's ability to abstain from answering
unknown questions, leading to an over 40% accuracy improvement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Linguistic Collapse: Neural Collapse in (Large) Language Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17767v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17767v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert Wu, Vardan Papyan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural collapse ($\mathcal{NC}$) is a phenomenon observed in classification
tasks where top-layer representations collapse into their class means, which
become equinorm, equiangular and aligned with the classifiers. These behaviors
-- associated with generalization and robustness -- would manifest under
specific conditions: models are trained towards zero loss, with noise-free
labels belonging to balanced classes, which do not outnumber the model's hidden
dimension. Recent studies have explored $\mathcal{NC}$ in the absence of one or
more of these conditions to extend and capitalize on the associated benefits of
ideal geometries. Language modeling presents a curious frontier, as
\textit{training by token prediction} constitutes a classification task where
none of the conditions exist: the vocabulary is imbalanced and exceeds the
embedding dimension; different tokens might correspond to similar contextual
embeddings; and large language models (LLMs) in particular are typically only
trained for a few epochs. This paper empirically investigates the impact of
scaling the architectures and training of causal language models (CLMs) on
their progression towards $\mathcal{NC}$. We find that $\mathcal{NC}$
properties that develop with scale (and regularization) are linked to
generalization. Moreover, there is evidence of some relationship between
$\mathcal{NC}$ and generalization independent of scale. Our work thereby
underscores the generality of $\mathcal{NC}$ as it extends to the novel and
more challenging setting of language modeling. Downstream, we seek to inspire
further research on the phenomenon to deepen our understanding of LLMs -- and
neural networks at large -- and improve existing architectures based on
$\mathcal{NC}$-related properties. Our code is hosted on GitHub at
https://github.com/rhubarbwu/linguistic-collapse .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024; 36 pages; 30 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Long-form factuality in large language models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18802v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18802v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Jie Huang, Dustin Tran, Daiyi Peng, Ruibo Liu, Da Huang, Cosmo Du, Quoc V. Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) often generate content that contains factual
errors when responding to fact-seeking prompts on open-ended topics. To
benchmark a model's long-form factuality in open domains, we first use GPT-4 to
generate LongFact, a prompt set comprising thousands of questions spanning 38
topics. We then propose that LLM agents can be used as automated evaluators for
long-form factuality through a method which we call Search-Augmented Factuality
Evaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into
a set of individual facts and to evaluate the accuracy of each fact using a
multi-step reasoning process comprising sending search queries to Google Search
and determining whether a fact is supported by the search results. Furthermore,
we propose extending F1 score as an aggregated metric for long-form factuality.
To do so, we balance the percentage of supported facts in a response
(precision) with the percentage of provided facts relative to a hyperparameter
representing a user's preferred response length (recall).
  Empirically, we demonstrate that LLM agents can outperform crowdsourced human
annotators - on a set of ~16k individual facts, SAFE agrees with crowdsourced
human annotators 72% of the time, and on a random subset of 100 disagreement
cases, SAFE wins 76% of the time. At the same time, SAFE is more than 20 times
cheaper than human annotators. We also benchmark thirteen language models on
LongFact across four model families (Gemini, GPT, Claude, and PaLM-2), finding
that larger language models generally achieve better long-form factuality.
LongFact, SAFE, and all experimental code are available at
https://github.com/google-deepmind/long-form-factuality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024; 72 pages, 18 figures, 30 tables. Code at
  https://github.com/google-deepmind/long-form-factuality</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HealthQ: Unveiling Questioning Capabilities of LLM Chains in Healthcare
  Conversations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19487v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19487v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyu Wang, Hao Li, Di Huang, Amir M. Rahmani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In digital healthcare, large language models (LLMs) have primarily been
utilized to enhance question-answering capabilities and improve patient
interactions. However, effective patient care necessitates LLM chains that can
actively gather information by posing relevant questions. This paper presents
HealthQ, a novel framework designed to evaluate the questioning capabilities of
LLM healthcare chains. We implemented several LLM chains, including
Retrieval-Augmented Generation (RAG), Chain of Thought (CoT), and reflective
chains, and introduced an LLM judge to assess the relevance and informativeness
of the generated questions. To validate HealthQ, we employed traditional
Natural Language Processing (NLP) metrics such as Recall-Oriented Understudy
for Gisting Evaluation (ROUGE) and Named Entity Recognition (NER)-based set
comparison, and constructed two custom datasets from public medical note
datasets, ChatDoctor and MTS-Dialog. Our contributions are threefold: we
provide the first comprehensive study on the questioning capabilities of LLMs
in healthcare conversations, develop a novel dataset generation pipeline, and
propose a detailed evaluation methodology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deploying Multi-task Online Server with Large Language Model <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03644v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03644v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yincen Qu, Chao Ma, Xiangying Dai, Hui Zhou, Yiting Wu, Hengyue Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the industry, numerous tasks are deployed online. Traditional approaches
often tackle each task separately by its own network, which leads to excessive
costs for developing and scaling models, especially in the context of large
language models. Although multi-task methods can save costs through parameter
sharing, they often struggle to outperform single-task methods in real-world
applications. To tackle these challenges, we present a three-stage multi-task
learning framework for large language models. It involves task filtering,
followed by fine-tuning on high-resource tasks, and finally fine-tuning on all
tasks. We conducted comprehensive experiments in single-task and multi-task
settings. Our approach, exemplified on different benchmarks, demonstrates that
it is able to achieve performance comparable to the single-task method while
reducing up to 90.9\% of its overhead.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by COLING 2025 Industry Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ $B^4$: A Black-Box Scrubbing Attack on LLM Watermarks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01222v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01222v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baizhou Huang, Xiao Pu, Xiaojun Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Watermarking has emerged as a prominent technique for LLM-generated content
detection by embedding imperceptible patterns. Despite supreme performance, its
robustness against adversarial attacks remains underexplored. Previous work
typically considers a grey-box attack setting, where the specific type of
watermark is already known. Some even necessitates knowledge about
hyperparameters of the watermarking method. Such prerequisites are unattainable
in real-world scenarios. Targeting at a more realistic black-box threat model
with fewer assumptions, we here propose $B^4$, a black-box scrubbing attack on
watermarks. Specifically, we formulate the watermark scrubbing attack as a
constrained optimization problem by capturing its objectives with two
distributions, a Watermark Distribution and a Fidelity Distribution. This
optimization problem can be approximately solved using two proxy distributions.
Experimental results across 12 different settings demonstrate the superior
performance of $B^4$ compared with other baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FinCon: A Synthesized LLM Multi-Agent System with Conceptual Verbal
  Reinforcement for Enhanced Financial Decision Making 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.06567v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.06567v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangyang Yu, Zhiyuan Yao, Haohang Li, Zhiyang Deng, Yupeng Cao, Zhi Chen, Jordan W. Suchow, Rong Liu, Zhenyu Cui, Zhaozhuo Xu, Denghui Zhang, Koduvayur Subbalakshmi, Guojun Xiong, Yueru He, Jimin Huang, Dong Li, Qianqian Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated notable potential in
conducting complex tasks and are increasingly utilized in various financial
applications. However, high-quality sequential financial investment
decision-making remains challenging. These tasks require multiple interactions
with a volatile environment for every decision, demanding sufficient
intelligence to maximize returns and manage risks. Although LLMs have been used
to develop agent systems that surpass human teams and yield impressive
investment returns, opportunities to enhance multi-sourced information
synthesis and optimize decision-making outcomes through timely experience
refinement remain unexplored. Here, we introduce the FinCon, an LLM-based
multi-agent framework with CONceptual verbal reinforcement tailored for diverse
FINancial tasks. Inspired by effective real-world investment firm
organizational structures, FinCon utilizes a manager-analyst communication
hierarchy. This structure allows for synchronized cross-functional agent
collaboration towards unified goals through natural language interactions and
equips each agent with greater memory capacity than humans. Additionally, a
risk-control component in FinCon enhances decision quality by episodically
initiating a self-critiquing mechanism to update systematic investment beliefs.
The conceptualized beliefs serve as verbal reinforcement for the future agent's
behavior and can be selectively propagated to the appropriate node that
requires knowledge updates. This feature significantly improves performance
while reducing unnecessary peer-to-peer communication costs. Moreover, FinCon
demonstrates strong generalization capabilities in various financial tasks,
including single stock trading and portfolio management.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LLM Applications, LLM Agents, Financial Technology, Quantitative
  Finance, Algorithmic Trading, Cognitive Science</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Birdie: Advancing State Space Models with Reward-Driven Objectives and
  Curricula <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01030v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01030v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sam Blouir, Jimmy T. H. Smith, Antonios Anastasopoulos, Amarda Shehu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient state space models (SSMs), such as linear recurrent neural networks
and linear attention variants, offer computational advantages over Transformers
but struggle with tasks requiring long-range in-context retrieval-like text
copying, associative recall, and question answering over long contexts.
Previous efforts to address these challenges have focused on architectural
modifications, often reintroducing computational inefficiencies. In this paper,
we propose a novel training procedure, Birdie, that significantly enhances the
in-context retrieval capabilities of SSMs without altering their architecture.
Our approach combines bidirectional input processing with dynamic mixtures of
specialized pre-training objectives, optimized via reinforcement learning. We
introduce a new bidirectional SSM architecture that seamlessly transitions from
bidirectional context processing to causal generation. Experimental evaluations
demonstrate that Birdie markedly improves performance on retrieval-intensive
tasks such as multi-number phone book lookup, long paragraph
question-answering, and infilling. This narrows the performance gap with
Transformers, while retaining computational efficiency. Our findings highlight
the importance of training procedures in leveraging the fixed-state capacity of
SSMs, offering a new direction to advance their capabilities. All code and
pre-trained models are available at https://www.github.com/samblouir/birdie,
with support for JAX and PyTorch.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 (Main Conference)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">127</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SVDQunat: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05007v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05007v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muyang Li, Yujun Lin, Zhekai Zhang, Tianle Cai, Xiuyu Li, Junxian Guo, Enze Xie, Chenlin Meng, Jun-Yan Zhu, Song Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have been proven highly effective at generating high-quality
images. However, as these models grow larger, they require significantly more
memory and suffer from higher latency, posing substantial challenges for
deployment. In this work, we aim to accelerate diffusion models by quantizing
their weights and activations to 4 bits. At such an aggressive level, both
weights and activations are highly sensitive, where conventional post-training
quantization methods for large language models like smoothing become
insufficient. To overcome this limitation, we propose SVDQuant, a new 4-bit
quantization paradigm. Different from smoothing which redistributes outliers
between weights and activations, our approach absorbs these outliers using a
low-rank branch. We first consolidate the outliers by shifting them from
activations to weights, then employ a high-precision low-rank branch to take in
the weight outliers with Singular Value Decomposition (SVD). This process eases
the quantization on both sides. However, na\"{\i}vely running the low-rank
branch independently incurs significant overhead due to extra data movement of
activations, negating the quantization speedup. To address this, we co-design
an inference engine Nunchaku that fuses the kernels of the low-rank branch into
those of the low-bit branch to cut off redundant memory access. It can also
seamlessly support off-the-shelf low-rank adapters (LoRAs) without the need for
re-quantization. Extensive experiments on SDXL, PixArt-$\Sigma$, and FLUX.1
validate the effectiveness of SVDQuant in preserving image quality. We reduce
the memory usage for the 12B FLUX.1 models by 3.5$\times$, achieving
3.0$\times$ speedup over the 4-bit weight-only quantized baseline on the 16GB
laptop 4090 GPU, paving the way for more interactive applications on PCs. Our
quantization library and inference engine are open-sourced.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Quantization Library: https://github.com/mit-han-lab/deepcompressor
  Inference Engine: https://github.com/mit-han-lab/nunchaku Website:
  https://hanlab.mit.edu/projects/svdquant Demo: https://svdquant.mit.edu Blog:
  https://hanlab.mit.edu/blog/svdquant</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ProEdit: Simple Progression is All You Need for High-Quality 3D Scene
  Editing <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05006v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05006v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun-Kun Chen, Yu-Xiong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes ProEdit - a simple yet effective framework for
high-quality 3D scene editing guided by diffusion distillation in a novel
progressive manner. Inspired by the crucial observation that multi-view
inconsistency in scene editing is rooted in the diffusion model's large
feasible output space (FOS), our framework controls the size of FOS and reduces
inconsistency by decomposing the overall editing task into several subtasks,
which are then executed progressively on the scene. Within this framework, we
design a difficulty-aware subtask decomposition scheduler and an adaptive 3D
Gaussian splatting (3DGS) training strategy, ensuring high quality and
efficiency in performing each subtask. Extensive evaluation shows that our
ProEdit achieves state-of-the-art results in various scenes and challenging
editing tasks, all through a simple framework without any expensive or
sophisticated add-ons like distillation losses, components, or training
procedures. Notably, ProEdit also provides a new way to control, preview, and
select the "aggressivity" of editing operation during the editing process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024. Project Page: https://immortalco.github.io/ProEdit/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diff-2-in-1: Bridging Generation and Dense Perception with Diffusion
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05005v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05005v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuhong Zheng, Zhipeng Bao, Ruoyu Zhao, Martial Hebert, Yu-Xiong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Beyond high-fidelity image synthesis, diffusion models have recently
exhibited promising results in dense visual perception tasks. However, most
existing work treats diffusion models as a standalone component for perception
tasks, employing them either solely for off-the-shelf data augmentation or as
mere feature extractors. In contrast to these isolated and thus sub-optimal
efforts, we introduce a unified, versatile, diffusion-based framework,
Diff-2-in-1, that can simultaneously handle both multi-modal data generation
and dense visual perception, through a unique exploitation of the
diffusion-denoising process. Within this framework, we further enhance
discriminative visual perception via multi-modal generation, by utilizing the
denoising network to create multi-modal data that mirror the distribution of
the original training set. Importantly, Diff-2-in-1 optimizes the utilization
of the created diverse and faithful data by leveraging a novel self-improving
learning mechanism. Comprehensive experimental evaluations validate the
effectiveness of our framework, showcasing consistent performance improvements
across various discriminative backbones and high-quality multi-modal data
generation characterized by both realism and usefulness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReCapture: Generative Video Camera Controls for User-Provided Videos
  using Masked Video <span class="highlight-title">Fine-Tuning</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05003v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05003v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Junhao Zhang, Roni Paiss, Shiran Zada, Nikhil Karnad, David E. Jacobs, Yael Pritch, Inbar Mosseri, Mike Zheng Shou, Neal Wadhwa, Nataniel Ruiz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, breakthroughs in video modeling have allowed for controllable
camera trajectories in generated videos. However, these methods cannot be
directly applied to user-provided videos that are not generated by a video
model. In this paper, we present ReCapture, a method for generating new videos
with novel camera trajectories from a single user-provided video. Our method
allows us to re-generate the reference video, with all its existing scene
motion, from vastly different angles and with cinematic camera motion. Notably,
using our method we can also plausibly hallucinate parts of the scene that were
not observable in the reference video. Our method works by (1) generating a
noisy anchor video with a new camera trajectory using multiview diffusion
models or depth-based point cloud rendering and then (2) regenerating the
anchor video into a clean and temporally consistent reangled video using our
proposed masked video fine-tuning technique.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>project page: https://generative-video-camera-controls.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analyzing The Language of Visual Tokens 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05001v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05001v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David M. Chan, Rodolfo Corona, Joonyong Park, Cheol Jun Cho, Yutong Bai, Trevor Darrell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the introduction of transformer-based models for vision and language
tasks, such as LLaVA and Chameleon, there has been renewed interest in the
discrete tokenized representation of images. These models often treat image
patches as discrete tokens, analogous to words in natural language, learning
joint alignments between visual and human languages. However, little is known
about the statistical behavior of these visual languages - whether they follow
similar frequency distributions, grammatical structures, or topologies as
natural languages. In this paper, we take a natural-language-centric approach
to analyzing discrete visual languages and uncover striking similarities and
fundamental differences. We demonstrate that, although visual languages adhere
to Zipfian distributions, higher token innovation drives greater entropy and
lower compression, with tokens predominantly representing object parts,
indicating intermediate granularity. We also show that visual languages lack
cohesive grammatical structures, leading to higher perplexity and weaker
hierarchical organization compared to natural languages. Finally, we
demonstrate that, while vision models align more closely with natural languages
than other models, this alignment remains significantly weaker than the
cohesion found within natural languages. Through these experiments, we
demonstrate how understanding the statistical properties of discrete visual
languages can inform the design of more effective computer vision models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM2<span class="highlight-title">CLIP</span>: Powerful Language Model Unlock Richer Visual Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04997v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04997v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiquan Huang, Aoqi Wu, Yifan Yang, Xufang Luo, Yuqing Yang, Liang Hu, Qi Dai, Xiyang Dai, Dongdong Chen, Chong Luo, Lili Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  CLIP is one of the most important multimodal foundational models today. What
powers CLIP's capabilities? The rich supervision signals provided by natural
language, the carrier of human knowledge, shape a powerful cross-modal
representation space. However, with the rapid advancements in large language
models LLMs like GPT-4 and LLaMA, the boundaries of language comprehension and
generation are continually being pushed. This raises an intriguing question:
can the capabilities of LLMs be harnessed to further improve multimodal
representation learning? The potential benefits of incorporating LLMs into CLIP
are clear. LLMs' strong textual understanding can fundamentally improve CLIP's
ability to handle image captions, drastically enhancing its ability to process
long and complex texts, a well-known limitation of vanilla CLIP. Moreover, LLMs
are trained on a vast corpus of text, possessing open-world knowledge. This
allows them to expand on caption information during training, increasing the
efficiency of the learning process. In this paper, we propose LLM2CLIP, a novel
approach that embraces the power of LLMs to unlock CLIP's potential. By
fine-tuning the LLM in the caption space with contrastive learning, we extract
its textual capabilities into the output embeddings, significantly improving
the output layer's textual discriminability. We then design an efficient
training process where the fine-tuned LLM acts as a powerful teacher for CLIP's
visual encoder. Thanks to the LLM's presence, we can now incorporate longer and
more complex captions without being restricted by vanilla CLIP's text encoder's
context window and ability limitations. Our experiments demonstrate that this
approach brings substantial improvements in cross-modal tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HourVideo: 1-Hour Video-Language Understanding <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04998v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04998v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keshigeyan Chandrasegaran, Agrim Gupta, Lea M. Hadzic, Taran Kota, Jimming He, Cristóbal Eyzaguirre, Zane Durante, Manling Li, Jiajun Wu, Li Fei-Fei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present HourVideo, a benchmark dataset for hour-long video-language
understanding. Our dataset consists of a novel task suite comprising
summarization, perception (recall, tracking), visual reasoning (spatial,
temporal, predictive, causal, counterfactual), and navigation (room-to-room,
object retrieval) tasks. HourVideo includes 500 manually curated egocentric
videos from the Ego4D dataset, spanning durations of 20 to 120 minutes, and
features 12,976 high-quality, five-way multiple-choice questions. Benchmarking
results reveal that multimodal models, including GPT-4 and LLaVA-NeXT, achieve
marginal improvements over random chance. In stark contrast, human experts
significantly outperform the state-of-the-art long-context multimodal model,
Gemini Pro 1.5 (85.0% vs. 37.3%), highlighting a substantial gap in multimodal
capabilities. Our benchmark, evaluation toolkit, prompts, and documentation are
available at https://hourvideo.stanford.edu
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 Datasets and Benchmarks Track; 28 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LoFi: Scalable Local Image Reconstruction with Implicit Neural
  Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04995v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04995v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        AmirEhsan Khorashadizadeh, Tobías I. Liaudat, Tianlin Liu, Jason D. McEwen, Ivan Dokmanić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural fields or implicit neural representations (INRs) have attracted
significant attention in machine learning and signal processing due to their
efficient continuous representation of images and 3D volumes. In this work, we
build on INRs and introduce a coordinate-based local processing framework for
solving imaging inverse problems, termed LoFi (Local Field). Unlike
conventional methods for image reconstruction, LoFi processes local information
at each coordinate \textit{separately} by multi-layer perceptrons (MLPs),
recovering the object at that specific coordinate. Similar to INRs, LoFi can
recover images at any continuous coordinate, enabling image reconstruction at
multiple resolutions. With comparable or better performance than standard CNNs
for image reconstruction, LoFi achieves excellent generalization to
out-of-distribution data and memory usage almost independent of image
resolution. Remarkably, training on $1024 \times 1024$ images requires just 3GB
of memory -- over 20 times less than the memory typically needed by standard
CNNs. Additionally, LoFi's local design allows it to train on extremely small
datasets with less than 10 samples, without overfitting or the need for
regularization or early stopping. Finally, we use LoFi as a denoising prior in
a plug-and-play framework for solving general inverse problems to benefit from
its continuous image representation and strong generalization. Although trained
on low-resolution images, LoFi can be used as a low-dimensional prior to solve
inverse problems at any resolution. We validate our framework across a variety
of imaging modalities, from low-dose computed tomography to radio
interferometric imaging.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SG-I2V: Self-Guided Trajectory Control in Image-to-Video Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04989v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04989v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Koichi Namekata, Sherwin Bahmani, Ziyi Wu, Yash Kant, Igor Gilitschenski, David B. Lindell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Methods for image-to-video generation have achieved impressive,
photo-realistic quality. However, adjusting specific elements in generated
videos, such as object motion or camera movement, is often a tedious process of
trial and error, e.g., involving re-generating videos with different random
seeds. Recent techniques address this issue by fine-tuning a pre-trained model
to follow conditioning signals, such as bounding boxes or point trajectories.
Yet, this fine-tuning procedure can be computationally expensive, and it
requires datasets with annotated object motion, which can be difficult to
procure. In this work, we introduce SG-I2V, a framework for controllable
image-to-video generation that is self-guided$\unicode{x2013}$offering
zero-shot control by relying solely on the knowledge present in a pre-trained
image-to-video diffusion model without the need for fine-tuning or external
knowledge. Our zero-shot method outperforms unsupervised baselines while being
competitive with supervised models in terms of visual quality and motion
fidelity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://kmcode1.github.io/Projects/SG-I2V/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Planar Reflection-Aware Neural Radiance Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04984v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04984v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Gao, Yipeng Wang, Changil Kim, Jia-Bin Huang, Johannes Kopf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Radiance Fields (NeRF) have demonstrated exceptional capabilities in
reconstructing complex scenes with high fidelity. However, NeRF's view
dependency can only handle low-frequency reflections. It falls short when
handling complex planar reflections, often interpreting them as erroneous scene
geometries and leading to duplicated and inaccurate scene representations. To
address this challenge, we introduce a reflection-aware NeRF that jointly
models planar reflectors, such as windows, and explicitly casts reflected rays
to capture the source of the high-frequency reflections. We query a single
radiance field to render the primary color and the source of the reflection. We
propose a sparse edge regularization to help utilize the true sources of
reflections for rendering planar reflections rather than creating a duplicate
along the primary ray at the same depth. As a result, we obtain accurate scene
geometry. Rendering along the primary ray results in a clean, reflection-free
view, while explicitly rendering along the reflected ray allows us to
reconstruct highly detailed reflections. Our extensive quantitative and
qualitative evaluations of real-world datasets demonstrate our method's
enhanced performance in accurately handling reflections.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AsCAN: Asymmetric Convolution-Attention Networks for Efficient
  Recognition and Generation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04967v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04967v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anil Kag, Huseyin Coskun, Jierun Chen, Junli Cao, Willi Menapace, Aliaksandr Siarohin, Sergey Tulyakov, Jian Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural network architecture design requires making many crucial decisions.
The common desiderata is that similar decisions, with little modifications, can
be reused in a variety of tasks and applications. To satisfy that,
architectures must provide promising latency and performance trade-offs,
support a variety of tasks, scale efficiently with respect to the amounts of
data and compute, leverage available data from other tasks, and efficiently
support various hardware. To this end, we introduce AsCAN -- a hybrid
architecture, combining both convolutional and transformer blocks. We revisit
the key design principles of hybrid architectures and propose a simple and
effective \emph{asymmetric} architecture, where the distribution of
convolutional and transformer blocks is \emph{asymmetric}, containing more
convolutional blocks in the earlier stages, followed by more transformer blocks
in later stages. AsCAN supports a variety of tasks: recognition, segmentation,
class-conditional image generation, and features a superior trade-off between
performance and latency. We then scale the same architecture to solve a
large-scale text-to-image task and show state-of-the-art performance compared
to the most recent public and commercial models. Notably, even without any
computation optimization for transformer blocks, our models still yield faster
inference speed than existing works featuring efficient attention mechanisms,
highlighting the advantages and the value of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024. Project Page:
  https://snap-research.github.io/snap_image/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VAIR: Visuo-Acoustic Implicit Representations for Low-Cost, <span class="highlight-title">Multi-Modal</span>
  Transparent Surface Reconstruction in Indoor Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04963v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04963v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Advaith V. Sethuraman, Onur Bagoren, Harikrishnan Seetharaman, Dalton Richardson, Joseph Taylor, Katherine A. Skinner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mobile robots operating indoors must be prepared to navigate challenging
scenes that contain transparent surfaces. This paper proposes a novel method
for the fusion of acoustic and visual sensing modalities through implicit
neural representations to enable dense reconstruction of transparent surfaces
in indoor scenes. We propose a novel model that leverages generative latent
optimization to learn an implicit representation of indoor scenes consisting of
transparent surfaces. We demonstrate that we can query the implicit
representation to enable volumetric rendering in image space or 3D geometry
reconstruction (point clouds or mesh) with transparent surface prediction. We
evaluate our method's effectiveness qualitatively and quantitatively on a new
dataset collected using a custom, low-cost sensing platform featuring RGB-D
cameras and ultrasonic sensors. Our method exhibits significant improvement
over state-of-the-art for transparent surface reconstruction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://umfieldrobotics.github.io/VAIR_site/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncovering Hidden Subspaces in Video Diffusion Models Using
  Re-Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04956v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04956v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mischa Dombrowski, Hadrien Reynaud, Bernhard Kainz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Latent Video Diffusion Models can easily deceive casual observers and domain
experts alike thanks to the produced image quality and temporal consistency.
Beyond entertainment, this creates opportunities around safe data sharing of
fully synthetic datasets, which are crucial in healthcare, as well as other
domains relying on sensitive personal information. However, privacy concerns
with this approach have not fully been addressed yet, and models trained on
synthetic data for specific downstream tasks still perform worse than those
trained on real data. This discrepancy may be partly due to the sampling space
being a subspace of the training videos, effectively reducing the training data
size for downstream models. Additionally, the reduced temporal consistency when
generating long videos could be a contributing factor.
  In this paper, we first show that training privacy-preserving models in
latent space is computationally more efficient and generalize better.
Furthermore, to investigate downstream degradation factors, we propose to use a
re-identification model, previously employed as a privacy preservation filter.
We demonstrate that it is sufficient to train this model on the latent space of
the video generator. Subsequently, we use these models to evaluate the subspace
covered by synthetic video datasets and thus introduce a new way to measure the
faithfulness of generative machine learning models. We focus on a specific
application in healthcare echocardiography to illustrate the effectiveness of
our novel methods. Our findings indicate that only up to 30.8% of the training
videos are learned in latent video diffusion models, which could explain the
lack of performance when training downstream tasks on synthetic data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 tables, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CAD-MLLM: Unifying <span class="highlight-title">Multimodal</span>ity-Conditioned CAD Generation With MLLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04954v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04954v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingwei Xu, Chenyu Wang, Zibo Zhao, Wen Liu, Yi Ma, Shenghua Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper aims to design a unified Computer-Aided Design (CAD) generation
system that can easily generate CAD models based on the user's inputs in the
form of textual description, images, point clouds, or even a combination of
them. Towards this goal, we introduce the CAD-MLLM, the first system capable of
generating parametric CAD models conditioned on the multimodal input.
Specifically, within the CAD-MLLM framework, we leverage the command sequences
of CAD models and then employ advanced large language models (LLMs) to align
the feature space across these diverse multi-modalities data and CAD models'
vectorized representations. To facilitate the model training, we design a
comprehensive data construction and annotation pipeline that equips each CAD
model with corresponding multimodal data. Our resulting dataset, named
Omni-CAD, is the first multimodal CAD dataset that contains textual
description, multi-view images, points, and command sequence for each CAD
model. It contains approximately 450K instances and their CAD construction
sequences. To thoroughly evaluate the quality of our generated CAD models, we
go beyond current evaluation metrics that focus on reconstruction quality by
introducing additional metrics that assess topology quality and surface
enclosure extent. Extensive experimental results demonstrate that CAD-MLLM
significantly outperforms existing conditional generative methods and remains
highly robust to noises and missing points. The project page and more
visualizations can be found at: https://cad-mllm.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://cad-mllm.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ M3DocRAG: <span class="highlight-title">Multi-modal</span> Retrieval is What You Need for Multi-page
  Multi-document Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04952v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04952v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaemin Cho, Debanjan Mahata, Ozan Irsoy, Yujie He, Mohit Bansal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Document visual question answering (DocVQA) pipelines that answer questions
from documents have broad applications. Existing methods focus on handling
single-page documents with multi-modal language models (MLMs), or rely on
text-based retrieval-augmented generation (RAG) that uses text extraction tools
such as optical character recognition (OCR). However, there are difficulties in
applying these methods in real-world scenarios: (a) questions often require
information across different pages or documents, where MLMs cannot handle many
long documents; (b) documents often have important information in visual
elements such as figures, but text extraction tools ignore them. We introduce
M3DocRAG, a novel multi-modal RAG framework that flexibly accommodates various
document contexts (closed-domain and open-domain), question hops (single-hop
and multi-hop), and evidence modalities (text, chart, figure, etc.). M3DocRAG
finds relevant documents and answers questions using a multi-modal retriever
and an MLM, so that it can efficiently handle single or many documents while
preserving visual information. Since previous DocVQA datasets ask questions in
the context of a specific document, we also present M3DocVQA, a new benchmark
for evaluating open-domain DocVQA over 3,000+ PDF documents with 40,000+ pages.
In three benchmarks (M3DocVQA/MMLongBench-Doc/MP-DocVQA), empirical results
show that M3DocRAG with ColPali and Qwen2-VL 7B achieves superior performance
than many strong baselines, including state-of-the-art performance in
MP-DocVQA. We provide comprehensive analyses of different indexing, MLMs, and
retrieval models. Lastly, we qualitatively show that M3DocRAG can successfully
handle various scenarios, such as when relevant information exists across
multiple pages and when answer evidence only exists in images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project webpage: https://m3docrag.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Reinforcement Learning-Based Automatic Video Editing Method Using
  Pre-trained <span class="highlight-title">Vision-Language</span> Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04942v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04942v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Panwen Hu, Nan Xiao, Feifei Li, Yongquan Chen, Rui Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this era of videos, automatic video editing techniques attract more and
more attention from industry and academia since they can reduce workloads and
lower the requirements for human editors. Existing automatic editing systems
are mainly scene- or event-specific, e.g., soccer game broadcasting, yet the
automatic systems for general editing, e.g., movie or vlog editing which covers
various scenes and events, were rarely studied before, and converting the
event-driven editing method to a general scene is nontrivial. In this paper, we
propose a two-stage scheme for general editing. Firstly, unlike previous works
that extract scene-specific features, we leverage the pre-trained
Vision-Language Model (VLM) to extract the editing-relevant representations as
editing context. Moreover, to close the gap between the professional-looking
videos and the automatic productions generated with simple guidelines, we
propose a Reinforcement Learning (RL)-based editing framework to formulate the
editing problem and train the virtual editor to make better sequential editing
decisions. Finally, we evaluate the proposed method on a more general editing
task with a real movie dataset. Experimental results demonstrate the
effectiveness and benefits of the proposed context representation and the
learning ability of our RL-based editing framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SaSR-Net: Source-Aware Semantic Representation Network for Enhancing
  Audio-Visual Question Answering <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04933v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04933v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        ianyu Yang, Yiyang Nan, Lisen Dai, Zhenwen Liang, Yapeng Tian, Xiangliang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-Visual Question Answering (AVQA) is a challenging task that involves
answering questions based on both auditory and visual information in videos. A
significant challenge is interpreting complex multi-modal scenes, which include
both visual objects and sound sources, and connecting them to the given
question. In this paper, we introduce the Source-aware Semantic Representation
Network (SaSR-Net), a novel model designed for AVQA. SaSR-Net utilizes
source-wise learnable tokens to efficiently capture and align audio-visual
elements with the corresponding question. It streamlines the fusion of audio
and visual information using spatial and temporal attention mechanisms to
identify answers in multi-modal scenes. Extensive experiments on the Music-AVQA
and AVQA-Yang datasets show that SaSR-Net outperforms state-of-the-art AVQA
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DimensionX: Create Any 3D and 4D Scenes from a Single Image with
  Controllable Video Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04928v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04928v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqiang Sun, Shuo Chen, Fangfu Liu, Zilong Chen, Yueqi Duan, Jun Zhang, Yikai Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce \textbf{DimensionX}, a framework designed to
generate photorealistic 3D and 4D scenes from just a single image with video
diffusion. Our approach begins with the insight that both the spatial structure
of a 3D scene and the temporal evolution of a 4D scene can be effectively
represented through sequences of video frames. While recent video diffusion
models have shown remarkable success in producing vivid visuals, they face
limitations in directly recovering 3D/4D scenes due to limited spatial and
temporal controllability during generation. To overcome this, we propose
ST-Director, which decouples spatial and temporal factors in video diffusion by
learning dimension-aware LoRAs from dimension-variant data. This controllable
video diffusion approach enables precise manipulation of spatial structure and
temporal dynamics, allowing us to reconstruct both 3D and 4D representations
from sequential frames with the combination of spatial and temporal dimensions.
Additionally, to bridge the gap between generated videos and real-world scenes,
we introduce a trajectory-aware mechanism for 3D generation and an
identity-preserving denoising strategy for 4D generation. Extensive experiments
on various real-world and synthetic datasets demonstrate that DimensionX
achieves superior results in controllable video generation, as well as in 3D
and 4D scene generation, compared with previous methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://chenshuo20.github.io/DimensionX/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ StoryAgent: Customized Storytelling Video Generation via Multi-Agent
  Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04925v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04925v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Panwen Hu, Jin Jiang, Jianqi Chen, Mingfei Han, Shengcai Liao, Xiaojun Chang, Xiaodan Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of AI-Generated Content (AIGC) has spurred research into automated
video generation to streamline conventional processes. However, automating
storytelling video production, particularly for customized narratives, remains
challenging due to the complexity of maintaining subject consistency across
shots. While existing approaches like Mora and AesopAgent integrate multiple
agents for Story-to-Video (S2V) generation, they fall short in preserving
protagonist consistency and supporting Customized Storytelling Video Generation
(CSVG). To address these limitations, we propose StoryAgent, a multi-agent
framework designed for CSVG. StoryAgent decomposes CSVG into distinct subtasks
assigned to specialized agents, mirroring the professional production process.
Notably, our framework includes agents for story design, storyboard generation,
video creation, agent coordination, and result evaluation. Leveraging the
strengths of different models, StoryAgent enhances control over the generation
process, significantly improving character consistency. Specifically, we
introduce a customized Image-to-Video (I2V) method, LoRA-BE, to enhance
intra-shot temporal consistency, while a novel storyboard generation pipeline
is proposed to maintain subject consistency across shots. Extensive experiments
demonstrate the effectiveness of our approach in synthesizing highly consistent
storytelling videos, outperforming state-of-the-art methods. Our contributions
include the introduction of StoryAgent, a versatile framework for video
generation tasks, and novel techniques for preserving protagonist consistency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MVSplat360: Feed-Forward 360 Scene Synthesis from Sparse Views <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04924v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04924v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuedong Chen, Chuanxia Zheng, Haofei Xu, Bohan Zhuang, Andrea Vedaldi, Tat-Jen Cham, Jianfei Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce MVSplat360, a feed-forward approach for 360{\deg} novel view
synthesis (NVS) of diverse real-world scenes, using only sparse observations.
This setting is inherently ill-posed due to minimal overlap among input views
and insufficient visual information provided, making it challenging for
conventional methods to achieve high-quality results. Our MVSplat360 addresses
this by effectively combining geometry-aware 3D reconstruction with temporally
consistent video generation. Specifically, it refactors a feed-forward 3D
Gaussian Splatting (3DGS) model to render features directly into the latent
space of a pre-trained Stable Video Diffusion (SVD) model, where these features
then act as pose and visual cues to guide the denoising process and produce
photorealistic 3D-consistent views. Our model is end-to-end trainable and
supports rendering arbitrary views with as few as 5 sparse input views. To
evaluate MVSplat360's performance, we introduce a new benchmark using the
challenging DL3DV-10K dataset, where MVSplat360 achieves superior visual
quality compared to state-of-the-art methods on wide-sweeping or even 360{\deg}
NVS tasks. Experiments on the existing benchmark RealEstate10K also confirm the
effectiveness of our model. The video results are available on our project
page: https://donydchen.github.io/mvsplat360.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024, Project page: https://donydchen.github.io/mvsplat360,
  Code: https://github.com/donydchen/mvsplat360</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VideoGLaMM: A Large <span class="highlight-title">Multimodal</span> Model for Pixel-Level Visual Grounding in
  Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04923v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04923v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shehan Munasinghe, Hanan Gani, Wenqi Zhu, Jiale Cao, Eric Xing, Fahad Shahbaz Khan, Salman Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-grained alignment between videos and text is challenging due to complex
spatial and temporal dynamics in videos. Existing video-based Large Multimodal
Models (LMMs) handle basic conversations but struggle with precise pixel-level
grounding in videos. To address this, we introduce VideoGLaMM, a LMM designed
for fine-grained pixel-level grounding in videos based on user-provided textual
inputs. Our design seamlessly connects three key components: a Large Language
Model, a dual vision encoder that emphasizes both spatial and temporal details,
and a spatio-temporal decoder for accurate mask generation. This connection is
facilitated via tunable V-L and L-V adapters that enable close Vision-Language
(VL) alignment. The architecture is trained to synchronize both spatial and
temporal elements of video content with textual instructions. To enable
fine-grained grounding, we curate a multimodal dataset featuring detailed
visually-grounded conversations using a semiautomatic annotation pipeline,
resulting in a diverse set of 38k video-QA triplets along with 83k objects and
671k masks. We evaluate VideoGLaMM on three challenging tasks: Grounded
Conversation Generation, Visual Grounding, and Referring Video Segmentation.
Experimental results show that our model consistently outperforms existing
approaches across all three tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report of VideoGLaMM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stem-OB: Generalizable Visual Imitation Learning with Stem-Like
  Convergent Observation through Diffusion Inversion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04919v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04919v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaizhe Hu, Zihang Rui, Yao He, Yuyao Liu, Pu Hua, Huazhe Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual imitation learning methods demonstrate strong performance, yet they
lack generalization when faced with visual input perturbations, including
variations in lighting and textures, impeding their real-world application. We
propose Stem-OB that utilizes pretrained image diffusion models to suppress
low-level visual differences while maintaining high-level scene structures.
This image inversion process is akin to transforming the observation into a
shared representation, from which other observations stem, with extraneous
details removed. Stem-OB contrasts with data-augmentation approaches as it is
robust to various unspecified appearance changes without the need for
additional training. Our method is a simple yet highly effective plug-and-play
solution. Empirical results confirm the effectiveness of our approach in
simulated tasks and show an exceptionally significant improvement in real-world
applications, with an average increase of 22.2% in success rates compared to
the best baseline. See https://hukz18.github.io/Stem-Ob/ for more info.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Arxiv preprint version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Iris Centre Localisation for Assistive Eye-Gaze Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04912v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04912v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nipun Sandamal Ranasekara Pathiranage, Stefania Cristina, Kenneth P. Camilleri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this research work, we address the problem of robust iris centre
localisation in unconstrained conditions as a core component of our eye-gaze
tracking platform. We investigate the application of U-Net variants for
segmentation-based and regression-based approaches to improve our iris centre
localisation, which was previously based on Bayes' classification. The achieved
results are comparable to or better than the state-of-the-art, offering a
drastic improvement over those achieved by the Bayes' classifier, and without
sacrificing the real-time performance of our eye-gaze tracking platform.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ In the Era of <span class="highlight-title">Prompt</span> Learning with <span class="highlight-title">Vision-Language</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04892v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04892v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ankit Jha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale foundation models like CLIP have shown strong zero-shot
generalization but struggle with domain shifts, limiting their adaptability. In
our work, we introduce \textsc{StyLIP}, a novel domain-agnostic prompt learning
strategy for Domain Generalization (DG). StyLIP disentangles visual style and
content in CLIP`s vision encoder by using style projectors to learn
domain-specific prompt tokens and combining them with content features. Trained
contrastively, this approach enables seamless adaptation across domains,
outperforming state-of-the-art methods on multiple DG benchmarks. Additionally,
we propose AD-CLIP for unsupervised domain adaptation (DA), leveraging CLIP`s
frozen vision backbone to learn domain-invariant prompts through image style
and content features. By aligning domains in embedding space with entropy
minimization, AD-CLIP effectively handles domain shifts, even when only target
domain samples are available. Lastly, we outline future work on class discovery
using prompt learning for semantic segmentation in remote sensing, focusing on
identifying novel or rare classes in unstructured environments. This paves the
way for more adaptive and generalizable models in complex, real-world
scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICVGIP 2024, Young Faculty Symposium</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ZAHA: Introducing the Level of Facade Generalization and the Large-Scale
  Point Cloud Facade Semantic Segmentation Benchmark Dataset <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04865v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04865v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olaf Wysocki, Yue Tan, Thomas Froech, Yan Xia, Magdalena Wysocki, Ludwig Hoegner, Daniel Cremers, Christoph Holst
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facade semantic segmentation is a long-standing challenge in photogrammetry
and computer vision. Although the last decades have witnessed the influx of
facade segmentation methods, there is a lack of comprehensive facade classes
and data covering the architectural variability. In ZAHA, we introduce Level of
Facade Generalization (LoFG), novel hierarchical facade classes designed based
on international urban modeling standards, ensuring compatibility with
real-world challenging classes and uniform methods' comparison. Realizing the
LoFG, we present to date the largest semantic 3D facade segmentation dataset,
providing 601 million annotated points at five and 15 classes of LoFG2 and
LoFG3, respectively. Moreover, we analyze the performance of baseline semantic
segmentation methods on our introduced LoFG classes and data, complementing it
with a discussion on the unresolved challenges for facade segmentation. We
firmly believe that ZAHA shall facilitate further development of 3D facade
semantic segmentation methods, enabling robust segmentation indispensable in
creating urban digital twins.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to WACV 2025 (IEEE/CVF Winter Conference on Applications of
  Computer Vision (WACV))</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A multi-purpose automatic editing system based on lecture semantics for
  remote education 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04859v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04859v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Panwen Hu, Rui Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Remote teaching has become popular recently due to its convenience and
safety, especially under extreme circumstances like a pandemic. However, online
students usually have a poor experience since the information acquired from the
views provided by the broadcast platforms is limited. One potential solution is
to show more camera views simultaneously, but it is technically challenging and
distracting for the viewers. Therefore, an automatic multi-camera
directing/editing system, which aims at selecting the most concerned view at
each time instance to guide the attention of online students, is in urgent
demand. However, existing systems mostly make simple assumptions and focus on
tracking the position of the speaker instead of the real lecture semantics, and
therefore have limited capacities to deliver optimal information flow. To this
end, this paper proposes an automatic multi-purpose editing system based on the
lecture semantics, which can both direct the multiple video streams for
real-time broadcasting and edit the optimal video offline for review purposes.
Our system directs the views by semantically analyzing the class events while
following the professional directing rules, mimicking a human director to
capture the regions of interest from the viewpoint of the onsite students. We
conduct both qualitative and quantitative analyses to verify the effectiveness
of the proposed system and its components.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differentiable Gaussian Representation for Incomplete CT Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04844v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04844v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaokai Wu, Yuxiang Lu, Wei Ji, Suizhi Huang, Fengyu Yang, Shalayiding Sirejiding, Qichen He, Jing Tong, Yanbiao Ji, Yue Ding, Hongtao Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incomplete Computed Tomography (CT) benefits patients by reducing radiation
exposure. However, reconstructing high-fidelity images from limited views or
angles remains challenging due to the ill-posed nature of the problem. Deep
Learning Reconstruction (DLR) methods have shown promise in enhancing image
quality, but the paradox between training data diversity and high
generalization ability remains unsolved. In this paper, we propose a novel
Gaussian Representation for Incomplete CT Reconstruction (GRCT) without the
usage of any neural networks or full-dose CT data. Specifically, we model the
3D volume as a set of learnable Gaussians, which are optimized directly from
the incomplete sinogram. Our method can be applied to multiple views and angles
without changing the architecture. Additionally, we propose a differentiable
Fast CT Reconstruction method for efficient clinical usage. Extensive
experiments on multiple datasets and settings demonstrate significant
improvements in reconstruction quality metrics and high efficiency. We plan to
release our code as open-source.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ D$^3$epth: Self-Supervised Depth Estimation with Dynamic Mask in Dynamic
  Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04826v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04826v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyu Chen, Hong Liu, Wenhao Li, Ying Zhu, Guoquan Wang, Jianbing Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Depth estimation is a crucial technology in robotics. Recently,
self-supervised depth estimation methods have demonstrated great potential as
they can efficiently leverage large amounts of unlabelled real-world data.
However, most existing methods are designed under the assumption of static
scenes, which hinders their adaptability in dynamic environments. To address
this issue, we present D$^3$epth, a novel method for self-supervised depth
estimation in dynamic scenes. It tackles the challenge of dynamic objects from
two key perspectives. First, within the self-supervised framework, we design a
reprojection constraint to identify regions likely to contain dynamic objects,
allowing the construction of a dynamic mask that mitigates their impact at the
loss level. Second, for multi-frame depth estimation, we introduce a cost
volume auto-masking strategy that leverages adjacent frames to identify regions
associated with dynamic objects and generate corresponding masks. This provides
guidance for subsequent processes. Furthermore, we propose a spectral entropy
uncertainty module that incorporates spectral entropy to guide uncertainty
estimation during depth fusion, effectively addressing issues arising from cost
volume computation in dynamic environments. Extensive experiments on KITTI and
Cityscapes datasets demonstrate that the proposed method consistently
outperforms existing self-supervised monocular depth estimation baselines. Code
is available at \url{https://github.com/Csyunling/D3epth}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Open sourced</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ End-to-end Inception-Unet based Generative Adversarial Networks for Snow
  and Rain Removals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04821v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04821v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ibrahim Kajo, Mohamed Kas, Yassine Ruichek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The superior performance introduced by deep learning approaches in removing
atmospheric particles such as snow and rain from a single image; favors their
usage over classical ones. However, deep learning-based approaches still suffer
from challenges related to the particle appearance characteristics such as
size, type, and transparency. Furthermore, due to the unique characteristics of
rain and snow particles, single network based deep learning approaches struggle
in handling both degradation scenarios simultaneously. In this paper, a global
framework that consists of two Generative Adversarial Networks (GANs) is
proposed where each handles the removal of each particle individually. The
architectures of both desnowing and deraining GANs introduce the integration of
a feature extraction phase with the classical U-net generator network which in
turn enhances the removal performance in the presence of severe variations in
size and appearance. Furthermore, a realistic dataset that contains pairs of
snowy images next to their groundtruth images estimated using a low-rank
approximation approach; is presented. The experiments show that the proposed
desnowing and deraining approaches achieve significant improvements in
comparison to the state-of-the-art approaches when tested on both synthetic and
realistic datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GANESH: Generalizable NeRF for Lensless Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04810v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04810v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rakesh Raj Madavan, Akshat Kaimal, Badhrinarayanan K V, Vinayak Gupta, Rohit Choudhary, Chandrakala Shanmuganathan, Kaushik Mitra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lensless imaging offers a significant opportunity to develop ultra-compact
cameras by removing the conventional bulky lens system. However, without a
focusing element, the sensor's output is no longer a direct image but a complex
multiplexed scene representation. Traditional methods have attempted to address
this challenge by employing learnable inversions and refinement models, but
these methods are primarily designed for 2D reconstruction and do not
generalize well to 3D reconstruction. We introduce GANESH, a novel framework
designed to enable simultaneous refinement and novel view synthesis from
multi-view lensless images. Unlike existing methods that require scene-specific
training, our approach supports on-the-fly inference without retraining on each
scene. Moreover, our framework allows us to tune our model to specific scenes,
enhancing the rendering and refinement quality. To facilitate research in this
area, we also present the first multi-view lensless dataset, LenslessScenes.
Extensive experiments demonstrate that our method outperforms current
approaches in reconstruction accuracy and refinement quality. Code and video
results are available at https://rakesh-123-cryp.github.io/Rakesh.github.io/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MPVO: Motion-Prior based Visual Odometry for PointGoal Navigation <span class="chip">ECCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04796v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04796v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sayan Paul, Ruddra dev Roychoudhury, Brojeshwar Bhowmick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual odometry (VO) is essential for enabling accurate point-goal navigation
of embodied agents in indoor environments where GPS and compass sensors are
unreliable and inaccurate. However, traditional VO methods face challenges in
wide-baseline scenarios, where fast robot motions and low frames per second
(FPS) during inference hinder their performance, leading to drift and
catastrophic failures in point-goal navigation. Recent deep-learned VO methods
show robust performance but suffer from sample inefficiency during training;
hence, they require huge datasets and compute resources. So, we propose a
robust and sample-efficient VO pipeline based on motion priors available while
an agent is navigating an environment. It consists of a training-free
action-prior based geometric VO module that estimates a coarse relative pose
which is further consumed as a motion prior by a deep-learned VO model, which
finally produces a fine relative pose to be used by the navigation policy. This
strategy helps our pipeline achieve up to 2x sample efficiency during training
and demonstrates superior accuracy and robustness in point-goal navigation
tasks compared to state-of-the-art VO method(s). Realistic indoor environments
of the Gibson dataset is used in the AI-Habitat simulator to evaluate the
proposed approach using navigation metrics (like success/SPL) and pose metrics
(like RPE/ATE). We hope this method further opens a direction of work where
motion priors from various sources can be utilized to improve VO estimates and
achieve better results in embodied navigation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in 50SFM Workshop of the 18th European Conference on
  Computer Vision (ECCV) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Effective Pipeline for Whole-Slide Image Glomerulus Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04782v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04782v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quan Huu Cap
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Whole-slide images (WSI) glomerulus segmentation is essential for accurately
diagnosing kidney diseases. In this work, we propose a practical pipeline for
glomerulus segmentation that effectively enhances both patch-level and
WSI-level segmentation tasks. Our approach leverages stitching on overlapping
patches, increasing the detection coverage, especially when glomeruli are
located near patch image borders. In addition, we conduct comprehensive
evaluations from different segmentation models across two large and diverse
datasets with over 30K glomerulus annotations. Experimental results demonstrate
that models using our pipeline outperform the previous state-of-the-art method,
achieving superior results across both datasets and setting a new benchmark for
glomerulus segmentation in WSIs. The code and pre-trained models are available
at https://github.com/huuquan1994/wsi_glomerulus_seg.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Taming Rectified Flow for Inversion and Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04746v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04746v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiangshan Wang, Junfu Pu, Zhongang Qi, Jiayi Guo, Yue Ma, Nisha Huang, Yuxin Chen, Xiu Li, Ying Shan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rectified-flow-based diffusion transformers, such as FLUX and OpenSora, have
demonstrated exceptional performance in the field of image and video
generation. Despite their robust generative capabilities, these models often
suffer from inaccurate inversion, which could further limit their effectiveness
in downstream tasks such as image and video editing. To address this issue, we
propose RF-Solver, a novel training-free sampler that enhances inversion
precision by reducing errors in the process of solving rectified flow ODEs.
Specifically, we derive the exact formulation of the rectified flow ODE and
perform a high-order Taylor expansion to estimate its nonlinear components,
significantly decreasing the approximation error at each timestep. Building
upon RF-Solver, we further design RF-Edit, which comprises specialized
sub-modules for image and video editing. By sharing self-attention layer
features during the editing process, RF-Edit effectively preserves the
structural information of the source image or video while achieving
high-quality editing results. Our approach is compatible with any pre-trained
rectified-flow-based models for image and video tasks, requiring no additional
training or optimization. Extensive experiments on text-to-image generation,
image & video inversion, and image & video editing demonstrate the robust
performance and adaptability of our methods. Code is available at
https://github.com/wangjiangshan0725/RF-Solver-Edit.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Convolutional Differentiable Logic Gate Networks <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04732v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04732v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Petersen, Hilde Kuehne, Christian Borgelt, Julian Welzel, Stefano Ermon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increasing inference cost of machine learning models, there is a
growing interest in models with fast and efficient inference. Recently, an
approach for learning logic gate networks directly via a differentiable
relaxation was proposed. Logic gate networks are faster than conventional
neural network approaches because their inference only requires logic gate
operators such as NAND, OR, and XOR, which are the underlying building blocks
of current hardware and can be efficiently executed. We build on this idea,
extending it by deep logic gate tree convolutions, logical OR pooling, and
residual initializations. This allows scaling logic gate networks up by over
one order of magnitude and utilizing the paradigm of convolution. On CIFAR-10,
we achieve an accuracy of 86.29% using only 61 million logic gates, which
improves over the SOTA while being 29x smaller.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at NeurIPS 2024 (Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Controlling Human Shape and Pose in Text-to-Image Diffusion Models via
  Domain Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04724v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04724v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benito Buchheim, Max Reimann, Jürgen Döllner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a methodology for conditional control of human shape and pose in
pretrained text-to-image diffusion models using a 3D human parametric model
(SMPL). Fine-tuning these diffusion models to adhere to new conditions requires
large datasets and high-quality annotations, which can be more cost-effectively
acquired through synthetic data generation rather than real-world data.
However, the domain gap and low scene diversity of synthetic data can
compromise the pretrained model's visual fidelity. We propose a
domain-adaptation technique that maintains image quality by isolating
synthetically trained conditional information in the classifier-free guidance
vector and composing it with another control network to adapt the generated
images to the input domain. To achieve SMPL control, we fine-tune a
ControlNet-based architecture on the synthetic SURREAL dataset of rendered
humans and apply our domain adaptation at generation time. Experiments
demonstrate that our model achieves greater shape and pose diversity than the
2d pose-based ControlNet, while maintaining the visual fidelity and improving
stability, proving its usefulness for downstream tasks such as human animation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Subspace-Constrained Quadratic Matrix Factorization: Algorithm and
  Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04717v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04717v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Zhai, Xiaohui Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Matrix Factorization has emerged as a widely adopted framework for modeling
data exhibiting low-rank structures. To address challenges in manifold
learning, this paper presents a subspace-constrained quadratic matrix
factorization model. The model is designed to jointly learn key low-dimensional
structures, including the tangent space, the normal subspace, and the quadratic
form that links the tangent space to a low-dimensional representation. We solve
the proposed factorization model using an alternating minimization method,
involving an in-depth investigation of nonlinear regression and projection
subproblems. Theoretical properties of the quadratic projection problem and
convergence characteristics of the alternating strategy are also investigated.
To validate our approach, we conduct numerical experiments on synthetic and
real-world datasets. Results demonstrate that our model outperforms existing
methods, highlighting its robustness and efficacy in capturing core
low-dimensional structures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeuroFly: A framework for whole-brain single neuron reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04715v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04715v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rubin Zhao, Yang Liu, Shiqi Zhang, Zijian Yi, Yanyang Xiao, Fang Xu, Yi Yang, Pencheng Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neurons, with their elongated, tree-like dendritic and axonal structures,
enable efficient signal integration and long-range communication across brain
regions. By reconstructing individual neurons' morphology, we can gain valuable
insights into brain connectivity, revealing the structure basis of cognition,
movement, and perception. Despite the accumulation of extensive 3D microscopic
imaging data, progress has been considerably hindered by the absence of
automated tools to streamline this process. Here we introduce NeuroFly, a
validated framework for large-scale automatic single neuron reconstruction.
This framework breaks down the process into three distinct stages:
segmentation, connection, and proofreading. In the segmentation stage, we
perform automatic segmentation followed by skeletonization to generate
over-segmented neuronal fragments without branches. During the connection
stage, we use a 3D image-based path following approach to extend each fragment
and connect it with other fragments of the same neuron. Finally, human
annotators are required only to proofread the few unresolved positions. The
first two stages of our process are clearly defined computer vision problems,
and we have trained robust baseline models to solve them. We validated
NeuroFly's efficiency using in-house datasets that include a variety of
challenging scenarios, such as dense arborizations, weak axons, images with
contamination. We will release the datasets along with a suite of visualization
and annotation tools for better reproducibility. Our goal is to foster
collaboration among researchers to address the neuron reconstruction challenge,
ultimately accelerating advancements in neuroscience research. The dataset and
code are available at https://github.com/beanli161514/neurofly
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Progressive Multi-Level Alignments for Semi-Supervised Domain Adaptation
  SAR Target Recognition Using Simulated Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04711v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04711v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinzheng Zhang, Hui Zhu, Hongqian Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, an intriguing research trend for automatic target recognition (ATR)
from synthetic aperture radar (SAR) imagery has arisen: using simulated data to
train ATR models is a feasible solution to the issue of inadequate measured
data. To close the domain gap that exists between the real and simulated data,
the unsupervised domain adaptation (UDA) techniques are frequently exploited to
construct ATR models. However, for UDA, the target domain lacks labeled data to
direct the model training, posing a great challenge to ATR performance. To
address the above problem, a semi-supervised domain adaptation (SSDA) framework
has been proposed adopting progressive multi-level alignments for simulated
data-aided SAR ATR. First, a progressive wavelet transform data augmentation
(PWTDA) is presented by analyzing the discrepancies of wavelet decomposition
sub-bands of two domain images, obtaining the domain-level alignment.
Specifically, the domain gap is narrowed by mixing the wavelet transform
high-frequency sub-band components. Second, we develop an asymptotic
instance-prototype alignment (AIPA) strategy to push the source domain
instances close to the corresponding target prototypes, aiming to achieve
category-level alignment. Moreover, the consistency alignment is implemented by
excavating the strong-weak augmentation consistency of both individual samples
and the multi-sample relationship, enhancing the generalization capability of
the model. Extensive experiments on the Synthetic and Measured Paired Labeled
Experiment (SAMPLE) dataset, indicate that our approach obtains recognition
accuracies of 99.63% and 98.91% in two common experimental settings with only
one labeled sample per class of the target domain, outperforming the most
advanced SSDA techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From CNN to ConvRNN: Adapting Visualization Techniques for Time-Series
  Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04707v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04707v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabien Poirier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, neural networks are commonly used to solve various problems.
Unfortunately, despite their effectiveness, they are often perceived as black
boxes capable of providing answers without explaining their decisions, which
raises numerous ethical and legal concerns. Fortunately, the field of
explainability helps users understand these results. This aspect of machine
learning allows users to grasp the decision-making process of a model and
verify the relevance of its outcomes. In this article, we focus on the learning
process carried out by a ``time distributed`` convRNN, which performs anomaly
detection from video data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ESC-MISR: Enhancing Spatial Correlations for Multi-Image
  Super-Resolution in Remote Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04706v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04706v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihui Zhang, Jinhui Pang, Jianan Li, Xiaoshuai Hao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-Image Super-Resolution (MISR) is a crucial yet challenging research
task in the remote sensing community. In this paper, we address the challenging
task of Multi-Image Super-Resolution in Remote Sensing (MISR-RS), aiming to
generate a High-Resolution (HR) image from multiple Low-Resolution (LR) images
obtained by satellites. Recently, the weak temporal correlations among LR
images have attracted increasing attention in the MISR-RS task. However,
existing MISR methods treat the LR images as sequences with strong temporal
correlations, overlooking spatial correlations and imposing temporal
dependencies. To address this problem, we propose a novel end-to-end framework
named Enhancing Spatial Correlations in MISR (ESC-MISR), which fully exploits
the spatial-temporal relations of multiple images for HR image reconstruction.
Specifically, we first introduce a novel fusion module named Multi-Image
Spatial Transformer (MIST), which emphasizes parts with clearer global spatial
features and enhances the spatial correlations between LR images. Besides, we
perform a random shuffle strategy for the sequential inputs of LR images to
attenuate temporal dependencies and capture weak temporal correlations in the
training stage. Compared with the state-of-the-art methods, our ESC-MISR
achieves 0.70dB and 0.76dB cPSNR improvements on the two bands of the PROBA-V
dataset respectively, demonstrating the superiority of our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Brightness Adaptation for Robust <span class="highlight-title">Multi-modal</span> Image Fusion <span class="chip">IJCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04697v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04697v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Sun, Bing Cao, Pengfei Zhu, Qinghua Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Infrared and visible image fusion aim to integrate modality strengths for
visually enhanced, informative images. Visible imaging in real-world scenarios
is susceptible to dynamic environmental brightness fluctuations, leading to
texture degradation. Existing fusion methods lack robustness against such
brightness perturbations, significantly compromising the visual fidelity of the
fused imagery. To address this challenge, we propose the Brightness Adaptive
multimodal dynamic fusion framework (BA-Fusion), which achieves robust image
fusion despite dynamic brightness fluctuations. Specifically, we introduce a
Brightness Adaptive Gate (BAG) module, which is designed to dynamically select
features from brightness-related channels for normalization, while preserving
brightness-independent structural information within the source images.
Furthermore, we propose a brightness consistency loss function to optimize the
BAG module. The entire framework is tuned via alternating training strategies.
Extensive experiments validate that our method surpasses state-of-the-art
methods in preserving multi-modal image information and visual fidelity, while
exhibiting remarkable robustness across varying brightness levels. Our code is
available: https://github.com/SunYM2020/BA-Fusion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IJCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reciprocal Point Learning Network with Large Electromagnetic Kernel for
  SAR Open-Set Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04693v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04693v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiayang Xiao, Zhuoxuan Li, Ruyi Zhang, Jiacheng Chen, Haipeng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The limitations of existing Synthetic Aperture Radar (SAR) Automatic Target
Recognition (ATR) methods lie in their confinement by the closed-environment
assumption, hindering their effective and robust handling of unknown target
categories in open environments. Open Set Recognition (OSR), a pivotal facet
for algorithmic practicality, intends to categorize known classes while
denoting unknown ones as "unknown." The chief challenge in OSR involves
concurrently mitigating risks associated with generalizing features from a
restricted set of known classes to numerous unknown samples and the open space
exposure to potential unknown data. To enhance open-set SAR classification, a
method called scattering kernel with reciprocal learning network is proposed.
Initially, a feature learning framework is constructed based on reciprocal
point learning (RPL), establishing a bounded space for potential unknown
classes. This approach indirectly introduces unknown information into a learner
confined to known classes, thereby acquiring more concise and discriminative
representations. Subsequently, considering the variability in the imaging of
targets at different angles and the discreteness of components in SAR images, a
proposal is made to design convolutional kernels based on large-sized attribute
scattering center models. This enhances the ability to extract intrinsic
non-linear features and specific scattering characteristics in SAR images,
thereby improving the discriminative features of the model and mitigating the
impact of imaging variations on classification performance. Experiments on the
MSTAR datasets substantiate the superior performance of the proposed approach
called ASC-RPL over mainstream methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Personalized Federated Learning for Cross-view Geo-localization <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04692v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04692v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christos Anagnostopoulos, Alexandros Gkillas, Nikos Piperigkos, Aris S. Lalos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we propose a methodology combining Federated Learning (FL) with
Cross-view Image Geo-localization (CVGL) techniques. We address the challenges
of data privacy and heterogeneity in autonomous vehicle environments by
proposing a personalized Federated Learning scenario that allows selective
sharing of model parameters. Our method implements a coarse-to-fine approach,
where clients share only the coarse feature extractors while keeping
fine-grained features specific to local environments. We evaluate our approach
against traditional centralized and single-client training schemes using the
KITTI dataset combined with satellite imagery. Results demonstrate that our
federated CVGL method achieves performance close to centralized training while
maintaining data privacy. The proposed partial model sharing strategy shows
comparable or slightly better performance than classical FL, offering
significant reduced communication overhead without sacrificing accuracy. Our
work contributes to more robust and privacy-preserving localization systems for
autonomous vehicles operating in diverse environments
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 figures, Preprint submitted to the IEEE 26th International
  Workshop on Multimedia Signal Processing (MMSP)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DNN-based 3D Cloud Retrieval for Variable Solar Illumination and
  Multiview Spaceborne Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04682v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04682v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tamar Klein, Tom Aizenberg, Roi Ronen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Climate studies often rely on remotely sensed images to retrieve
two-dimensional maps of cloud properties. To advance volumetric analysis, we
focus on recovering the three-dimensional (3D) heterogeneous extinction
coefficient field of shallow clouds using multiview remote sensing data.
Climate research requires large-scale worldwide statistics. To enable scalable
data processing, previous deep neural networks (DNNs) can infer at spaceborne
remote sensing downlink rates. However, prior methods are limited to a fixed
solar illumination direction. In this work, we introduce the first scalable
DNN-based system for 3D cloud retrieval that accommodates varying camera poses
and solar directions. By integrating multiview cloud intensity images with
camera poses and solar direction data, we achieve greater flexibility in
recovery. Training of the DNN is performed by a novel two-stage scheme to
address the high number of degrees of freedom in this problem. Our approach
shows substantial improvements over previous state-of-the-art, particularly in
handling variations in the sun's zenith angle.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CaPo: Cooperative Plan Optimization for Efficient Embodied Multi-Agent
  Cooperation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04679v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04679v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Liu, Pan Zhou, Yingjun Du, Ah-Hwee Tan, Cees G. M. Snoek, Jan-Jakob Sonke, Efstratios Gavves
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we address the cooperation problem among large language model
(LLM) based embodied agents, where agents must cooperate to achieve a common
goal. Previous methods often execute actions extemporaneously and incoherently,
without long-term strategic and cooperative planning, leading to redundant
steps, failures, and even serious repercussions in complex tasks like
search-and-rescue missions where discussion and cooperative plan are crucial.
To solve this issue, we propose Cooperative Plan Optimization (CaPo) to enhance
the cooperation efficiency of LLM-based embodied agents. Inspired by human
cooperation schemes, CaPo improves cooperation efficiency with two phases: 1)
meta-plan generation, and 2) progress-adaptive meta-plan and execution. In the
first phase, all agents analyze the task, discuss, and cooperatively create a
meta-plan that decomposes the task into subtasks with detailed steps, ensuring
a long-term strategic and coherent plan for efficient coordination. In the
second phase, agents execute tasks according to the meta-plan and dynamically
adjust it based on their latest progress (e.g., discovering a target object)
through multi-turn discussions. This progress-based adaptation eliminates
redundant actions, improving the overall cooperation efficiency of agents.
Experimental results on the ThreeDworld Multi-Agent Transport and Communicative
Watch-And-Help tasks demonstrate that CaPo achieves much higher task completion
rate and efficiency compared with state-of-the-arts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explainable Search and Discovery of Visual Cultural Heritage Collections
  with <span class="highlight-title">Multimodal</span> Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04663v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04663v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taylor Arnold, Lauren Tilton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many cultural institutions have made large digitized visual collections
available online, often under permissible re-use licences. Creating interfaces
for exploring and searching these collections is difficult, particularly in the
absence of granular metadata. In this paper, we introduce a method for using
state-of-the-art multimodal large language models (LLMs) to enable an
open-ended, explainable search and discovery interface for visual collections.
We show how our approach can create novel clustering and recommendation systems
that avoid common pitfalls of methods based directly on visual embeddings. Of
particular interest is the ability to offer concrete textual explanations of
each recommendation without the need to preselect the features of interest.
Together, these features can create a digital interface that is more open-ended
and flexible while also being better suited to addressing privacy and ethical
concerns. Through a case study using a collection of documentary photographs,
we provide several metrics showing the efficacy and possibilities of our
approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, CHR 2024: Computational Humanities Research Conference,
  December 4 - 6, 2024, Aarhus University, Denmark</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated Image Color Mapping for a Historic Photographic Collection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04659v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04659v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taylor Arnold, Lauren Tilton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the 1970s, the United States Environmental Protection Agency sponsored
Documerica, a large-scale photography initiative to document environmental
subjects nation-wide. While over 15,000 digitized public-domain photographs
from the collection are available online, most of the images were scanned from
damaged copies of the original prints. We present and evaluate a modified
histogram matching technique based on the underlying chemistry of the prints
for correcting the damaged images by using training data collected from a small
set of undamaged prints. The entire set of color-adjusted Documerica images is
made available in an open repository.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, CHR 2024: Computational Humanities Research Conference,
  December 4 - 6, 2024, Aarhus University, Denmark</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ICH-SCNet: Intracerebral Hemorrhage Segmentation and Prognosis
  Classification Network Using <span class="highlight-title">CLIP</span>-guided SAM mechanism 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04656v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04656v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinlei Yu, Ahmed Elazab, Ruiquan Ge, Hui Jin, Xinchen Jiang, Gangyong Jia, Qing Wu, Qinglei Shi, Changmiao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intracerebral hemorrhage (ICH) is the most fatal subtype of stroke and is
characterized by a high incidence of disability. Accurate segmentation of the
ICH region and prognosis prediction are critically important for developing and
refining treatment plans for post-ICH patients. However, existing approaches
address these two tasks independently and predominantly focus on imaging data
alone, thereby neglecting the intrinsic correlation between the tasks and
modalities. This paper introduces a multi-task network, ICH-SCNet, designed for
both ICH segmentation and prognosis classification. Specifically, we integrate
a SAM-CLIP cross-modal interaction mechanism that combines medical text and
segmentation auxiliary information with neuroimaging data to enhance
cross-modal feature recognition. Additionally, we develop an effective feature
fusion module and a multi-task loss function to improve performance further.
Extensive experiments on an ICH dataset reveal that our approach surpasses
other state-of-the-art methods. It excels in the overall performance of
classification tasks and outperforms competing models in all segmentation task
metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 figures, 3 tables, published to BIBM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DanceFusion: A Spatio-Temporal Skeleton Diffusion Transformer for
  Audio-Driven Dance Motion Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04646v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04646v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Zhao, Zhengmin Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces DanceFusion, a novel framework for reconstructing and
generating dance movements synchronized to music, utilizing a Spatio-Temporal
Skeleton Diffusion Transformer. The framework adeptly handles incomplete and
noisy skeletal data common in short-form dance videos on social media platforms
like TikTok. DanceFusion incorporates a hierarchical Transformer-based
Variational Autoencoder (VAE) integrated with a diffusion model, significantly
enhancing motion realism and accuracy. Our approach introduces sophisticated
masking techniques and a unique iterative diffusion process that refines the
motion sequences, ensuring high fidelity in both motion generation and
synchronization with accompanying audio cues. Comprehensive evaluations
demonstrate that DanceFusion surpasses existing methods, providing
state-of-the-art performance in generating dynamic, realistic, and
stylistically diverse dance motions. Potential applications of this framework
extend to content creation, virtual reality, and interactive entertainment,
promising substantial advancements in automated dance generation. Visit our
project page at https://th-mlab.github.io/DanceFusion/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TAP-VL: Text Layout-Aware Pre-training for Enriched <span class="highlight-title">Vision-Language</span>
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04642v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04642v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Fhima, Elad Ben Avraham, Oren Nuriel, Yair Kittenplon, Roy Ganz, Aviad Aberdam, Ron Litman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language (VL) models have garnered considerable research interest;
however, they still face challenges in effectively handling text within images.
To address this limitation, researchers have developed two approaches. The
first method involves utilizing external Optical Character Recognition (OCR)
tools to extract textual information from images, which is then prepended to
other textual inputs. The second strategy focuses on employing extremely
high-resolution images to improve text recognition capabilities. In this paper,
we focus on enhancing the first strategy by introducing a novel method, named
TAP-VL, which treats OCR information as a distinct modality and seamlessly
integrates it into any VL model. TAP-VL employs a lightweight transformer-based
OCR module to receive OCR with layout information, compressing it into a short
fixed-length sequence for input into the LLM. Initially, we conduct
model-agnostic pretraining of the OCR module on unlabeled documents, followed
by its integration into any VL architecture through brief fine-tuning.
Extensive experiments demonstrate consistent performance improvements when
applying TAP-VL to top-performing VL models, across scene-text and
document-based VL benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improved Multi-Task Brain Tumour Segmentation with Synthetic Data
  Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04632v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04632v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        André Ferreira, Tiago Jesus, Behrus Puladi, Jens Kleesiek, Victor Alves, Jan Egger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the winning solution of task 1 and the third-placed
solution of task 3 of the BraTS challenge. The use of automated tools in
clinical practice has increased due to the development of more and more
sophisticated and reliable algorithms. However, achieving clinical standards
and developing tools for real-life scenarios is a major challenge. To this end,
BraTS has organised tasks to find the most advanced solutions for specific
purposes. In this paper, we propose the use of synthetic data to train
state-of-the-art frameworks in order to improve the segmentation of adult
gliomas in a post-treatment scenario, and the segmentation of meningioma for
radiotherapy planning. Our results suggest that the use of synthetic data leads
to more robust algorithms, although the synthetic data generation pipeline is
not directly suited to the meningioma task. The code for these tasks is
available at https://github.com/ShadowTwin41/BraTS_2023_2024_solutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Brain Tumour Removing and Missing Modality Generation using 3D WDM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04630v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04630v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        André Ferreira, Gijs Luijten, Behrus Puladi, Jens Kleesiek, Victor Alves, Jan Egger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the second-placed solution for task 8 and the
participation solution for task 7 of BraTS 2024. The adoption of automated
brain analysis algorithms to support clinical practice is increasing. However,
many of these algorithms struggle with the presence of brain lesions or the
absence of certain MRI modalities. The alterations in the brain's morphology
leads to high variability and thus poor performance of predictive models that
were trained only on healthy brains. The lack of information that is usually
provided by some of the missing MRI modalities also reduces the reliability of
the prediction models trained with all modalities. In order to improve the
performance of these models, we propose the use of conditional 3D wavelet
diffusion models. The wavelet transform enabled full-resolution image training
and prediction on a GPU with 48 GB VRAM, without patching or downsampling,
preserving all information for prediction. For the inpainting task of BraTS
2024, the use of a large and variable number of healthy masks and the stability
and efficiency of the 3D wavelet diffusion model resulted in 0.007, 22.61 and
0.842 in the validation set and 0.07 , 22.8 and 0.91 in the testing set (MSE,
PSNR and SSIM respectively). The code for these tasks is available at
https://github.com/ShadowTwin41/BraTS_2023_2024_solutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-temporal crack segmentation in concrete structure using deep
  learning approaches 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04620v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04620v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Said Harb, Pedro Achanccaray, Mehdi Maboudi, Markus Gerke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cracks are among the earliest indicators of deterioration in concrete
structures. Early automatic detection of these cracks can significantly extend
the lifespan of critical infrastructures, such as bridges, buildings, and
tunnels, while simultaneously reducing maintenance costs and facilitating
efficient structural health monitoring. This study investigates whether
leveraging multi-temporal data for crack segmentation can enhance segmentation
quality. Therefore, we compare a Swin UNETR trained on multi-temporal data with
a U-Net trained on mono-temporal data to assess the effect of temporal
information compared with conventional single-epoch approaches. To this end, a
multi-temporal dataset comprising 1356 images, each with 32 sequential crack
propagation images, was created. After training the models, experiments were
conducted to analyze their generalization ability, temporal consistency, and
segmentation quality. The multi-temporal approach consistently outperformed its
mono-temporal counterpart, achieving an IoU of $82.72\%$ and a F1-score of
$90.54\%$, representing a significant improvement over the mono-temporal
model's IoU of $76.69\%$ and F1-score of $86.18\%$, despite requiring only half
of the trainable parameters. The multi-temporal model also displayed a more
consistent segmentation quality, with reduced noise and fewer errors. These
results suggest that temporal information significantly enhances the
performance of segmentation models, offering a promising solution for improved
crack detection and the long-term monitoring of concrete structures, even with
limited sequential data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Population estimation using 3D city modelling and Carto2S datasets -- A
  case study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04612v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04612v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jai G Singla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the launch of Carto2S series of satellites, high resolution images
(0.6-1.0 meters) are acquired and available for use. High resolution Digital
Elevation Model (DEM) with better accuracies can be generated using C2S
multi-view and multi date datasets. DEMs are further used as an input to derive
Digital terrain models (DTMs) and to extract accurate heights of the objects
(building and tree) over the surface of the Earth. Extracted building heights
are validated with ground control points and can be used for generation of city
modelling and resource estimation like population estimation, health planning,
water and transport resource estimations. In this study, an attempt is made to
assess the population of a township using high-resolution Indian remote sensing
satellite datasets. We used Carto 2S multi-view data and generated a precise
DEM and DTM over a city area. Using DEM and DTM datasets, accurate heights of
the buildings are extracted which are further validated with ground data.
Accurate building heights and high resolution imagery are used for generating
accurate virtual 3D city model and assessing the number of floor and carpet
area of the houses/ flats/ apartments. Population estimation of the area is
made using derived information of no of houses/ flats/ apartments from the
satellite datasets. Further, information about number of hospital and schools
around the residential area is extracted from open street maps (OSM).
Population estimation using satellite data and derived information from OSM
datasets can prove to be very good tool for local administrator and decision
makers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Solar potential analysis over Indian cities using high-resolution
  satellite imagery and DEM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04610v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04610v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jai Singla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most of the research work in the solar potential analysis is performed
utilizing aerial imagery, LiDAR data, and satellite imagery. However, in the
existing studies using satellite data, parameters such as trees/ vegetation
shadow, adjacent higher architectural structures, and eccentric roof structures
in urban areas were not considered, and relatively coarser-resolution datasets
were used for analysis. In this work, we have implemented a novel approach to
estimate rooftop solar potential using inputs of high-resolution satellite
imagery (0.5 cm), a digital elevation model (1m), along with ground station
radiation data. Solar radiation analysis is performed using the diffusion
proportion and transmissivity ratio derived from the ground station data hosted
by IMD. It was observed that due to seasonal variations, environmental effects
and technical reasons such as solar panel structure etc., there can be a
significant loss of electricity generation up to 50%. Based on the results, it
is also understood that using 1m DEM and 50cm satellite imagery, more authentic
results are produced over the urban areas.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross- and Intra-image Prototypical Learning for Multi-label Disease
  Diagnosis and Interpretation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04607v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04607v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chong Wang, Fengbei Liu, Yuanhong Chen, Helen Frazer, Gustavo Carneiro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in prototypical learning have shown remarkable potential to
provide useful decision interpretations associating activation maps and
predictions with class-specific training prototypes. Such prototypical learning
has been well-studied for various single-label diseases, but for quite relevant
and more challenging multi-label diagnosis, where multiple diseases are often
concurrent within an image, existing prototypical learning models struggle to
obtain meaningful activation maps and effective class prototypes due to the
entanglement of the multiple diseases. In this paper, we present a novel Cross-
and Intra-image Prototypical Learning (CIPL) framework, for accurate
multi-label disease diagnosis and interpretation from medical images. CIPL
takes advantage of common cross-image semantics to disentangle the multiple
diseases when learning the prototypes, allowing a comprehensive understanding
of complicated pathological lesions. Furthermore, we propose a new two-level
alignment-based regularisation strategy that effectively leverages consistent
intra-image information to enhance interpretation robustness and predictive
performance. Extensive experiments show that our CIPL attains the
state-of-the-art (SOTA) classification accuracy in two public multi-label
benchmarks of disease diagnosis: thoracic radiography and fundus images.
Quantitative interpretability results show that CIPL also has superiority in
weakly-supervised thoracic disease localisation over other leading saliency-
and prototype-based explanation methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Social EgoMesh Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04598v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04598v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Scofano, Alessio Sampieri, Edoardo De Matteis, Indro Spinelli, Fabio Galasso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately estimating the 3D pose of the camera wearer in egocentric video
sequences is crucial to modeling human behavior in virtual and augmented
reality applications. The task presents unique challenges due to the limited
visibility of the user's body caused by the front-facing camera mounted on
their head. Recent research has explored the utilization of the scene and
ego-motion, but it has overlooked humans' interactive nature. We propose a
novel framework for Social Egocentric Estimation of body MEshes (SEE-ME). Our
approach is the first to estimate the wearer's mesh using only a latent
probabilistic diffusion model, which we condition on the scene and, for the
first time, on the social wearer-interactee interactions. Our in-depth study
sheds light on when social interaction matters most for ego-mesh estimation; it
quantifies the impact of interpersonal distance and gaze direction. Overall,
SEE-ME surpasses the current best technique, reducing the pose estimation error
(MPJPE) by 53%. The code is available at https://github.com/L-Scofano/SEEME.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Impact of Semi-Supervised Learning on Line Segment Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04596v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04596v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johanna Engman, Karl Åström, Magnus Oskarsson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we present a method for line segment detection in images, based
on a semi-supervised framework. Leveraging the use of a consistency loss based
on differently augmented and perturbed unlabeled images with a small amount of
labeled data, we show comparable results to fully supervised methods. This
opens up application scenarios where annotation is difficult or expensive, and
for domain specific adaptation of models. We are specifically interested in
real-time and online applications, and investigate small and efficient learning
backbones. Our method is to our knowledge the first to target line detection
using modern state-of-the-art methodologies for semi-supervised learning. We
test the method on both standard benchmarks and domain specific scenarios for
forestry applications, showing the tractability of the proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 6 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TexLiverNet: Leveraging Medical Knowledge and Spatial-Frequency
  Perception for Enhanced Liver Tumor Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04595v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04595v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyan Jiang, Zhi Zhou, Hailing Wang, Guozhong Wang, Zhijun Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrating textual data with imaging in liver tumor segmentation is
essential for enhancing diagnostic accuracy. However, current multi-modal
medical datasets offer only general text annotations, lacking lesion-specific
details critical for extracting nuanced features, especially for fine-grained
segmentation of tumor boundaries and small lesions. To address these
limitations, we developed datasets with lesion-specific text annotations for
liver tumors and introduced the TexLiverNet model. TexLiverNet employs an
agent-based cross-attention module that integrates text features efficiently
with visual features, significantly reducing computational costs. Additionally,
enhanced spatial and adaptive frequency domain perception is proposed to
precisely delineate lesion boundaries, reduce background interference, and
recover fine details in small lesions. Comprehensive evaluations on public and
private datasets demonstrate that TexLiverNet achieves superior performance
compared to current state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Verification of Neural Networks against Convolutional Perturbations via
  Parameterised Kernels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04594v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04594v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benedikt Brückner, Alessio Lomuscio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop a method for the efficient verification of neural networks against
convolutional perturbations such as blurring or sharpening. To define input
perturbations we use well-known camera shake, box blur and sharpen kernels. We
demonstrate that these kernels can be linearly parameterised in a way that
allows for a variation of the perturbation strength while preserving desired
kernel properties. To facilitate their use in neural network verification, we
develop an efficient way of convolving a given input with these parameterised
kernels. The result of this convolution can be used to encode the perturbation
in a verification setting by prepending a linear layer to a given network. This
leads to tight bounds and a high effectiveness in the resulting verification
step. We add further precision by employing input splitting as a branch and
bound strategy. We demonstrate that we are able to verify robustness on a
number of standard benchmarks where the baseline is unable to provide any
safety certificates. To the best of our knowledge, this is the first solution
for verifying robustness against specific convolutional perturbations such as
camera shake.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Inherent Robustness of One-Stage Object Detection against
  Out-of-Distribution Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04586v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04586v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aitor Martinez-Seras, Javier Del Ser, Alain Andres, Pablo Garcia-Bringas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robustness is a fundamental aspect for developing safe and trustworthy
models, particularly when they are deployed in the open world. In this work we
analyze the inherent capability of one-stage object detectors to robustly
operate in the presence of out-of-distribution (OoD) data. Specifically, we
propose a novel detection algorithm for detecting unknown objects in image
data, which leverages the features extracted by the model from each sample.
Differently from other recent approaches in the literature, our proposal does
not require retraining the object detector, thereby allowing for the use of
pretrained models. Our proposed OoD detector exploits the application of
supervised dimensionality reduction techniques to mitigate the effects of the
curse of dimensionality on the features extracted by the model. Furthermore, it
utilizes high-resolution feature maps to identify potential unknown objects in
an unsupervised fashion. Our experiments analyze the Pareto trade-off between
the performance detecting known and unknown objects resulting from different
algorithmic configurations and inference confidence thresholds. We also compare
the performance of our proposed algorithm to that of logits-based post-hoc OoD
methods, as well as possible fusion strategies. Finally, we discuss on the
competitiveness of all tested methods against state-of-the-art OoD approaches
for object detection models over the recently published Unknown Object
Detection benchmark. The obtained results verify that the performance of
avant-garde post-hoc OoD detectors can be further improved when combined with
our proposed algorithm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 figures, 4 tables, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PASSION for Dermatology: Bridging the Diversity Gap with Pigmented Skin
  Images from Sub-Saharan Africa <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04584v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04584v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philippe Gottfrois, Fabian Gröger, Faly Herizo Andriambololoniaina, Ludovic Amruthalingam, Alvaro Gonzalez-Jimenez, Christophe Hsu, Agnes Kessy, Simone Lionetti, Daudi Mavura, Wingston Ng'ambi, Dingase Faith Ngongonda, Marc Pouly, Mendrika Fifaliana Rakotoarisaona, Fahafahantsoa Rapelanoro Rabenja, Ibrahima Traoré, Alexander A. Navarini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Africa faces a huge shortage of dermatologists, with less than one per
million people. This is in stark contrast to the high demand for dermatologic
care, with 80% of the paediatric population suffering from largely untreated
skin conditions. The integration of AI into healthcare sparks significant hope
for treatment accessibility, especially through the development of AI-supported
teledermatology. Current AI models are predominantly trained on white-skinned
patients and do not generalize well enough to pigmented patients. The PASSION
project aims to address this issue by collecting images of skin diseases in
Sub-Saharan countries with the aim of open-sourcing this data. This dataset is
the first of its kind, consisting of 1,653 patients for a total of 4,901
images. The images are representative of telemedicine settings and encompass
the most common paediatric conditions: eczema, fungals, scabies, and impetigo.
We also provide a baseline machine learning model trained on the dataset and a
detailed performance analysis for the subpopulations represented in the
dataset. The project website can be found at https://passionderm.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MICCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DomainGallery: Few-shot Domain-driven Image Generation by
  Attribute-centric Fine<span class="highlight-title">tuning</span> <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04571v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04571v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Duan, Yan Hong, Bo Zhang, Jun Lan, Huijia Zhu, Weiqiang Wang, Jianfu Zhang, Li Niu, Liqing Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent progress in text-to-image models pretrained on large-scale
datasets has enabled us to generate various images as long as we provide a text
prompt describing what we want. Nevertheless, the availability of these models
is still limited when we expect to generate images that fall into a specific
domain either hard to describe or just unseen to the models. In this work, we
propose DomainGallery, a few-shot domain-driven image generation method which
aims at finetuning pretrained Stable Diffusion on few-shot target datasets in
an attribute-centric manner. Specifically, DomainGallery features prior
attribute erasure, attribute disentanglement, regularization and enhancement.
These techniques are tailored to few-shot domain-driven generation in order to
solve key issues that previous works have failed to settle. Extensive
experiments are given to validate the superior performance of DomainGallery on
a variety of domain-driven generation scenarios. Codes are available at
https://github.com/Ldhlwh/DomainGallery.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Fingerprints for Adversarial Attack Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04533v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04533v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haim Fisher, Moni Shahar, Yehezkel S. Resheff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning models for image classification have become standard tools in
recent years. A well known vulnerability of these models is their
susceptibility to adversarial examples. These are generated by slightly
altering an image of a certain class in a way that is imperceptible to humans
but causes the model to classify it wrongly as another class. Many algorithms
have been proposed to address this problem, falling generally into one of two
categories: (i) building robust classifiers (ii) directly detecting attacked
images. Despite the good performance of these detectors, we argue that in a
white-box setting, where the attacker knows the configuration and weights of
the network and the detector, they can overcome the detector by running many
examples on a local copy, and sending only those that were not detected to the
actual model. This problem is common in security applications where even a very
good model is not sufficient to ensure safety. In this paper we propose to
overcome this inherent limitation of any static defence with randomization. To
do so, one must generate a very large family of detectors with consistent
performance, and select one or more of them randomly for each input. For the
individual detectors, we suggest the method of neural fingerprints. In the
training phase, for each class we repeatedly sample a tiny random subset of
neurons from certain layers of the network, and if their average is
sufficiently different between clean and attacked images of the focal class
they are considered a fingerprint and added to the detector bank. During test
time, we sample fingerprints from the bank associated with the label predicted
by the model, and detect attacks using a likelihood ratio test. We evaluate our
detectors on ImageNet with different attack methods and model architectures,
and show near-perfect detection with low rates of false detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ l0-Regularized Sparse Coding-based Interpretable Network for <span class="highlight-title">Multi-Modal</span>
  Image Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04519v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04519v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gargi Panda, Soumitra Kundu, Saumik Bhattacharya, Aurobinda Routray
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal image fusion (MMIF) enhances the information content of the fused
image by combining the unique as well as common features obtained from
different modality sensor images, improving visualization, object detection,
and many more tasks. In this work, we introduce an interpretable network for
the MMIF task, named FNet, based on an l0-regularized multi-modal convolutional
sparse coding (MCSC) model. Specifically, for solving the l0-regularized CSC
problem, we develop an algorithm unrolling-based l0-regularized sparse coding
(LZSC) block. Given different modality source images, FNet first separates the
unique and common features from them using the LZSC block and then these
features are combined to generate the final fused image. Additionally, we
propose an l0-regularized MCSC model for the inverse fusion process. Based on
this model, we introduce an interpretable inverse fusion network named IFNet,
which is utilized during FNet's training. Extensive experiments show that FNet
achieves high-quality fusion results across five different MMIF tasks.
Furthermore, we show that FNet enhances downstream object detection in
visible-thermal image pairs. We have also visualized the intermediate results
of FNet, which demonstrates the good interpretability of our network.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continuous Sign Language Recognition System using Deep Learning with
  MediaPipe Holistic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04517v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04517v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sharvani Srivastava, Sudhakar Singh,  Pooja, Shiv Prakash
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sign languages are the language of hearing-impaired people who use visuals
like the hand, facial, and body movements for communication. There are
different signs and gestures representing alphabets, words, and phrases.
Nowadays approximately 300 sign languages are being practiced worldwide such as
American Sign Language (ASL), Chinese Sign Language (CSL), Indian Sign Language
(ISL), and many more. Sign languages are dependent on the vocal language of a
place. Unlike vocal or spoken languages, there are no helping words in sign
language like is, am, are, was, were, will, be, etc. As only a limited
population is well-versed in sign language, this lack of familiarity of sign
language hinders hearing-impaired people from communicating freely and easily
with everyone. This issue can be addressed by a sign language recognition (SLR)
system which has the capability to translate the sign language into vocal
language. In this paper, a continuous SLR system is proposed using a deep
learning model employing Long Short-Term Memory (LSTM), trained and tested on
an ISL primary dataset. This dataset is created using MediaPipe Holistic
pipeline for tracking face, hand, and body movements and collecting landmarks.
The system recognizes the signs and gestures in real-time with 88.23% accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 4 figures, Wireless Pers Commun</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedDP: Privacy-preserving method based on federated learning for
  histopathology image segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04509v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04509v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liangrui Pan, Mao Huang, Lian Wang, Pinle Qin, Shaoliang Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hematoxylin and Eosin (H&E) staining of whole slide images (WSIs) is
considered the gold standard for pathologists and medical practitioners for
tumor diagnosis, surgical planning, and post-operative assessment. With the
rapid advancement of deep learning technologies, the development of numerous
models based on convolutional neural networks and transformer-based models has
been applied to the precise segmentation of WSIs. However, due to privacy
regulations and the need to protect patient confidentiality, centralized
storage and processing of image data are impractical. Training a centralized
model directly is challenging to implement in medical settings due to these
privacy concerns.This paper addresses the dispersed nature and privacy
sensitivity of medical image data by employing a federated learning framework,
allowing medical institutions to collaboratively learn while protecting patient
privacy. Additionally, to address the issue of original data reconstruction
through gradient inversion during the federated learning training process,
differential privacy introduces noise into the model updates, preventing
attackers from inferring the contributions of individual samples, thereby
protecting the privacy of the training data.Experimental results show that the
proposed method, FedDP, minimally impacts model accuracy while effectively
safeguarding the privacy of cancer pathology image data, with only a slight
decrease in Dice, Jaccard, and Acc indices by 0.55%, 0.63%, and 0.42%,
respectively. This approach facilitates cross-institutional collaboration and
knowledge sharing while protecting sensitive data privacy, providing a viable
solution for further research and application in the medical field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in BIBM2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pose2Trajectory: Using Transformers on Body Pose to Predict Tennis
  Player's Trajectory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04501v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04501v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali K. AlShami, Terrance Boult, Jugal Kalita
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tracking the trajectory of tennis players can help camera operators in
production. Predicting future movement enables cameras to automatically track
and predict a player's future trajectory without human intervention. Predicting
future human movement in the context of complex physical tasks is also
intellectually satisfying. Swift advancements in sports analytics and the wide
availability of videos for tennis have inspired us to propose a novel method
called Pose2Trajectory, which predicts a tennis player's future trajectory as a
sequence derived from their body joints' data and ball position. Demonstrating
impressive accuracy, our approach capitalizes on body joint information to
provide a comprehensive understanding of the human body's geometry and motion,
thereby enhancing the prediction of the player's trajectory. We use
encoder-decoder Transformer architecture trained on the joints and trajectory
information of the players with ball positions. The predicted sequence can
provide information to help close-up cameras to keep tracking the tennis
player, following centroid coordinates. We generate a high-quality dataset from
multiple videos to assist tennis player movement prediction using object
detection and human pose estimation methods. It contains bounding boxes and
joint information for tennis players and ball positions in singles tennis
games. Our method shows promising results in predicting the tennis player's
movement trajectory with different sequence prediction lengths using the joints
and trajectory information with the ball position.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synergy-Guided Regional Supervision of Pseudo Labels for Semi-Supervised
  Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04493v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04493v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Wang, Xinlin Zhang, Yuanbin Chen, Yuanbo Zhou, Longxuan Zhao, Tao Tan, Tong Tong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised learning has received considerable attention for its
potential to leverage abundant unlabeled data to enhance model robustness.
Pseudo labeling is a widely used strategy in semi supervised learning. However,
existing methods often suffer from noise contamination, which can undermine
model performance. To tackle this challenge, we introduce a novel
Synergy-Guided Regional Supervision of Pseudo Labels (SGRS-Net) framework.
Built upon the mean teacher network, we employ a Mix Augmentation module to
enhance the unlabeled data. By evaluating the synergy before and after
augmentation, we strategically partition the pseudo labels into distinct
regions. Additionally, we introduce a Region Loss Evaluation module to assess
the loss across each delineated area. Extensive experiments conducted on the LA
dataset have demonstrated superior performance over state-of-the-art
techniques, underscoring the efficiency and practicality of our framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CFPNet: Improving Lightweight ToF Depth Completion via Cross-zone
  Feature Propagation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04480v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04480v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laiyan Ding, Hualie Jiang, Rui Xu, Rui Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Depth completion using lightweight time-of-flight (ToF) depth sensors is
attractive due to their low cost. However, lightweight ToF sensors usually have
a limited field of view (FOV) compared with cameras. Thus, only pixels in the
zone area of the image can be associated with depth signals. Previous methods
fail to propagate depth features from the zone area to the outside-zone area
effectively, thus suffering from degraded depth completion performance outside
the zone. To this end, this paper proposes the CFPNet to achieve cross-zone
feature propagation from the zone area to the outside-zone area with two novel
modules. The first is a direct-attention-based propagation module (DAPM), which
enforces direct cross-zone feature acquisition. The second is a
large-kernel-based propagation module (LKPM), which realizes cross-zone feature
propagation by utilizing convolution layers with kernel sizes up to 31. CFPNet
achieves state-of-the-art (SOTA) depth completion performance by combining
these two modules properly, as verified by extensive experimental results on
the ZJU-L5 dataset. The code will be made public.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning Models for UAV-Assisted Bridge Inspection: A YOLO
  Benchmark Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04475v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04475v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Trong-Nhan Phan, Hoang-Hai Nguyen, Thi-Thu-Hien Ha, Huy-Tan Thai, Kim-Hung Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual inspections of bridges are critical to ensure their safety and
identify potential failures early. This inspection process can be rapidly and
accurately automated by using unmanned aerial vehicles (UAVs) integrated with
deep learning models. However, choosing an appropriate model that is
lightweight enough to integrate into the UAV and fulfills the strict
requirements for inference time and accuracy is challenging. Therefore, our
work contributes to the advancement of this model selection process by
conducting a benchmark of 23 models belonging to the four newest YOLO variants
(YOLOv5, YOLOv6, YOLOv7, YOLOv8) on COCO-Bridge-2021+, a dataset for bridge
details detection. Through comprehensive benchmarking, we identify YOLOv8n,
YOLOv7tiny, YOLOv6m, and YOLOv6m6 as the models offering an optimal balance
between accuracy and processing speed, with mAP@50 scores of 0.803, 0.837,
0.853, and 0.872, and inference times of 5.3ms, 7.5ms, 14.06ms, and 39.33ms,
respectively. Our findings accelerate the model selection process for UAVs,
enabling more efficient and reliable bridge inspections.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FreeCap: Hybrid Calibration-Free Motion Capture in Open Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04469v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04469v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aoru Xue, Yiming Ren, Zining Song, Mao Ye, Xinge Zhu, Yuexin Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel hybrid calibration-free method FreeCap to accurately
capture global multi-person motions in open environments. Our system combines a
single LiDAR with expandable moving cameras, allowing for flexible and precise
motion estimation in a unified world coordinate. In particular, We introduce a
local-to-global pose-aware cross-sensor human-matching module that predicts the
alignment among each sensor, even in the absence of calibration. Additionally,
our coarse-to-fine sensor-expandable pose optimizer further optimizes the 3D
human key points and the alignments, it is also capable of incorporating
additional cameras to enhance accuracy. Extensive experiments on Human-M3 and
FreeMotion datasets demonstrate that our method significantly outperforms
state-of-the-art single-modal methods, offering an expandable and efficient
solution for multi-person motion capture across various applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient single image non-uniformity correction algorithm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04457v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04457v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yohann Tendero, Jerome Gilles, Stephane Landeau, Jean-Michel Morel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a new way to correct the non-uniformity (NU) in
uncooled infrared-type images. The main defect of these uncooled images is the
lack of a column (resp. line) time-dependent cross-calibration, resulting in a
strong column (resp. line) and time dependent noise. This problem can be
considered as a 1D flicker of the columns inside each frame. Thus, classic
movie deflickering algorithms can be adapted, to equalize the columns (resp.
the lines). The proposed method therefore applies to the series formed by the
columns of an infrared image a movie deflickering algorithm. The obtained
single image method works on static images, and therefore requires no
registration, no camera motion compensation, and no closed aperture sensor
equalization. Thus, the method has only one camera dependent parameter, and is
landscape independent. This simple method will be compared to a state of the
art total variation single image correction on raw real and simulated images.
The method is real time, requiring only two operations per pixel. It involves
no test-pattern calibration and produces no "ghost artifacts".
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2411.03615</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Properties of BV-G structures + textures decomposition models.
  Application to road detection in satellite images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04456v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04456v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jerome Gilles, Yves Meyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we present some theoretical results about a structures-textures
image decomposition model which was proposed by the second author. We prove a
theorem which gives the behavior of this model in different cases. Finally, as
a consequence of the theorem we derive an algorithm for the detection of long
and thin objects applied to a road networks detection application in aerial or
satellite images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BendVLM: Test-Time Debiasing of <span class="highlight-title">Vision-Language</span> Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04420v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04420v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Walter Gerych, Haoran Zhang, Kimia Hamidieh, Eileen Pan, Maanas Sharma, Thomas Hartvigsen, Marzyeh Ghassemi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language model (VLM) embeddings have been shown to encode biases
present in their training data, such as societal biases that prescribe negative
characteristics to members of various racial and gender identities. VLMs are
being quickly adopted for a variety of tasks ranging from few-shot
classification to text-guided image generation, making debiasing VLM embeddings
crucial. Debiasing approaches that fine-tune the VLM often suffer from
catastrophic forgetting. On the other hand, fine-tuning-free methods typically
utilize a "one-size-fits-all" approach that assumes that correlation with the
spurious attribute can be explained using a single linear direction across all
possible inputs. In this work, we propose Bend-VLM, a nonlinear,
fine-tuning-free approach for VLM embedding debiasing that tailors the
debiasing operation to each unique input. This allows for a more flexible
debiasing approach. Additionally, we do not require knowledge of the set of
inputs a priori to inference time, making our method more appropriate for
online, open-set tasks such as retrieval and text guided image generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Image Understanding Makes for A Good Tokenizer for Image Generation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04406v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04406v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luting Wang, Yang Zhao, Zijian Zhang, Jiashi Feng, Si Liu, Bingyi Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Abstract Modern image generation (IG) models have been shown to capture rich
semantics valuable for image understanding (IU) tasks. However, the potential
of IU models to improve IG performance remains uncharted. We address this issue
using a token-based IG framework, which relies on effective tokenizers to
project images into token sequences. Currently, pixel reconstruction (e.g.,
VQGAN) dominates the training objective for image tokenizers. In contrast, our
approach adopts the feature reconstruction objective, where tokenizers are
trained by distilling knowledge from pretrained IU encoders. Comprehensive
comparisons indicate that tokenizers with strong IU capabilities achieve
superior IG performance across a variety of metrics, datasets, tasks, and
proposal networks. Notably, VQ-KD CLIP achieves $4.10$ FID on ImageNet-1k
(IN-1k). Visualization suggests that the superiority of VQ-KD can be partly
attributed to the rich semantics within the VQ-KD codebook. We further
introduce a straightforward pipeline to directly transform IU encoders into
tokenizers, demonstrating exceptional effectiveness for IG tasks. These
discoveries may energize further exploration into image tokenizer research and
inspire the community to reassess the relationship between IU and IG. The code
is released at https://github.com/magic-research/vector_quantization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Bronchoscopy Depth Estimation through Synthetic-to-Real Domain
  Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04404v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04404v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyao Tian, Huai Liao, Xinyan Huang, Lujie Li, Hongbin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monocular depth estimation has shown promise in general imaging tasks, aiding
in localization and 3D reconstruction. While effective in various domains, its
application to bronchoscopic images is hindered by the lack of labeled data,
challenging the use of supervised learning methods. In this work, we propose a
transfer learning framework that leverages synthetic data with depth labels for
training and adapts domain knowledge for accurate depth estimation in real
bronchoscope data. Our network demonstrates improved depth prediction on real
footage using domain adaptation compared to training solely on synthetic data,
validating our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ProGraph: Temporally-alignable Probability Guided Graph Topological
  Modeling for 3D Human Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04399v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04399v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongsheng Wang, Zehui Feng, Tong Xiao, Genfan Yang, Shengyu Zhang, Fei Wu, Feng Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current 3D human motion reconstruction methods from monocular videos rely on
features within the current reconstruction window, leading to distortion and
deformations in the human structure under local occlusions or blurriness in
video frames. To estimate realistic 3D human mesh sequences based on incomplete
features, we propose Temporally-alignable Probability Guided Graph Topological
Modeling for 3D Human Reconstruction (ProGraph). For missing parts recovery, we
exploit the explicit topological-aware probability distribution across the
entire motion sequence. To restore the complete human, Graph Topological
Modeling (GTM) learns the underlying topological structure, focusing on the
relationships inherent in the individual parts. Next, to generate blurred
motion parts, Temporal-alignable Probability Distribution (TPDist) utilizes the
GTM to predict features based on distribution. This interactive mechanism
facilitates motion consistency, allowing the restoration of human parts.
Furthermore, Hierarchical Human Loss (HHLoss) constrains the probability
distribution errors of inter-frame features during topological structure
variation. Our Method achieves superior results than other SOTA methods in
addressing occlusions and blurriness on 3DPW.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MegaPortrait: Revisiting Diffusion Control for High-fidelity Portrait
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04357v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04357v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Yang, Sotiris Anagnostidis, Enis Simsar, Thomas Hofmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose MegaPortrait. It's an innovative system for creating personalized
portrait images in computer vision. It has three modules: Identity Net, Shading
Net, and Harmonization Net. Identity Net generates learned identity using a
customized model fine-tuned with source images. Shading Net re-renders
portraits using extracted representations. Harmonization Net fuses pasted faces
and the reference image's body for coherent results. Our approach with
off-the-shelf Controlnets is better than state-of-the-art AI portrait products
in identity preservation and image fidelity. MegaPortrait has a simple but
effective design and we compare it with other methods and products to show its
superiority.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LidaRefer: Outdoor 3D Visual Grounding for Autonomous Driving with
  Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04351v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04351v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yeong-Seung Baek, Heung-Seon Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D visual grounding (VG) aims to locate relevant objects or regions within 3D
scenes based on natural language descriptions. Although recent methods for
indoor 3D VG have successfully transformer-based architectures to capture
global contextual information and enable fine-grained cross-modal fusion, they
are unsuitable for outdoor environments due to differences in the distribution
of point clouds between indoor and outdoor settings. Specifically, first,
extensive LiDAR point clouds demand unacceptable computational and memory
resources within transformers due to the high-dimensional visual features.
Second, dominant background points and empty spaces in sparse LiDAR point
clouds complicate cross-modal fusion owing to their irrelevant visual
information. To address these challenges, we propose LidaRefer, a
transformer-based 3D VG framework designed for large-scale outdoor scenes.
Moreover, during training, we introduce a simple and effective localization
method, which supervises the decoder's queries to localize not only a target
object but also ambiguous objects that might be confused as the target due to
the exhibition of similar attributes in a scene or the incorrect understanding
of a language description. This supervision enhances the model's ability to
distinguish ambiguous objects from a target by learning the differences in
their spatial relationships and attributes. LidaRefer achieves state-of-the-art
performance on Talk2Car-3D, a 3D VG dataset for autonomous driving, with
significant improvements under various evaluation settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UEVAVD: A Dataset for Developing UAV's Eye View Active Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04348v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04348v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinhua Jiang, Tianpeng Liu, Li Liu, Zhen Liu, Yongxiang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Occlusion is a longstanding difficulty that challenges the UAV-based object
detection. Many works address this problem by adapting the detection model.
However, few of them exploit that the UAV could fundamentally improve detection
performance by changing its viewpoint. Active Object Detection (AOD) offers an
effective way to achieve this purpose. Through Deep Reinforcement Learning
(DRL), AOD endows the UAV with the ability of autonomous path planning to
search for the observation that is more conducive to target identification.
Unfortunately, there exists no available dataset for developing the UAV AOD
method. To fill this gap, we released a UAV's eye view active vision dataset
named UEVAVD and hope it can facilitate research on the UAV AOD problem.
Additionally, we improve the existing DRL-based AOD method by incorporating the
inductive bias when learning the state representation. First, due to the
partial observability, we use the gated recurrent unit to extract state
representations from the observation sequence instead of the single-view
observation. Second, we pre-decompose the scene with the Segment Anything Model
(SAM) and filter out the irrelevant information with the derived masks. With
these practices, the agent could learn an active viewing policy with better
generalization capability. The effectiveness of our innovations is validated by
the experiments on the UEVAVD dataset. Our dataset will soon be available at
https://github.com/Leo000ooo/UEVAVD_dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GazeGen: Gaze-Driven User Interaction for Visual Content Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04335v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04335v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        He-Yen Hsieh, Ziyun Li, Sai Qian Zhang, Wei-Te Mark Ting, Kao-Den Chang, Barbara De Salvo, Chiao Liu, H. T. Kung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present GazeGen, a user interaction system that generates visual content
(images and videos) for locations indicated by the user's eye gaze. GazeGen
allows intuitive manipulation of visual content by targeting regions of
interest with gaze. Using advanced techniques in object detection and
generative AI, GazeGen performs gaze-controlled image adding/deleting,
repositioning, and surface material changes of image objects, and converts
static images into videos. Central to GazeGen is the DFT Gaze (Distilled and
Fine-Tuned Gaze) agent, an ultra-lightweight model with only 281K parameters,
performing accurate real-time gaze predictions tailored to individual users'
eyes on small edge devices. GazeGen is the first system to combine visual
content generation with real-time gaze estimation, made possible exclusively by
DFT Gaze. This real-time gaze estimation enables various visual content
generation tasks, all controlled by the user's gaze. The input for DFT Gaze is
the user's eye images, while the inputs for visual content generation are the
user's view and the predicted gaze point from DFT Gaze. To achieve efficient
gaze predictions, we derive the small model from a large model (10x larger) via
novel knowledge distillation and personal adaptation techniques. We integrate
knowledge distillation with a masked autoencoder, developing a compact yet
powerful gaze estimation model. This model is further fine-tuned with Adapters,
enabling highly accurate and personalized gaze predictions with minimal user
input. DFT Gaze ensures low-latency and precise gaze tracking, supporting a
wide range of gaze-driven tasks. We validate the performance of DFT Gaze on AEA
and OpenEDS2020 benchmarks, demonstrating low angular gaze error and low
latency on the edge device (Raspberry Pi 4). Furthermore, we describe
applications of GazeGen, illustrating its versatility and effectiveness in
various usage scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HandCraft: Anatomically Correct Restoration of Malformed Hands in
  Diffusion Generated Images <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04332v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04332v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyue Qin, Yiqun Zhang, Yang Liu, Dylan Campbell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative text-to-image models, such as Stable Diffusion, have demonstrated
a remarkable ability to generate diverse, high-quality images. However, they
are surprisingly inept when it comes to rendering human hands, which are often
anatomically incorrect or reside in the "uncanny valley". In this paper, we
propose a method HandCraft for restoring such malformed hands. This is achieved
by automatically constructing masks and depth images for hands as conditioning
signals using a parametric model, allowing a diffusion-based image editor to
fix the hand's anatomy and adjust its pose while seamlessly integrating the
changes into the original image, preserving pose, color, and style. Our
plug-and-play hand restoration solution is compatible with existing pretrained
diffusion models, and the restoration process facilitates adoption by eschewing
any fine-tuning or training requirements for the diffusion models. We also
contribute MalHand datasets that contain generated images with a wide variety
of malformed hands in several styles for hand detector training and hand
restoration benchmarking, and demonstrate through qualitative and quantitative
evaluation that HandCraft not only restores anatomical correctness but also
maintains the integrity of the overall image.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WACV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comparative Analysis of U-Net-based models for Segmentation of Cardiac
  MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.09980v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.09980v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ketan Suhaas Saichandran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical imaging refers to the technologies and methods utilized to view the
human body and its inside, in order to diagnose, monitor, or even treat medical
disorders. This paper aims to explore the application of deep learning
techniques in the semantic segmentation of Cardiac short-axis MRI (Magnetic
Resonance Imaging) images, aiming to enhance the diagnosis, monitoring, and
treatment of medical disorders related to the heart. The focus centers on
implementing various architectures that are derivatives of U-Net, to
effectively isolate specific parts of the heart for comprehensive anatomical
and functional analysis. Through a combination of images, graphs, and
quantitative metrics, the efficacy of the models and their predictions are
showcased. Additionally, this paper addresses encountered challenges and
outline strategies for future improvements. This abstract provides a concise
overview of the efforts in utilizing deep learning for cardiac image
segmentation, emphasizing both the accomplishments and areas for further
refinement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GD doesn't make the cut: Three ways that non-differentiability affects
  neural network training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.08426v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.08426v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddharth Krishna Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper critically examines the fundamental distinctions between gradient
methods applied to non-differentiable functions (NGDMs) and classical gradient
descents (GDs) for differentiable functions, revealing significant gaps in
current deep learning optimization theory. We demonstrate that NGDMs exhibit
markedly different convergence properties compared to GDs, strongly challenging
the applicability of extensive neural network convergence literature based on
$L-smoothness$ to non-smooth neural networks. Our analysis reveals paradoxical
behavior of NDGM solutions for $L_{1}$-regularized problems, where increasing
regularization counterintuitively leads to larger $L_{1}$ norms of optimal
solutions. This finding calls into question widely adopted $L_{1}$ penalization
techniques for network pruning. We further challenge the common assumption that
optimization algorithms like RMSProp behave similarly in differentiable and
non-differentiable contexts. Expanding on the Edge of Stability phenomenon, we
demonstrate its occurrence in a broader class of functions, including Lipschitz
continuous convex differentiable functions. This finding raises important
questions about its relevance and interpretation in non-convex,
non-differentiable neural networks, particularly those using ReLU activations.
Our work identifies critical misunderstandings of NDGMs in influential
literature, stemming from an overreliance on strong smoothness assumptions.
These findings necessitate a reevaluation of optimization dynamics in deep
learning, emphasizing the crucial need for more nuanced theoretical foundations
in analyzing these complex systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring QUIC Dynamics: A Large-Scale Dataset for Encrypted Traffic
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03728v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03728v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Barak Gahtan, Robert J. Shahla, Alex M. Bronstein, Reuven Cohen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  QUIC, a new and increasingly used transport protocol, addresses and resolves
the limitations of TCP by offering improved security, performance, and features
such as stream multiplexing and connection migration. These features, however,
also present challenges for network operators who need to monitor and analyze
web traffic. In this paper, we introduce VisQUIC, a labeled dataset comprising
over 100,000 QUIC traces from more than 44,000 websites (URLs), collected over
a four-month period. These traces provide the foundation for generating more
than seven million images, with configurable parameters of window length, pixel
resolution, normalization, and labels. These images enable an observer looking
at the interactions between a client and a server to analyze and gain insights
about QUIC encrypted connections. To illustrate the dataset's potential, we
offer a use-case example of an observer estimating the number of HTTP/3
responses/requests pairs in a given QUIC, which can reveal server behavior,
client--server interactions, and the load imposed by an observed connection. We
formulate the problem as a discrete regression problem, train a machine
learning (ML) model for it, and then evaluate it using the proposed dataset on
an example use case.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The dataset and the supplementary material can be provided upon
  request</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ C3T: Cross-modal Transfer Through Time for Human Action Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.16803v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.16803v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhi Kamboj, Anh Duy Nguyen, Minh Do
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In order to unlock the potential of diverse sensors, we investigate a method
to transfer knowledge between modalities using the structure of a unified
multimodal representation space for Human Action Recognition (HAR). We
formalize and explore an understudied cross-modal transfer setting we term
Unsupervised Modality Adaptation (UMA), where the modality used in testing is
not used in supervised training, i.e. zero labeled instances of the test
modality are available during training. We develop three methods to perform
UMA: Student-Teacher (ST), Contrastive Alignment (CA), and Cross-modal Transfer
Through Time (C3T). Our extensive experiments on various camera+IMU datasets
compare these methods to each other in the UMA setting, and to their empirical
upper bound in the supervised setting. The results indicate C3T is the most
robust and highest performing by at least a margin of 8%, and nears the
supervised setting performance even in the presence of temporal noise. This
method introduces a novel mechanism for aligning signals across time-varying
latent vectors, extracted from the receptive field of temporal convolutions.
Our findings suggest that C3T has significant potential for developing
generalizable models for time-series sensor data, opening new avenues for
multi-modal learning in various applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Caching for Faster Video Generation with Diffusion Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02397v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02397v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kumara Kahatapitiya, Haozhe Liu, Sen He, Ding Liu, Menglin Jia, Chenyang Zhang, Michael S. Ryoo, Tian Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating temporally-consistent high-fidelity videos can be computationally
expensive, especially over longer temporal spans. More-recent Diffusion
Transformers (DiTs) -- despite making significant headway in this context --
have only heightened such challenges as they rely on larger models and heavier
attention mechanisms, resulting in slower inference speeds. In this paper, we
introduce a training-free method to accelerate video DiTs, termed Adaptive
Caching (AdaCache), which is motivated by the fact that "not all videos are
created equal": meaning, some videos require fewer denoising steps to attain a
reasonable quality than others. Building on this, we not only cache
computations through the diffusion process, but also devise a caching schedule
tailored to each video generation, maximizing the quality-latency trade-off. We
further introduce a Motion Regularization (MoReg) scheme to utilize video
information within AdaCache, essentially controlling the compute allocation
based on motion content. Altogether, our plug-and-play contributions grant
significant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video
generation) without sacrificing the generation quality, across multiple video
DiT baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project-page is available at https://adacache-dit.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CardioSpectrum: Comprehensive Myocardium Motion Analysis with 3D Deep
  Learning and Geometric Insights <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.03794v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.03794v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shahar Zuler, Shai Tejman-Yarden, Dan Raviv
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to map left ventricle (LV) myocardial motion using computed
tomography angiography (CTA) is essential to diagnosing cardiovascular
conditions and guiding interventional procedures. Due to their inherent
locality, conventional neural networks typically have difficulty predicting
subtle tangential movements, which considerably lessens the level of precision
at which myocardium three-dimensional (3D) mapping can be performed. Using 3D
optical flow techniques and Functional Maps (FMs), we present a comprehensive
approach to address this problem. FMs are known for their capacity to capture
global geometric features, thus providing a fuller understanding of 3D
geometry. As an alternative to traditional segmentation-based priors, we employ
surface-based two-dimensional (2D) constraints derived from spectral
correspondence methods. Our 3D deep learning architecture, based on the ARFlow
model, is optimized to handle complex 3D motion analysis tasks. By
incorporating FMs, we can capture the subtle tangential movements of the
myocardium surface precisely, hence significantly improving the accuracy of 3D
mapping of the myocardium. The experimental results confirm the effectiveness
of this method in enhancing myocardium motion analysis. This approach can
contribute to improving cardiovascular diagnosis and treatment. Our code and
additional resources are available at:
https://shaharzuler.github.io/CardioSpectrumPage
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been early accepted to MICCAI 2024, LNCS 15005,
  Springer, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pediatric Wrist Fracture Detection Using Feature Context Excitation
  Modules in X-ray Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01031v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01031v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui-Yang Ju, Chun-Tse Chien, Enkaer Xieerke, Jen-Shiun Chiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Children often suffer wrist trauma in daily life, while they usually need
radiologists to analyze and interpret X-ray images before surgical treatment by
surgeons. The development of deep learning has enabled neural networks to serve
as computer-assisted diagnosis (CAD) tools to help doctors and experts in
medical image diagnostics. Since YOLOv8 model has obtained the satisfactory
success in object detection tasks, it has been applied to various fracture
detection. This work introduces four variants of Feature Contexts
Excitation-YOLOv8 (FCE-YOLOv8) model, each incorporating a different FCE module
(i.e., modules of Squeeze-and-Excitation (SE), Global Context (GC),
Gather-Excite (GE), and Gaussian Context Transformer (GCT)) to enhance the
model performance. Experimental results on GRAZPEDWRI-DX dataset demonstrate
that our proposed YOLOv8+GC-M3 model improves the mAP@50 value from 65.78% to
66.32%, outperforming the state-of-the-art (SOTA) model while reducing
inference time. Furthermore, our proposed YOLOv8+SE-M3 model achieves the
highest mAP@50 value of 67.07%, exceeding the SOTA performance. The
implementation of this work is available at
https://github.com/RuiyangJu/FCE-YOLOv8.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2407.03163</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Knowledge Graphs of Driving Scenes to Empower the Emerging Capabilities
  of Neurosymbolic AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03225v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03225v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruwan Wickramarachchi, Cory Henson, Amit Sheth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the era of Generative AI, Neurosymbolic AI is emerging as a powerful
approach for tasks spanning from perception to cognition. The use of
Neurosymbolic AI has been shown to achieve enhanced capabilities, including
improved grounding, alignment, explainability, and reliability. However, due to
its nascent stage, there is a lack of widely available real-world benchmark
datasets tailored to Neurosymbolic AI tasks. To address this gap and support
the evaluation of current and future methods, we introduce DSceneKG -- a suite
of knowledge graphs of driving scenes built from real-world, high-quality
scenes from multiple open autonomous driving datasets. In this article, we
detail the construction process of DSceneKG and highlight its application in
seven different tasks. DSceneKG is publicly accessible at:
https://github.com/ruwantw/DSceneKG
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interpreting <span class="highlight-title">CLIP</span>: Insights on the Robustness to ImageNet Distribution
  Shifts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.13040v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.13040v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Crabbé, Pau Rodríguez, Vaishaal Shankar, Luca Zappella, Arno Blaas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  What distinguishes robust models from non-robust ones? While for ImageNet
distribution shifts it has been shown that such differences in robustness can
be traced back predominantly to differences in training data, so far it is not
known what that translates to in terms of what the model has learned. In this
work, we bridge this gap by probing the representation spaces of 16 robust
zero-shot CLIP vision encoders with various backbones (ResNets and ViTs) and
pretraining sets (OpenAI, LAION-400M, LAION-2B, YFCC15M, CC12M and {DataComp}),
and comparing them to the representation spaces of less robust models with
identical backbones, but different (pre)training sets or objectives (CLIP
pretraining on ImageNet-Captions, and supervised training or finetuning on
ImageNet).Through this analysis, we generate three novel insights. Firstly, we
detect the presence of outlier features in robust zero-shot CLIP vision
encoders, which to the best of our knowledge is the first time these are
observed in non-language and non-transformer models. Secondly, we find the
existence of outlier features to be an indication of ImageNet shift robustness
in models, since we only find them in robust models in our analysis. Lastly, we
also investigate the number of unique encoded concepts in the representation
space and find zero-shot CLIP models to encode a higher number of unique
concepts in their representation space. However, we do not find this to be an
indicator of ImageNet shift robustness and hypothesize that it is rather
related to the language supervision. Since the presence of outlier features can
be detected without access to any data from shifted datasets, we believe that
they could be a useful tool for practitioners to get a feeling for the
distribution shift robustness of a pretrained model during deployment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in TMLR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mini-InternVL: A Flexible-Transfer Pocket <span class="highlight-title">Multimodal</span> Model with 5%
  Parameters and 90% Performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16261v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16261v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhangwei Gao, Zhe Chen, Erfei Cui, Yiming Ren, Weiyun Wang, Jinguo Zhu, Hao Tian, Shenglong Ye, Junjun He, Xizhou Zhu, Lewei Lu, Tong Lu, Yu Qiao, Jifeng Dai, Wenhai Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) have demonstrated impressive
performance in vision-language tasks across a broad spectrum of domains.
However, the large model scale and associated high computational costs pose
significant challenges for training and deploying MLLMs on consumer-grade GPUs
or edge devices, thereby hindering their widespread application. In this work,
we introduce Mini-InternVL, a series of MLLMs with parameters ranging from 1B
to 4B, which achieves 90% of the performance with only 5% of the parameters.
This significant improvement in efficiency and effectiveness makes our models
more accessible and applicable in various real-world scenarios. To further
promote the adoption of our models, we develop a unified adaptation framework
for Mini-InternVL, which enables our models to transfer and outperform
specialized models in downstream tasks, including autonomous driving, medical
images, and remote sensing. We believe that our study can provide valuable
insights and resources to advance the development of efficient and effective
MLLMs. Code is available at https://github.com/OpenGVLab/InternVL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BrainSegFounder: Towards 3D Foundation Models for Neuroimage
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10395v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10395v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joseph Cox, Peng Liu, Skylar E. Stolte, Yunchao Yang, Kang Liu, Kyle B. See, Huiwen Ju, Ruogu Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The burgeoning field of brain health research increasingly leverages
artificial intelligence (AI) to interpret and analyze neurological data. This
study introduces a novel approach towards the creation of medical foundation
models by integrating a large-scale multi-modal magnetic resonance imaging
(MRI) dataset derived from 41,400 participants in its own. Our method involves
a novel two-stage pretraining approach using vision transformers. The first
stage is dedicated to encoding anatomical structures in generally healthy
brains, identifying key features such as shapes and sizes of different brain
regions. The second stage concentrates on spatial information, encompassing
aspects like location and the relative positioning of brain structures. We
rigorously evaluate our model, BrainFounder, using the Brain Tumor Segmentation
(BraTS) challenge and Anatomical Tracings of Lesions After Stroke v2.0 (ATLAS
v2.0) datasets. BrainFounder demonstrates a significant performance gain,
surpassing the achievements of the previous winning solutions using fully
supervised learning. Our findings underscore the impact of scaling up both the
complexity of the model and the volume of unlabeled training data derived from
generally healthy brains, which enhances the accuracy and predictive
capabilities of the model in complex neuroimaging tasks with MRI. The
implications of this research provide transformative insights and practical
applications in healthcare and make substantial steps towards the creation of
foundation models for Medical AI. Our pretrained models and training code can
be found at https://github.com/lab-smile/GatorBrain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 5 figures, to be published in Medical Image Analysis</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiT4Edit: Diffusion Transformer for Image Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03286v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03286v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunyu Feng, Yue Ma, Bingyuan Wang, Chenyang Qi, Haozhe Chen, Qifeng Chen, Zeyu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent advances in UNet-based image editing, methods for shape-aware
object editing in high-resolution images are still lacking. Compared to UNet,
Diffusion Transformers (DiT) demonstrate superior capabilities to effectively
capture the long-range dependencies among patches, leading to higher-quality
image generation. In this paper, we propose DiT4Edit, the first Diffusion
Transformer-based image editing framework. Specifically, DiT4Edit uses the
DPM-Solver inversion algorithm to obtain the inverted latents, reducing the
number of steps compared to the DDIM inversion algorithm commonly used in
UNet-based frameworks. Additionally, we design unified attention control and
patches merging, tailored for transformer computation streams. This integration
allows our framework to generate higher-quality edited images faster. Our
design leverages the advantages of DiT, enabling it to surpass UNet structures
in image editing, especially in high-resolution and arbitrary-size images.
Extensive experiments demonstrate the strong performance of DiT4Edit across
various editing scenarios, highlighting the potential of Diffusion Transformers
in supporting image editing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SpikeBottleNet: Spike-Driven Feature Compression Architecture for
  Edge-Cloud Co-Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08673v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08673v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maruf Hassan, Steven Davy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Edge-cloud co-inference enables efficient deep neural network (DNN)
deployment by splitting the architecture between an edge device and cloud
server, crucial for resource-constraint edge devices. This approach requires
balancing on-device computations and communication costs, often achieved
through compressed intermediate feature transmission. Conventional DNN
architectures require continuous data processing and floating point
activations, leading to considerable energy consumption and increased feature
sizes, thus raising transmission costs. This challenge motivates exploring
binary, event-driven activations using spiking neural networks (SNNs), known
for their extreme energy efficiency. In this research, we propose
SpikeBottleNet, a novel architecture for edge-cloud co-inference systems that
integrates a spiking neuron model to significantly reduce energy consumption on
edge devices. A key innovation of our study is an intermediate feature
compression technique tailored for SNNs for efficient feature transmission.
This technique leverages a split computing approach to strategically place
encoder-decoder bottleneck units within complex deep architectures like ResNet
and MobileNet. Experimental results demonstrate that SpikeBottleNet achieves up
to 256x bit compression in the final convolutional layer of ResNet, with
minimal accuracy loss (0.16%). Additionally, our approach enhances edge device
energy efficiency by up to 144x compared to the baseline BottleNet, making it
ideal for resource-limited edge devices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Local Padding in Patch-Based GANs for Seamless Infinite-Sized Texture
  Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.02340v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.02340v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alhasan Abdellatif, Ahmed H. Elsheikh, Hannah P. Menke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Texture models based on Generative Adversarial Networks (GANs) use
zero-padding to implicitly encode positional information of the image features.
However, when extending the spatial input to generate images at large sizes,
zero-padding can often lead to degradation in image quality due to the
incorrect positional information at the center of the image. Moreover,
zero-padding can limit the diversity within the generated large images. In this
paper, we propose a novel approach for generating stochastic texture images at
large arbitrary sizes using GANs based on patch-by-patch generation. Instead of
zero-padding, the model uses \textit{local padding} in the generator that
shares border features between the generated patches; providing positional
context and ensuring consistency at the boundaries. The proposed models are
trainable on a single texture image and have a constant GPU scalability with
respect to the output image size, and hence can generate images of infinite
sizes. We show in the experiments that our method has a significant advancement
beyond existing GANs-based texture models in terms of the quality and diversity
of the generated textures. Furthermore, the implementation of local padding in
the state-of-the-art super-resolution models effectively eliminates tiling
artifacts enabling large-scale super-resolution. Our code is available at
\url{https://github.com/ai4netzero/Infinite_Texture_GANs}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DN-Splatter: Depth and Normal Priors for Gaussian Splatting and Meshing <span class="chip">WACV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17822v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17822v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matias Turkulainen, Xuqian Ren, Iaroslav Melekhov, Otto Seiskari, Esa Rahtu, Juho Kannala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-fidelity 3D reconstruction of common indoor scenes is crucial for VR and
AR applications. 3D Gaussian splatting, a novel differentiable rendering
technique, has achieved state-of-the-art novel view synthesis results with high
rendering speeds and relatively low training times. However, its performance on
scenes commonly seen in indoor datasets is poor due to the lack of geometric
constraints during optimization. In this work, we explore the use of readily
accessible geometric cues to enhance Gaussian splatting optimization in
challenging, ill-posed, and textureless scenes. We extend 3D Gaussian splatting
with depth and normal cues to tackle challenging indoor datasets and showcase
techniques for efficient mesh extraction. Specifically, we regularize the
optimization procedure with depth information, enforce local smoothness of
nearby Gaussians, and use off-the-shelf monocular networks to achieve better
alignment with the true scene geometry. We propose an adaptive depth loss based
on the gradient of color images, improving depth estimation and novel view
synthesis results over various baselines. Our simple yet effective
regularization technique enables direct mesh extraction from the Gaussian
representation, yielding more physically accurate reconstructions of indoor
scenes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in 2025 IEEE/CVF Winter Conference on Applications of
  Computer Vision (WACV)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Representing Domain-Mixing Optical Degradation for Real-World
  Computational Aberration Correction via Vector Quantization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10012v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10012v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Jiang, Zhonghua Yi, Shaohua Gao, Yao Gao, Xiaolong Qian, Hao Shi, Lei Sun, JinXing Niu, Kaiwei Wang, Kailun Yang, Jian Bai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Relying on paired synthetic data, existing learning-based Computational
Aberration Correction (CAC) methods are confronted with the intricate and
multifaceted synthetic-to-real domain gap, which leads to suboptimal
performance in real-world applications. In this paper, in contrast to improving
the simulation pipeline, we deliver a novel insight into real-world CAC from
the perspective of Unsupervised Domain Adaptation (UDA). By incorporating
readily accessible unpaired real-world data into training, we formalize the
Domain Adaptive CAC (DACAC) task, and then introduce a comprehensive Real-world
aberrated images (Realab) dataset to benchmark it. The setup task presents a
formidable challenge due to the intricacy of understanding the target optical
degradation domain. To this intent, we propose a novel Quantized Domain-Mixing
Representation (QDMR) framework as a potent solution to the issue. Centering
around representing and quantizing the optical degradation which is consistent
across different images, QDMR adapts the CAC model to the target domain from
three key aspects: (1) reconstructing aberrated images of both domains by a
VQGAN to learn a Domain-Mixing Codebook (DMC) characterizing the optical
degradation; (2) modulating the deep features in CAC model with DMC to transfer
the target domain knowledge; and (3) leveraging the trained VQGAN to generate
pseudo target aberrated images from the source ones for convincing target
domain supervision. Extensive experiments on both synthetic and real-world
benchmarks reveal that the models with QDMR consistently surpass the
competitive methods in mitigating the synthetic-to-real gap, which produces
visually pleasant real-world CAC results with fewer artifacts. Codes and
datasets are made publicly available at https://github.com/zju-jiangqi/QDMR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Optics & Laser Technology. Codes and datasets are made
  publicly available at https://github.com/zju-jiangqi/QDMR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating alignment between humans and neural network representations
  in image-based learning tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.09377v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.09377v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Can Demircan, Tankred Saanum, Leonardo Pettini, Marcel Binz, Blazej M Baczkowski, Christian F Doeller, Mona M Garvert, Eric Schulz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans represent scenes and objects in rich feature spaces, carrying
information that allows us to generalise about category memberships and
abstract functions with few examples. What determines whether a neural network
model generalises like a human? We tested how well the representations of $86$
pretrained neural network models mapped to human learning trajectories across
two tasks where humans had to learn continuous relationships and categories of
natural images. In these tasks, both human participants and neural networks
successfully identified the relevant stimulus features within a few trials,
demonstrating effective generalisation. We found that while training dataset
size was a core determinant of alignment with human choices, contrastive
training with multi-modal data (text and imagery) was a common feature of
currently publicly available models that predicted human generalisation.
Intrinsic dimensionality of representations had different effects on alignment
for different model types. Lastly, we tested three sets of human-aligned
representations and found no consistent improvements in predictive accuracy
compared to the baselines. In conclusion, pretrained neural networks can serve
to extract representations for cognitive models, as they appear to capture some
fundamental aspects of cognition that are transferable across tasks. Both our
paradigms and modelling approach offer a novel way to quantify alignment
between neural networks and humans and extend cognitive science into more
naturalistic domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentially Private Integrated Decision Gradients (IDG-DP) for
  Radar-based Human Activity Recognition <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02099v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02099v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Idris Zakariyya, Linda Tran, Kaushik Bhargav Sivangi, Paul Henderson, Fani Deligianni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human motion analysis offers significant potential for healthcare monitoring
and early detection of diseases. The advent of radar-based sensing systems has
captured the spotlight for they are able to operate without physical contact
and they can integrate with pre-existing Wi-Fi networks. They are also seen as
less privacy-invasive compared to camera-based systems. However, recent
research has shown high accuracy in recognizing subjects or gender from radar
gait patterns, raising privacy concerns. This study addresses these issues by
investigating privacy vulnerabilities in radar-based Human Activity Recognition
(HAR) systems and proposing a novel method for privacy preservation using
Differential Privacy (DP) driven by attributions derived with Integrated
Decision Gradient (IDG) algorithm. We investigate Black-box Membership
Inference Attack (MIA) Models in HAR settings across various levels of
attacker-accessible information. We extensively evaluated the effectiveness of
the proposed IDG-DP method by designing a CNN-based HAR model and rigorously
assessing its resilience against MIAs. Experimental results demonstrate the
potential of IDG-DP in mitigating privacy attacks while maintaining utility
across all settings, particularly excelling against label-only and shadow model
black-box MIA attacks. This work represents a crucial step towards balancing
the need for effective radar-based HAR with robust privacy protection in
healthcare environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at WACV 2025. 12 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ICAL: Implicit Character-Aided Learning for Enhanced Handwritten
  Mathematical Expression Recognition <span class="chip">ICDAR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.09032v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.09032v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianhua Zhu, Liangcai Gao, Wenqi Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Significant progress has been made in the field of handwritten mathematical
expression recognition, while existing encoder-decoder methods are usually
difficult to model global information in $LaTeX$. Therefore, this paper
introduces a novel approach, Implicit Character-Aided Learning (ICAL), to mine
the global expression information and enhance handwritten mathematical
expression recognition. Specifically, we propose the Implicit Character
Construction Module (ICCM) to predict implicit character sequences and use a
Fusion Module to merge the outputs of the ICCM and the decoder, thereby
producing corrected predictions. By modeling and utilizing implicit character
information, ICAL achieves a more accurate and context-aware interpretation of
handwritten mathematical expressions. Experimental results demonstrate that
ICAL notably surpasses the state-of-the-art(SOTA) models, improving the
expression recognition rate (ExpRate) by 2.25\%/1.81\%/1.39\% on the CROHME
2014/2016/2019 datasets respectively, and achieves a remarkable 69.06\% on the
challenging HME100k test set. We make our code available on the GitHub:
https://github.com/qingzhenduyu/ICAL
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICDAR 2024 Oral Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CapS-Adapter: Caption-based <span class="highlight-title">MultiModal</span> Adapter in Zero-Shot
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.16591v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.16591v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qijie Wang, Guandu Liu, Bin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in vision-language foundational models, such as CLIP, have
demonstrated significant strides in zero-shot classification. However, the
extensive parameterization of models like CLIP necessitates a
resource-intensive fine-tuning process. In response, TIP-Adapter and SuS-X have
introduced training-free methods aimed at bolstering the efficacy of downstream
tasks. While these approaches incorporate support sets to maintain data
distribution consistency between knowledge cache and test sets, they often fall
short in terms of generalization on the test set, particularly when faced with
test data exhibiting substantial distributional variations. In this work, we
present CapS-Adapter, an innovative method that employs a caption-based support
set, effectively harnessing both image and caption features to exceed existing
state-of-the-art techniques in training-free scenarios. CapS-Adapter adeptly
constructs support sets that closely mirror target distributions, utilizing
instance-level distribution features extracted from multimodal large models. By
leveraging CLIP's single and cross-modal strengths, CapS-Adapter enhances
predictive accuracy through the use of multimodal support sets. Our method
achieves outstanding zero-shot classification results across 19 benchmark
datasets, improving accuracy by 2.19\% over the previous leading method. Our
contributions are substantiated through extensive validation on multiple
benchmark datasets, demonstrating superior performance and robust
generalization capabilities. Our code is made publicly available at
https://github.com/WLuLi/CapS-Adapter.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACM Multimedia 2024 Poster</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Classification by Coupling Data Mollification with Label
  Smoothing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.01494v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.01494v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Markus Heinonen, Ba-Hien Tran, Michael Kampffmeyer, Maurizio Filippone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Introducing training-time augmentations is a key technique to enhance
generalization and prepare deep neural networks against test-time corruptions.
Inspired by the success of generative diffusion models, we propose a novel
approach of coupling data mollification, in the form of image noising and
blurring, with label smoothing to align predicted label confidences with image
degradation. The method is simple to implement, introduces negligible
overheads, and can be combined with existing augmentations. We demonstrate
improved robustness and uncertainty quantification on the corrupted image
benchmarks of the CIFAR and TinyImageNet datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LOVA3: Learning to Visual Question Answering, Asking and Assessment <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14974v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14974v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Henry Hengyuan Zhao, Pan Zhou, Difei Gao, Zechen Bai, Mike Zheng Shou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Question answering, asking, and assessment are three innate human traits
crucial for understanding the world and acquiring knowledge. By enhancing these
capabilities, humans can more effectively utilize data, leading to better
comprehension and learning outcomes. Current Multimodal Large Language Models
(MLLMs) primarily focus on question answering, often neglecting the full
potential of questioning and assessment skills. Inspired by the human learning
mechanism, we introduce LOVA3, an innovative framework named "Learning tO
Visual question Answering, Asking and Assessment," designed to equip MLLMs with
these additional capabilities. Our approach involves the creation of two
supplementary training tasks GenQA and EvalQA, aiming at fostering the skills
of asking and assessing questions in the context of images. To develop the
questioning ability, we compile a comprehensive set of multimodal foundational
tasks. For assessment, we introduce a new benchmark called EvalQABench,
comprising 64,000 training samples (split evenly between positive and negative
samples) and 5,000 validation and testing samples. We posit that enhancing
MLLMs with the capabilities to answer, ask, and assess questions will enhance
their multimodal comprehension, ultimately improving overall performance. To
validate this hypothesis, we train MLLMs using the LOVA3 framework and evaluate
them on a range of multimodal datasets and benchmarks. Our results demonstrate
consistent performance gains, underscoring the critical role of these
additional tasks in fostering comprehensive intelligence in MLLMs. The code is
available at https://github.com/showlab/LOVA3.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024. The code is available at
  https://github.com/showlab/LOVA3</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning from Pattern Completion: Self-supervised Controllable
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18694v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18694v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiqiang Chen, Guofan Fan, Jinying Gao, Lei Ma, Bo Lei, Tiejun Huang, Shan Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The human brain exhibits a strong ability to spontaneously associate
different visual attributes of the same or similar visual scene, such as
associating sketches and graffiti with real-world visual objects, usually
without supervising information. In contrast, in the field of artificial
intelligence, controllable generation methods like ControlNet heavily rely on
annotated training datasets such as depth maps, semantic segmentation maps, and
poses, which limits the method's scalability. Inspired by the neural mechanisms
that may contribute to the brain's associative power, specifically the cortical
modularization and hippocampal pattern completion, here we propose a
self-supervised controllable generation (SCG) framework. Firstly, we introduce
an equivariant constraint to promote inter-module independence and intra-module
correlation in a modular autoencoder network, thereby achieving functional
specialization. Subsequently, based on these specialized modules, we employ a
self-supervised pattern completion approach for controllable generation
training. Experimental results demonstrate that the proposed modular
autoencoder effectively achieves functional specialization, including the
modular processing of color, brightness, and edge detection, and exhibits
brain-like features including orientation selectivity, color antagonism, and
center-surround receptive fields. Through self-supervised training, associative
generation capabilities spontaneously emerge in SCG, demonstrating excellent
generalization ability to various tasks such as associative generation on
painting, sketches, and ancient graffiti. Compared to the previous
representative method ControlNet, our proposed approach not only demonstrates
superior robustness in more challenging high-noise scenarios but also possesses
more promising scalability potential due to its self-supervised manner.Codes
are released on Github and Gitee.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An efficient dual-branch framework via implicit self-texture enhancement
  for arbitrary-scale histopathology image super-resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.15613v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.15613v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minghong Duan, Linhao Qu, Zhiwei Yang, Manning Wang, Chenxi Zhang, Zhijian Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-quality whole-slide scanning is expensive, complex, and time-consuming,
thus limiting the acquisition and utilization of high-resolution histopathology
images in daily clinical work. Deep learning-based single-image
super-resolution (SISR) techniques provide an effective way to solve this
problem. However, the existing SISR models applied in histopathology images can
only work in fixed integer scaling factors, decreasing their applicability.
Though methods based on implicit neural representation (INR) have shown
promising results in arbitrary-scale super-resolution (SR) of natural images,
applying them directly to histopathology images is inadequate because they have
unique fine-grained image textures different from natural images. Thus, we
propose an Implicit Self-Texture Enhancement-based dual-branch framework (ISTE)
for arbitrary-scale SR of histopathology images to address this challenge. The
proposed ISTE contains a feature aggregation branch and a texture learning
branch. We employ the feature aggregation branch to enhance the learning of the
local details for SR images while utilizing the texture learning branch to
enhance the learning of high-frequency texture details. Then, we design a
two-stage texture enhancement strategy to fuse the features from the two
branches to obtain the SR images. Experiments on publicly available datasets,
including TMA, HistoSR, and the TCGA lung cancer datasets, demonstrate that
ISTE outperforms existing fixed-scale and arbitrary-scale SR algorithms across
various scaling factors. Additionally, extensive experiments have shown that
the histopathology images reconstructed by the proposed ISTE are applicable to
downstream pathology image analysis tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Learning for Surgical Instrument Recognition and Segmentation in
  Robotic-Assisted Surgeries: A Systematic Review 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07269v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07269v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fatimaelzahraa Ali Ahmed, Mahmoud Yousef, Mariam Ali Ahmed, Hasan Omar Ali, Anns Mahboob, Hazrat Ali, Zubair Shah, Omar Aboumarzouk, Abdulla Al Ansari, Shidin Balakrishnan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Applying deep learning (DL) for annotating surgical instruments in
robot-assisted minimally invasive surgeries (MIS) represents a significant
advancement in surgical technology. This systematic review examines 48 studies
that and advanced DL methods and architectures. These sophisticated DL models
have shown notable improvements in the precision and efficiency of detecting
and segmenting surgical tools. The enhanced capabilities of these models
support various clinical applications, including real-time intraoperative
guidance, comprehensive postoperative evaluations, and objective assessments of
surgical skills. By accurately identifying and segmenting surgical instruments
in video data, DL models provide detailed feedback to surgeons, thereby
improving surgical outcomes and reducing complication risks. Furthermore, the
application of DL in surgical education is transformative. The review
underscores the significant impact of DL on improving the accuracy of skill
assessments and the overall quality of surgical training programs. However,
implementing DL in surgical tool detection and segmentation faces challenges,
such as the need for large, accurately annotated datasets to train these models
effectively. The manual annotation process is labor-intensive and
time-consuming, posing a significant bottleneck. Future research should focus
on automating the detection and segmentation process and enhancing the
robustness of DL models against environmental variations. Expanding the
application of DL models across various surgical specialties will be essential
to fully realize this technology's potential. Integrating DL with other
emerging technologies, such as augmented reality (AR), also offers promising
opportunities to further enhance the precision and efficacy of surgical
procedures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>57 pages, 9 figures, Published in Artificial Intelligence Reviews
  journal <https://link.springer.com/journal/10462></span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TaE: Task-aware Expandable Representation for Long Tail Class
  Incremental Learning <span class="chip">ACCV2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.05797v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.05797v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linjie Li, Zhenyu Wu, Jiaming Liu, Yang Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class-incremental learning is dedicated to the development of deep learning
models that are capable of acquiring new knowledge while retaining previously
learned information. Most methods focus on balanced data distribution for each
task, overlooking real-world long-tailed distributions. Therefore, Long-Tailed
Class-Incremental Learning has been introduced, which trains on data where head
classes have more samples than tail classes. Existing methods mainly focus on
preserving representative samples from previous classes to combat catastrophic
forgetting. Recently, dynamic network algorithms freeze old network structures
and expand new ones, achieving significant performance. However, with the
introduction of the long-tail problem, merely extending Determined blocks can
lead to miscalibrated predictions, while expanding the entire backbone results
in an explosion of memory size. To address these issues, we introduce a novel
Task-aware Expandable (TaE) framework, dynamically allocating and updating
task-specific trainable parameters to learn diverse representations from each
incremental task while resisting forgetting through the majority of frozen
model parameters. To further encourage the class-specific feature
representation, we develop a Centroid-Enhanced (CEd) method to guide the update
of these task-aware parameters. This approach is designed to adaptively
allocate feature space for every class by adjusting the distance between intra-
and inter-class features, which can extend to all "training from sketch"
algorithms. Extensive experiments demonstrate that TaE achieves
state-of-the-art performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACCV2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GS2Pose: Two-stage 6D Object Pose Estimation Guided by Gaussian
  Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03807v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03807v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jilan Mei, Junbo Li, Cai Meng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a new method for accurate and robust 6D pose estimation
of novel objects, named GS2Pose. By introducing 3D Gaussian splatting, GS2Pose
can utilize the reconstruction results without requiring a high-quality CAD
model, which means it only requires segmented RGBD images as input.
Specifically, GS2Pose employs a two-stage structure consisting of coarse
estimation followed by refined estimation. In the coarse stage, a lightweight
U-Net network with a polarization attention mechanism, called Pose-Net, is
designed. By using the 3DGS model for supervised training, Pose-Net can
generate NOCS images to compute a coarse pose. In the refinement stage, GS2Pose
formulates a pose regression algorithm following the idea of reprojection or
Bundle Adjustment (BA), referred to as GS-Refiner. By leveraging Lie algebra to
extend 3DGS, GS-Refiner obtains a pose-differentiable rendering pipeline that
refines the coarse pose by comparing the input images with the rendered images.
GS-Refiner also selectively updates parameters in the 3DGS model to achieve
environmental adaptation, thereby enhancing the algorithm's robustness and
flexibility to illuminative variation, occlusion, and other challenging
disruptive factors. GS2Pose was evaluated through experiments conducted on the
LineMod dataset, where it was compared with similar algorithms, yielding highly
competitive results. The code for GS2Pose will soon be released on GitHub.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Training-free Zero-shot Composed Image Retrieval via Weighted Modality
  Fusion and Similarity <span class="chip">TAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.04918v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.04918v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ren-Di Wu, Yu-Yen Lin, Huei-Fang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Composed image retrieval (CIR), which formulates the query as a combination
of a reference image and modified text, has emerged as a new form of image
search due to its enhanced ability to capture user intent. However, training a
CIR model in a supervised manner typically requires labor-intensive collection
of (reference image, text modifier, target image) triplets. While existing
zero-shot CIR (ZS-CIR) methods eliminate the need for training on specific
downstream datasets, they still require additional pretraining on large-scale
image datasets. In this paper, we introduce a training-free approach for
ZS-CIR. Our approach, Weighted Modality fusion and similarity for CIR
(WeiMoCIR), operates under the assumption that image and text modalities can be
effectively combined using a simple weighted average. This allows the query
representation to be constructed directly from the reference image and text
modifier. To further enhance retrieval performance, we employ multimodal large
language models (MLLMs) to generate image captions for the database images and
incorporate these textual captions into the similarity computation by combining
them with image information using a weighted average. Our approach is simple,
easy to implement, and its effectiveness is validated through experiments on
the FashionIQ and CIRR datasets. Code is available at
https://github.com/whats2000/WeiMoCIR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 6 figures, International Conference on Technologies and
  Applications of Artificial Intelligence (TAAI) Camera Ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Calibrated Robust <span class="highlight-title">Fine-Tuning</span> of <span class="highlight-title">Vision-Language</span> Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.01723v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.01723v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changdae Oh, Hyesu Lim, Mijoo Kim, Dongyoon Han, Sangdoo Yun, Jaegul Choo, Alexander Hauptmann, Zhi-Qi Cheng, Kyungwoo Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Improving out-of-distribution (OOD) generalization during in-distribution
(ID) adaptation is a primary goal of robust fine-tuning of zero-shot models
beyond naive fine-tuning. However, despite decent OOD generalization
performance from recent robust fine-tuning methods, confidence calibration for
reliable model output has not been fully addressed. This work proposes a robust
fine-tuning method that improves both OOD accuracy and confidence calibration
simultaneously in vision language models. Firstly, we show that both OOD
classification and OOD calibration errors have a shared upper bound consisting
of two terms of ID data: 1) ID calibration error and 2) the smallest singular
value of the ID input covariance matrix. Based on this insight, we design a
novel framework that conducts fine-tuning with a constrained multimodal
contrastive loss enforcing a larger smallest singular value, which is further
guided by the self-distillation of a moving-averaged model to achieve
calibrated prediction as well. Starting from empirical evidence supporting our
theoretical statements, we provide extensive experimental results on ImageNet
distribution shift benchmarks that demonstrate the effectiveness of our theorem
and its practical implementation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 (a short version was presented at the NeurIPS 2023
  Workshop on Distribution Shifts); Major modification of (v7): Fixing the
  x-axis of Figure 3 and Pearson correlation, accordingly</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LightAvatar: Efficient Head Avatar as Dynamic Neural Light Field <span class="chip">ECCV'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18057v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18057v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huan Wang, Feitong Tan, Ziqian Bai, Yinda Zhang, Shichen Liu, Qiangeng Xu, Menglei Chai, Anish Prabhu, Rohit Pandey, Sean Fanello, Zeng Huang, Yun Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works have shown that neural radiance fields (NeRFs) on top of
parametric models have reached SOTA quality to build photorealistic head
avatars from a monocular video. However, one major limitation of the NeRF-based
avatars is the slow rendering speed due to the dense point sampling of NeRF,
preventing them from broader utility on resource-constrained devices. We
introduce LightAvatar, the first head avatar model based on neural light fields
(NeLFs). LightAvatar renders an image from 3DMM parameters and a camera pose
via a single network forward pass, without using mesh or volume rendering. The
proposed approach, while being conceptually appealing, poses a significant
challenge towards real-time efficiency and training stability. To resolve them,
we introduce dedicated network designs to obtain proper representations for the
NeLF model and maintain a low FLOPs budget. Meanwhile, we tap into a
distillation-based training strategy that uses a pretrained avatar model as
teacher to synthesize abundant pseudo data for training. A warping field
network is introduced to correct the fitting error in the real data so that the
model can learn better. Extensive experiments suggest that our method can
achieve new SOTA image quality quantitatively or qualitatively, while being
significantly faster than the counterparts, reporting 174.1 FPS (512x512
resolution) on a consumer-grade GPU (RTX3090) with no customized optimization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV'24 CADL Workshop. Code:
  https://github.com/MingSun-Tse/LightAvatar-TensorFlow. V2: Corrected speed
  benchmark with GaussianAvatar</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GeNIe: Generative Hard Negative Images Through Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.02548v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.02548v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soroush Abbasi Koohpayegani, Anuj Singh, K L Navaneet, Hamed Pirsiavash, Hadi Jamali-Rad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data augmentation is crucial in training deep models, preventing them from
overfitting to limited data. Recent advances in generative AI, e.g., diffusion
models, have enabled more sophisticated augmentation techniques that produce
data resembling natural images. We introduce GeNIe a novel augmentation method
which leverages a latent diffusion model conditioned on a text prompt to
combine two contrasting data points (an image from the source category and a
text prompt from the target category) to generate challenging augmentations. To
achieve this, we adjust the noise level (equivalently, number of diffusion
iterations) to ensure the generated image retains low-level and background
features from the source image while representing the target category,
resulting in a hard negative sample for the source category. We further
automate and enhance GeNIe by adaptively adjusting the noise level selection on
a per image basis (coined as GeNIe-Ada), leading to further performance
improvements. Our extensive experiments, in both few-shot and long-tail
distribution settings, demonstrate the effectiveness of our novel augmentation
method and its superior performance over the prior art. Our code is available
at: https://github.com/UCDvision/GeNIe
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our code is available https://github.com/UCDvision/GeNIe</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SeafloorAI: A Large-scale <span class="highlight-title">Vision-Language</span> Dataset for Seafloor
  Geological Survey 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00172v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00172v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kien X. Nguyen, Fengchun Qiao, Arthur Trembanis, Xi Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A major obstacle to the advancements of machine learning models in marine
science, particularly in sonar imagery analysis, is the scarcity of AI-ready
datasets. While there have been efforts to make AI-ready sonar image dataset
publicly available, they suffer from limitations in terms of environment
setting and scale. To bridge this gap, we introduce SeafloorAI, the first
extensive AI-ready datasets for seafloor mapping across 5 geological layers
that is curated in collaboration with marine scientists. We further extend the
dataset to SeafloorGenAI by incorporating the language component in order to
facilitate the development of both vision- and language-capable machine
learning models for sonar imagery. The dataset consists of 62 geo-distributed
data surveys spanning 17,300 square kilometers, with 696K sonar images, 827K
annotated segmentation masks, 696K detailed language descriptions and
approximately 7M question-answer pairs. By making our data processing source
code publicly available, we aim to engage the marine science community to
enrich the data pool and inspire the machine learning community to develop more
robust models. This collaborative approach will enhance the capabilities and
applications of our datasets within both fields.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Typicalness-Aware Learning for Failure Detection <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01981v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01981v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijun Liu, Jiequan Cui, Zhuotao Tian, Senqiao Yang, Qingdong He, Xiaoling Wang, Jingyong Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) often suffer from the overconfidence issue, where
incorrect predictions are made with high confidence scores, hindering the
applications in critical systems. In this paper, we propose a novel approach
called Typicalness-Aware Learning (TAL) to address this issue and improve
failure detection performance. We observe that, with the cross-entropy loss,
model predictions are optimized to align with the corresponding labels via
increasing logit magnitude or refining logit direction. However, regarding
atypical samples, the image content and their labels may exhibit disparities.
This discrepancy can lead to overfitting on atypical samples, ultimately
resulting in the overconfidence issue that we aim to address. To tackle the
problem, we have devised a metric that quantifies the typicalness of each
sample, enabling the dynamic adjustment of the logit magnitude during the
training process. By allowing atypical samples to be adequately fitted while
preserving reliable logit direction, the problem of overconfidence can be
mitigated. TAL has been extensively evaluated on benchmark datasets, and the
results demonstrate its superiority over existing failure detection methods.
Specifically, TAL achieves a more than 5% improvement on CIFAR100 in terms of
the Area Under the Risk-Coverage Curve (AURC) compared to the state-of-the-art.
Code is available at https://github.com/liuyijungoon/TAL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Undermining Image and Text Classification Algorithms Using Adversarial
  Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03348v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03348v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Langalibalele Lunga, Suhas Sreehari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning models are prone to adversarial attacks, where inputs can be
manipulated in order to cause misclassifications. While previous research has
focused on techniques like Generative Adversarial Networks (GANs), there's
limited exploration of GANs and Synthetic Minority Oversampling Technique
(SMOTE) in text and image classification models to perform adversarial attacks.
Our study addresses this gap by training various machine learning models and
using GANs and SMOTE to generate additional data points aimed at attacking text
classification models. Furthermore, we extend our investigation to face
recognition models, training a Convolutional Neural Network(CNN) and subjecting
it to adversarial attacks with fast gradient sign perturbations on key features
identified by GradCAM, a technique used to highlight key image characteristics
CNNs use in classification. Our experiments reveal a significant vulnerability
in classification models. Specifically, we observe a 20 % decrease in accuracy
for the top-performing text classification models post-attack, along with a 30
% decrease in facial recognition accuracy. This highlights the susceptibility
of these models to manipulation of input data. Adversarial attacks not only
compromise the security but also undermine the reliability of machine learning
systems. By showcasing the impact of adversarial attacks on both text
classification and face recognition models, our study underscores the urgent
need for develop robust defenses against such vulnerabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for presentation at Electronic Imaging Conference 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Aligning Text-to-Image Diffusion Models with Reward Backpropagation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03739v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03739v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mihir Prabhudesai, Anirudh Goyal, Deepak Pathak, Katerina Fragkiadaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image diffusion models have recently emerged at the forefront of
image generation, powered by very large-scale unsupervised or weakly supervised
text-to-image training datasets. Due to their unsupervised training,
controlling their behavior in downstream tasks, such as maximizing
human-perceived image quality, image-text alignment, or ethical image
generation, is difficult. Recent works finetune diffusion models to downstream
reward functions using vanilla reinforcement learning, notorious for the high
variance of the gradient estimators. In this paper, we propose AlignProp, a
method that aligns diffusion models to downstream reward functions using
end-to-end backpropagation of the reward gradient through the denoising
process. While naive implementation of such backpropagation would require
prohibitive memory resources for storing the partial derivatives of modern
text-to-image models, AlignProp finetunes low-rank adapter weight modules and
uses gradient checkpointing, to render its memory usage viable. We test
AlignProp in finetuning diffusion models to various objectives, such as
image-text semantic alignment, aesthetics, compressibility and controllability
of the number of objects present, as well as their combinations. We show
AlignProp achieves higher rewards in fewer training steps than alternatives,
while being conceptually simpler, making it a straightforward choice for
optimizing diffusion models for differentiable reward functions of interest.
Code and Visualization results are available at https://align-prop.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is subsumed by a later paper of ours: arXiv:2407.08737</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Accuracy: Ensuring Correct Predictions With Correct Rationales <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00132v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00132v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tang Li, Mengmeng Ma, Xi Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large pretrained foundation models demonstrate exceptional performance and,
in some high-stakes applications, even surpass human experts. However, most of
these models are currently evaluated primarily on prediction accuracy,
overlooking the validity of the rationales behind their accurate predictions.
For the safe deployment of foundation models, there is a pressing need to
ensure double-correct predictions, i.e., correct prediction backed by correct
rationales. To achieve this, we propose a two-phase scheme: First, we curate a
new dataset that offers structured rationales for visual recognition tasks.
Second, we propose a rationale-informed optimization method to guide the model
in disentangling and localizing visual evidence for each rationale, without
requiring manual annotations. Extensive experiments and ablation studies
demonstrate that our model outperforms state-of-the-art models by up to 10.1%
in prediction accuracy across a wide range of tasks. Furthermore, our method
significantly improves the model's rationale correctness, improving
localization by 7.5% and disentanglement by 36.5%. Our dataset, source code,
and pretrained weights: https://github.com/deep-real/DCP
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings of the 38th Conference on Neural Information
  Processing Systems (NeurIPS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ bit2bit: 1-bit quanta video reconstruction via self-supervised photon
  prediction <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23247v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23247v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yehe Liu, Alexander Krull, Hector Basevi, Ales Leonardis, Michael W. Jenkins
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quanta image sensors, such as SPAD arrays, are an emerging sensor technology,
producing 1-bit arrays representing photon detection events over exposures as
short as a few nanoseconds. In practice, raw data are post-processed using
heavy spatiotemporal binning to create more useful and interpretable images at
the cost of degrading spatiotemporal resolution. In this work, we propose
bit2bit, a new method for reconstructing high-quality image stacks at the
original spatiotemporal resolution from sparse binary quanta image data.
Inspired by recent work on Poisson denoising, we developed an algorithm that
creates a dense image sequence from sparse binary photon data by predicting the
photon arrival location probability distribution. However, due to the binary
nature of the data, we show that the assumption of a Poisson distribution is
inadequate. Instead, we model the process with a Bernoulli lattice process from
the truncated Poisson. This leads to the proposal of a novel self-supervised
solution based on a masked loss function. We evaluate our method using both
simulated and real data. On simulated data from a conventional video, we
achieve 34.35 mean PSNR with extremely photon-sparse binary input (<0.06
photons per pixel per frame). We also present a novel dataset containing a wide
range of real SPAD high-speed videos under various challenging imaging
conditions. The scenes cover strong/weak ambient light, strong motion,
ultra-fast events, etc., which will be made available to the community, on
which we demonstrate the promise of our approach. Both reconstruction quality
and throughput substantially surpass the state-of-the-art methods (e.g., Quanta
Burst Photography (QBP)). Our approach significantly enhances the visualization
and usability of the data, enabling the application of existing analysis
techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting Surgical Instrument Segmentation Without Human Intervention:
  A Graph Partitioning View 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.14789v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.14789v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyu Sheng, Jianan Fan, Dongnan Liu, Ron Kikinis, Weidong Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surgical instrument segmentation (SIS) on endoscopic images stands as a
long-standing and essential task in the context of computer-assisted
interventions for boosting minimally invasive surgery. Given the recent surge
of deep learning methodologies and their data-hungry nature, training a neural
predictive model based on massive expert-curated annotations has been
dominating and served as an off-the-shelf approach in the field, which could,
however, impose prohibitive burden to clinicians for preparing fine-grained
pixel-wise labels corresponding to the collected surgical video frames. In this
work, we propose an unsupervised method by reframing the video frame
segmentation as a graph partitioning problem and regarding image pixels as
graph nodes, which is significantly different from the previous efforts. A
self-supervised pre-trained model is firstly leveraged as a feature extractor
to capture high-level semantic features. Then, Laplacian matrixs are computed
from the features and are eigendecomposed for graph partitioning. On the "deep"
eigenvectors, a surgical video frame is meaningfully segmented into different
modules such as tools and tissues, providing distinguishable semantic
information like locations, classes, and relations. The segmentation problem
can then be naturally tackled by applying clustering or threshold on the
eigenvectors. Extensive experiments are conducted on various datasets (e.g.,
EndoVis2017, EndoVis2018, UCL, etc.) for different clinical endpoints. Across
all the challenging scenarios, our method demonstrates outstanding performance
and robustness higher than unsupervised state-of-the-art (SOTA) methods. The
code is released at https://github.com/MingyuShengSMY/GraphClusteringSIS.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by The 32nd ACM International Conference on Multimedia (ACM
  MM 2024) Workshop on Multimedia Computing for Health and Medicine (MCHM)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AMNCutter: Affinity-Attention-Guided Multi-View Normalized Cutter for
  Unsupervised Surgical Instrument Segmentation <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03695v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03695v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyu Sheng, Jianan Fan, Dongnan Liu, Ron Kikinis, Weidong Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surgical instrument segmentation (SIS) is pivotal for robotic-assisted
minimally invasive surgery, assisting surgeons by identifying surgical
instruments in endoscopic video frames. Recent unsupervised surgical instrument
segmentation (USIS) methods primarily rely on pseudo-labels derived from
low-level features such as color and optical flow, but these methods show
limited effectiveness and generalizability in complex and unseen endoscopic
scenarios. In this work, we propose a label-free unsupervised model featuring a
novel module named Multi-View Normalized Cutter (m-NCutter). Different from
previous USIS works, our model is trained using a graph-cutting loss function
that leverages patch affinities for supervision, eliminating the need for
pseudo-labels. The framework adaptively determines which affinities from which
levels should be prioritized. Therefore, the low- and high-level features and
their affinities are effectively integrated to train a label-free unsupervised
model, showing superior effectiveness and generalization ability. We conduct
comprehensive experiments across multiple SIS datasets to validate our
approach's state-of-the-art (SOTA) performance, robustness, and exceptional
potential as a pre-trained model. Our code is released at
https://github.com/MingyuShengSMY/AMNCutter.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the 2025 IEEE/CVF Winter Conference on Applications of
  Computer Vision (WACV 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Controllable Talking Face Generation by Implicit Facial Keypoints
  Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.02880v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.02880v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dong Zhao, Jiaying Shi, Wenjun Li, Shudong Wang, Shenghui Xu, Zhaoming Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-driven talking face generation has garnered significant interest within
the domain of digital human research. Existing methods are encumbered by
intricate model architectures that are intricately dependent on each other,
complicating the process of re-editing image or video inputs. In this work, we
present ControlTalk, a talking face generation method to control face
expression deformation based on driven audio, which can construct the head pose
and facial expression including lip motion for both single image or sequential
video inputs in a unified manner. By utilizing a pre-trained video synthesis
renderer and proposing the lightweight adaptation, ControlTalk achieves precise
and naturalistic lip synchronization while enabling quantitative control over
mouth opening shape. Our experiments show that our method is superior to
state-of-the-art performance on widely used benchmarks, including HDTF and
MEAD. The parameterized adaptation demonstrates remarkable generalization
capabilities, effectively handling expression deformation across same-ID and
cross-ID scenarios, and extending its utility to out-of-domain portraits,
regardless of languages. Code is available at
https://github.com/NetEase-Media/ControlTalk.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Principled Probabilistic Imaging using Diffusion Models as Plug-and-Play
  Priors <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.18782v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.18782v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihui Wu, Yu Sun, Yifan Chen, Bingliang Zhang, Yisong Yue, Katherine L. Bouman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models (DMs) have recently shown outstanding capabilities in
modeling complex image distributions, making them expressive image priors for
solving Bayesian inverse problems. However, most existing DM-based methods rely
on approximations in the generative process to be generic to different inverse
problems, leading to inaccurate sample distributions that deviate from the
target posterior defined within the Bayesian framework. To harness the
generative power of DMs while avoiding such approximations, we propose a Markov
chain Monte Carlo algorithm that performs posterior sampling for general
inverse problems by reducing it to sampling the posterior of a Gaussian
denoising problem. Crucially, we leverage a general DM formulation as a unified
interface that allows for rigorously solving the denoising problem with a range
of state-of-the-art DMs. We demonstrate the effectiveness of the proposed
method on six inverse problems (three linear and three nonlinear), including a
real-world black hole imaging problem. Experimental results indicate that our
proposed method offers more accurate reconstructions and posterior estimation
compared to existing DM-based imaging inverse methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TLCM: Training-efficient Latent Consistency Model for Image Generation
  with 2-8 Steps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05768v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05768v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingsong Xie, Zhenyi Liao, Zhijie Deng, Chen chen, Haonan Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distilling latent diffusion models (LDMs) into ones that are fast to sample
from is attracting growing research interest. However, the majority of existing
methods face two critical challenges: (1) They hinge on long training using a
huge volume of real data. (2) They routinely lead to quality degradation for
generation, especially in text-image alignment. This paper proposes a novel
training-efficient Latent Consistency Model (TLCM) to overcome these
challenges. Our method first accelerates LDMs via data-free multistep latent
consistency distillation (MLCD), and then data-free latent consistency
distillation is proposed to efficiently guarantee the inter-segment consistency
in MLCD. Furthermore, we introduce bags of techniques, e.g., distribution
matching, adversarial learning, and preference learning, to enhance TLCM's
performance at few-step inference without any real data. TLCM demonstrates a
high level of flexibility by enabling adjustment of sampling steps within the
range of 2 to 8 while still producing competitive outputs compared to full-step
approaches. Notably, TLCM enjoys the data-free merit by employing synthetic
data from the teacher for distillation. With just 70 training hours on an A100
GPU, a 3-step TLCM distilled from SDXL achieves an impressive CLIP Score of
33.68 and an Aesthetic Score of 5.97 on the MSCOCO-2017 5K benchmark,
surpassing various accelerated models and even outperforming the teacher model
in human preference metrics. We also demonstrate the versatility of TLCMs in
applications including image style transfer, controllable generation, and
Chinese-to-image generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Variational Zero-shot Multispectral Pansharpening 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.06633v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.06633v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangyu Rui, Xiangyong Cao, Yining Li, Deyu Meng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pansharpening aims to generate a high spatial resolution multispectral image
(HRMS) by fusing a low spatial resolution multispectral image (LRMS) and a
panchromatic image (PAN). The most challenging issue for this task is that only
the to-be-fused LRMS and PAN are available, and the existing deep
learning-based methods are unsuitable since they rely on many training pairs.
Traditional variational optimization (VO) based methods are well-suited for
addressing such a problem. They focus on carefully designing explicit fusion
rules as well as regularizations for an optimization problem, which are based
on the researcher's discovery of the image relationships and image structures.
Unlike previous VO-based methods, in this work, we explore such complex
relationships by a parameterized term rather than a manually designed one.
Specifically, we propose a zero-shot pansharpening method by introducing a
neural network into the optimization objective. This network estimates a
representation component of HRMS, which mainly describes the relationship
between HRMS and PAN. In this way, the network achieves a similar goal to the
so-called deep image prior because it implicitly regulates the relationship
between the HRMS and PAN images through its inherent structure. We directly
minimize this optimization objective via network parameters and the expected
HRMS image through iterative updating. Extensive experiments on various
benchmark datasets demonstrate that our proposed method can achieve better
performance compared with other state-of-the-art methods. The codes are
available at https://github.com/xyrui/PSDip.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SS3DM: Benchmarking Street-View Surface Reconstruction with a Synthetic
  3D Mesh Dataset <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21739v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21739v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yubin Hu, Kairui Wen, Heng Zhou, Xiaoyang Guo, Yong-Jin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing accurate 3D surfaces for street-view scenarios is crucial for
applications such as digital entertainment and autonomous driving simulation.
However, existing street-view datasets, including KITTI, Waymo, and nuScenes,
only offer noisy LiDAR points as ground-truth data for geometric evaluation of
reconstructed surfaces. These geometric ground-truths often lack the necessary
precision to evaluate surface positions and do not provide data for assessing
surface normals. To overcome these challenges, we introduce the SS3DM dataset,
comprising precise \textbf{S}ynthetic \textbf{S}treet-view \textbf{3D}
\textbf{M}esh models exported from the CARLA simulator. These mesh models
facilitate accurate position evaluation and include normal vectors for
evaluating surface normal. To simulate the input data in realistic driving
scenarios for 3D reconstruction, we virtually drive a vehicle equipped with six
RGB cameras and five LiDAR sensors in diverse outdoor scenes. Leveraging this
dataset, we establish a benchmark for state-of-the-art surface reconstruction
methods, providing a comprehensive evaluation of the associated challenges.
  For more information, visit our homepage at https://ss3dm.top.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024, Track on Datasets and Benchmarks</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-11-06T00:00:00Z">2024-11-06</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">92</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Medical Adaptation of Large Language and <span class="highlight-title">Vision-Language</span> Models: Are We
  Making Progress? <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04118v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04118v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel P. Jeong, Saurabh Garg, Zachary C. Lipton, Michael Oberst
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Several recent works seek to develop foundation models specifically for
medical applications, adapting general-purpose large language models (LLMs) and
vision-language models (VLMs) via continued pretraining on publicly available
biomedical corpora. These works typically claim that such domain-adaptive
pretraining (DAPT) improves performance on downstream medical tasks, such as
answering medical licensing exam questions. In this paper, we compare seven
public "medical" LLMs and two VLMs against their corresponding base models,
arriving at a different conclusion: all medical VLMs and nearly all medical
LLMs fail to consistently improve over their base models in the zero-/few-shot
prompting regime for medical question-answering (QA) tasks. For instance,
across the tasks and model pairs we consider in the 3-shot setting, medical
LLMs only outperform their base models in 12.1% of cases, reach a (statistical)
tie in 49.8% of cases, and are significantly worse than their base models in
the remaining 38.2% of cases. Our conclusions are based on (i) comparing each
medical model head-to-head, directly against the corresponding base model; (ii)
optimizing the prompts for each model separately; and (iii) accounting for
statistical uncertainty in comparisons. While these basic practices are not
consistently adopted in the literature, our ablations show that they
substantially impact conclusions. Our findings suggest that state-of-the-art
general-domain models may already exhibit strong medical knowledge and
reasoning capabilities, and offer recommendations to strengthen the conclusions
of future studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Main Conference as Long Paper (Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Consistency Preference Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04109v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04109v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Archiki Prasad, Weizhe Yuan, Richard Yuanzhe Pang, Jing Xu, Maryam Fazel-Zarandi, Mohit Bansal, Sainbayar Sukhbaatar, Jason Weston, Jane Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-alignment, whereby models learn to improve themselves without human
annotation, is a rapidly growing research area. However, existing techniques
often fail to improve complex reasoning tasks due to the difficulty of
assigning correct rewards. An orthogonal approach that is known to improve
correctness is self-consistency, a method applied at inference time based on
multiple sampling in order to find the most consistent answer. In this work, we
extend the self-consistency concept to help train models. We thus introduce
self-consistency preference optimization (ScPO), which iteratively trains
consistent answers to be preferred over inconsistent ones on unsupervised new
problems. We show ScPO leads to large improvements over conventional reward
model training on reasoning tasks such as GSM8K and MATH, closing the gap with
supervised training with gold answers or preferences, and that combining ScPO
with standard supervised learning improves results even further. On ZebraLogic,
ScPO finetunes Llama-3 8B to be superior to Llama-3 70B, Gemma-2 27B, and
Claude-3 Haiku.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Transformers Solve Propositional Logic Problems: A Mechanistic
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04105v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04105v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guan Zhe Hong, Nishanth Dikkala, Enming Luo, Cyrus Rashtchian, Rina Panigrahy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown amazing performance on tasks that
require planning and reasoning. Motivated by this, we investigate the internal
mechanisms that underpin a network's ability to perform complex logical
reasoning. We first construct a synthetic propositional logic problem that
serves as a concrete test-bed for network training and evaluation. Crucially,
this problem demands nontrivial planning to solve, but we can train a small
transformer to achieve perfect accuracy. Building on our set-up, we then pursue
an understanding of precisely how a three-layer transformer, trained from
scratch, solves this problem. We are able to identify certain "planning" and
"reasoning" circuits in the network that necessitate cooperation between the
attention blocks to implement the desired logic. To expand our findings, we
then study a larger model, Mistral 7B. Using activation patching, we
characterize internal components that are critical in solving our logic
problem. Overall, our work systemically uncovers novel aspects of small and
large transformers, and continues the study of how they plan and reason.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Summarization of Opinionated Political Documents with Varied
  Perspectives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04093v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04093v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicholas Deas, Kathleen McKeown
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Global partisan hostility and polarization has increased, and this
polarization is heightened around presidential elections. Models capable of
generating accurate summaries of diverse perspectives can help reduce such
polarization by exposing users to alternative perspectives. In this work, we
introduce a novel dataset and task for independently summarizing each political
perspective in a set of passages from opinionated news articles. For this task,
we propose a framework for evaluating different dimensions of perspective
summary performance. We benchmark 10 models of varying sizes and architectures
through both automatic and human evaluation. While recent models like GPT-4o
perform well on this task, we find that all models struggle to generate
summaries faithful to the intended perspective. Our analysis of summaries
focuses on how extraction behavior depends on the features of the input
documents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Collaborative Content Moderation Framework for Toxicity Detection
  based on Conformalized Estimates of Annotation Disagreement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04090v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04090v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guillermo Villate-Castillo, Javier Del Ser, Borja Sanz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Content moderation typically combines the efforts of human moderators and
machine learning models.However, these systems often rely on data where
significant disagreement occurs during moderation, reflecting the subjective
nature of toxicity perception.Rather than dismissing this disagreement as
noise, we interpret it as a valuable signal that highlights the inherent
ambiguity of the content,an insight missed when only the majority label is
considered.In this work, we introduce a novel content moderation framework that
emphasizes the importance of capturing annotation disagreement. Our approach
uses multitask learning, where toxicity classification serves as the primary
task and annotation disagreement is addressed as an auxiliary
task.Additionally, we leverage uncertainty estimation techniques, specifically
Conformal Prediction, to account for both the ambiguity in comment annotations
and the model's inherent uncertainty in predicting toxicity and
disagreement.The framework also allows moderators to adjust thresholds for
annotation disagreement, offering flexibility in determining when ambiguity
should trigger a review.We demonstrate that our joint approach enhances model
performance, calibration, and uncertainty estimation, while offering greater
parameter efficiency and improving the review process in comparison to
single-task methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ M3SciQA: A <span class="highlight-title">Multi-Modal</span> Multi-Document Scientific QA Benchmark for
  Evaluating Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04075v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04075v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuhan Li, Ziyao Shangguan, Yilun Zhao, Deyuan Li, Yixin Liu, Arman Cohan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing benchmarks for evaluating foundation models mainly focus on
single-document, text-only tasks. However, they often fail to fully capture the
complexity of research workflows, which typically involve interpreting
non-textual data and gathering information across multiple documents. To
address this gap, we introduce M3SciQA, a multi-modal, multi-document
scientific question answering benchmark designed for a more comprehensive
evaluation of foundation models. M3SciQA consists of 1,452 expert-annotated
questions spanning 70 natural language processing paper clusters, where each
cluster represents a primary paper along with all its cited documents,
mirroring the workflow of comprehending a single paper by requiring multi-modal
and multi-document data. With M3SciQA, we conduct a comprehensive evaluation of
18 foundation models. Our results indicate that current foundation models still
significantly underperform compared to human experts in multi-modal information
retrieval and in reasoning across multiple scientific documents. Additionally,
we explore the implications of these findings for the future advancement of
applying foundation models in multi-modal scientific literature analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beemo: Benchmark of Expert-edited Machine-generated Outputs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04032v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04032v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ekaterina Artemova, Jason Lucas, Saranya Venkatraman, Jooyoung Lee, Sergei Tilga, Adaku Uchendu, Vladislav Mikhailov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid proliferation of large language models (LLMs) has increased the
volume of machine-generated texts (MGTs) and blurred text authorship in various
domains. However, most existing MGT benchmarks include single-author texts
(human-written and machine-generated). This conventional design fails to
capture more practical multi-author scenarios, where the user refines the LLM
response for natural flow, coherence, and factual correctness. Our paper
introduces the Benchmark of Expert-edited Machine-generated Outputs (Beemo),
which includes 6.5k texts written by humans, generated by ten
instruction-finetuned LLMs, and edited by experts for various use cases,
ranging from creative writing to summarization. Beemo additionally comprises
13.1k machine-generated and LLM-edited texts, allowing for diverse MGT
detection evaluation across various edit types. We document Beemo's creation
protocol and present the results of benchmarking 33 configurations of MGT
detectors in different experimental setups. We find that expert-based editing
evades MGT detection, while LLM-edited texts are unlikely to be recognized as
human-written. Beemo and all materials are publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Prompt</span> Engineering Using GPT for Word-Level Code-Mixed Language
  Identification in Low-Resource Dravidian Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04025v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04025v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aniket Deroy, Subhankar Maity
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language Identification (LI) is crucial for various natural language
processing tasks, serving as a foundational step in applications such as
sentiment analysis, machine translation, and information retrieval. In
multilingual societies like India, particularly among the youth engaging on
social media, text often exhibits code-mixing, blending local languages with
English at different linguistic levels. This phenomenon presents formidable
challenges for LI systems, especially when languages intermingle within single
words. Dravidian languages, prevalent in southern India, possess rich
morphological structures yet suffer from under-representation in digital
platforms, leading to the adoption of Roman or hybrid scripts for
communication. This paper introduces a prompt based method for a shared task
aimed at addressing word-level LI challenges in Dravidian languages. In this
work, we leveraged GPT-3.5 Turbo to understand whether the large language
models is able to correctly classify words into correct categories. Our
findings show that the Kannada model consistently outperformed the Tamil model
across most metrics, indicating a higher accuracy and reliability in
identifying and categorizing Kannada language instances. In contrast, the Tamil
model showed moderate performance, particularly needing improvement in
precision and recall.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at FIRE 2024 (Track: Word-level Language Identification in
  Dravidian Languages)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WorryWords: Norms of Anxiety Association for over 44k English Words 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03966v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03966v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saif M. Mohammad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anxiety, the anticipatory unease about a potential negative outcome, is a
common and beneficial human emotion. However, there is still much that is not
known, such as how anxiety relates to our body and how it manifests in
language. This is especially pertinent given the increasing impact of
anxiety-related disorders. In this work, we introduce WorryWords, the first
large-scale repository of manually derived word--anxiety associations for over
44,450 English words. We show that the anxiety associations are highly
reliable. We use WorryWords to study the relationship between anxiety and other
emotion constructs, as well as the rate at which children acquire anxiety words
with age. Finally, we show that using WorryWords alone, one can accurately
track the change of anxiety in streams of text. The lexicon enables a wide
variety of anxiety-related research in psychology, NLP, public health, and
social sciences. WorryWords (and its translations to over 100 languages) is
freely available. http://saifmohammad.com/worrywords.html
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What Really is Commonsense Knowledge? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03964v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03964v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quyet V. Do, Junze Li, Tung-Duong Vuong, Zhaowei Wang, Yangqiu Song, Xiaojuan Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Commonsense datasets have been well developed in Natural Language Processing,
mainly through crowdsource human annotation. However, there are debates on the
genuineness of commonsense reasoning benchmarks. In specific, a significant
portion of instances in some commonsense benchmarks do not concern commonsense
knowledge. That problem would undermine the measurement of the true commonsense
reasoning ability of evaluated models. It is also suggested that the problem
originated from a blurry concept of commonsense knowledge, as distinguished
from other types of knowledge. To demystify all of the above claims, in this
study, we survey existing definitions of commonsense knowledge, ground into the
three frameworks for defining concepts, and consolidate them into a
multi-framework unified definition of commonsense knowledge (so-called
consolidated definition). We then use the consolidated definition for
annotations and experiments on the CommonsenseQA and CommonsenseQA 2.0 datasets
to examine the above claims. Our study shows that there exists a large portion
of non-commonsense-knowledge instances in the two datasets, and a large
performance gap on these two subsets where Large Language Models (LLMs) perform
worse on commonsense-knowledge instances.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and data will be released together with the next version of the
  paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Does A Text Preprocessing Pipeline Affect Ontology Syntactic
  Matching? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03962v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03962v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhangcheng Qiang, Kerry Taylor, Weiqing Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The generic text preprocessing pipeline, comprising Tokenisation,
Normalisation, Stop Words Removal, and Stemming/Lemmatisation, has been
implemented in many ontology matching (OM) systems. However, the lack of
standardisation in text preprocessing creates diversity in mapping results. In
this paper, we investigate the effect of the text preprocessing pipeline on OM
tasks at syntactic levels. Our experiments on 8 Ontology Alignment Evaluation
Initiative (OAEI) track repositories with 49 distinct alignments indicate: (1)
Tokenisation and Normalisation are currently more effective than Stop Words
Removal and Stemming/Lemmatisation; and (2) The selection of Lemmatisation and
Stemming is task-specific. We recommend standalone Lemmatisation or Stemming
with post-hoc corrections. We find that (3) Porter Stemmer and Snowball Stemmer
perform better than Lancaster Stemmer; and that (4) Part-of-Speech (POS)
Tagging does not help Lemmatisation. To repair less effective Stop Words
Removal and Stemming/Lemmatisation used in OM tasks, we propose a novel
context-based pipeline repair approach that significantly improves matching
correctness and overall matching performance. We also discuss the use of text
preprocessing pipeline in the new era of large language models (LLMs).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 26 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interactions Across Blocks in Post-Training Quantization of Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03934v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03934v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khasmamad Shabanovi, Lukas Wiest, Vladimir Golkov, Daniel Cremers, Thomas Pfeil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Post-training quantization is widely employed to reduce the computational
demands of neural networks. Typically, individual substructures, such as layers
or blocks of layers, are quantized with the objective of minimizing
quantization errors in their pre-activations by fine-tuning the corresponding
weights. Deriving this local objective from the global objective of minimizing
task loss involves two key simplifications: assuming substructures are mutually
independent and ignoring the knowledge of subsequent substructures as well as
the task loss. In this work, we assess the effects of these simplifications on
weight-only quantization of large language models. We introduce two multi-block
fine-tuning strategies and compare them against the baseline of fine-tuning
single transformer blocks. The first captures correlations of weights across
blocks by jointly optimizing multiple quantized blocks. The second incorporates
knowledge of subsequent blocks by minimizing the error in downstream
pre-activations rather than focusing solely on the quantized block. Our
findings indicate that the effectiveness of these methods depends on the
specific network model, with no impact on some models but demonstrating
significant benefits for others.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluation data contamination in LLMs: how do we measure it and (when)
  does it matter? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03923v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03923v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aaditya K. Singh, Muhammed Yusuf Kocyigit, Andrew Poulton, David Esiobu, Maria Lomeli, Gergely Szilvasy, Dieuwke Hupkes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hampering the interpretation of benchmark scores, evaluation data
contamination has become a growing concern in the evaluation of LLMs, and an
active area of research studies its effects. While evaluation data
contamination is easily understood intuitively, it is surprisingly difficult to
define precisely which samples should be considered contaminated and,
consequently, how it impacts benchmark scores. We propose that these questions
should be addressed together and that contamination metrics can be assessed
based on whether models benefit from the examples they mark contaminated. We
propose a novel analysis method called ConTAM, and show with a large scale
survey of existing and novel n-gram based contamination metrics across 13
benchmarks and 7 models from 2 different families that ConTAM can be used to
better understand evaluation data contamination and its effects. We find that
contamination may have a much larger effect than reported in recent LLM
releases and benefits models differently at different scales. We also find that
considering only the longest contaminated substring provides a better signal
than considering a union of all contaminated substrings, and that doing model
and benchmark specific threshold analysis greatly increases the specificity of
the results. Lastly, we investigate the impact of hyperparameter choices,
finding that, among other things, both using larger values of n and
disregarding matches that are infrequent in the pre-training data lead to many
false negatives. With ConTAM, we provide a method to empirically ground
evaluation data contamination metrics in downstream effects. With our
exploration, we shed light on how evaluation data contamination can impact LLMs
and provide insight into the considerations important when doing contamination
analysis. We end our paper by discussing these in more detail and providing
concrete suggestions for future work.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAGulator: Lightweight Out-of-Context Detectors for Grounded Text
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03920v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03920v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ian Poey, Jiajun Liu, Qishuai Zhong, Adrien Chenailler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-time detection of out-of-context LLM outputs is crucial for enterprises
looking to safely adopt RAG applications. In this work, we train lightweight
models to discriminate LLM-generated text that is semantically out-of-context
from retrieved text documents. We preprocess a combination of summarisation and
semantic textual similarity datasets to construct training data using minimal
resources. We find that DeBERTa is not only the best-performing model under
this pipeline, but it is also fast and does not require additional text
preprocessing or feature engineering. While emerging work demonstrates that
generative LLMs can also be fine-tuned and used in complex data pipelines to
achieve state-of-the-art performance, we note that speed and resource limits
are important considerations for on-premise deployment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lexicalization Is All You Need: Examining the Impact of Lexical
  Knowledge in a Compositional QALD System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03906v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03906v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Maria Schmidt, Mohammad Fazleh Elahi, Philipp Cimiano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we examine the impact of lexicalization on Question Answering
over Linked Data (QALD). It is well known that one of the key challenges in
interpreting natural language questions with respect to SPARQL lies in bridging
the lexical gap, that is mapping the words in the query to the correct
vocabulary elements. We argue in this paper that lexicalization, that is
explicit knowledge about the potential interpretations of a word with respect
to the given vocabulary, significantly eases the task and increases the
performance of QA systems. Towards this goal, we present a compositional QA
system that can leverage explicit lexical knowledge in a compositional manner
to infer the meaning of a question in terms of a SPARQL query. We show that
such a system, given lexical knowledge, has a performance well beyond current
QA systems, achieving up to a $35.8\%$ increase in the micro $F_1$ score
compared to the best QA system on QALD-9. This shows the importance and
potential of including explicit lexical knowledge. In contrast, we show that
LLMs have limited abilities to exploit lexical knowledge, with only marginal
improvements compared to a version without lexical knowledge. This shows that
LLMs have no ability to compositionally interpret a question on the basis of
the meaning of its parts, a key feature of compositional approaches. Taken
together, our work shows new avenues for QALD research, emphasizing the
importance of lexicalization and compositionality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24th International Conference on Knowledge Engineering and Knowledge
  Management (EKAW 2024), November 26-28, 2024, Amsterdam, The Netherlands</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Computational Analysis of Gender Depiction in the Comedias of Calderón
  de la Barca 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03895v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03895v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Allison Keith, Antonio Rojas Castro, Sebastian Padó
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In theatre, playwrights use the portrayal of characters to explore culturally
based gender norms. In this paper, we develop quantitative methods to study
gender depiction in the non-religious works (comedias) of Pedro Calder\'on de
la Barca, a prolific Spanish 17th century author. We gather insights from a
corpus of more than 100 plays by using a gender classifier and applying model
explainability (attribution) methods to determine which text features are most
influential in the model's decision to classify speech as 'male' or 'female',
indicating the most gendered elements of dialogue in Calder\'on's comedias in a
human accessible manner. We find that female and male characters are portrayed
differently and can be identified by the gender prediction model at practically
useful accuracies (up to f=0.83). Analysis reveals semantic aspects of gender
portrayal, and demonstrates that the model is even useful in providing a
relatively accurate scene-by-scene prediction of cross-dressing characters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi3Hate: <span class="highlight-title">Multimodal</span>, Multilingual, and Multicultural Hate Speech
  Detection with <span class="highlight-title">Vision-Language</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03888v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03888v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minh Duc Bui, Katharina von der Wense, Anne Lauscher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Warning: this paper contains content that may be offensive or upsetting
  Hate speech moderation on global platforms poses unique challenges due to the
multimodal and multilingual nature of content, along with the varying cultural
perceptions. How well do current vision-language models (VLMs) navigate these
nuances? To investigate this, we create the first multimodal and multilingual
parallel hate speech dataset, annotated by a multicultural set of annotators,
called Multi3Hate. It contains 300 parallel meme samples across 5 languages:
English, German, Spanish, Hindi, and Mandarin. We demonstrate that cultural
background significantly affects multimodal hate speech annotation in our
dataset. The average pairwise agreement among countries is just 74%,
significantly lower than that of randomly selected annotator groups. Our
qualitative analysis indicates that the lowest pairwise label agreement-only
67% between the USA and India-can be attributed to cultural factors. We then
conduct experiments with 5 large VLMs in a zero-shot setting, finding that
these models align more closely with annotations from the US than with those
from other cultures, even when the memes and prompts are presented in the
dominant language of the other culture. Code and dataset are available at
https://github.com/MinhDucBui/Multi3Hate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Polynomial Composition Activations: Unleashing the Dynamics of Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03884v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03884v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhijian Zhuo, Ya Wang, Yutao Zeng, Xiaoqing Li, Xun Zhou, Jinwen Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have found extensive applications across various domains due to
the powerful fitting capabilities. This success can be partially attributed to
their inherent nonlinearity. Thus, in addition to the ReLU function employed in
the original transformer architecture, researchers have explored alternative
modules such as GeLU and SwishGLU to enhance nonlinearity and thereby augment
representational capacity. In this paper, we propose a novel category of
polynomial composition activations (PolyCom), designed to optimize the dynamics
of transformers. Theoretically, we provide a comprehensive mathematical
analysis of PolyCom, highlighting its enhanced expressivity and efficacy
relative to other activation functions. Notably, we demonstrate that networks
incorporating PolyCom achieve the $\textbf{optimal approximation rate}$,
indicating that PolyCom networks require minimal parameters to approximate
general smooth functions in Sobolev spaces. We conduct empirical experiments on
the pre-training configurations of large language models (LLMs), including both
dense and sparse architectures. By substituting conventional activation
functions with PolyCom, we enable LLMs to capture higher-order interactions
within the data, thus improving performance metrics in terms of accuracy and
convergence rates. Extensive experimental results demonstrate the effectiveness
of our method, showing substantial improvements over other activation
functions. Code is available at https://github.com/BryceZhuo/PolyCom.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MEG: Medical Knowledge-Augmented Large Language Models for Question
  Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03883v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03883v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laura Cabello, Carmen Martin-Turrero, Uchenna Akujuobi, Anders Søgaard, Carlos Bobed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Question answering is a natural language understanding task that involves
reasoning over both explicit context and unstated, relevant domain knowledge.
Large language models (LLMs), which underpin most contemporary question
answering systems, struggle to induce how concepts relate in specialized
domains such as medicine. Existing medical LLMs are also costly to train. In
this work, we present MEG, a parameter-efficient approach for medical
knowledge-augmented LLMs. MEG uses a lightweight mapping network to integrate
graph embeddings into the LLM, enabling it to leverage external knowledge in a
cost-effective way. We evaluate our method on four popular medical
multiple-choice datasets and show that LLMs greatly benefit from the factual
grounding provided by knowledge graph embeddings. MEG attains an average of
+10.2% accuracy over the Mistral-Instruct baseline, and +6.7% over specialized
models like BioMistral. We also show results based on Llama-3. Finally, we show
that MEG's performance remains robust to the choice of graph encoder.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Performance evaluation of SLAM-ASR: The Good, the Bad, the Ugly, and the
  Way Forward <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03866v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03866v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shashi Kumar, Iuliia Thorbecke, Sergio Burdisso, Esaú Villatoro-Tello, Manjunath K E, Kadri Hacioğlu, Pradeep Rangappa, Petr Motlicek, Aravind Ganapathiraju, Andreas Stolcke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research has demonstrated that training a linear connector between
speech foundation encoders and large language models (LLMs) enables this
architecture to achieve strong ASR capabilities. Despite the impressive
results, it remains unclear whether these simple approaches are robust enough
across different scenarios and speech conditions, such as domain shifts and
different speech perturbations. In this paper, we address these questions by
conducting various ablation experiments using a recent and widely adopted
approach called SLAM-ASR. We present novel empirical findings that offer
insights on how to effectively utilize the SLAM-ASR architecture across a wide
range of settings. Our main findings indicate that the SLAM-ASR exhibits poor
performance in cross-domain evaluation settings. Additionally, speech
perturbations within in-domain data, such as changes in speed or the presence
of additive noise, can significantly impact performance. Our findings offer
critical insights for fine-tuning and configuring robust LLM-based ASR models,
tailored to different data characteristics and computational resources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICASSP 2025 SALMA Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MambaPEFT: Exploring Parameter-Efficient <span class="highlight-title">Fine-Tuning</span> for Mamba 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03855v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03855v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masakazu Yoshimura, Teruaki Hayashi, Yota Maeda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An ecosystem of Transformer-based models has been established by building
large models with extensive data. Parameter-efficient fine-tuning (PEFT) is a
crucial technology for deploying these models to downstream tasks with minimal
cost while achieving effective performance. Recently, Mamba, a State Space
Model (SSM)-based model, has attracted attention as a potential alternative to
Transformers. While many large-scale Mamba-based models have been proposed,
efficiently adapting pre-trained Mamba-based models to downstream tasks remains
unexplored. In this paper, we conduct an exploratory analysis of PEFT methods
for Mamba. We investigate the effectiveness of existing PEFT methods for
Transformers when applied to Mamba. We also modify these methods to better
align with the Mamba architecture. Additionally, we propose new Mamba-specific
PEFT methods that leverage the distinctive structure of Mamba. Our experiments
indicate that PEFT performs more effectively for Mamba than Transformers.
Lastly, we demonstrate how to effectively combine multiple PEFT methods and
provide a framework that outperforms previous works. To ensure reproducibility,
we will release the code after publication.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Novice to Expert: LLM Agent Policy Optimization via Step-wise
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03817v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03817v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhirui Deng, Zhicheng Dou, Yutao Zhu, Ji-Rong Wen, Ruibin Xiong, Mang Wang, Weipeng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The outstanding capabilities of large language models (LLMs) render them a
crucial component in various autonomous agent systems. While traditional
methods depend on the inherent knowledge of LLMs without fine-tuning, more
recent approaches have shifted toward the reinforcement learning strategy to
further enhance agents' ability to solve complex interactive tasks with
environments and tools. However, previous approaches are constrained by the
sparse reward issue, where existing datasets solely provide a final scalar
reward for each multi-step reasoning chain, potentially leading to
ineffectiveness and inefficiency in policy learning. In this paper, we
introduce StepAgent, which utilizes step-wise reward to optimize the agent's
reinforcement learning process. Inheriting the spirit of novice-to-expert
theory, we first compare the actions of the expert and the agent to
automatically generate intermediate rewards for fine-grained optimization.
Additionally, we propose implicit-reward and inverse reinforcement learning
techniques to facilitate agent reflection and policy adjustment. Further
theoretical analysis demonstrates that the action distribution of the agent can
converge toward the expert action distribution over multiple training cycles.
Experimental results across various datasets indicate that StepAgent
outperforms existing baseline methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MRJ-Agent: An Effective Jailbreak Agent for Multi-Round Dialogue 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03814v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03814v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fengxiang Wang, Ranjie Duan, Peng Xiao, Xiaojun Jia, YueFeng Chen, Chongwen Wang, Jialing Tao, Hang Su, Jun Zhu, Hui Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) demonstrate outstanding performance in their
reservoir of knowledge and understanding capabilities, but they have also been
shown to be prone to illegal or unethical reactions when subjected to jailbreak
attacks. To ensure their responsible deployment in critical applications, it is
crucial to understand the safety capabilities and vulnerabilities of LLMs.
Previous works mainly focus on jailbreak in single-round dialogue, overlooking
the potential jailbreak risks in multi-round dialogues, which are a vital way
humans interact with and extract information from LLMs. Some studies have
increasingly concentrated on the risks associated with jailbreak in multi-round
dialogues. These efforts typically involve the use of manually crafted
templates or prompt engineering techniques. However, due to the inherent
complexity of multi-round dialogues, their jailbreak performance is limited. To
solve this problem, we propose a novel multi-round dialogue jailbreaking agent,
emphasizing the importance of stealthiness in identifying and mitigating
potential threats to human values posed by LLMs. We propose a risk
decomposition strategy that distributes risks across multiple rounds of queries
and utilizes psychological strategies to enhance attack strength. Extensive
experiments show that our proposed method surpasses other attack methods and
achieves state-of-the-art attack success rate. We will make the corresponding
code and dataset available for future research. The code will be released soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The natural stability of autonomous morphology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03811v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03811v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erich Round, Louise Esher, Sacha Beniamine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous morphology, such as inflection class systems and paradigmatic
distribution patterns, is widespread and diachronically resilient in natural
language. Why this should be so has remained unclear given that autonomous
morphology imposes learning costs, offers no clear benefit relative to its
absence and could easily be removed by the analogical forces which are
constantly reshaping it. Here we propose an explanation for the resilience of
autonomous morphology, in terms of a diachronic dynamic of attraction and
repulsion between morphomic categories, which emerges spontaneously from a
simple paradigm cell filling process. Employing computational evolutionary
models, our key innovation is to bring to light the role of `dissociative
evidence', i.e., evidence for inflectional distinctiveness which a rational
reasoner will have access to during analogical inference. Dissociative evidence
creates a repulsion dynamic which prevents morphomic classes from collapsing
together entirely, i.e., undergoing complete levelling. As we probe alternative
models, we reveal the limits of conditional entropy as a measure for
predictability in systems that are undergoing change. Finally, we demonstrate
that autonomous morphology, far from being `unnatural' (e.g.
\citealt{Aronoff1994}), is rather the natural (emergent) consequence of a
natural (rational) process of inference applied to inflectional systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication by the journal Morphology</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding the Effects of Human-written Paraphrases in LLM-generated
  Text Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03806v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03806v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hiu Ting Lau, Arkaitz Zubiaga
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural Language Generation has been rapidly developing with the advent of
large language models (LLMs). While their usage has sparked significant
attention from the general public, it is important for readers to be aware when
a piece of text is LLM-generated. This has brought about the need for building
models that enable automated LLM-generated text detection, with the aim of
mitigating potential negative outcomes of such content. Existing LLM-generated
detectors show competitive performances in telling apart LLM-generated and
human-written text, but this performance is likely to deteriorate when
paraphrased texts are considered. In this study, we devise a new data
collection strategy to collect Human & LLM Paraphrase Collection (HLPC), a
first-of-its-kind dataset that incorporates human-written texts and
paraphrases, as well as LLM-generated texts and paraphrases. With the aim of
understanding the effects of human-written paraphrases on the performance of
state-of-the-art LLM-generated text detectors OpenAI RoBERTa and watermark
detectors, we perform classification experiments that incorporate human-written
paraphrases, watermarked and non-watermarked LLM-generated documents from GPT
and OPT, and LLM-generated paraphrases from DIPPER and BART. The results show
that the inclusion of human-written paraphrases has a significant impact of
LLM-generated detector performance, promoting TPR@1%FPR with a possible
trade-off of AUROC and accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comparative Study of Recent Large Language Models on Generating
  Hospital Discharge Summaries for Lung Cancer Patients 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03805v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03805v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Li, Fang Li, Kirk Roberts, Licong Cui, Cui Tao, Hua Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating discharge summaries is a crucial yet time-consuming task in
clinical practice, essential for conveying pertinent patient information and
facilitating continuity of care. Recent advancements in large language models
(LLMs) have significantly enhanced their capability in understanding and
summarizing complex medical texts. This research aims to explore how LLMs can
alleviate the burden of manual summarization, streamline workflow efficiencies,
and support informed decision-making in healthcare settings. Clinical notes
from a cohort of 1,099 lung cancer patients were utilized, with a subset of 50
patients for testing purposes, and 102 patients used for model fine-tuning.
This study evaluates the performance of multiple LLMs, including GPT-3.5,
GPT-4, GPT-4o, and LLaMA 3 8b, in generating discharge summaries. Evaluation
metrics included token-level analysis (BLEU, ROUGE-1, ROUGE-2, ROUGE-L) and
semantic similarity scores between model-generated summaries and
physician-written gold standards. LLaMA 3 8b was further tested on clinical
notes of varying lengths to examine the stability of its performance. The study
found notable variations in summarization capabilities among LLMs. GPT-4o and
fine-tuned LLaMA 3 demonstrated superior token-level evaluation metrics, while
LLaMA 3 consistently produced concise summaries across different input lengths.
Semantic similarity scores indicated GPT-4o and LLaMA 3 as leading models in
capturing clinical relevance. This study contributes insights into the efficacy
of LLMs for generating discharge summaries, highlighting LLaMA 3's robust
performance in maintaining clarity and relevance across varying clinical
contexts. These findings underscore the potential of automated summarization
tools to enhance documentation precision and efficiency, ultimately improving
patient care and operational capability in healthcare settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ No Culture Left Behind: ArtELingo-28, a Benchmark of WikiArt with
  Captions in 28 Languages <span class="chip">EMNLP 24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03769v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03769v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youssef Mohamed, Runjia Li, Ibrahim Said Ahmad, Kilichbek Haydarov, Philip Torr, Kenneth Ward Church, Mohamed Elhoseiny
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research in vision and language has made considerable progress thanks to
benchmarks such as COCO. COCO captions focused on unambiguous facts in English;
ArtEmis introduced subjective emotions and ArtELingo introduced some
multilinguality (Chinese and Arabic). However we believe there should be more
multilinguality. Hence, we present ArtELingo-28, a vision-language benchmark
that spans $\textbf{28}$ languages and encompasses approximately
$\textbf{200,000}$ annotations ($\textbf{140}$ annotations per image).
Traditionally, vision research focused on unambiguous class labels, whereas
ArtELingo-28 emphasizes diversity of opinions over languages and cultures. The
challenge is to build machine learning systems that assign emotional captions
to images. Baseline results will be presented for three novel conditions:
Zero-Shot, Few-Shot and One-vs-All Zero-Shot. We find that cross-lingual
transfer is more successful for culturally-related languages. Data and code are
provided at www.artelingo.org.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, Accepted at EMNLP 24, for more details see www.artelingo.org</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Number Cookbook: Number Understanding of Language Models and How to
  Improve It 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03766v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03766v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haotong Yang, Yi Hu, Shijia Kang, Zhouchen Lin, Muhan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) can solve an increasing number of complex
reasoning tasks while making surprising mistakes in basic numerical
understanding and processing (such as 9.11 > 9.9). The latter ability is
essential for tackling complex arithmetic and mathematical problems and serves
as a foundation for most reasoning tasks, but previous work paid little
attention to it or only discussed several restricted tasks (like integer
addition). In this paper, we comprehensively investigate the numerical
understanding and processing ability (NUPA) of LLMs. Firstly, we introduce a
benchmark covering four common numerical representations and 17 distinct
numerical tasks in four major categories, resulting in 41 meaningful
combinations in total. These tasks are derived from primary and secondary
education curricula, encompassing nearly all everyday numerical understanding
and processing scenarios, and the rules of these tasks are very simple and
clear. Through the benchmark, we find that current LLMs fail frequently in many
of the tasks. To study the problem, we train small models with existing and
potential techniques for enhancing NUPA (such as special tokenizers, PEs, and
number formats), comprehensively evaluating their effectiveness using our
testbed. We also finetune practical-scale LLMs on our proposed NUPA tasks and
find that 1) naive finetuning can improve NUPA a lot on many but not all tasks,
and 2) surprisingly, techniques designed to enhance NUPA prove ineffective for
finetuning pretrained models. We further explore the impact of chain-of-thought
techniques on NUPA. Our work takes a preliminary step towards understanding and
improving NUPA of LLMs. Our benchmark and code are released at
https://github.com/GraphPKU/number_cookbook.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Root Shapes the Fruit: On the Persistence of Gender-Exclusive Harms
  in Aligned Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03700v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03700v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anaelia Ovalle, Krunoslav Lehman Pavasovic, Louis Martin, Luke Zettlemoyer, Eric Michael Smith, Adina Williams, Levent Sagun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural-language assistants are designed to provide users with helpful
responses while avoiding harmful outputs, largely achieved through alignment to
human preferences. Yet there is limited understanding of whether alignment
techniques may inadvertently perpetuate or even amplify harmful biases
inherited from their pre-aligned base models. This issue is compounded by the
choice of bias evaluation benchmarks in popular preference-finetuned models,
which predominantly focus on dominant social categories, such as binary gender,
thereby limiting insights into biases affecting underrepresented groups.
Towards addressing this gap, we center transgender, nonbinary, and other
gender-diverse identities to investigate how alignment procedures interact with
pre-existing gender-diverse bias in LLMs. Our key contributions include: 1) a
comprehensive survey of bias evaluation modalities across leading
preference-finetuned LLMs, highlighting critical gaps in gender-diverse
representation, 2) systematic evaluation of gender-diverse biases across 12
models spanning Direct Preference Optimization (DPO) stages, uncovering harms
popular bias benchmarks fail to detect, and 3) a flexible framework for
measuring harmful biases in implicit reward signals applicable to other social
contexts. Our findings reveal that DPO-aligned models are particularly
sensitive to supervised finetuning (SFT), and can amplify two forms of
real-world gender-diverse harms from their base models: stigmatization and
gender non-affirmative language. We conclude with recommendations tailored to
DPO and broader alignment practices, advocating for the adoption of
community-informed bias evaluation frameworks to more effectively identify and
address underrepresented harms in LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to 2024 Neurips Queer in AI Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ QUILL: Quotation Generation Enhancement of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03675v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03675v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin Xiao, Bowei Zhang, Qianyu He, Jiaqing Liang, Feng Wei, Jinglei Chen, Zujie Liang, Deqing Yang, Yanghua Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Large language models (LLMs) have become excellent writing assistants,
they still struggle with quotation generation. This is because they either
hallucinate when providing factual quotations or fail to provide quotes that
exceed human expectations. To bridge the gap, we systematically study how to
evaluate and improve LLMs' performance in quotation generation tasks. We first
establish a holistic and automatic evaluation system for quotation generation
task, which consists of five criteria each with corresponding automatic metric.
To improve the LLMs' quotation generation abilities, we construct a bilingual
knowledge base that is broad in scope and rich in dimensions, containing up to
32,022 quotes. Moreover, guided by our critiria, we further design a
quotation-specific metric to rerank the retrieved quotations from the knowledge
base. Extensive experiments show that our metrics strongly correlate with human
preferences. Existing LLMs struggle to generate desired quotes, but our
quotation knowledge base and reranking metric help narrow this gap. Our dataset
and code are publicly available at https://github.com/GraceXiaoo/QUILL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Moral Beliefs across LLMs through a Pluralistic Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03665v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03665v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuelin Liu, Yanfei Zhu, Shucheng Zhu, Pengyuan Liu, Ying Liu, Dong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Proper moral beliefs are fundamental for language models, yet assessing these
beliefs poses a significant challenge. This study introduces a novel
three-module framework to evaluate the moral beliefs of four prominent large
language models. Initially, we constructed a dataset containing 472 moral
choice scenarios in Chinese, derived from moral words. The decision-making
process of the models in these scenarios reveals their moral principle
preferences. By ranking these moral choices, we discern the varying moral
beliefs held by different language models. Additionally, through moral debates,
we investigate the firmness of these models to their moral choices. Our
findings indicate that English language models, namely ChatGPT and Gemini,
closely mirror moral decisions of the sample of Chinese university students,
demonstrating strong adherence to their choices and a preference for
individualistic moral beliefs. In contrast, Chinese models such as Ernie and
ChatGLM lean towards collectivist moral beliefs, exhibiting ambiguity in their
moral choices and debates. This study also uncovers gender bias embedded within
the moral beliefs of all examined language models. Our methodology offers an
innovative means to assess moral beliefs in both artificial and human
intelligence, facilitating a comparison of moral values across different
cultures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deploying Multi-task Online Server with Large Language Model <span class="chip">COLING2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03644v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03644v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yincen Qu, Chao Ma, Yiting Wu, Xiangying Dai, Hui Zhou, Hengyue Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the industry, numerous tasks are deployed online. Traditional approaches
often tackle each task separately by its own network, which leads to excessive
costs for developing and scaling models, especially in the context of large
language models. Although multi-task methods can save costs through parameter
sharing, they often struggle to outperform single-task methods in real-world
applications. To tackle these challenges, we present a three-stage multi-task
learning framework for large language models. It involves task filtering,
followed by fine-tuning on high-resource tasks, and finally fine-tuning on all
tasks. We conducted comprehensive experiments in single-task and multi-task
settings. Our approach, exemplified on different benchmarks, demonstrates that
it is able to achieve performance comparable to the single-task method while
reducing up to 90.9\% of its overhead.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>COLING2025 under submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Med<span class="highlight-title">prompt</span> to o1: Exploration of Run-Time Strategies for Medical
  Challenge Problems and Beyond 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03590v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03590v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harsha Nori, Naoto Usuyama, Nicholas King, Scott Mayer McKinney, Xavier Fernandes, Sheng Zhang, Eric Horvitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Run-time steering strategies like Medprompt are valuable for guiding large
language models (LLMs) to top performance on challenging tasks. Medprompt
demonstrates that a general LLM can be focused to deliver state-of-the-art
performance on specialized domains like medicine by using a prompt to elicit a
run-time strategy involving chain of thought reasoning and ensembling. OpenAI's
o1-preview model represents a new paradigm, where a model is designed to do
run-time reasoning before generating final responses. We seek to understand the
behavior of o1-preview on a diverse set of medical challenge problem
benchmarks. Following on the Medprompt study with GPT-4, we systematically
evaluate the o1-preview model across various medical benchmarks. Notably, even
without prompting techniques, o1-preview largely outperforms the GPT-4 series
with Medprompt. We further systematically study the efficacy of classic prompt
engineering strategies, as represented by Medprompt, within the new paradigm of
reasoning models. We found that few-shot prompting hinders o1's performance,
suggesting that in-context learning may no longer be an effective steering
approach for reasoning-native models. While ensembling remains viable, it is
resource-intensive and requires careful cost-performance optimization. Our cost
and accuracy analysis across run-time strategies reveals a Pareto frontier,
with GPT-4o representing a more affordable option and o1-preview achieving
state-of-the-art performance at higher cost. Although o1-preview offers top
performance, GPT-4o with steering strategies like Medprompt retains value in
specific contexts. Moreover, we note that the o1-preview model has reached
near-saturation on many existing medical benchmarks, underscoring the need for
new, challenging benchmarks. We close with reflections on general directions
for inference-time computation with LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The American Sign Language Knowledge Graph: Infusing ASL Models with
  Linguistic Knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03568v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03568v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lee Kezar, Nidhi Munikote, Zian Zeng, Zed Sehyr, Naomi Caselli, Jesse Thomason
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models for American Sign Language (ASL) could make language
technologies substantially more accessible to those who sign. To train models
on tasks such as isolated sign recognition (ISR) and ASL-to-English
translation, datasets provide annotated video examples of ASL signs. To
facilitate the generalizability and explainability of these models, we
introduce the American Sign Language Knowledge Graph (ASLKG), compiled from
twelve sources of expert linguistic knowledge. We use the ASLKG to train
neuro-symbolic models for 3 ASL understanding tasks, achieving accuracies of
91% on ISR, 14% for predicting the semantic features of unseen signs, and 36%
for classifying the topic of Youtube-ASL videos.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Multilingual Sentiment Lexicon for Low-Resource Language Translation
  using Large Languages Models and Explainable AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04316v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04316v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Melusi Malinga, Isaac Lupanda, Mike Wa Nkongolo, Phil van Deventer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  South Africa and the Democratic Republic of Congo (DRC) present a complex
linguistic landscape with languages such as Zulu, Sepedi, Afrikaans, French,
English, and Tshiluba (Ciluba), which creates unique challenges for AI-driven
translation and sentiment analysis systems due to a lack of accurately labeled
data. This study seeks to address these challenges by developing a multilingual
lexicon designed for French and Tshiluba, now expanded to include translations
in English, Afrikaans, Sepedi, and Zulu. The lexicon enhances cultural
relevance in sentiment classification by integrating language-specific
sentiment scores. A comprehensive testing corpus is created to support
translation and sentiment analysis tasks, with machine learning models such as
Random Forest, Support Vector Machine (SVM), Decision Trees, and Gaussian Naive
Bayes (GNB) trained to predict sentiment across low resource languages (LRLs).
Among them, the Random Forest model performed particularly well, capturing
sentiment polarity and handling language-specific nuances effectively.
Furthermore, Bidirectional Encoder Representations from Transformers (BERT), a
Large Language Model (LLM), is applied to predict context-based sentiment with
high accuracy, achieving 99% accuracy and 98% precision, outperforming other
models. The BERT predictions were clarified using Explainable AI (XAI),
improving transparency and fostering confidence in sentiment classification.
Overall, findings demonstrate that the proposed lexicon and machine learning
models significantly enhance translation and sentiment analysis for LRLs in
South Africa and the DRC, laying a foundation for future AI models that support
underrepresented languages, with applications across education, governance, and
business in multilingual contexts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work is part of a PhD proposal in Information Technology at the
  University of Pretoria, supervised by Dr. Mike Wa Nkongolo and co-supervised
  by Dr. Phil van Deventer, under the Low-Resource Language Processing Lab in
  the Department of Informatics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Bilingual Capabilities of Language Models to Support Diverse
  Linguistic Practices in Education 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04308v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04308v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anand Syamkumar, Nora Tseng, Kaycie Barron, Shanglin Yang, Shamya Karumbaiah, Rheeya Uppal, Junjie Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) offer promise in generating educational content,
providing instructor feedback, and reducing teacher workload on assessments.
While prior studies have focused on studying LLM-powered learning analytics,
limited research has examined how effective LLMs are in a bilingual context. In
this paper, we study the effectiveness of multilingual large language models
(MLLMs) across monolingual (English-only, Spanish-only) and bilingual
(Spanglish) student writing. We present a learning analytics use case that
details LLM performance in assessing acceptable and unacceptable explanations
of Science and Social Science concepts. Our findings reveal a significant bias
in the grading performance of pre-trained models for bilingual writing compared
to English-only and Spanish-only writing. Following this, we fine-tune
open-source MLLMs including Llama 3.1 and Mistral NeMo using synthetic datasets
generated in English, Spanish, and Spanglish. Our experiments indicate that the
models perform significantly better for all three languages after fine-tuning
with bilingual data. This study highlights the potential of enhancing MLLM
effectiveness to support authentic language practices amongst bilingual
learners. It also aims to illustrate the value of incorporating non-English
languages into the design and implementation of language models in education.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Capabilities Approach to Studying Bias and Harm in Language
  Technologies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04298v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04298v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hellina Hailu Nigatu, Zeerak Talat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mainstream Natural Language Processing (NLP) research has ignored the
majority of the world's languages. In moving from excluding the majority of the
world's languages to blindly adopting what we make for English, we first risk
importing the same harms we have at best mitigated and at least measured for
English. However, in evaluating and mitigating harms arising from adopting new
technologies into such contexts, we often disregard (1) the actual community
needs of Language Technologies, and (2) biases and fairness issues within the
context of the communities. In this extended abstract, we consider fairness,
bias, and inclusion in Language Technologies through the lens of the
Capabilities Approach. The Capabilities Approach centers on what people are
capable of achieving, given their intersectional social, political, and
economic contexts instead of what resources are (theoretically) available to
them. We detail the Capabilities Approach, its relationship to multilingual and
multicultural evaluation, and how the framework affords meaningful
collaboration with community members in defining and measuring the harms of
Language Technologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the New Perspectives on Bias and Discrimination in
  Language Technology workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unfair Alignment: Examining Safety Alignment Across Vision Encoder
  Layers in <span class="highlight-title">Vision-Language</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04291v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04291v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saketh Bachu, Erfan Shayegani, Trishna Chakraborty, Rohit Lal, Arindam Dutta, Chengyu Song, Yue Dong, Nael Abu-Ghazaleh, Amit K. Roy-Chowdhury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models (VLMs) have improved significantly in multi-modal
tasks, but their more complex architecture makes their safety alignment more
challenging than the alignment of large language models (LLMs). In this paper,
we reveal an unfair distribution of safety across the layers of VLM's vision
encoder, with earlier and middle layers being disproportionately vulnerable to
malicious inputs compared to the more robust final layers. This 'cross-layer'
vulnerability stems from the model's inability to generalize its safety
training from the default architectural settings used during training to unseen
or out-of-distribution scenarios, leaving certain layers exposed. We conduct a
comprehensive analysis by projecting activations from various intermediate
layers and demonstrate that these layers are more likely to generate harmful
outputs when exposed to malicious inputs. Our experiments with LLaVA-1.5 and
Llama 3.2 show discrepancies in attack success rates and toxicity scores across
layers, indicating that current safety alignment strategies focused on a single
default layer are insufficient.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint, Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Models are Hidden Reasoners: Unlocking Latent Reasoning
  Capabilities via Self-Rewarding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04282v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04282v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haolin Chen, Yihao Feng, Zuxin Liu, Weiran Yao, Akshara Prabhakar, Shelby Heinecke, Ricky Ho, Phil Mui, Silvio Savarese, Caiming Xiong, Huan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown impressive capabilities, but still
struggle with complex reasoning tasks requiring multiple steps. While
prompt-based methods like Chain-of-Thought (CoT) can improve LLM reasoning at
inference time, optimizing reasoning capabilities during training remains
challenging. We introduce LaTent Reasoning Optimization (LaTRO), a principled
framework that formulates reasoning as sampling from a latent distribution and
optimizes it via variational approaches. LaTRO enables LLMs to concurrently
improve both their reasoning process and ability to evaluate reasoning quality,
without requiring external feedback or reward models. We validate LaTRO through
experiments on GSM8K and ARC-Challenge datasets using multiple model
architectures. On GSM8K, LaTRO improves zero-shot accuracy by an average of
12.5% over base models and 9.6% over supervised fine-tuning across
Phi-3.5-mini, Mistral-7B, and Llama-3.1-8B. Our findings suggest that
pre-trained LLMs possess latent reasoning capabilities that can be unlocked and
enhanced through our proposed optimization approach in a self-improvement
manner. The code of LaTRO is available at
\url{https://github.com/SalesforceAIResearch/LaTRO}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diversity Helps Jailbreak Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04223v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04223v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiliang Zhao, Daniel Ben-Levi, Junfeng Yang, Chengzhi Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We have uncovered a powerful jailbreak technique that leverages large
language models' ability to diverge from prior context, enabling them to bypass
safety constraints and generate harmful outputs. By simply instructing the LLM
to deviate and obfuscate previous attacks, our method dramatically outperforms
existing approaches, achieving up to a 62% higher success rate in compromising
nine leading chatbots, including GPT-4, Gemini, and Llama, while using only 13%
of the queries. This revelation exposes a critical flaw in current LLM safety
training, suggesting that existing methods may merely mask vulnerabilities
rather than eliminate them. Our findings sound an urgent alarm for the need to
revolutionize testing methodologies to ensure robust and reliable LLM security.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2312.02119</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analyzing <span class="highlight-title">Multimodal</span> Features of Spontaneous Voice Assistant Commands
  for Mild Cognitive Impairment Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04158v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04158v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nana Lin, Youxiang Zhu, Xiaohui Liang, John A. Batsis, Caroline Summerour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mild cognitive impairment (MCI) is a major public health concern due to its
high risk of progressing to dementia. This study investigates the potential of
detecting MCI with spontaneous voice assistant (VA) commands from 35 older
adults in a controlled setting. Specifically, a command-generation task is
designed with pre-defined intents for participants to freely generate commands
that are more associated with cognitive ability than read commands. We develop
MCI classification and regression models with audio, textual, intent, and
multimodal fusion features. We find the command-generation task outperforms the
command-reading task with an average classification accuracy of 82%, achieved
by leveraging multimodal fusion features. In addition, generated commands
correlate more strongly with memory and attention subdomains than read
commands. Our results confirm the effectiveness of the command-generation task
and imply the promise of using longitudinal in-home commands for MCI detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Both Text and Images Leaked! A Systematic Analysis of <span class="highlight-title">Multimodal</span> LLM
  Data Contamination 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03823v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03823v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dingjie Song, Sicheng Lai, Shunian Chen, Lichao Sun, Benyou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid progression of multimodal large language models (MLLMs) has
demonstrated superior performance on various multimodal benchmarks. However,
the issue of data contamination during training creates challenges in
performance evaluation and comparison. While numerous methods exist for
detecting dataset contamination in large language models (LLMs), they are less
effective for MLLMs due to their various modalities and multiple training
phases. In this study, we introduce a multimodal data contamination detection
framework, MM-Detect, designed for MLLMs. Our experimental results indicate
that MM-Detect is sensitive to varying degrees of contamination and can
highlight significant performance improvements due to leakage of the training
set of multimodal benchmarks. Furthermore, We also explore the possibility of
contamination originating from the pre-training phase of LLMs used by MLLMs and
the fine-tuning phase of MLLMs, offering new insights into the stages at which
contamination may be introduced.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Crystal: Illuminating LLM Abilities on Language and Code 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04156v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04156v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianhua Tao, Junbo Li, Bowen Tan, Hongyi Wang, William Marshall, Bhargav M Kanakiya, Joel Hestness, Natalia Vassilieva, Zhiqiang Shen, Eric P. Xing, Zhengzhong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) specializing in code generation (which are also
often referred to as code LLMs), e.g., StarCoder and Code Llama, play
increasingly critical roles in various software development scenarios. It is
also crucial for code LLMs to possess both code generation and natural language
abilities for many specific applications, such as code snippet retrieval using
natural language or code explanations. The intricate interaction between
acquiring language and coding skills complicates the development of strong code
LLMs. Furthermore, there is a lack of thorough prior studies on the LLM
pretraining strategy that mixes code and natural language. In this work, we
propose a pretraining strategy to enhance the integration of natural language
and coding capabilities within a single LLM. Specifically, it includes two
phases of training with appropriately adjusted code/language ratios. The
resulting model, Crystal, demonstrates remarkable capabilities in both domains.
Specifically, it has natural language and coding performance comparable to that
of Llama 2 and Code Llama, respectively. Crystal exhibits better data
efficiency, using 1.4 trillion tokens compared to the more than 2 trillion
tokens used by Llama 2 and Code Llama. We verify our pretraining strategy by
analyzing the training process and observe consistent improvements in most
benchmarks. We also adopted a typical application adaptation phase with a
code-centric data mixture, only to find that it did not lead to enhanced
performance or training efficiency, underlining the importance of a carefully
designed data recipe. To foster research within the community, we commit to
open-sourcing every detail of the pretraining, including our training datasets,
code, loggings and 136 checkpoints throughout the training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at COLM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ No Train, all Gain: Self-Supervised Gradients Improve Deep Frozen
  Representations <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10964v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10964v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Walter Simoncini, Spyros Gidaris, Andrei Bursuc, Yuki M. Asano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces FUNGI, Features from UNsupervised GradIents, a method
to enhance the features of transformer encoders by leveraging self-supervised
gradients. Our method is simple: given any pretrained model, we first compute
gradients from various self-supervised objectives for each input. These
gradients are projected to a lower dimension and then concatenated with the
model's output embedding. The resulting features are evaluated on k-nearest
neighbor classification over 11 datasets from vision, 5 from natural language
processing, and 2 from audio. Across backbones spanning various sizes and
pretraining strategies, FUNGI features provide consistent performance
improvements over the embeddings. We also show that using FUNGI features can
benefit linear classification, clustering and image retrieval, and that they
significantly improve the retrieval-based in-context scene understanding
abilities of pretrained models, for example improving upon DINO by +17% for
semantic segmentation - without any training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024. Code available at
  https://github.com/WalterSimoncini/fungivision</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OpenFactCheck: A Unified Framework for Factuality Evaluation of LLMs <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.11832v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.11832v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hasan Iqbal, Yuxia Wang, Minghan Wang, Georgi Georgiev, Jiahui Geng, Iryna Gurevych, Preslav Nakov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increased use of large language models (LLMs) across a variety of
real-world applications calls for automatic tools to check the factual accuracy
of their outputs, as LLMs often hallucinate. This is difficult as it requires
assessing the factuality of free-form open-domain responses. While there has
been a lot of research on this topic, different papers use different evaluation
benchmarks and measures, which makes them hard to compare and hampers future
progress. To mitigate these issues, we developed OpenFactCheck, a unified
framework, with three modules: (i) RESPONSEEVAL, which allows users to easily
customize an automatic fact-checking system and to assess the factuality of all
claims in an input document using that system, (ii) LLMEVAL, which assesses the
overall factuality of an LLM, and (iii) CHECKEREVAL, a module to evaluate
automatic fact-checking systems. OpenFactCheck is open-sourced
(https://github.com/mbzuai-nlp/openfactcheck) and publicly released as a Python
library (https://pypi.org/project/openfactcheck/) and also as a web service
(http://app.openfactcheck.com). A video describing the system is available at
https://youtu.be/-i9VKL0HleI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 4 Figures, 3 Tables, Accepted at EMNLP 2024 System
  Demonstration. arXiv admin note: substantial text overlap with
  arXiv:2405.05583</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Counterfactual Token Generation in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17027v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17027v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ivi Chatzi, Nina Corvelo Benz, Eleni Straitouri, Stratis Tsirtsis, Manuel Gomez-Rodriguez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  "Sure, I am happy to generate a story for you: Captain Lyra stood at the helm
of her trusty ship, the Maelstrom's Fury, gazing out at the endless sea. [...]
Lyra's eyes welled up with tears as she realized the bitter truth - she had
sacrificed everything for fleeting riches, and lost the love of her crew, her
family, and herself." Although this story, generated by a large language model,
is captivating, one may wonder -- how would the story have unfolded if the
model had chosen "Captain Maeve" as the protagonist instead? We cannot know.
State-of-the-art large language models are stateless -- they maintain no
internal memory or state. Given a prompt, they generate a sequence of tokens as
an output using an autoregressive process. As a consequence, they cannot reason
about counterfactual alternatives to tokens they have generated in the past. In
this work, our goal is to enhance them with this functionality. To this end, we
develop a causal model of token generation that builds upon the Gumbel-Max
structural causal model. Our model allows any large language model to perform
counterfactual token generation at almost no cost in comparison with vanilla
token generation, it is embarrassingly simple to implement, and it does not
require any fine-tuning nor prompt engineering. We implement our model on Llama
3 8B-Instruct and Ministral-8B-Instruct and conduct a qualitative and a
quantitative analysis of counterfactually generated text. We conclude with a
demonstrative application of counterfactual token generation for bias
detection, unveiling interesting insights about the model of the world
constructed by large language models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Teaching Models to Improve on Tape 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01483v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01483v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liat Bezalel, Eyal Orgad, Amir Globerson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) often struggle when prompted to generate content
under specific constraints. However, in such cases it is often easy to check
whether these constraints are satisfied or violated. Recent works have shown
that LLMs can benefit from such "corrective feedback". Here we claim that this
skill of LLMs can be significantly enhanced via training. We introduce an RL
framework for teaching models to use such rewards, by simulating interaction
sessions, and rewarding the model according to its ability to satisfy the
constraints. We refer to our method as CORGI (Controlled Generation with RL for
Guided Interaction), and evaluate it on a variety of controlled generation
tasks using unlabeled training data. We find that CORGI consistently
outperforms the baseline reinforcement learning method that does not
incorporate conversational feedback. Furthermore, CORGI's interactive framework
enables meta-learning, allowing the LLM to generalize better to guided
interaction in new tasks. Our results clearly show that conversational
optimization, when combined with reinforcement learning, significantly improves
the effectiveness of LLMs in controlled generation contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diverging Preferences: When do Annotators Disagree and do Models Know? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14632v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14632v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael JQ Zhang, Zhilin Wang, Jena D. Hwang, Yi Dong, Olivier Delalleau, Yejin Choi, Eunsol Choi, Xiang Ren, Valentina Pyatkin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We examine diverging preferences in human-labeled preference datasets. We
develop a taxonomy of disagreement sources spanning 10 categories across four
high-level classes -- task underspecification, response style, refusals, and
annotation errors. We find that the majority of disagreements are in opposition
with standard reward modeling approaches, which are designed with the
assumption that annotator disagreement is noise. We then explore how these
findings impact two areas of LLM development: reward modeling and evaluation.
In our experiments, we demonstrate how standard reward modeling methods, like
the Bradley-Terry model, fail to differentiate whether a given preference
judgment is the result of unanimous agreement among annotators or the majority
opinion among diverging user preferences. We also find that these tendencies
are also echoed by popular LLM-as-Judge evaluation methods, which consistently
identify a winning response in cases of diverging preferences. These findings
highlight remaining challenges in LLM evaluations, which are greatly influenced
by divisive features like response style, and in developing pluralistically
aligned LLMs. To address these issues, we develop methods for identifying
diverging preferences to mitigate their influence on evaluation and training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pretraining and Updates of Domain-Specific LLM: A Case Study in the
  Japanese Business Domain <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.08262v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.08262v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kosuke Takahashi, Takahiro Omi, Kosuke Arima, Tatsuya Ishigaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of Large Language Models (LLMs) in various languages has been
advancing, but the combination of non-English languages with domain-specific
contexts remains underexplored. This paper presents our findings from training
and evaluating a Japanese business domain-specific LLM designed to better
understand business-related documents, such as the news on current affairs,
technical reports, and patents. Additionally, LLMs in this domain require
regular updates to incorporate the most recent knowledge. Therefore, we also
report our findings from the first experiments and evaluations involving
updates to this LLM using the latest article data, which is an important
problem setting that has not been addressed in previous research. From our
experiments on a newly created benchmark dataset for question answering in the
target domain, we found that (1) our pretrained model improves QA accuracy
without losing general knowledge, and (2) a proper mixture of the latest and
older texts in the training data for the update is necessary. Our pretrained
model and business domain benchmark are publicly available to support further
studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at PACLIC 38</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ News Reporter: A Multi-lingual LLM Framework for Broadcast T.V News <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07520v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07520v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tarun Jain, Yufei Gao, Sridhar Vanga, Karan Singla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have fast become an essential tools to many
conversational chatbots due to their ability to provide coherent answers for
varied queries. Datasets used to train these LLMs are often a mix of generic
and synthetic samples, thus lacking the verification needed to provide correct
and verifiable answers for T.V. News.
  We collect and share a large collection of QA pairs extracted from
transcripts of news recordings from various news-channels across the United
States. Resultant QA pairs are then used to fine-tune an off-the-shelf LLM
model. Our model surpasses base models of similar size on several open LLM
benchmarks. We further integrate and propose a RAG method to improve
contextualization of our answers and also point it to a verifiable news
recording.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, under review at ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Context-Aware Preference Modeling for Language Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.14916v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.14916v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Silviu Pitis, Ziang Xiao, Nicolas Le Roux, Alessandro Sordoni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While finetuning language models from pairwise preferences has proven
remarkably effective, the underspecified nature of natural language presents
critical challenges. Direct preference feedback is uninterpretable, difficult
to provide where multidimensional criteria may apply, and often inconsistent,
either because it is based on incomplete instructions or provided by diverse
principals. To address these challenges, we consider the two-step preference
modeling procedure that first resolves the under-specification by selecting a
context, and then evaluates preference with respect to the chosen context. We
decompose reward modeling error according to these two steps, which suggests
that supervising context in addition to context-specific preference may be a
viable approach to aligning models with diverse human preferences. For this to
work, the ability of models to evaluate context-specific preference is
critical. To this end, we contribute context-conditioned preference datasets
and accompanying experiments that investigate the ability of language models to
evaluate context-specific preference. We use our datasets to (1) show that
existing preference models benefit from, but fail to fully consider, added
context, (2) finetune a context-aware reward model with context-specific
performance exceeding that of GPT-4 and Llama 3 70B on tested datasets, and (3)
investigate the value of context-aware preference modeling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024. 10 pages (29 with references and appendix)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CLIBD: Bridging Vision and Genomics for Biodiversity Monitoring at Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17537v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17537v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        ZeMing Gong, Austin T. Wang, Xiaoliang Huo, Joakim Bruslund Haurum, Scott C. Lowe, Graham W. Taylor, Angel X. Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Measuring biodiversity is crucial for understanding ecosystem health. While
prior works have developed machine learning models for taxonomic classification
of photographic images and DNA separately, in this work, we introduce a
multimodal approach combining both, using CLIP-style contrastive learning to
align images, barcode DNA, and text-based representations of taxonomic labels
in a unified embedding space. This allows for accurate classification of both
known and unknown insect species without task-specific fine-tuning, leveraging
contrastive learning for the first time to fuse DNA and image data. Our method
surpasses previous single-modality approaches in accuracy by over 8% on
zero-shot learning tasks, showcasing its effectiveness in biodiversity studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages with 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Causal Reasoning in Large Language Models: A Survey 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16676v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16676v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Longxuan Yu, Delin Chen, Siheng Xiong, Qingyang Wu, Qingzhen Liu, Dawei Li, Zhikai Chen, Xiaoze Liu, Liangming Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal reasoning (CR) is a crucial aspect of intelligence, essential for
problem-solving, decision-making, and understanding the world. While large
language models (LLMs) can generate rationales for their outputs, their ability
to reliably perform causal reasoning remains uncertain, often falling short in
tasks requiring a deep understanding of causality. In this survey, we provide a
comprehensive review of research aimed at enhancing LLMs for causal reasoning.
We categorize existing methods based on the role of LLMs: either as reasoning
engines or as helpers providing knowledge or data to traditional CR methods,
followed by a detailed discussion of the methodologies in each category. We
then evaluate the performance of LLMs on various causal reasoning tasks,
providing key findings and in-depth analysis. Finally, we provide insights from
current studies and highlight promising directions for future research. We aim
for this work to serve as a comprehensive resource, fostering further
advancements in causal reasoning with LLMs. Resources are available at
https://github.com/chendl02/Awesome-LLM-causal-reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BABILong: Testing the Limits of LLMs with Long Context
  Reasoning-in-a-Haystack <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10149v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10149v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuri Kuratov, Aydar Bulatov, Petr Anokhin, Ivan Rodkin, Dmitry Sorokin, Artyom Sorokin, Mikhail Burtsev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, the input context sizes of large language models (LLMs) have
increased dramatically. However, existing evaluation methods have not kept
pace, failing to comprehensively assess the efficiency of models in handling
long contexts. To bridge this gap, we introduce the BABILong benchmark,
designed to test language models' ability to reason across facts distributed in
extremely long documents. BABILong includes a diverse set of 20 reasoning
tasks, including fact chaining, simple induction, deduction, counting, and
handling lists/sets. These tasks are challenging on their own, and even more
demanding when the required facts are scattered across long natural text. Our
evaluations show that popular LLMs effectively utilize only 10-20\% of the
context and their performance declines sharply with increased reasoning
complexity. Among alternatives to in-context reasoning, Retrieval-Augmented
Generation methods achieve a modest 60\% accuracy on single-fact question
answering, independent of context length. Among context extension methods, the
highest performance is demonstrated by recurrent memory transformers after
fine-tuning, enabling the processing of lengths up to 50 million tokens. The
BABILong benchmark is extendable to any length to support the evaluation of new
upcoming models with increased capabilities, and we provide splits up to 10
million token lengths.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 Datasets and Benchmarks Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Morphological Compositional Generalization in Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12656v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12656v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mete Ismayilzada, Defne Circi, Jonne Sälevä, Hale Sirin, Abdullatif Köksal, Bhuwan Dhingra, Antoine Bosselut, Lonneke van der Plas, Duygu Ataman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated significant progress in
various natural language generation and understanding tasks. However, their
linguistic generalization capabilities remain questionable, raising doubts
about whether these models learn language similarly to humans. While humans
exhibit compositional generalization and linguistic creativity in language use,
the extent to which LLMs replicate these abilities, particularly in morphology,
is under-explored. In this work, we systematically investigate the
morphological generalization abilities of LLMs through the lens of
compositionality. We define morphemes as compositional primitives and design a
novel suite of generative and discriminative tasks to assess morphological
productivity and systematicity. Focusing on agglutinative languages such as
Turkish and Finnish, we evaluate several state-of-the-art instruction-finetuned
multilingual models, including GPT-4 and Gemini. Our analysis shows that LLMs
struggle with morphological compositional generalization particularly when
applied to novel word roots, with performance declining sharply as
morphological complexity increases. While models can identify individual
morphological combinations better than chance, their performance lacks
systematicity, leading to significant accuracy gaps compared to humans.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ChartInsights: Evaluating <span class="highlight-title">Multimodal</span> Large Language Models for Low-Level
  Chart Question Answering <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.07001v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.07001v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Wu, Lutao Yan, Leixian Shen, Yunhai Wang, Nan Tang, Yuyu Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chart question answering (ChartQA) tasks play a critical role in interpreting
and extracting insights from visualization charts. While recent advancements in
multimodal large language models (MLLMs) like GPT-4o have shown promise in
high-level ChartQA tasks, such as chart captioning, their effectiveness in
low-level ChartQA tasks (e.g., identifying correlations) remains underexplored.
In this paper, we address this gap by evaluating MLLMs on low-level ChartQA
using a newly curated dataset, ChartInsights, which consists of 22,347 (chart,
task, query, answer) covering 10 data analysis tasks across 7 chart types. We
systematically evaluate 19 advanced MLLMs, including 12 open-source and 7
closed-source models. The average accuracy rate across these models is 39.8%,
with GPT-4o achieving the highest accuracy at 69.17%. To further explore the
limitations of MLLMs in low-level ChartQA, we conduct experiments that alter
visual elements of charts (e.g., changing color schemes, adding image noise) to
assess their impact on the task effectiveness. Furthermore, we propose a new
textual prompt strategy, Chain-of-Charts, tailored for low-level ChartQA tasks,
which boosts performance by 14.41%, achieving an accuracy of 83.58%. Finally,
incorporating a visual prompt strategy that directs attention to relevant
visual elements further improves accuracy to 84.32%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Conference Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Benchmarking <span class="highlight-title">Multimodal</span> Retrieval Augmented Generation with Dynamic VQA
  Dataset and Self-adaptive Planning Agent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02937v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02937v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangning Li, Yinghui Li, Xinyu Wang, Yong Jiang, Zhen Zhang, Xinran Zheng, Hui Wang, Hai-Tao Zheng, Pengjun Xie, Philip S. Yu, Fei Huang, Jingren Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Retrieval Augmented Generation (mRAG) plays an important role in
mitigating the "hallucination" issue inherent in multimodal large language
models (MLLMs). Although promising, existing heuristic mRAGs typically
predefined fixed retrieval processes, which causes two issues: (1) Non-adaptive
Retrieval Queries. (2) Overloaded Retrieval Queries. However, these flaws
cannot be adequately reflected by current knowledge-seeking visual question
answering (VQA) datasets, since the most required knowledge can be readily
obtained with a standard two-step retrieval. To bridge the dataset gap, we
first construct Dyn-VQA dataset, consisting of three types of "dynamic"
questions, which require complex knowledge retrieval strategies variable in
query, tool, and time: (1) Questions with rapidly changing answers. (2)
Questions requiring multi-modal knowledge. (3) Multi-hop questions. Experiments
on Dyn-VQA reveal that existing heuristic mRAGs struggle to provide sufficient
and precisely relevant knowledge for dynamic questions due to their rigid
retrieval processes. Hence, we further propose the first self-adaptive planning
agent for multimodal retrieval, OmniSearch. The underlying idea is to emulate
the human behavior in question solution which dynamically decomposes complex
multimodal questions into sub-question chains with retrieval action. Extensive
experiments prove the effectiveness of our OmniSearch, also provide direction
for advancing mRAG. The code and dataset will be open-sourced at
https://github.com/Alibaba-NLP/OmniSearch.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ElectionSim: Massive Population Election Simulation Powered by Large
  Language Model Driven Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20746v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20746v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinnong Zhang, Jiayu Lin, Libo Sun, Weihong Qi, Yihang Yang, Yue Chen, Hanjia Lyu, Xinyi Mou, Siming Chen, Jiebo Luo, Xuanjing Huang, Shiping Tang, Zhongyu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The massive population election simulation aims to model the preferences of
specific groups in particular election scenarios. It has garnered significant
attention for its potential to forecast real-world social trends. Traditional
agent-based modeling (ABM) methods are constrained by their ability to
incorporate complex individual background information and provide interactive
prediction results. In this paper, we introduce ElectionSim, an innovative
election simulation framework based on large language models, designed to
support accurate voter simulations and customized distributions, together with
an interactive platform to dialogue with simulated voters. We present a
million-level voter pool sampled from social media platforms to support
accurate individual simulation. We also introduce PPE, a poll-based
presidential election benchmark to assess the performance of our framework
under the U.S. presidential election scenario. Through extensive experiments
and analyses, we demonstrate the effectiveness and robustness of our framework
in U.S. presidential election simulations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>42 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CIBench: Evaluating Your LLMs with a Code Interpreter Plugin 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10499v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10499v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuyu Zhang, Songyang Zhang, Yingfan Hu, Haowen Shen, Kuikun Liu, Zerun Ma, Fengzhe Zhou, Wenwei Zhang, Xuming He, Dahua Lin, Kai Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While LLM-Based agents, which use external tools to solve complex problems,
have made significant progress, benchmarking their ability is challenging,
thereby hindering a clear understanding of their limitations. In this paper, we
propose an interactive evaluation framework, named CIBench, to comprehensively
assess LLMs' ability to utilize code interpreters for data science tasks. Our
evaluation framework includes an evaluation dataset and two evaluation modes.
The evaluation dataset is constructed using an LLM-human cooperative approach
and simulates an authentic workflow by leveraging consecutive and interactive
IPython sessions. The two evaluation modes assess LLMs' ability with and
without human assistance. We conduct extensive experiments to analyze the
ability of 24 LLMs on CIBench and provide valuable insights for future LLMs in
code interpreter utilization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review. The first three authors contribute equally, and
  Songyang Zhang is the project leader</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Fine Line: Navigating Large Language Model Pretraining with
  Down-streaming Capability Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.01204v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.01204v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Yang, Junzhuo Li, Xinyao Niu, Xinrun Du, Songyang Gao, Haoran Zhang, Zhaoliang Chen, Xingwei Qu, Ruibin Yuan, Yizhi Li, Jiaheng Liu, Stephen W. Huang, Shawn Yue, Ge Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Uncovering early-stage metrics that reflect final model performance is one
core principle for large-scale pretraining. The existing scaling law
demonstrates the power-law correlation between pretraining loss and training
flops, which serves as an important indicator of the current training state for
large language models. However, this principle only focuses on the model's
compression properties on the training data, resulting in an inconsistency with
the ability improvements on the downstream tasks. Some follow-up works
attempted to extend the scaling-law to more complex metrics (such as
hyperparameters), but still lacked a comprehensive analysis of the dynamic
differences among various capabilities during pretraining. To address the
aforementioned limitations, this paper undertakes a comprehensive comparison of
model capabilities at various pretraining intermediate checkpoints. Through
this analysis, we confirm that specific downstream metrics exhibit similar
training dynamics across models of different sizes, up to 67 billion
parameters. In addition to our core findings, we've reproduced Amber and
OpenLLaMA, releasing their intermediate checkpoints. This initiative offers
valuable resources to the research community and facilitates the verification
and exploration of LLM pretraining by open-source researchers. Besides, we
provide empirical summaries, including performance comparisons of different
models and capabilities, and tuition of key metrics for different training
phases. Based on these findings, we provide a more user-friendly strategy for
evaluating the optimization state, offering guidance for establishing a stable
pretraining process.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ShifCon: Enhancing Non-Dominant Language Capabilities with a Shift-based
  Contrastive Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.19453v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.19453v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hengyuan Zhang, Chenming Shang, Sizhe Wang, Dongdong Zhang, Renliang Sun, Yiyao Yu, Yujiu Yang, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although fine-tuning Large Language Models (LLMs) with multilingual data can
rapidly enhance the multilingual capabilities of LLMs, they still exhibit a
performance gap between the dominant language (e.g., English) and non-dominant
ones due to the imbalance of training data across languages. To further enhance
the performance of non-dominant languages, we propose ShifCon, a Shift-based
Contrastive framework that aligns the internal forward process of other
languages toward that of the dominant one. Specifically, it shifts the
representations of non-dominant languages into the dominant language subspace,
allowing them to access relatively rich information encoded in the model
parameters. The enriched representations are then shifted back into their
original language subspace before generation. Moreover, we introduce a subspace
distance metric to pinpoint the optimal layer area for shifting representations
and employ multilingual contrastive learning to further enhance the alignment
of representations within this area. Experiments demonstrate that our ShifCon
framework significantly enhances the performance of non-dominant languages,
particularly for low-resource ones. Further analysis offers extra insights to
verify the effectiveness of ShifCon and propel future research
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PersianRAG: A Retrieval-Augmented Generation System for Persian Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02832v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02832v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hossein Hosseini, Mohammad Sobhan Zare, Amir Hossein Mohammadi, Arefeh Kazemi, Zahra Zojaji, Mohammad Ali Nematbakhsh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval augmented generation (RAG) models, which integrate large-scale
pre-trained generative models with external retrieval mechanisms, have shown
significant success in various natural language processing (NLP) tasks.
However, applying RAG models in Persian language as a low-resource language,
poses distinct challenges. These challenges primarily involve the
preprocessing, embedding, retrieval, prompt construction, language modeling,
and response evaluation of the system. In this paper, we address the challenges
towards implementing a real-world RAG system for Persian language called
PersianRAG. We propose novel solutions to overcome these obstacles and evaluate
our approach using several Persian benchmark datasets. Our experimental results
demonstrate the capability of the PersianRAG framework to enhance question
answering task in Persian.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Swan and ArabicMTEB: Dialect-Aware, Arabic-Centric, Cross-Lingual, and
  Cross-Cultural Embedding Models and Benchmarks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01192v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01192v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gagan Bhatia, El Moatez Billah Nagoudi, Abdellah El Mekki, Fakhraddin Alwajih, Muhammad Abdul-Mageed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce {\bf Swan}, a family of embedding models centred around the
Arabic language, addressing both small-scale and large-scale use cases. Swan
includes two variants: Swan-Small, based on ARBERTv2, and Swan-Large, built on
ArMistral, a pretrained Arabic large language model. To evaluate these models,
we propose ArabicMTEB, a comprehensive benchmark suite that assesses
cross-lingual, multi-dialectal, multi-domain, and multi-cultural Arabic text
embedding performance, covering eight diverse tasks and spanning 94 datasets.
Swan-Large achieves state-of-the-art results, outperforming
Multilingual-E5-large in most Arabic tasks, while the Swan-Small consistently
surpasses Multilingual-E5-base. Our extensive evaluations demonstrate that Swan
models are both dialectally and culturally aware, excelling across various
Arabic domains while offering significant monetary efficiency. This work
significantly advances the field of Arabic language modelling and provides
valuable resources for future research and applications in Arabic natural
language processing. Our models and benchmark will be made publicly accessible
for research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Single-Audio: Advancing Multi-Audio Processing in Audio Large
  Language Models <span class="chip">EMNLP24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18680v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18680v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Chen, Xianghu Yue, Xiaoxue Gao, Chen Zhang, Luis Fernando D'Haro, Robby T. Tan, Haizhou Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Various audio-LLMs (ALLMs) have been explored recently for tackling different
audio tasks simultaneously using a single, unified model. While existing
evaluations of ALLMs primarily focus on single-audio tasks, real-world
applications often involve processing multiple audio streams simultaneously. To
bridge this gap, we propose the first multi-audio evaluation (MAE) benchmark
that consists of 20 datasets from 11 multi-audio tasks encompassing both speech
and sound scenarios. Comprehensive experiments on MAE demonstrate that the
existing ALLMs, while being powerful in comprehending primary audio elements in
individual audio inputs, struggling to handle multi-audio scenarios. To this
end, we propose a novel multi-audio-LLM (MALLM) to capture audio context among
multiple similar audios using discriminative learning on our proposed synthetic
data. The results demonstrate that the proposed MALLM outperforms all baselines
and achieves high data efficiency using synthetic data without requiring human
annotations. The proposed MALLM opens the door for ALLMs towards multi-audio
processing era and brings us closer to replicating human auditory capabilities
in machines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP24 Findings. Data available at
  https://github.com/MatthewCYM/MALLM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeTikZify: Synthesizing Graphics Programs for Scientific Figures and
  Sketches with TikZ <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15306v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15306v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonas Belouadi, Simone Paolo Ponzetto, Steffen Eger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creating high-quality scientific figures can be time-consuming and
challenging, even though sketching ideas on paper is relatively easy.
Furthermore, recreating existing figures that are not stored in formats
preserving semantic information is equally complex. To tackle this problem, we
introduce DeTikZify, a novel multimodal language model that automatically
synthesizes scientific figures as semantics-preserving TikZ graphics programs
based on sketches and existing figures. To achieve this, we create three new
datasets: DaTikZv2, the largest TikZ dataset to date, containing over 360k
human-created TikZ graphics; SketchFig, a dataset that pairs hand-drawn
sketches with their corresponding scientific figures; and MetaFig, a collection
of diverse scientific figures and associated metadata. We train DeTikZify on
MetaFig and DaTikZv2, along with synthetically generated sketches learned from
SketchFig. We also introduce an MCTS-based inference algorithm that enables
DeTikZify to iteratively refine its outputs without the need for additional
training. Through both automatic and human evaluation, we demonstrate that
DeTikZify outperforms commercial Claude 3 and GPT-4V in synthesizing TikZ
programs, with the MCTS algorithm effectively boosting its performance. We make
our code, models, and datasets publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024 (spotlight); Project page:
  https://github.com/potamides/DeTikZify</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Compositional Data Augmentation for Scientific Keyphrase Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03039v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03039v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mael Houbre, Florian Boudin, Beatrice Daille, Akiko Aizawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art models for keyphrase generation require large amounts of
training data to achieve good performance. However, obtaining keyphrase-labeled
documents can be challenging and costly. To address this issue, we present a
self-compositional data augmentation method. More specifically, we measure the
relatedness of training documents based on their shared keyphrases, and combine
similar documents to generate synthetic samples. The advantage of our method
lies in its ability to create additional training samples that keep domain
coherence, without relying on external data or resources. Our results on
multiple datasets spanning three different domains, demonstrate that our method
consistently improves keyphrase generation. A qualitative analysis of the
generated keyphrases for the Computer Science domain confirms this improvement
towards their representativity property.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to JCDL 2024. This is the author's version of the work. It
  is posted here for your personal use. Not for redistribution. The definitive
  version was published in the proceedings of the 2024 ACM/IEEE Joint
  Conference on Digital Libraries (JCDL 24)
  https://doi.org/10.1145/3677389.3702504</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated
  Parameters by Tencent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02265v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02265v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingwu Sun, Yanfeng Chen, Yiqing Huang, Ruobing Xie, Jiaqi Zhu, Kai Zhang, Shuaipeng Li, Zhen Yang, Jonny Han, Xiaobo Shu, Jiahao Bu, Zhongzhi Chen, Xuemeng Huang, Fengzong Lian, Saiyong Yang, Jianfeng Yan, Yuyuan Zeng, Xiaoqin Ren, Chao Yu, Lulu Wu, Yue Mao, Jun Xia, Tao Yang, Suncong Zheng, Kan Wu, Dian Jiao, Jinbao Xue, Xipeng Zhang, Decheng Wu, Kai Liu, Dengpeng Wu, Guanghui Xu, Shaohua Chen, Shuang Chen, Xiao Feng, Yigeng Hong, Junqiang Zheng, Chengcheng Xu, Zongwei Li, Xiong Kuang, Jianglu Hu, Yiqi Chen, Yuchi Deng, Guiyang Li, Ao Liu, Chenchen Zhang, Shihui Hu, Zilong Zhao, Zifan Wu, Yao Ding, Weichao Wang, Han Liu, Roberts Wang, Hao Fei, Peijie Yu, Ze Zhao, Xun Cao, Hai Wang, Fusheng Xiang, Mengyuan Huang, Zhiyuan Xiong, Bin Hu, Xuebin Hou, Lei Jiang, Jianqiang Ma, Jiajia Wu, Yaping Deng, Yi Shen, Qian Wang, Weijie Liu, Jie Liu, Meng Chen, Liang Dong, Weiwen Jia, Hu Chen, Feifei Liu, Rui Yuan, Huilin Xu, Zhenxiang Yan, Tengfei Cao, Zhichao Hu, Xinhua Feng, Dong Du, Tinghao Yu, Yangyu Tao, Feng Zhang, Jianchen Zhu, Chengzhong Xu, Xirui Li, Chong Zha, Wen Ouyang, Yinben Xia, Xiang Li, Zekun He, Rongpeng Chen, Jiawei Song, Ruibin Chen, Fan Jiang, Chongqing Zhao, Bo Wang, Hao Gong, Rong Gan, Winston Hu, Zhanhui Kang, Yong Yang, Yuhong Liu, Di Wang, Jie Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce Hunyuan-Large, which is currently the largest
open-source Transformer-based mixture of experts model, with a total of 389
billion parameters and 52 billion activation parameters, capable of handling up
to 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior
performance across various benchmarks including language understanding and
generation, logical reasoning, mathematical problem-solving, coding,
long-context, and aggregated tasks, where it outperforms LLama3.1-70B and
exhibits comparable performance when compared to the significantly larger
LLama3.1-405B model. Key practice of Hunyuan-Large include large-scale
synthetic data that is orders larger than in previous literature, a mixed
expert routing strategy, a key-value cache compression technique, and an
expert-specific learning rate strategy. Additionally, we also investigate the
scaling laws and learning rate schedule of mixture of experts models, providing
valuable insights and guidances for future model development and optimization.
The code and checkpoints of Hunyuan-Large are released to facilitate future
innovations and applications.
  Codes: https://github.com/Tencent/Hunyuan-Large
  Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 4 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FactTest: Factuality Testing in Large Language Models with Finite-Sample
  and Distribution-Free Guarantees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02603v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02603v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Nie, Xiaotian Hou, Shuhang Lin, James Zou, Huaxiu Yao, Linjun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The propensity of Large Language Models (LLMs) to generate hallucinations and
non-factual content undermines their reliability in high-stakes domains, where
rigorous control over Type I errors (the conditional probability of incorrectly
classifying hallucinations as truthful content) is essential. Despite its
importance, formal verification of LLM factuality with such guarantees remains
largely unexplored. In this paper, we introduce FactTest, a novel framework
that statistically assesses whether an LLM can confidently provide correct
answers to given questions with high-probability correctness guarantees. We
formulate factuality testing as hypothesis testing problem to enforce an upper
bound of Type I errors at user-specified significance levels. Notably, we prove
that our framework also ensures strong Type II error control under mild
conditions and can be extended to maintain its effectiveness when covariate
shifts exist. %These analyses are amenable to the principled NP framework. Our
approach is distribution-free and works for any number of human-annotated
samples. It is model-agnostic and applies to any black-box or white-box LM.
Extensive experiments on question-answering (QA) and multiple-choice benchmarks
demonstrate that \approach effectively detects hallucinations and improves the
model's ability to abstain from answering unknown questions, leading to an over
40% accuracy improvement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Harnessing Webpage UIs for Text-Rich Visual Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13824v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13824v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junpeng Liu, Tianyue Ou, Yifan Song, Yuxiao Qu, Wai Lam, Chenyan Xiong, Wenhu Chen, Graham Neubig, Xiang Yue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-rich visual understanding-the ability to process environments where
dense textual content is integrated with visuals-is crucial for multimodal
large language models (MLLMs) to interact effectively with structured
environments. To enhance this capability, we propose synthesizing general
multimodal instructions from webpage UIs using text-based large language models
(LLMs). Despite lacking direct visual input, text-based LLMs are able to
process structured text representations from webpage accessibility trees. These
instructions are then paired with UI screenshots to train multimodal models. We
introduce MultiUI, a dataset containing 7.3 million samples from 1 million
websites, covering diverse multimodal tasks and UI layouts. Models trained on
MultiUI not only excel in web UI tasks-achieving up to a 48% improvement on
VisualWebBench and a 19.1% boost in element accuracy on a web agent dataset
Mind2Web-but also generalize surprisingly well to non-web UI tasks and even to
non-UI domains, such as document understanding, OCR, and chart interpretation.
These results highlight the broad applicability of web UI data for advancing
text-rich visual understanding across various scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Packing Analysis: Packing Is More Appropriate for Large Models or
  Datasets in Supervised <span class="highlight-title">Fine-tuning</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08081v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08081v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuhe Wang, Guoyin Wang, Yizhong Wang, Jiwei Li, Eduard Hovy, Chen Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Packing, initially utilized in the pre-training phase, is an optimization
technique designed to maximize hardware resource efficiency by combining
different training sequences to fit the model's maximum input length. Although
it has demonstrated effectiveness during pre-training, there remains a lack of
comprehensive analysis for the supervised fine-tuning (SFT) stage on the
following points: (1) whether packing can effectively enhance training
efficiency while maintaining performance, (2) the suitable size of the model
and dataset for fine-tuning with the packing method, and (3) whether packing
unrelated or related training samples might cause the model to either
excessively disregard or over-rely on the context.
  In this paper, we perform extensive comparisons between SFT methods using
padding and packing, covering SFT datasets ranging from 69K to 1.2M and models
from 8B to 70B. This provides the first comprehensive analysis of the
advantages and limitations of packing versus padding, as well as practical
considerations for implementing packing in various training scenarios. Our
analysis covers various benchmarks, including knowledge, reasoning, and coding,
as well as GPT-based evaluations, time efficiency, and other fine-tuning
parameters. We also open-source our code for fine-tuning and evaluation and
provide checkpoints fine-tuned on datasets of different sizes, aiming to
advance future research on packing methods. Code is available at:
https://github.com/ShuheWang1998/Packing-Analysis?tab=readme-ov-file.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Power of Question Translation Training in Multilingual Reasoning:
  Broadened Scope and Deepened Insights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.01345v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.01345v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Zhu, Shujian Huang, Fei Yuan, Cheng Chen, Jiajun Chen, Alexandra Birch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bridging the significant gap between large language model's English and
non-English performance presents a great challenge. While some previous studies
attempt to mitigate this gap with translated training data, the recently
proposed question alignment framework leverages the model's English expertise
to improve multilingual performance with minimum usage of expensive,
error-prone translation. In this paper, we explore how broadly this method can
be applied by examining its effects in reasoning with and without
chain-of-thought, as well as with program-of-thought. We also explore applying
this framework to extremely large language models in an efficient manner, such
as through proxy-tuning. Experiment results on multilingual reasoning
benchmarks mGSM, mSVAMP, xCSQA and xNLI demonstrate that we can extend question
alignment framework to boost multilingual performance across diverse reasoning
scenarios, model families, and sizes. For instance, when applied to the LLaMA2
models, it brings an average accuracy improvements of 12.2% on mGSM even with
the 70B model. To understand the mechanism of its success, we analyze
representation space, generated response and data scales, and reveal how
question translation training strengthens language alignment within LLMs and
shapes their working patterns.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DropBP: Accelerating <span class="highlight-title">Fine-Tuning</span> of Large Language Models by Dropping
  Backward Propagation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17812v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17812v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunghyeon Woo, Baeseong Park, Byeongwook Kim, Minjung Jo, Se Jung Kwon, Dongsuk Jeon, Dongsoo Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have achieved significant success across various
domains. However, training these LLMs typically involves substantial memory and
computational costs during both forward and backward propagation. While
parameter-efficient fine-tuning (PEFT) considerably reduces the training memory
associated with parameters, it does not address the significant computational
costs and activation memory. In this paper, we propose Dropping Backward
Propagation (DropBP), a novel approach designed to reduce computational costs
and activation memory while maintaining accuracy. DropBP randomly drops layers
during backward propagation, which is essentially equivalent to training
shallow submodules generated by undropped layers and residual connections.
Additionally, DropBP calculates the sensitivity of each layer to assign an
appropriate drop rate, thereby stabilizing the training process. DropBP is not
only applicable to full fine-tuning but can also be orthogonally integrated
with all types of PEFT by dropping layers during backward propagation.
Specifically, DropBP can reduce training time by 44% with comparable accuracy
to the baseline, accelerate convergence to the same perplexity by 1.5x, and
enable training with a sequence length 6.2x larger on a single NVIDIA-A100 GPU.
Furthermore, our DropBP enabled a throughput increase of 79% on a NVIDIA A100
GPU and 117% on an Intel Gaudi2 HPU. The code is available at
https://github.com/WooSunghyeon/dropbp.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09324v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09324v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhao Xu, Fan Liu, Hao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although Large Language Models (LLMs) have demonstrated significant
capabilities in executing complex tasks in a zero-shot manner, they are
susceptible to jailbreak attacks and can be manipulated to produce harmful
outputs. Recently, a growing body of research has categorized jailbreak attacks
into token-level and prompt-level attacks. However, previous work primarily
overlooks the diverse key factors of jailbreak attacks, with most studies
concentrating on LLM vulnerabilities and lacking exploration of
defense-enhanced LLMs. To address these issues, we introduced
$\textbf{JailTrickBench}$ to evaluate the impact of various attack settings on
LLM performance and provide a baseline for jailbreak attacks, encouraging the
adoption of a standardized evaluation framework. Specifically, we evaluate the
eight key factors of implementing jailbreak attacks on LLMs from both
target-level and attack-level perspectives. We further conduct seven
representative jailbreak attacks on six defense methods across two widely used
datasets, encompassing approximately 354 experiments with about 55,000 GPU
hours on A800-80G. Our experimental results highlight the need for standardized
benchmarking to evaluate these attacks on defense-enhanced LLMs. Our code is
available at https://github.com/usail-hkust/JailTrickBench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CheX-GPT: Harnessing Large Language Models for Enhanced Chest X-ray
  Report Labeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.11505v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.11505v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jawook Gu, Kihyun You, Han-Cheol Cho, Jiho Kim, Eun Kyoung Hong, Byungseok Roh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Free-text radiology reports present a rich data source for various medical
tasks, but effectively labeling these texts remains challenging. Traditional
rule-based labeling methods fall short of capturing the nuances of diverse
free-text patterns. Moreover, models using expert-annotated data are limited by
data scarcity and pre-defined classes, impacting their performance, flexibility
and scalability. To address these issues, our study offers three main
contributions: 1) We demonstrate the potential of GPT as an adept labeler using
carefully designed prompts. 2) Utilizing only the data labeled by GPT, we
trained a BERT-based labeler, CheX-GPT, which operates faster and more
efficiently than its GPT counterpart. 3) To benchmark labeler performance, we
introduced a publicly available expert-annotated test set, MIMIC-500,
comprising 500 cases from the MIMIC validation set. Our findings demonstrate
that CheX-GPT not only excels in labeling accuracy over existing models, but
also showcases superior efficiency, flexibility, and scalability, supported by
our introduction of the MIMIC-500 dataset for robust benchmarking. Code and
models are available at https://github.com/Soombit-ai/CheXGPT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Wave Network: An Ultra-Small Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02674v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02674v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Zhang, Victor S. Sheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose an innovative token representation and update method in a new
ultra-small language model: the Wave network. Specifically, we use a complex
vector to represent each token, encoding both global and local semantics of the
input text. A complex vector consists of two components: a magnitude vector
representing the global semantics of the input text, and a phase vector
capturing the relationships between individual tokens and global semantics.
Experiments on the AG News text classification task demonstrate that, when
generating complex vectors from randomly initialized token embeddings, our
single-layer Wave Network achieves 90.91% accuracy with wave interference and
91.66% with wave modulation - outperforming a single Transformer layer using
BERT pre-trained embeddings by 19.23% and 19.98%, respectively, and approaching
the accuracy of the pre-trained and fine-tuned BERT base model (94.64%).
Additionally, compared to BERT base, the Wave Network reduces video memory
usage and training time by 77.34% and 85.62% during wave modulation. In
summary, we used a 2.4-million-parameter small language model to achieve
accuracy comparable to a 100-million-parameter BERT model in text
classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMLU-Pro: A More Robust and Challenging Multi-Task Language
  Understanding Benchmark <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.01574v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.01574v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, Wenhu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the age of large-scale language models, benchmarks like the Massive
Multitask Language Understanding (MMLU) have been pivotal in pushing the
boundaries of what AI can achieve in language comprehension and reasoning
across diverse domains. However, as models continue to improve, their
performance on these benchmarks has begun to plateau, making it increasingly
difficult to discern differences in model capabilities. This paper introduces
MMLU-Pro, an enhanced dataset designed to extend the mostly knowledge-driven
MMLU benchmark by integrating more challenging, reasoning-focused questions and
expanding the choice set from four to ten options. Additionally, MMLU-Pro
eliminates the trivial and noisy questions in MMLU. Our experimental results
show that MMLU-Pro not only raises the challenge, causing a significant drop in
accuracy by 16% to 33% compared to MMLU but also demonstrates greater stability
under varying prompts. With 24 different prompt styles tested, the sensitivity
of model scores to prompt variations decreased from 4-5% in MMLU to just 2% in
MMLU-Pro. Additionally, we found that models utilizing Chain of Thought (CoT)
reasoning achieved better performance on MMLU-Pro compared to direct answering,
which is in stark contrast to the findings on the original MMLU, indicating
that MMLU-Pro includes more complex reasoning questions. Our assessments
confirm that MMLU-Pro is a more discriminative benchmark to better track
progress in the field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This version has been accepted and published at NeurIPS 2024 Track
  Datasets and Benchmarks (Spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Perception Compressor:A training-free <span class="highlight-title">prompt</span> compression method in long
  context scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19272v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19272v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiwei Tang, Jin Xu, Tingwei Lu, Zhicheng Zhang, Yiming Zhao, Lin Hai, Hai-Tao Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) demonstrate exceptional capabilities in various
scenarios. However, they suffer from much redundant information and are
sensitive to the position of key information (relevant to the input question)
in long context scenarios, leading to inferior performance. To address these
challenges, we present Perception Compressor, a training-free prompt
compression method. It includes a perception retriever that leverages guiding
questions and instruction to retrieve the most relevant demonstrations, a
dual-slope ratio allocator to dynamically allocate compression ratios and
open-book ratios, and a semi-guided iterative compression that retains key
information at the token level while removing tokens that distract the LLM. We
conduct extensive experiments on long context benchmarks, i.e.,
NaturalQuestions, LongBench, and MuSiQue. Experiment results show that
Perception Compressor outperforms existing methods by a large margin, achieving
state-of-the-art performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AudioBench: A Universal Benchmark for Audio Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16020v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16020v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin Wang, Xunlong Zou, Geyu Lin, Shuo Sun, Zhuohan Liu, Wenyu Zhang, Zhengyuan Liu, AiTi Aw, Nancy F. Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce AudioBench, a universal benchmark designed to evaluate Audio
Large Language Models (AudioLLMs). It encompasses 8 distinct tasks and 26
datasets, among which, 7 are newly proposed datasets. The evaluation targets
three main aspects: speech understanding, audio scene understanding, and voice
understanding (paralinguistic). Despite recent advancements, there lacks a
comprehensive benchmark for AudioLLMs on instruction following capabilities
conditioned on audio signals. AudioBench addresses this gap by setting up
datasets as well as desired evaluation metrics. Besides, we also evaluated the
capabilities of five popular models and found that no single model excels
consistently across all tasks. We outline the research outlook for AudioLLMs
and anticipate that our open-sourced evaluation toolkit, data, and leaderboard
will offer a robust testbed for future model developments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v4 - Add acknowledgment and slight update on structure; Code:
  https://github.com/AudioLLMs/AudioBench</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ $B^4$: A Black-Box Scrubbing Attack on LLM Watermarks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01222v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01222v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baizhou Huang, Xiao Pu, Xiaojun Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Watermarking has emerged as a prominent technique for LLM-generated content
detection by embedding imperceptible patterns. Despite supreme performance, its
robustness against adversarial attacks remains underexplored. Previous work
typically considers a grey-box attack setting, where the specific type of
watermark is already known. Some even necessitates knowledge about
hyperparameters of the watermarking method. Such prerequisites are unattainable
in real-world scenarios. Targeting at a more realistic black-box threat model
with fewer assumptions, we here propose $\mathcal{B}^4$, a black-box scrubbing
attack on watermarks. Specifically, we formulate the watermark scrubbing attack
as a constrained optimization problem by capturing its objectives with two
distributions, a Watermark Distribution and a Fidelity Distribution. This
optimization problem can be approximately solved using two proxy distributions.
Experimental results across 12 different settings demonstrate the superior
performance of $\mathcal{B}^4$ compared with other baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multilingual Pretraining Using a Large Corpus Machine-Translated from a
  Single Source Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23956v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23956v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayi Wang, Yao Lu, Maurice Weber, Max Ryabinin, Yihong Chen, Raphael Tang, Pontus Stenetorp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  English, as a very high-resource language, enables the pretraining of
high-quality large language models (LLMs). The same cannot be said for most
other languages, as leading LLMs still underperform for non-English languages,
likely due to a gap in the quality and diversity of the available multilingual
pretraining corpora. In this work, we find that machine-translated text from a
single high-quality source language can contribute significantly to the
pretraining of multilingual LLMs. We translate FineWeb-Edu, a high-quality
English web dataset, into French, German, and Spanish, resulting in a final
300B-token dataset, which we call TransWeb-Edu, and pretrain a 1.3B-parameter
model, CuatroLLM, from scratch on this dataset. Across five non-English
reasoning tasks, we show that CuatroLLM matches or outperforms state-of-the-art
multilingual models trained using closed data, such as Llama3.2 and Gemma2,
despite using an order of magnitude less data, such as about 6% of the tokens
used for Llama3.2's training. We further demonstrate that with additional
domain-specific pretraining, amounting to less than 1% of TransWeb-Edu,
CuatroLLM surpasses the state of the art in multilingual reasoning. To promote
reproducibility, we release our corpus, models, and training pipeline under
open licenses at hf.co/britllm/CuatroLLM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficiently and Effectively: A Two-stage Approach to Balance Plaintext
  and Encrypted Text for Traffic Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.19687v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.19687v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Peng, Lei Cui, Wei Cai, Zhenquan Ding, Zhiyu Hao, Xiaochun Yun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Encrypted traffic classification is the task of identifying the application
or service associated with encrypted network traffic. One effective approach
for this task is to use deep learning methods to encode the raw traffic bytes
directly and automatically extract features for classification (byte-based
models). However, current byte-based models input raw traffic bytes, whether
plaintext or encrypted text, for automated feature extraction, neglecting the
distinct impacts of plaintext and encrypted text on downstream tasks.
Additionally, these models primarily focus on improving classification
accuracy, with little emphasis on the efficiency of models. In this paper, for
the first time, we analyze the impact of plaintext and encrypted text on the
model's effectiveness and efficiency. Based on our observations and findings,
we propose a two-phase approach to balance the trade-off between plaintext and
encrypted text in traffic classification. Specifically, Stage one is to
Determine whether the Plain text is enough to be accurately Classified (DPC)
using the proposed DPC Selector. This stage quickly identifies samples that can
be classified using plaintext, leveraging explicit byte features in plaintext
to enhance model's efficiency. Stage two aims to adaptively make a
classification with the result from stage one. This stage incorporates
encrypted text information for samples that cannot be classified using
plaintext alone, ensuring the model's effectiveness on traffic classification
tasks. Experiments on two datasets demonstrate that our proposed model achieves
state-of-the-art results in both effectiveness and efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating LLMs for Targeted Concept Simplification for Domain-Specific
  Texts <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20763v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20763v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sumit Asthana, Hannah Rashkin, Elizabeth Clark, Fantine Huot, Mirella Lapata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One useful application of NLP models is to support people in reading complex
text from unfamiliar domains (e.g., scientific articles). Simplifying the
entire text makes it understandable but sometimes removes important details. On
the contrary, helping adult readers understand difficult concepts in context
can enhance their vocabulary and knowledge. In a preliminary human study, we
first identify that lack of context and unfamiliarity with difficult concepts
is a major reason for adult readers' difficulty with domain-specific text. We
then introduce "targeted concept simplification," a simplification task for
rewriting text to help readers comprehend text containing unfamiliar concepts.
We also introduce WikiDomains, a new dataset of 22k definitions from 13
academic domains paired with a difficult concept within each definition. We
benchmark the performance of open-source and commercial LLMs and a simple
dictionary baseline on this task across human judgments of ease of
understanding and meaning preservation. Interestingly, our human judges
preferred explanations about the difficult concept more than simplification of
the concept phrase. Further, no single model achieved superior performance
across all quality dimensions, and automated metrics also show low correlations
with human evaluations of concept simplification ($\sim0.2$), opening up rich
avenues for research on personalized human reading comprehension support.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to appear in proceedings of EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Document Parsing Unveiled: Techniques, Challenges, and Prospects for
  Structured Information Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21169v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21169v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qintong Zhang, Victor Shea-Jay Huang, Bin Wang, Junyuan Zhang, Zhengren Wang, Hao Liang, Shawn Wang, Matthieu Lin, Conghui He, Wentao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Document parsing is essential for converting unstructured and semi-structured
documents-such as contracts, academic papers, and invoices-into structured,
machine-readable data. Document parsing extract reliable structured data from
unstructured inputs, providing huge convenience for numerous applications.
Especially with recent achievements in Large Language Models, document parsing
plays an indispensable role in both knowledge base construction and training
data generation. This survey presents a comprehensive review of the current
state of document parsing, covering key methodologies, from modular pipeline
systems to end-to-end models driven by large vision-language models. Core
components such as layout detection, content extraction (including text,
tables, and mathematical expressions), and multi-modal data integration are
examined in detail. Additionally, this paper discusses the challenges faced by
modular document parsing systems and vision-language models in handling complex
layouts, integrating multiple modules, and recognizing high-density text. It
emphasizes the importance of developing larger and more diverse datasets and
outlines future research directions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Creative Short Story Generation in Humans and Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02316v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02316v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mete Ismayilzada, Claire Stevenson, Lonneke van der Plas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Storytelling is a fundamental aspect of human communication, relying heavily
on creativity to produce narratives that are novel, appropriate, and
surprising. While large language models (LLMs) have recently demonstrated the
ability to generate high-quality stories, their creative capabilities remain
underexplored. Previous research has either focused on creativity tests
requiring short responses or primarily compared model performance in story
generation to that of professional writers. However, the question of whether
LLMs exhibit creativity in writing short stories on par with the average human
remains unanswered. In this work, we conduct a systematic analysis of
creativity in short story generation across LLMs and everyday people. Using a
five-sentence creative story task, commonly employed in psychology to assess
human creativity, we automatically evaluate model- and human-generated stories
across several dimensions of creativity, including novelty, surprise, and
diversity. Our findings reveal that while LLMs can generate stylistically
complex stories, they tend to fall short in terms of creativity when compared
to average human writers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GRSQA -- Graph Reasoning-Structured Question Answering Dataset 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00369v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00369v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anish Pahilajani, Devasha Trivedi, Jincen Shuai, Khin S. Yone, Samyak Rajesh Jain, Namyong Park, Ryan A. Rossi, Nesreen K. Ahmed, Franck Dernoncourt, Yu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have excelled in multi-hop question-answering
(M-QA) due to their advanced reasoning abilities. However, the impact of the
inherent reasoning structures on LLM M-QA performance remains unclear, largely
due to the absence of QA datasets that provide fine-grained reasoning
structures. To address this gap, we introduce the Graph Reasoning-Structured
Question Answering Dataset (GRS-QA), which includes both semantic contexts and
reasoning structures for QA pairs. Unlike existing M-QA datasets, where
different reasoning structures are entangled together, GRS-QA explicitly
captures intricate reasoning pathways by constructing reasoning graphs, where
nodes represent textual contexts and edges denote logical flows. These
reasoning graphs of different structures enable a fine-grained evaluation of
LLM reasoning capabilities across various reasoning structures. Our empirical
analysis reveals that LLMs perform differently when handling questions with
varying reasoning structures. This finding facilitates the exploration of
textual structures as compared with semantics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 24 figures, 10 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transcoders Find Interpretable LLM Feature Circuits <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11944v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11944v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacob Dunefsky, Philippe Chlenski, Neel Nanda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A key goal in mechanistic interpretability is circuit analysis: finding
sparse subgraphs of models corresponding to specific behaviors or capabilities.
However, MLP sublayers make fine-grained circuit analysis on transformer-based
language models difficult. In particular, interpretable features -- such as
those found by sparse autoencoders (SAEs) -- are typically linear combinations
of extremely many neurons, each with its own nonlinearity to account for.
Circuit analysis in this setting thus either yields intractably large circuits
or fails to disentangle local and global behavior. To address this we explore
transcoders, which seek to faithfully approximate a densely activating MLP
layer with a wider, sparsely-activating MLP layer. We introduce a novel method
for using transcoders to perform weights-based circuit analysis through MLP
sublayers. The resulting circuits neatly factorize into input-dependent and
input-invariant terms. We then successfully train transcoders on language
models with 120M, 410M, and 1.4B parameters, and find them to perform at least
on par with SAEs in terms of sparsity, faithfulness, and
human-interpretability. Finally, we apply transcoders to reverse-engineer
unknown circuits in the model, and we obtain novel insights regarding the
"greater-than circuit" in GPT2-small. Our results suggest that transcoders can
prove effective in decomposing model computations involving MLPs into
interpretable circuits. Code is available at
https://github.com/jacobdunefsky/transcoder_circuits/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 6 figures, 4 tables, 2 algorithms. NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Teach Better or Show Smarter? On Instructions and Exemplars in Automatic
  <span class="highlight-title">Prompt</span> Optimization <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15708v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15708v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingchen Wan, Ruoxi Sun, Hootan Nakhost, Sercan O. Arik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have demonstrated remarkable capabilities, but their
performance is heavily reliant on effective prompt engineering. Automatic
prompt optimization (APO) methods are designed to automate this and can be
broadly categorized into those targeting instructions (instruction
optimization, IO) vs. those targeting exemplars (exemplar optimization, EO).
Despite their shared objective, these have evolved rather independently, with
IO receiving more research attention recently. This paper seeks to bridge this
gap by comprehensively comparing the performance of representative IO and EO
techniques both isolation and combination on a diverse set of challenging
tasks. Our findings reveal that intelligently reusing model-generated
input-output pairs obtained from evaluating prompts on the validation set as
exemplars, consistently improves performance on top of IO methods but is
currently under-investigated. We also find that despite the recent focus on IO,
how we select exemplars can outweigh how we optimize instructions, with EO
strategies as simple as random search outperforming state-of-the-art IO methods
with seed instructions without any optimization. Moreover, we observe a synergy
between EO and IO, with optimal combinations surpassing the individual
contributions. We conclude that studying exemplar optimization both as a
standalone method and its optimal combination with instruction optimization
remain a crucial aspect of APO and deserve greater consideration in future
research, even in the era of highly capable instruction-following models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Expanded version of the NeurIPS 2024 paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Perceptions to Beliefs: Exploring Precursory Inferences for Theory of
  Mind in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.06004v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.06004v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chani Jung, Dongkwan Kim, Jiho Jin, Jiseon Kim, Yeon Seonwoo, Yejin Choi, Alice Oh, Hyunwoo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While humans naturally develop theory of mind (ToM), the capability to
understand other people's mental states and beliefs, state-of-the-art large
language models (LLMs) underperform on simple ToM benchmarks. We posit that we
can extend our understanding of LLMs' ToM abilities by evaluating key human ToM
precursors$-$perception inference and perception-to-belief inference$-$in LLMs.
We introduce two datasets, Percept-ToMi and Percept-FANToM, to evaluate these
precursory inferences for ToM in LLMs by annotating characters' perceptions on
ToMi and FANToM, respectively. Our evaluation of eight state-of-the-art LLMs
reveals that the models generally perform well in perception inference while
exhibiting limited capability in perception-to-belief inference (e.g., lack of
inhibitory control). Based on these results, we present PercepToM, a novel ToM
method leveraging LLMs' strong perception inference capability while
supplementing their limited perception-to-belief inference. Experimental
results demonstrate that PercepToM significantly enhances LLM's performance,
especially in false belief scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interpretable Differential Diagnosis with Dual-Inference Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.07330v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.07330v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuang Zhou, Mingquan Lin, Sirui Ding, Jiashuo Wang, Genevieve B. Melton, James Zou, Rui Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic differential diagnosis (DDx) is an essential medical task that
generates a list of potential diseases as differentials based on patient
symptom descriptions. In practice, interpreting these differential diagnoses
yields significant value but remains under-explored. Given the powerful
capabilities of large language models (LLMs), we investigated using LLMs for
interpretable DDx. Specifically, we curated the first DDx dataset with
expert-derived interpretation on 570 clinical notes. Besides, we proposed
Dual-Inf, a novel framework that enabled LLMs to conduct bidirectional
inference (i.e., from symptoms to diagnoses and vice versa) for DDx
interpretation. Both human and automated evaluation validated its efficacy in
predicting and elucidating differentials across four base LLMs. In addition,
Dual-Inf could reduce interpretation errors and hold promise for rare disease
explanations. To the best of our knowledge, it is the first work that
customizes LLMs for DDx explanation and comprehensively evaluates their
interpretation performance. Overall, our study bridges a critical gap in DDx
interpretation and enhances clinical decision-making.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Textless Speech-to-Speech Translation With Limited Parallel Data <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.15405v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.15405v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anuj Diwan, Anirudh Srinivasan, David Harwath, Eunsol Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing speech-to-speech translation (S2ST) models fall into two camps: they
either leverage text as an intermediate step or require hundreds of hours of
parallel speech data. Both approaches are incompatible with textless languages
or language pairs with limited parallel data. We present PFB, a framework for
training textless S2ST models that require just dozens of hours of parallel
speech data. We first pretrain a model on large-scale monolingual speech data,
finetune it with a small amount of parallel speech data (20-60 hours), and
lastly train with an unsupervised backtranslation objective. We train and
evaluate our models for English-to-German, German-to-English and
Marathi-to-English translation on three different domains (European Parliament,
Common Voice, and All India Radio) with single-speaker synthesized speech.
Evaluated using the ASR-BLEU metric, our models achieve reasonable performance
on all three domains, with some being within 1-2 points of our higher-resourced
topline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GPT-4V Cannot Generate Radiology Reports Yet 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12176v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12176v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuyang Jiang, Chacha Chen, Dang Nguyen, Benjamin M. Mervak, Chenhao Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  GPT-4V's purported strong multimodal abilities raise interests in using it to
automate radiology report writing, but there lacks thorough evaluations. In
this work, we perform a systematic evaluation of GPT-4V in generating radiology
reports on two chest X-ray report datasets: MIMIC-CXR and IU X-Ray. We attempt
to directly generate reports using GPT-4V through different prompting
strategies and find that it fails terribly in both lexical metrics and clinical
efficacy metrics. To understand the low performance, we decompose the task into
two steps: 1) the medical image reasoning step of predicting medical condition
labels from images; and 2) the report synthesis step of generating reports from
(groundtruth) conditions. We show that GPT-4V's performance in image reasoning
is consistently low across different prompts. In fact, the distributions of
model-predicted labels remain constant regardless of which groundtruth
conditions are present on the image, suggesting that the model is not
interpreting chest X-rays meaningfully. Even when given groundtruth conditions
in report synthesis, its generated reports are less correct and less
natural-sounding than a finetuned LLaMA-2. Altogether, our findings cast doubt
on the viability of using GPT-4V in a radiology workflow.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 3 figures, code:
  https://github.com/ChicagoHAI/cxr-eval-gpt-4v</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ INQUIRE: A Natural World Text-to-Image Retrieval Benchmark <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02537v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02537v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edward Vendrow, Omiros Pantazis, Alexander Shepard, Gabriel Brostow, Kate E. Jones, Oisin Mac Aodha, Sara Beery, Grant Van Horn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce INQUIRE, a text-to-image retrieval benchmark designed to
challenge multimodal vision-language models on expert-level queries. INQUIRE
includes iNaturalist 2024 (iNat24), a new dataset of five million natural world
images, along with 250 expert-level retrieval queries. These queries are paired
with all relevant images comprehensively labeled within iNat24, comprising
33,000 total matches. Queries span categories such as species identification,
context, behavior, and appearance, emphasizing tasks that require nuanced image
understanding and domain expertise. Our benchmark evaluates two core retrieval
tasks: (1) INQUIRE-Fullrank, a full dataset ranking task, and (2)
INQUIRE-Rerank, a reranking task for refining top-100 retrievals. Detailed
evaluation of a range of recent multimodal models demonstrates that INQUIRE
poses a significant challenge, with the best models failing to achieve an
mAP@50 above 50%. In addition, we show that reranking with more powerful
multimodal models can enhance retrieval performance, yet there remains a
significant margin for improvement. By focusing on scientifically-motivated
ecological challenges, INQUIRE aims to bridge the gap between AI capabilities
and the needs of real-world scientific inquiry, encouraging the development of
retrieval systems that can assist with accelerating ecological and biodiversity
research. Our dataset and code are available at
https://inquire-benchmark.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in NeurIPS 2024, Datasets and Benchmarks Track</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">116</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Community Forensics: Using Thousands of Generators to Train Fake Image
  Detectors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04125v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04125v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeongsoo Park, Andrew Owens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the key challenges of detecting AI-generated images is spotting images
that have been created by previously unseen generative models. We argue that
the limited diversity of the training data is a major obstacle to addressing
this problem, and we propose a new dataset that is significantly larger and
more diverse than prior work. As part of creating this dataset, we
systematically download thousands of text-to-image latent diffusion models and
sample images from them. We also collect images from dozens of popular open
source and commercial models. The resulting dataset contains 2.7M images that
have been sampled from 4803 different models. These images collectively capture
a wide range of scene content, generator architectures, and image processing
settings. Using this dataset, we study the generalization abilities of fake
image detectors. Our experiments suggest that detection performance improves as
the number of models in the training set increases, even when these models have
similar architectures. We also find that detection performance improves as the
diversity of the models increases, and that our trained detectors generalize
better than those trained on other datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fed-EC: Bandwidth-Efficient Clustering-Based Federated Learning For
  Autonomous Visual Robot Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04112v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04112v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shreya Gummadi, Mateus V. Gasparino, Deepak Vasisht, Girish Chowdhary
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Centralized learning requires data to be aggregated at a central server,
which poses significant challenges in terms of data privacy and bandwidth
consumption. Federated learning presents a compelling alternative, however,
vanilla federated learning methods deployed in robotics aim to learn a single
global model across robots that works ideally for all. But in practice one
model may not be well suited for robots deployed in various environments. This
paper proposes Federated-EmbedCluster (Fed-EC), a clustering-based federated
learning framework that is deployed with vision based autonomous robot
navigation in diverse outdoor environments. The framework addresses the key
federated learning challenge of deteriorating model performance of a single
global model due to the presence of non-IID data across real-world robots.
Extensive real-world experiments validate that Fed-EC reduces the communication
size by 23x for each robot while matching the performance of centralized
learning for goal-oriented navigation and outperforms local learning. Fed-EC
can transfer previously learnt models to new robots that join the cluster.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RaVL: Discovering and Mitigating Spurious Correlations in Fine-Tuned
  <span class="highlight-title">Vision-Language</span> Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04097v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04097v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maya Varma, Jean-Benoit Delbrouck, Zhihong Chen, Akshay Chaudhari, Curtis Langlotz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuned vision-language models (VLMs) often capture spurious correlations
between image features and textual attributes, resulting in degraded zero-shot
performance at test time. Existing approaches for addressing spurious
correlations (i) primarily operate at the global image-level rather than
intervening directly on fine-grained image features and (ii) are predominantly
designed for unimodal settings. In this work, we present RaVL, which takes a
fine-grained perspective on VLM robustness by discovering and mitigating
spurious correlations using local image features rather than operating at the
global image level. Given a fine-tuned VLM, RaVL first discovers spurious
correlations by leveraging a region-level clustering approach to identify
precise image features contributing to zero-shot classification errors. Then,
RaVL mitigates the identified spurious correlation with a novel region-aware
loss function that enables the VLM to focus on relevant regions and ignore
spurious relationships during fine-tuning. We evaluate RaVL on 654 VLMs with
various model architectures, data domains, and learned spurious correlations.
Our results show that RaVL accurately discovers (191% improvement over the
closest baseline) and mitigates (8.2% improvement on worst-group image
classification accuracy) spurious correlations. Qualitative evaluations on
general-domain and medical-domain VLMs confirm our findings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Textual Decomposition Then Sub-motion-space Scattering for
  Open-Vocabulary Motion Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04079v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04079v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke Fan, Jiangning Zhang, Ran Yi, Jingyu Gong, Yabiao Wang, Yating Wang, Xin Tan, Chengjie Wang, Lizhuang Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-motion generation is a crucial task in computer vision, which
generates the target 3D motion by the given text. The existing annotated
datasets are limited in scale, resulting in most existing methods overfitting
to the small datasets and unable to generalize to the motions of the open
domain. Some methods attempt to solve the open-vocabulary motion generation
problem by aligning to the CLIP space or using the Pretrain-then-Finetuning
paradigm. However, the current annotated dataset's limited scale only allows
them to achieve mapping from sub-text-space to sub-motion-space, instead of
mapping between full-text-space and full-motion-space (full mapping), which is
the key to attaining open-vocabulary motion generation. To this end, this paper
proposes to leverage the atomic motion (simple body part motions over a short
time period) as an intermediate representation, and leverage two orderly
coupled steps, i.e., Textual Decomposition and Sub-motion-space Scattering, to
address the full mapping problem. For Textual Decomposition, we design a
fine-grained description conversion algorithm, and combine it with the
generalization ability of a large language model to convert any given motion
text into atomic texts. Sub-motion-space Scattering learns the compositional
process from atomic motions to the target motions, to make the learned
sub-motion-space scattered to form the full-motion-space. For a given motion of
the open domain, it transforms the extrapolation into interpolation and thereby
significantly improves generalization. Our network, $DSO$-Net, combines textual
$d$ecomposition and sub-motion-space $s$cattering to solve the
$o$pen-vocabulary motion generation. Extensive experiments demonstrate that our
DSO-Net achieves significant improvements over the state-of-the-art methods on
open-vocabulary motion generation. Code is available at
https://vankouf.github.io/DSONet/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>project page: https://vankouf.github.io/DSONet/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ H-POPE: Hierarchical Polling-based Probing Evaluation of Hallucinations
  in Large <span class="highlight-title">Vision-Language</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04077v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04077v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nhi Pham, Michael Schott
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  By leveraging both texts and images, large vision language models (LVLMs)
have shown significant progress in various multi-modal tasks. Nevertheless,
these models often suffer from hallucinations, e.g., they exhibit
inconsistencies between the visual input and the textual output. To address
this, we propose H-POPE, a coarse-to-fine-grained benchmark that systematically
assesses hallucination in object existence and attributes. Our evaluation shows
that models are prone to hallucinations on object existence, and even more so
on fine-grained attributes. We further investigate whether these models rely on
visual input to formulate the output texts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Poster at https://sites.google.com/berkeley.edu/bb-stat/home</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pseudo-labeling with Keyword Refining for Few-Supervised Video
  Captioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04059v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04059v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ping Li, Tao Wang, Xinkui Zhao, Xianghua Xu, Mingli Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video captioning generate a sentence that describes the video content.
Existing methods always require a number of captions (\eg, 10 or 20) per video
to train the model, which is quite costly. In this work, we explore the
possibility of using only one or very few ground-truth sentences, and introduce
a new task named few-supervised video captioning. Specifically, we propose a
few-supervised video captioning framework that consists of lexically
constrained pseudo-labeling module and keyword-refined captioning module.
Unlike the random sampling in natural language processing that may cause
invalid modifications (\ie, edit words), the former module guides the model to
edit words using some actions (\eg, copy, replace, insert, and delete) by a
pretrained token-level classifier, and then fine-tunes candidate sentences by a
pretrained language model. Meanwhile, the former employs the repetition
penalized sampling to encourage the model to yield concise pseudo-labeled
sentences with less repetition, and selects the most relevant sentences upon a
pretrained video-text model. Moreover, to keep semantic consistency between
pseudo-labeled sentences and video content, we develop the transformer-based
keyword refiner with the video-keyword gated fusion strategy to emphasize more
on relevant words. Extensive experiments on several benchmarks demonstrate the
advantages of the proposed approach in both few-supervised and fully-supervised
scenarios. The code implementation is available at
https://github.com/mlvccn/PKG_VidCap
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 figures, Accepted in Pattern Recognition</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-branch Spatio-Temporal Graph Neural Network For Efficient Ice
  Layer Thickness Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04055v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04055v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zesheng Liu, Maryam Rahnemoonfar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding spatio-temporal patterns in polar ice layers is essential for
tracking changes in ice sheet balance and assessing ice dynamics. While
convolutional neural networks are widely used in learning ice layer patterns
from raw echogram images captured by airborne snow radar sensors, noise in the
echogram images prevents researchers from getting high-quality results.
Instead, we focus on geometric deep learning using graph neural networks,
aiming to build a spatio-temporal graph neural network that learns from
thickness information of the top ice layers and predicts for deeper layers. In
this paper, we developed a novel multi-branch spatio-temporal graph neural
network that used the GraphSAGE framework for spatio features learning and a
temporal convolution operation to capture temporal changes, enabling different
branches of the network to be more specialized and focusing on a single
learning task. We found that our proposed multi-branch network can consistently
outperform the current fused spatio-temporal graph neural network in both
accuracy and efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aligning Characteristic Descriptors with Images for Human-Expert-like
  Explainability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04008v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04008v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bharat Chandra Yalavarthi, Nalini Ratha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In mission-critical domains such as law enforcement and medical diagnosis,
the ability to explain and interpret the outputs of deep learning models is
crucial for ensuring user trust and supporting informed decision-making.
Despite advancements in explainability, existing methods often fall short in
providing explanations that mirror the depth and clarity of those given by
human experts. Such expert-level explanations are essential for the dependable
application of deep learning models in law enforcement and medical contexts.
Additionally, we recognize that most explanations in real-world scenarios are
communicated primarily through natural language. Addressing these needs, we
propose a novel approach that utilizes characteristic descriptors to explain
model decisions by identifying their presence in images, thereby generating
expert-like explanations. Our method incorporates a concept bottleneck layer
within the model architecture, which calculates the similarity between image
and descriptor encodings to deliver inherent and faithful explanations. Through
experiments in face recognition and chest X-ray diagnosis, we demonstrate that
our approach offers a significant contrast over existing techniques, which are
often limited to the use of saliency maps. We believe our approach represents a
significant step toward making deep learning systems more accountable,
transparent, and trustworthy in the critical domains of face recognition and
medical diagnosis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synomaly Noise and Multi-Stage Diffusion: A Novel Approach for
  Unsupervised Anomaly Detection in Ultrasound Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04004v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04004v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Bi, Lucie Huang, Ricarda Clarenbach, Reza Ghotbi, Angelos Karlas, Nassir Navab, Zhongliang Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ultrasound (US) imaging is widely used in routine clinical practice due to
its advantages of being radiation-free, cost-effective, and portable. However,
the low reproducibility and quality of US images, combined with the scarcity of
expert-level annotation, make the training of fully supervised segmentation
models challenging. To address these issues, we propose a novel unsupervised
anomaly detection framework based on a diffusion model that incorporates a
synthetic anomaly (Synomaly) noise function and a multi-stage diffusion
process. Synomaly noise introduces synthetic anomalies into healthy images
during training, allowing the model to effectively learn anomaly removal. The
multi-stage diffusion process is introduced to progressively denoise images,
preserving fine details while improving the quality of anomaly-free
reconstructions. The generated high-fidelity counterfactual healthy images can
further enhance the interpretability of the segmentation models, as well as
provide a reliable baseline for evaluating the extent of anomalies and
supporting clinical decision-making. Notably, the unsupervised anomaly
detection model is trained purely on healthy images, eliminating the need for
anomalous training samples and pixel-level annotations. We validate the
proposed approach on carotid US, brain MRI, and liver CT datasets. The
experimental results demonstrate that the proposed framework outperforms
existing state-of-the-art unsupervised anomaly detection methods, achieving
performance comparable to fully supervised segmentation models in the US
dataset. Additionally, ablation studies underline the importance of
hyperparameter selection for Synomaly noise and the effectiveness of the
multi-stage diffusion process in enhancing model performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Local vs distributed representations: What is the right basis for
  interpretability? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03993v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03993v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julien Colin, Lore Goetschalckx, Thomas Fel, Victor Boutin, Jay Gopal, Thomas Serre, Nuria Oliver
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Much of the research on the interpretability of deep neural networks has
focused on studying the visual features that maximally activate individual
neurons. However, recent work has cast doubts on the usefulness of such local
representations for understanding the behavior of deep neural networks because
individual neurons tend to respond to multiple unrelated visual patterns, a
phenomenon referred to as "superposition". A promising alternative to
disentangle these complex patterns is learning sparsely distributed vector
representations from entire network layers, as the resulting basis vectors
seemingly encode single identifiable visual patterns consistently. Thus, one
would expect the resulting code to align better with human perceivable visual
patterns, but supporting evidence remains, at best, anecdotal. To fill this
gap, we conducted three large-scale psychophysics experiments collected from a
pool of 560 participants. Our findings provide (i) strong evidence that
features obtained from sparse distributed representations are easier to
interpret by human observers and (ii) that this effect is more pronounced in
the deepest layers of a neural network. Complementary analyses also reveal that
(iii) features derived from sparse distributed representations contribute more
to the model's decision. Overall, our results highlight that distributed
representations constitute a superior basis for interpretability, underscoring
a need for the field to move beyond the interpretation of local neural codes in
favor of sparsely distributed ones.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ET-SEED: Efficient Trajectory-Level SE(3) Equivariant Diffusion Policy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03990v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03990v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenrui Tie, Yue Chen, Ruihai Wu, Boxuan Dong, Zeyi Li, Chongkai Gao, Hao Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Imitation learning, e.g., diffusion policy, has been proven effective in
various robotic manipulation tasks. However, extensive demonstrations are
required for policy robustness and generalization. To reduce the demonstration
reliance, we leverage spatial symmetry and propose ET-SEED, an efficient
trajectory-level SE(3) equivariant diffusion model for generating action
sequences in complex robot manipulation tasks. Further, previous equivariant
diffusion models require the per-step equivariance in the Markov process,
making it difficult to learn policy under such strong constraints. We
theoretically extend equivariant Markov kernels and simplify the condition of
equivariant diffusion process, thereby significantly improving training
efficiency for trajectory-level SE(3) equivariant diffusion policy in an
end-to-end manner. We evaluate ET-SEED on representative robotic manipulation
tasks, involving rigid body, articulated and deformable object. Experiments
demonstrate superior data efficiency and manipulation proficiency of our
proposed method, as well as its ability to generalize to unseen configurations
with only a few demonstrations. Website: https://et-seed.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accept to CoRL 2024 Workshop on X-Embodiment Robot Learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReEdit: <span class="highlight-title">Multimodal</span> Exemplar-Based Image Editing with Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03982v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03982v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashutosh Srivastava, Tarun Ram Menta, Abhinav Java, Avadhoot Jadhav, Silky Singh, Surgan Jandial, Balaji Krishnamurthy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern Text-to-Image (T2I) Diffusion models have revolutionized image editing
by enabling the generation of high-quality photorealistic images. While the de
facto method for performing edits with T2I models is through text instructions,
this approach non-trivial due to the complex many-to-many mapping between
natural language and images. In this work, we address exemplar-based image
editing -- the task of transferring an edit from an exemplar pair to a content
image(s). We propose ReEdit, a modular and efficient end-to-end framework that
captures edits in both text and image modalities while ensuring the fidelity of
the edited image. We validate the effectiveness of ReEdit through extensive
comparisons with state-of-the-art baselines and sensitivity analyses of key
design choices. Our results demonstrate that ReEdit consistently outperforms
contemporary approaches both qualitatively and quantitatively. Additionally,
ReEdit boasts high practical applicability, as it does not require any
task-specific optimization and is four times faster than the next best
baseline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>First three authors contributed equally to this work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HRDecoder: High-Resolution Decoder Network for Fundus Image Lesion
  Segmentation <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03976v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03976v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyuan Ding, Yixiong Liang, Shichao Kan, Qing Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High resolution is crucial for precise segmentation in fundus images, yet
handling high-resolution inputs incurs considerable GPU memory costs, with
diminishing performance gains as overhead increases. To address this issue
while tackling the challenge of segmenting tiny objects, recent studies have
explored local-global fusion methods. These methods preserve fine details using
local regions and capture long-range context information from downscaled global
images. However, the necessity of multiple forward passes inevitably incurs
significant computational overhead, adversely affecting inference speed. In
this paper, we propose HRDecoder, a simple High-Resolution Decoder network for
fundus lesion segmentation. It integrates a high-resolution representation
learning module to capture fine-grained local features and a high-resolution
fusion module to fuse multi-scale predictions. Our method effectively improves
the overall segmentation accuracy of fundus lesions while consuming reasonable
memory and computational overhead, and maintaining satisfying inference speed.
Experimental results on the IDRID and DDR datasets demonstrate the
effectiveness of our method. Code is available at
https://github.com/CVIU-CSU/HRDecoder.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 3 figures, accepted by MICCAI 2024, the revised version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Face Reconstruction from Face Embeddings using Adapter to a Face
  Foundation Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03960v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03960v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hatef Otroshi Shahreza, Anjith George, Sébastien Marcel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Face recognition systems extract embedding vectors from face images and use
these embeddings to verify or identify individuals. Face reconstruction attack
(also known as template inversion) refers to reconstructing face images from
face embeddings and using the reconstructed face image to enter a face
recognition system. In this paper, we propose to use a face foundation model to
reconstruct face images from the embeddings of a blackbox face recognition
model. The foundation model is trained with 42M images to generate face images
from the facial embeddings of a fixed face recognition model. We propose to use
an adapter to translate target embeddings into the embedding space of the
foundation model. The generated images are evaluated on different face
recognition models and different datasets, demonstrating the effectiveness of
our method to translate embeddings of different face recognition models. We
also evaluate the transferability of reconstructed face images when attacking
different face recognition models. Our experimental results show that our
reconstructed face images outperform previous reconstruction attacks against
face recognition models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Energy Score-based Pseudo-Label Filtering and Adaptive Loss for
  Imbalanced Semi-supervised SAR target recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03959v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03959v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinzheng Zhang, Yuqing Luo, Guopeng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic target recognition (ATR) is an important use case for synthetic
aperture radar (SAR) image interpretation. Recent years have seen significant
advancements in SAR ATR technology based on semi-supervised learning. However,
existing semi-supervised SAR ATR algorithms show low recognition accuracy in
the case of class imbalance. This work offers a non-balanced semi-supervised
SAR target recognition approach using dynamic energy scores and adaptive loss.
First, an energy score-based method is developed to dynamically select
unlabeled samples near to the training distribution as pseudo-labels during
training, assuring pseudo-label reliability in long-tailed distribution
circumstances. Secondly, loss functions suitable for class imbalances are
proposed, including adaptive margin perception loss and adaptive hard triplet
loss, the former offsets inter-class confusion of classifiers, alleviating the
imbalance issue inherent in pseudo-label generation. The latter effectively
tackles the model's preference for the majority class by focusing on complex
difficult samples during training. Experimental results on extremely imbalanced
SAR datasets demonstrate that the proposed method performs well under the dual
constraints of scarce labels and data imbalance, effectively overcoming the
model bias caused by data imbalance and achieving high-precision target
recognition.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Act in Collusion: A Persistent Distributed Multi-Target Backdoor in
  Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03926v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03926v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Liu, Wu Yang, Chen Xu, Jiguang Lv, Huanran Wang, Yuhang Zhang, Shuchun Xu, Dapeng Man
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning, a novel paradigm designed to protect data privacy, is
vulnerable to backdoor attacks due to its distributed nature. Current research
often designs attacks based on a single attacker with a single backdoor,
overlooking more realistic and complex threats in federated learning. We
propose a more practical threat model for federated learning: the distributed
multi-target backdoor. In this model, multiple attackers control different
clients, embedding various triggers and targeting different classes,
collaboratively implanting backdoors into the global model via central
aggregation. Empirical validation shows that existing methods struggle to
maintain the effectiveness of multiple backdoors in the global model. Our key
insight is that similar backdoor triggers cause parameter conflicts and
injecting new backdoors disrupts gradient directions, significantly weakening
some backdoors performance. To solve this, we propose a Distributed
Multi-Target Backdoor Attack (DMBA), ensuring efficiency and persistence of
backdoors from different malicious clients. To avoid parameter conflicts, we
design a multi-channel dispersed frequency trigger strategy to maximize trigger
differences. To mitigate gradient interference, we introduce backdoor replay in
local training to neutralize conflicting gradients. Extensive validation shows
that 30 rounds after the attack, Attack Success Rates of three different
backdoors from various clients remain above 93%. The code will be made publicly
available after the review period.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-supervised Representation Learning for Cell Event Recognition
  through Time Arrow Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03924v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03924v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cangxiong Chen, Vinay P. Namboodiri, Julia E. Sero
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The spatio-temporal nature of live-cell microscopy data poses challenges in
the analysis of cell states which is fundamental in bioimaging. Deep-learning
based segmentation or tracking methods rely on large amount of high quality
annotations to work effectively. In this work, we explore an alternative
solution: using feature maps obtained from self-supervised representation
learning (SSRL) on time arrow prediction (TAP) for the downstream supervised
task of cell event recognition. We demonstrate through extensive experiments
and analysis that this approach can achieve better performance with limited
annotation compared to models trained from end to end using fully supervised
approach. Our analysis also provides insight into applications of the SSRL
using TAP in live-cell microscopy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ROBIN: Robust and Invisible Watermarks for Diffusion Models with
  Adversarial Optimization <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03862v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03862v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huayang Huang, Yu Wu, Qian Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Watermarking generative content serves as a vital tool for authentication,
ownership protection, and mitigation of potential misuse. Existing watermarking
methods face the challenge of balancing robustness and concealment. They
empirically inject a watermark that is both invisible and robust and passively
achieve concealment by limiting the strength of the watermark, thus reducing
the robustness. In this paper, we propose to explicitly introduce a watermark
hiding process to actively achieve concealment, thus allowing the embedding of
stronger watermarks. To be specific, we implant a robust watermark in an
intermediate diffusion state and then guide the model to hide the watermark in
the final generated image. We employ an adversarial optimization algorithm to
produce the optimal hiding prompt guiding signal for each watermark. The prompt
embedding is optimized to minimize artifacts in the generated image, while the
watermark is optimized to achieve maximum strength. The watermark can be
verified by reversing the generation process. Experiments on various diffusion
models demonstrate the watermark remains verifiable even under significant
image tampering and shows superior invisibility compared to other
state-of-the-art robust watermarking methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accept to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedRISE: Rating Induced Sign Election of Gradients for Byzantine
  Tolerant Federated Aggregation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03861v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03861v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joseph Geo Benjamin, Mothilal Asokan, Mohammad Yaqub, Karthik Nandakumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the most common defense strategies against model poisoning in
federated learning is to employ a robust aggregator mechanism that makes the
training more resilient. Many of the existing Byzantine robust aggregators
provide theoretical guarantees and are empirically effective against certain
categories of attacks. However, we observe that certain high-strength attacks
can subvert the aggregator and collapse the training. In addition, most
aggregators require identifying tolerant settings to converge. Impact of
attacks becomes more pronounced when the number of Byzantines is near-majority,
and becomes harder to evade if the attacker is omniscient with access to data,
honest updates and aggregation methods. Motivated by these observations, we
develop a robust aggregator called FedRISE for cross-silo FL that is consistent
and less susceptible to poisoning updates by an omniscient attacker. The
proposed method explicitly determines the optimal direction of each gradient
through a sign-voting strategy that uses variance-reduced sparse gradients. We
argue that vote weighting based on the cosine similarity of raw gradients is
misleading, and we introduce a sign-based gradient valuation function that
ignores the gradient magnitude. We compare our method against 8 robust
aggregators under 6 poisoning attacks on 3 datasets and architectures. Our
results show that existing robust aggregators collapse for at least some
attacks under severe settings, while FedRISE demonstrates better robustness
because of a stringent gradient inclusion formulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is a work under submission/review process</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MambaPEFT: Exploring Parameter-Efficient <span class="highlight-title">Fine-Tuning</span> for Mamba 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03855v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03855v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masakazu Yoshimura, Teruaki Hayashi, Yota Maeda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An ecosystem of Transformer-based models has been established by building
large models with extensive data. Parameter-efficient fine-tuning (PEFT) is a
crucial technology for deploying these models to downstream tasks with minimal
cost while achieving effective performance. Recently, Mamba, a State Space
Model (SSM)-based model, has attracted attention as a potential alternative to
Transformers. While many large-scale Mamba-based models have been proposed,
efficiently adapting pre-trained Mamba-based models to downstream tasks remains
unexplored. In this paper, we conduct an exploratory analysis of PEFT methods
for Mamba. We investigate the effectiveness of existing PEFT methods for
Transformers when applied to Mamba. We also modify these methods to better
align with the Mamba architecture. Additionally, we propose new Mamba-specific
PEFT methods that leverage the distinctive structure of Mamba. Our experiments
indicate that PEFT performs more effectively for Mamba than Transformers.
Lastly, we demonstrate how to effectively combine multiple PEFT methods and
provide a framework that outperforms previous works. To ensure reproducibility,
we will release the code after publication.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Edge Computing-Based Solution for Real-Time Leaf Disease
  Classification using Thermal Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03835v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03835v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Públio Elon Correa da Silva, Jurandy Almeida
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning (DL) technologies can transform agriculture by improving crop
health monitoring and management, thus improving food safety. In this paper, we
explore the potential of edge computing for real-time classification of leaf
diseases using thermal imaging. We present a thermal image dataset for plant
disease classification and evaluate deep learning models, including
InceptionV3, MobileNetV1, MobileNetV2, and VGG-16, on resource-constrained
devices like the Raspberry Pi 4B. Using pruning and quantization-aware
training, these models achieve inference times up to 1.48x faster on Edge TPU
Max for VGG16, and up to 2.13x faster with precision reduction on Intel NCS2
for MobileNetV1, compared to high-end GPUs like the RTX 3090, while maintaining
state-of-the-art accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Enhancement of Haar Cascade Algorithm Applied to Face Recognition for
  Gate Pass Security 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03831v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03831v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clarence A. Antipona, Romeo R. Magsino, Raymund M. Dioses, Khatalyn E. Mata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study is focused on enhancing the Haar Cascade Algorithm to decrease the
false positive and false negative rate in face matching and face detection to
increase the accuracy rate even under challenging conditions. The face
recognition library was implemented with Haar Cascade Algorithm in which the
128-dimensional vectors representing the unique features of a face are encoded.
A subprocess was applied where the grayscale image from Haar Cascade was
converted to RGB to improve the face encoding. Logical process and face
filtering are also used to decrease non-face detection. The Enhanced Haar
Cascade Algorithm produced a 98.39% accuracy rate (21.39% increase), 63.59%
precision rate, 98.30% recall rate, and 72.23% in F1 Score. In comparison, the
Haar Cascade Algorithm achieved a 46.70% to 77.00% accuracy rate, 44.15%
precision rate, 98.61% recall rate, and 47.01% in F1 Score. Both algorithms
used the Confusion Matrix Test with 301,950 comparisons using the same dataset
of 550 images. The 98.39% accuracy rate shows a significant decrease in false
positive and false negative rates in facial recognition. Face matching and face
detection are more accurate in images with complex backgrounds, lighting
variations, and occlusions, or even those with similar attributes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalize or Detect? Towards Robust Semantic Segmentation Under
  Multiple Distribution Shifts <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03829v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03829v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhitong Gao, Bingnan Li, Mathieu Salzmann, Xuming He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In open-world scenarios, where both novel classes and domains may exist, an
ideal segmentation model should detect anomaly classes for safety and
generalize to new domains. However, existing methods often struggle to
distinguish between domain-level and semantic-level distribution shifts,
leading to poor out-of-distribution (OOD) detection or domain generalization
performance. In this work, we aim to equip the model to generalize effectively
to covariate-shift regions while precisely identifying semantic-shift regions.
To achieve this, we design a novel generative augmentation method to produce
coherent images that incorporate both anomaly (or novel) objects and various
covariate shifts at both image and object levels. Furthermore, we introduce a
training strategy that recalibrates uncertainty specifically for semantic
shifts and enhances the feature extractor to align features associated with
domain shifts. We validate the effectiveness of our method across benchmarks
featuring both semantic and domain shifts. Our method achieves state-of-the-art
performance across all benchmarks for both OOD detection and domain
generalization. Code is available at
https://github.com/gaozhitong/MultiShiftSeg.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Both Text and Images Leaked! A Systematic Analysis of <span class="highlight-title">Multimodal</span> LLM
  Data Contamination 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03823v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03823v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dingjie Song, Sicheng Lai, Shunian Chen, Lichao Sun, Benyou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid progression of multimodal large language models (MLLMs) has
demonstrated superior performance on various multimodal benchmarks. However,
the issue of data contamination during training creates challenges in
performance evaluation and comparison. While numerous methods exist for
detecting dataset contamination in large language models (LLMs), they are less
effective for MLLMs due to their various modalities and multiple training
phases. In this study, we introduce a multimodal data contamination detection
framework, MM-Detect, designed for MLLMs. Our experimental results indicate
that MM-Detect is sensitive to varying degrees of contamination and can
highlight significant performance improvements due to leakage of the training
set of multimodal benchmarks. Furthermore, We also explore the possibility of
contamination originating from the pre-training phase of LLMs used by MLLMs and
the fine-tuning phase of MLLMs, offering new insights into the stages at which
contamination may be introduced.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SA3DIP: Segment Any 3D Instance with Potential 3D Priors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03819v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03819v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xi Yang, Xu Gu, Xingyilang Yin, Xinbo Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of 2D foundation models has sparked research into adapting
them for open-world 3D instance segmentation. Recent methods introduce a
paradigm that leverages superpoints as geometric primitives and incorporates 2D
multi-view masks from Segment Anything model (SAM) as merging guidance,
achieving outstanding zero-shot instance segmentation results. However, the
limited use of 3D priors restricts the segmentation performance. Previous
methods calculate the 3D superpoints solely based on estimated normal from
spatial coordinates, resulting in under-segmentation for instances with similar
geometry. Besides, the heavy reliance on SAM and hand-crafted algorithms in 2D
space suffers from over-segmentation due to SAM's inherent part-level
segmentation tendency. To address these issues, we propose SA3DIP, a novel
method for Segmenting Any 3D Instances via exploiting potential 3D Priors.
Specifically, on one hand, we generate complementary 3D primitives based on
both geometric and textural priors, which reduces the initial errors that
accumulate in subsequent procedures. On the other hand, we introduce
supplemental constraints from the 3D space by using a 3D detector to guide a
further merging process. Furthermore, we notice a considerable portion of
low-quality ground truth annotations in ScanNetV2 benchmark, which affect the
fair evaluations. Thus, we present ScanNetV2-INS with complete ground truth
labels and supplement additional instances for 3D class-agnostic instance
segmentation. Experimental evaluations on various 2D-3D datasets demonstrate
the effectiveness and robustness of our approach. Our code and proposed
ScanNetV2-INS dataset are available HERE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GS2Pose: Tow-stage 6D Object Pose Estimation Guided by Gaussian
  Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03807v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03807v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jilan Mei, Junbo Li, Cai Meng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a new method for accurate and robust 6D pose estimation
of novel objects, named GS2Pose. By introducing 3D Gaussian splatting, GS2Pose
can utilize the reconstruction results without requiring a high-quality CAD
model, which means it only requires segmented RGBD images as input.
Specifically, GS2Pose employs a two-stage structure consisting of coarse
estimation followed by refined estimation. In the coarse stage, a lightweight
U-Net network with a polarization attention mechanism, called Pose-Net, is
designed. By using the 3DGS model for supervised training, Pose-Net can
generate NOCS images to compute a coarse pose. In the refinement stage, GS2Pose
formulates a pose regression algorithm following the idea of reprojection or
Bundle Adjustment (BA), referred to as GS-Refiner. By leveraging Lie algebra to
extend 3DGS, GS-Refiner obtains a pose-differentiable rendering pipeline that
refines the coarse pose by comparing the input images with the rendered images.
GS-Refiner also selectively updates parameters in the 3DGS model to achieve
environmental adaptation, thereby enhancing the algorithm's robustness and
flexibility to illuminative variation, occlusion, and other challenging
disruptive factors. GS2Pose was evaluated through experiments conducted on the
LineMod dataset, where it was compared with similar algorithms, yielding highly
competitive results. The code for GS2Pose will soon be released on GitHub.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VQA$^2$:Visual Question Answering for Video Quality Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03795v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03795v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziheng Jia, Zicheng Zhang, Jiaying Qian, Haoning Wu, Wei Sun, Chunyi Li, Xiaohong Liu, Weisi Lin, Guangtao Zhai, Xiongkuo Min
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent and proliferation of large multi-modal models (LMMs) have
introduced a new paradigm to video-related computer vision fields, including
training and inference methods based on visual question answering (VQA). These
methods enable models to handle multiple downstream tasks robustly. Video
Quality Assessment (VQA), a classic field in low-level visual quality
evaluation, originally focused on quantitative video quality scoring. However,
driven by advances in LMMs, it is now evolving towards more comprehensive
visual quality understanding tasks. Visual question answering has significantly
improved low-level visual evaluation within the image domain recently. However,
related work is almost nonexistent in the video domain, leaving substantial
room for improvement. To address this gap, we introduce the VQA2 Instruction
Dataset the first visual question answering instruction dataset entirely
focuses on video quality assessment, and based on it, we propose the VQA2
series models The VQA2 Instruction Dataset consists of three stages and covers
various video types, containing 157,735 instruction question-answer pairs,
including both manually annotated and synthetic data. We conduct extensive
experiments on both video quality scoring and video quality understanding
tasks. Results demonstrate that the VQA2 series models achieve state-of-the-art
(SOTA) performance in quality scoring tasks, and their performance in visual
quality question answering surpasses the renowned GPT-4o. Additionally, our
final model, the VQA2-Assistant, performs well across both scoring and
question-answering tasks, validating its versatility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Harmformer: Harmonic Networks Meet Transformers for Continuous
  Roto-Translation Equivariance <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03794v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03794v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomáš Karella, Adam Harmanec, Jan Kotera, Jan Blažek, Filip Šroubek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  CNNs exhibit inherent equivariance to image translation, leading to efficient
parameter and data usage, faster learning, and improved robustness. The concept
of translation equivariant networks has been successfully extended to rotation
transformation using group convolution for discrete rotation groups and
harmonic functions for the continuous rotation group encompassing $360^\circ$.
We explore the compatibility of the SA mechanism with full rotation
equivariance, in contrast to previous studies that focused on discrete
rotation. We introduce the Harmformer, a harmonic transformer with a
convolutional stem that achieves equivariance for both translation and
continuous rotation. Accompanied by an end-to-end equivariance proof, the
Harmformer not only outperforms previous equivariant transformers, but also
demonstrates inherent stability under any continuous rotation, even without
seeing rotated samples during training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Appears in NeurIPS 2024 Workshop on Symmetry and Geometry in Neural
  Representations</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sub-DM:Subspace Diffusion Model with Orthogonal Decomposition for MRI
  Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03758v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03758v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Guan, Qinrong Cai, Wei Li, Qiuyun Fan, Dong Liang, Qiegen Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion model-based approaches recently achieved re-markable success in MRI
reconstruction, but integration into clinical routine remains challenging due
to its time-consuming convergence. This phenomenon is partic-ularly notable
when directly apply conventional diffusion process to k-space data without
considering the inherent properties of k-space sampling, limiting k-space
learning efficiency and image reconstruction quality. To tackle these
challenges, we introduce subspace diffusion model with orthogonal
decomposition, a method (referred to as Sub-DM) that restrict the diffusion
process via projections onto subspace as the k-space data distribution evolves
toward noise. Particularly, the subspace diffusion model circumvents the
inference challenges posed by the com-plex and high-dimensional characteristics
of k-space data, so the highly compact subspace ensures that diffusion process
requires only a few simple iterations to produce accurate prior information.
Furthermore, the orthogonal decomposition strategy based on wavelet transform
hin-ders the information loss during the migration of the vanilla diffusion
process to the subspace. Considering the strate-gy is approximately reversible,
such that the entire pro-cess can be reversed. As a result, it allows the
diffusion processes in different spaces to refine models through a mutual
feedback mechanism, enabling the learning of ac-curate prior even when dealing
with complex k-space data. Comprehensive experiments on different datasets
clearly demonstrate that the superiority of Sub-DM against state of-the-art
methods in terms of reconstruction speed and quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deferred Poisoning: Making the Model More Vulnerable via Hessian
  Singularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03752v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03752v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhao He, Jinyu Tian, Xianwei Zheng, Li Dong, Yuanman Li, Leo Yu Zhang, Jiantao Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have shown that deep learning models are very vulnerable to
poisoning attacks. Many defense methods have been proposed to address this
issue. However, traditional poisoning attacks are not as threatening as
commonly believed. This is because they often cause differences in how the
model performs on the training set compared to the validation set. Such
inconsistency can alert defenders that their data has been poisoned, allowing
them to take the necessary defensive actions. In this paper, we introduce a
more threatening type of poisoning attack called the Deferred Poisoning Attack.
This new attack allows the model to function normally during the training and
validation phases but makes it very sensitive to evasion attacks or even
natural noise. We achieve this by ensuring the poisoned model's loss function
has a similar value as a normally trained model at each input sample but with a
large local curvature. A similar model loss ensures that there is no obvious
inconsistency between the training and validation accuracy, demonstrating high
stealthiness. On the other hand, the large curvature implies that a small
perturbation may cause a significant increase in model loss, leading to
substantial performance degradation, which reflects a worse robustness. We
fulfill this purpose by making the model have singular Hessian information at
the optimal point via our proposed Singularization Regularization term. We have
conducted both theoretical and empirical analyses of the proposed method and
validated its effectiveness through experiments on image classification tasks.
Furthermore, we have confirmed the hazards of this form of poisoning attack
under more general scenarios using natural noise, offering a new perspective
for research in the field of security.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Homotopy Continuation Made Easy: Regression-based Online Simulation of
  Starting Problem-Solution Pairs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03745v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03745v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyue Zhang, Zijia Dai, Wanting Xu, Laurent Kneip
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While automatically generated polynomial elimination templates have sparked
great progress in the field of 3D computer vision, there remain many problems
for which the degree of the constraints or the number of unknowns leads to
intractability. In recent years, homotopy continuation has been introduced as a
plausible alternative. However, the method currently depends on expensive
parallel tracking of all possible solutions in the complex domain, or a
classification network for starting problem-solution pairs trained over a
limited set of real-world examples. Our innovation consists of employing a
regression network trained in simulation to directly predict a solution from
input correspondences, followed by an online simulator that invents a
consistent problem-solution pair. Subsequently, homotopy continuation is
applied to track that single solution back to the original problem. We apply
this elegant combination to generalized camera resectioning, and also introduce
a new solution to the challenging generalized relative pose and scale problem.
As demonstrated, the proposed method successfully compensates the raw error
committed by the regressor alone, and leads to state-of-the-art efficiency and
success rates while running on CPU resources, only.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeurIPS 2023 Competition: Privacy Preserving Federated Learning Document
  VQA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03730v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03730v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marlon Tobaben, Mohamed Ali Souibgui, Rubèn Tito, Khanh Nguyen, Raouf Kerkouche, Kangsoo Jung, Joonas Jälkö, Lei Kang, Andrey Barsky, Vincent Poulain d'Andecy, Aurélie Joseph, Aashiq Muhamed, Kevin Kuo, Virginia Smith, Yusuke Yamasaki, Takumi Fukami, Kenta Niwa, Iifan Tyou, Hiro Ishii, Rio Yokota, Ragul N, Rintu Kutum, Josep Llados, Ernest Valveny, Antti Honkela, Mario Fritz, Dimosthenis Karatzas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Privacy Preserving Federated Learning Document VQA (PFL-DocVQA)
competition challenged the community to develop provably private and
communication-efficient solutions in a federated setting for a real-life use
case: invoice processing. The competition introduced a dataset of real invoice
documents, along with associated questions and answers requiring information
extraction and reasoning over the document images. Thereby, it brings together
researchers and expertise from the document analysis, privacy, and federated
learning communities. Participants fine-tuned a pre-trained, state-of-the-art
Document Visual Question Answering model provided by the organizers for this
new domain, mimicking a typical federated invoice processing setup. The base
model is a multi-modal generative language model, and sensitive information
could be exposed through either the visual or textual input modality.
Participants proposed elegant solutions to reduce communication costs while
maintaining a minimum utility threshold in track 1 and to protect all
information from each document provider using differential privacy in track 2.
The competition served as a new testbed for developing and testing private
federated learning methods, simultaneously raising awareness about privacy
within the document image analysis and recognition community. Ultimately, the
competition analysis provides best practices and recommendations for
successfully running privacy-focused federated learning challenges in the
future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Relation Learning and Aggregate-attention for Multi-person Motion
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03729v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03729v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kehua Qu, Rui Ding, Jin Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-person motion prediction is an emerging and intricate task with broad
real-world applications. Unlike single person motion prediction, it considers
not just the skeleton structures or human trajectories but also the
interactions between others. Previous methods use various networks to achieve
impressive predictions but often overlook that the joints relations within an
individual (intra-relation) and interactions among groups (inter-relation) are
distinct types of representations. These methods often lack explicit
representation of inter&intra-relations, and inevitably introduce undesired
dependencies. To address this issue, we introduce a new collaborative framework
for multi-person motion prediction that explicitly modeling these relations:a
GCN-based network for intra-relations and a novel reasoning network for
inter-relations.Moreover, we propose a novel plug-and-play aggregation module
called the Interaction Aggregation Module (IAM), which employs an
aggregate-attention mechanism to seamlessly integrate these relations.
Experiments indicate that the module can also be applied to other dual-path
models. Extensive experiments on the 3DPW, 3DPW-RC, CMU-Mocap, MuPoTS-3D, as
well as synthesized datasets Mix1 & Mix2 (9 to 15 persons), demonstrate that
our method achieves state-of-the-art performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE Transactions on Multimedia</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Fourier Filtering Network with Contrastive Learning for
  UAV-based Unaligned Bi-modal Salient Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03728v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03728v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengfei Lyu, Pak-Hei Yeung, Xiufei Cheng, Xiaosheng Yu, Chengdong Wu, Jagath C. Rajapakse
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unmanned aerial vehicle (UAV)-based bi-modal salient object detection (BSOD)
aims to segment salient objects in a scene utilizing complementary cues in
unaligned RGB and thermal image pairs. However, the high computational expense
of existing UAV-based BSOD models limits their applicability to real-world UAV
devices. To address this problem, we propose an efficient Fourier filter
network with contrastive learning that achieves both real-time and accurate
performance. Specifically, we first design a semantic contrastive alignment
loss to align the two modalities at the semantic level, which facilitates
mutual refinement in a parameter-free way. Second, inspired by the fast Fourier
transform that obtains global relevance in linear complexity, we propose
synchronized alignment fusion, which aligns and fuses bi-modal features in the
channel and spatial dimensions by a hierarchical filtering mechanism. Our
proposed model, AlignSal, reduces the number of parameters by 70.0%, decreases
the floating point operations by 49.4%, and increases the inference speed by
152.5% compared to the cutting-edge BSOD model (i.e., MROS). Extensive
experiments on the UAV RGB-T 2400 and three weakly aligned datasets demonstrate
that AlignSal achieves both real-time inference speed and better performance
and generalizability compared to sixteen state-of-the-art BSOD models across
most evaluation metrics. In addition, our ablation studies further verify
AlignSal's potential in boosting the performance of existing aligned BSOD
models on UAV-based unaligned data. The code is available at:
https://github.com/JoshuaLPF/AlignSal.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PX2Tooth: Reconstructing the 3D Point Cloud Teeth from a Single
  Panoramic X-ray 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03725v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03725v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wen Ma, Huikai Wu, Zikai Xiao, Yang Feng, Jian Wu, Zuozhu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing the 3D anatomical structures of the oral cavity, which
originally reside in the cone-beam CT (CBCT), from a single 2D Panoramic
X-ray(PX) remains a critical yet challenging task, as it can effectively reduce
radiation risks and treatment costs during the diagnostic in digital dentistry.
However, current methods are either error-prone or only trained/evaluated on
small-scale datasets (less than 50 cases), resulting in compromised
trustworthiness. In this paper, we propose PX2Tooth, a novel approach to
reconstruct 3D teeth using a single PX image with a two-stage framework. First,
we design the PXSegNet to segment the permanent teeth from the PX images,
providing clear positional, morphological, and categorical information for each
tooth. Subsequently, we design a novel tooth generation network (TGNet) that
learns to transform random point clouds into 3D teeth. TGNet integrates the
segmented patch information and introduces a Prior Fusion Module (PFM) to
enhance the generation quality, especially in the root apex region. Moreover,
we construct a dataset comprising 499 pairs of CBCT and Panoramic X-rays.
Extensive experiments demonstrate that PX2Tooth can achieve an Intersection
over Union (IoU) of 0.793, significantly surpassing previous methods,
underscoring the great potential of artificial intelligence in digital
dentistry.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ma W, Wu H, Xiao Z, et al. PX2Tooth: Reconstructing the 3D Point
  Cloud Teeth from a Single Panoramic X-Ray[C]//International Conference on
  Medical Image Computing and Computer-Assisted Intervention. Cham: Springer
  Nature Switzerland, 2024: 411-421</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Estimation of Psychosocial Work Environment Exposures Through Video
  Object Detection. Proof of Concept Using CCTV Footage 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03724v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03724v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Claus D. Hansen, Thuy Hai Le, David Campos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper examines the use of computer vision algorithms to estimate aspects
of the psychosocial work environment using CCTV footage. We present a proof of
concept for a methodology that detects and tracks people in video footage and
estimates interactions between customers and employees by estimating their
poses and calculating the duration of their encounters. We propose a pipeline
that combines existing object detection and tracking algorithms (YOLOv8 and
DeepSORT) with pose estimation algorithms (BlazePose) to estimate the number of
customers and employees in the footage as well as the duration of their
encounters. We use a simple rule-based approach to classify the interactions as
positive, neutral or negative based on three different criteria: distance,
duration and pose. The proposed methodology is tested on a small dataset of
CCTV footage. While the data is quite limited in particular with respect to the
quality of the footage, we have chosen this case as it represents a typical
setting where the method could be applied. The results show that the object
detection and tracking part of the pipeline has a reasonable performance on the
dataset with a high degree of recall and reasonable accuracy. At this stage,
the pose estimation is still limited to fully detect the type of interactions
due to difficulties in tracking employees in the footage. We conclude that the
method is a promising alternative to self-reported measures of the psychosocial
work environment and could be used in future studies to obtain external
observations of the work environment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 9 figures, presented at IWOAR 9th International Workshop on
  Sensor-Based Activity Recognition and Artificial Intelligence, September
  26-27, Potsdam, Germany</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zero-shot Dynamic MRI Reconstruction with Global-to-local Diffusion
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03723v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03723v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Guan, Kunlong Zhang, Qi Qi, Dong Wang, Ziwen Ke, Shaoyu Wang, Dong Liang, Qiegen Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have recently demonstrated considerable advancement in the
generation and reconstruction of magnetic resonance imaging (MRI) data. These
models exhibit great potential in handling unsampled data and reducing noise,
highlighting their promise as generative models. However, their application in
dynamic MRI remains relatively underexplored. This is primarily due to the
substantial amount of fully-sampled data typically required for training, which
is difficult to obtain in dynamic MRI due to its spatio-temporal complexity and
high acquisition costs. To address this challenge, we propose a dynamic MRI
reconstruction method based on a time-interleaved acquisition scheme, termed
the Glob-al-to-local Diffusion Model. Specifically, fully encoded
full-resolution reference data are constructed by merging under-sampled k-space
data from adjacent time frames, generating two distinct bulk training datasets
for global and local models. The global-to-local diffusion framework
alternately optimizes global information and local image details, enabling
zero-shot reconstruction. Extensive experiments demonstrate that the proposed
method performs well in terms of noise reduction and detail preservation,
achieving reconstruction quality comparable to that of supervised approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ These Maps Are Made by Propagation: Adapting Deep Stereo Networks to
  Road Scenarios with Decisive Disparity Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03717v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03717v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuang-Wei Liu, Yikang Zhang, Qijun Chen, Ioannis Pitas, Rui Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stereo matching has emerged as a cost-effective solution for road surface 3D
reconstruction, garnering significant attention towards improving both
computational efficiency and accuracy. This article introduces decisive
disparity diffusion (D3Stereo), marking the first exploration of dense deep
feature matching that adapts pre-trained deep convolutional neural networks
(DCNNs) to previously unseen road scenarios. A pyramid of cost volumes is
initially created using various levels of learned representations.
Subsequently, a novel recursive bilateral filtering algorithm is employed to
aggregate these costs. A key innovation of D3Stereo lies in its alternating
decisive disparity diffusion strategy, wherein intra-scale diffusion is
employed to complete sparse disparity images, while inter-scale inheritance
provides valuable prior information for higher resolutions. Extensive
experiments conducted on our created UDTIRI-Stereo and Stereo-Road datasets
underscore the effectiveness of D3Stereo strategy in adapting pre-trained DCNNs
and its superior performance compared to all other explicit programming-based
algorithms designed specifically for road surface 3D reconstruction. Additional
experiments conducted on the Middlebury dataset with backbone DCNNs pre-trained
on the ImageNet database further validate the versatility of D3Stereo strategy
in tackling general stereo matching problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explaining Human Activity Recognition with SHAP: Validating Insights
  with Perturbation and Quantitative Measures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03714v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03714v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Tempel, Espen Alexander F. Ihlen, Lars Adde, Inga Strümke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Human Activity Recognition (HAR), understanding the intricacy of body
movements within high-risk applications is essential. This study uses SHapley
Additive exPlanations (SHAP) to explain the decision-making process of Graph
Convolution Networks (GCNs) when classifying activities with skeleton data. We
employ SHAP to explain two real-world datasets: one for cerebral palsy (CP)
classification and the widely used NTU RGB+D 60 action recognition dataset. To
test the explanation, we introduce a novel perturbation approach that modifies
the model's edge importance matrix, allowing us to evaluate the impact of
specific body key points on prediction outcomes. To assess the fidelity of our
explanations, we employ informed perturbation, targeting body key points
identified as important by SHAP and comparing them against random perturbation
as a control condition. This perturbation enables a judgment on whether the
body key points are truly influential or non-influential based on the SHAP
values. Results on both datasets show that body key points identified as
important through SHAP have the largest influence on the accuracy, specificity,
and sensitivity metrics. Our findings highlight that SHAP can provide granular
insights into the input feature contribution to the prediction outcome of GCNs
in HAR tasks. This demonstrates the potential for more interpretable and
trustworthy models in high-stakes applications like healthcare or
rehabilitation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Fine-Tuning</span> <span class="highlight-title">Vision-Language</span> Model for Automated Engineering Drawing
  Information Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03707v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03707v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Tayyab Khan, Lequn Chen, Ye Han Ng, Wenhe Feng, Nicholas Yew Jin Tan, Seung Ki Moon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Geometric Dimensioning and Tolerancing (GD&T) plays a critical role in
manufacturing by defining acceptable variations in part features to ensure
component quality and functionality. However, extracting GD&T information from
2D engineering drawings is a time-consuming and labor-intensive task, often
relying on manual efforts or semi-automated tools. To address these challenges,
this study proposes an automated and computationally efficient GD&T extraction
method by fine-tuning Florence-2, an open-source vision-language model (VLM).
The model is trained on a dataset of 400 drawings with ground truth annotations
provided by domain experts. For comparison, two state-of-the-art closed-source
VLMs, GPT-4o and Claude-3.5-Sonnet, are evaluated on the same dataset. All
models are assessed using precision, recall, F1-score, and hallucination
metrics. Due to the computational cost and impracticality of fine-tuning large
closed-source VLMs for domain-specific tasks, GPT-4o and Claude-3.5-Sonnet are
evaluated in a zero-shot setting. In contrast, Florence-2, a smaller model with
0.23 billion parameters, is optimized through full-parameter fine-tuning across
three distinct experiments, each utilizing datasets augmented to different
levels. The results show that Florence-2 achieves a 29.95% increase in
precision, a 37.75% increase in recall, a 52.40% improvement in F1-score, and a
43.15% reduction in hallucination rate compared to the best-performing
closed-source model. These findings highlight the effectiveness of fine-tuning
smaller, open-source VLMs like Florence-2, offering a practical and efficient
solution for automated GD&T extraction to support downstream manufacturing
tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper has been submitted to the 9th International Conference on
  Innovation in Artificial Intelligence (ICIAI 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3DGS-CD: 3D Gaussian Splatting-based Change Detection for Physical
  Object Rearrangement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03706v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03706v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqi Lu, Jianbo Ye, John Leonard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present 3DGS-CD, the first 3D Gaussian Splatting (3DGS)-based method for
detecting physical object rearrangements in 3D scenes. Our approach estimates
3D object-level changes by comparing two sets of unaligned images taken at
different times. Leveraging 3DGS's novel view rendering and EfficientSAM's
zero-shot segmentation capabilities, we detect 2D object-level changes, which
are then associated and fused across views to estimate 3D changes. Our method
can detect changes in cluttered environments using sparse post-change images
within as little as 18s, using as few as a single new image. It does not rely
on depth input, user instructions, object classes, or object models -- An
object is recognized simply if it has been re-arranged. Our approach is
evaluated on both public and self-collected real-world datasets, achieving up
to 14% higher accuracy and three orders of magnitude faster performance
compared to the state-of-the-art radiance-field-based change detection method.
This significant performance boost enables a broad range of downstream
applications, where we highlight three key use cases: object reconstruction,
robot workspace reset, and 3DGS model update. Our code and data will be made
available at https://github.com/520xyxyzq/3DGS-CD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph-Based <span class="highlight-title">Multi-Modal</span> Sensor Fusion for Autonomous Driving <span class="chip">ICPR'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03702v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03702v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Depanshu Sani, Saket Anand
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing demand for robust scene understanding in mobile robotics and
autonomous driving has highlighted the importance of integrating multiple
sensing modalities. By combining data from diverse sensors like cameras and
LIDARs, fusion techniques can overcome the limitations of individual sensors,
enabling a more complete and accurate perception of the environment. We
introduce a novel approach to multi-modal sensor fusion, focusing on developing
a graph-based state representation that supports critical decision-making
processes in autonomous driving. We present a Sensor-Agnostic Graph-Aware
Kalman Filter [3], the first online state estimation technique designed to fuse
multi-modal graphs derived from noisy multi-sensor data. The estimated
graph-based state representations serve as a foundation for advanced
applications like Multi-Object Tracking (MOT), offering a comprehensive
framework for enhancing the situational awareness and safety of autonomous
systems. We validate the effectiveness of our proposed framework through
extensive experiments conducted on both synthetic and real-world driving
datasets (nuScenes). Our results showcase an improvement in MOTA and a
reduction in estimated position errors (MOTP) and identity switches (IDS) for
tracked objects using the SAGA-KF. Furthermore, we highlight the capability of
such a framework to develop methods that can leverage heterogeneous information
(like semantic objects and geometric structures) from various sensing
modalities, enabling a more holistic approach to scene understanding and
enhancing the safety and effectiveness of autonomous systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>An extended abstract accepted at Young Researchers' Symposium, ICVGIP
  '24. This extended abstract contains the following: 1. Short summary of our
  work, SAGA-KF, accepted at ICPR'24. 2. A proposal that was awarded the
  Qualcomm Innovation Fellowship'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OccLoff: Learning Optimized Feature Fusion for 3D Occupancy Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03696v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03696v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ji Zhang, Yiran Ding, Zixin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D semantic occupancy prediction is crucial for finely representing the
surrounding environment, which is essential for ensuring the safety in
autonomous driving. Existing fusion-based occupancy methods typically involve
performing a 2D-to-3D view transformation on image features, followed by
computationally intensive 3D operations to fuse these with LiDAR features,
leading to high computational costs and reduced accuracy. Moreover, current
research on occupancy prediction predominantly focuses on designing specific
network architectures, often tailored to particular models, with limited
attention given to the more fundamental aspect of semantic feature learning.
This gap hinders the development of more transferable methods that could
enhance the performance of various occupancy models. To address these
challenges, we propose OccLoff, a framework that Learns to Optimize Feature
Fusion for 3D occupancy prediction. Specifically, we introduce a sparse fusion
encoder with entropy masks that directly fuses 3D and 2D features, improving
model accuracy while reducing computational overhead. Additionally, we propose
a transferable proxy-based loss function and an adaptive hard sample weighting
algorithm, which enhance the performance of several state-of-the-art methods.
Extensive evaluations on the nuScenes and SemanticKITTI benchmarks demonstrate
the superiority of our framework, and ablation studies confirm the
effectiveness of each proposed module.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AMNCutter: Affinity-Attention-Guided Multi-View Normalized Cutter for
  Unsupervised Surgical Instrument Segmentation <span class="chip">WACV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyu Sheng, Jianan Fan, Dongnan Liu, Ron Kikinis, Weidong Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surgical instrument segmentation (SIS) is pivotal for robotic-assisted
minimally invasive surgery, assisting surgeons by identifying surgical
instruments in endoscopic video frames. Recent unsupervised surgical instrument
segmentation (USIS) methods primarily rely on pseudo-labels derived from
low-level features such as color and optical flow, but these methods show
limited effectiveness and generalizability in complex and unseen endoscopic
scenarios. In this work, we propose a label-free unsupervised model featuring a
novel module named Multi-View Normalized Cutter (m-NCutter). Different from
previous USIS works, our model is trained using a graph-cutting loss function
that leverages patch affinities for supervision, eliminating the need for
pseudo-labels. The framework adaptively determines which affinities from which
levels should be prioritized. Therefore, the low- and high-level features and
their affinities are effectively integrated to train a label-free unsupervised
model, showing superior effectiveness and generalization ability. We conduct
comprehensive experiments across multiple SIS datasets to validate our
approach's state-of-the-art (SOTA) performance, robustness, and exceptional
potential as a pre-trained model. Our code is released at
https://github.com/MingyuShengSMY/AMNCutter.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper was accepted by the 2025 IEEE Winter Conference on
  Applications of Computer Vision (WACV)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Where Do We Stand with Implicit Neural Representations? A Technical and
  Performance Survey 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03688v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03688v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amer Essakine, Yanqi Cheng, Chun-Wun Cheng, Lipei Zhang, Zhongying Deng, Lei Zhu, Carola-Bibiane Schönlieb, Angelica I Aviles-Rivero
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Implicit Neural Representations (INRs) have emerged as a paradigm in
knowledge representation, offering exceptional flexibility and performance
across a diverse range of applications. INRs leverage multilayer perceptrons
(MLPs) to model data as continuous implicit functions, providing critical
advantages such as resolution independence, memory efficiency, and
generalisation beyond discretised data structures. Their ability to solve
complex inverse problems makes them particularly effective for tasks including
audio reconstruction, image representation, 3D object reconstruction, and
high-dimensional data synthesis. This survey provides a comprehensive review of
state-of-the-art INR methods, introducing a clear taxonomy that categorises
them into four key areas: activation functions, position encoding, combined
strategies, and network structure optimisation. We rigorously analyse their
critical properties, such as full differentiability, smoothness, compactness,
and adaptability to varying resolutions while also examining their strengths
and limitations in addressing locality biases and capturing fine details. Our
experimental comparison offers new insights into the trade-offs between
different approaches, showcasing the capabilities and challenges of the latest
INR techniques across various tasks. In addition to identifying areas where
current methods excel, we highlight key limitations and potential avenues for
improvement, such as developing more expressive activation functions, enhancing
positional encoding mechanisms, and improving scalability for complex,
high-dimensional data. This survey serves as a roadmap for researchers,
offering practical guidance for future exploration in the field of INRs. We aim
to foster new methodologies by outlining promising research directions for INRs
and applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards 3D Semantic Scene Completion for Autonomous Driving: A
  Meta-Learning Framework Empowered by Deformable Large-Kernel Attention and
  Mamba Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03672v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03672v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yansong Qu, Zilin Huang, Zihao Sheng, Tiantian Chen, Sikai Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic scene completion (SSC) is essential for achieving comprehensive
perception in autonomous driving systems. However, existing SSC methods often
overlook the high deployment costs in real-world applications. Traditional
architectures, such as 3D Convolutional Neural Networks (3D CNNs) and
self-attention mechanisms, face challenges in efficiently capturing long-range
dependencies within 3D voxel grids, limiting their effectiveness. To address
these issues, we introduce MetaSSC, a novel meta-learning-based framework for
SSC that leverages deformable convolution, large-kernel attention, and the
Mamba (D-LKA-M) model. Our approach begins with a voxel-based semantic
segmentation (SS) pretraining task, aimed at exploring the semantics and
geometry of incomplete regions while acquiring transferable meta-knowledge.
Using simulated cooperative perception datasets, we supervise the perception
training of a single vehicle using aggregated sensor data from multiple nearby
connected autonomous vehicles (CAVs), generating richer and more comprehensive
labels. This meta-knowledge is then adapted to the target domain through a
dual-phase training strategy that does not add extra model parameters, enabling
efficient deployment. To further enhance the model's capability in capturing
long-sequence relationships within 3D voxel grids, we integrate Mamba blocks
with deformable convolution and large-kernel attention into the backbone
network. Extensive experiments demonstrate that MetaSSC achieves
state-of-the-art performance, significantly outperforming competing models
while also reducing deployment costs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Touchstone Benchmark: Are We on the Right Way for Evaluating AI
  Algorithms for Medical Segmentation? <span class="chip">NeurIPS-2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03670v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03670v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro R. A. S. Bassi, Wenxuan Li, Yucheng Tang, Fabian Isensee, Zifu Wang, Jieneng Chen, Yu-Cheng Chou, Yannick Kirchhoff, Maximilian Rokuss, Ziyan Huang, Jin Ye, Junjun He, Tassilo Wald, Constantin Ulrich, Michael Baumgartner, Saikat Roy, Klaus H. Maier-Hein, Paul Jaeger, Yiwen Ye, Yutong Xie, Jianpeng Zhang, Ziyang Chen, Yong Xia, Zhaohu Xing, Lei Zhu, Yousef Sadegheih, Afshin Bozorgpour, Pratibha Kumari, Reza Azad, Dorit Merhof, Pengcheng Shi, Ting Ma, Yuxin Du, Fan Bai, Tiejun Huang, Bo Zhao, Haonan Wang, Xiaomeng Li, Hanxue Gu, Haoyu Dong, Jichen Yang, Maciej A. Mazurowski, Saumya Gupta, Linshan Wu, Jiaxin Zhuang, Hao Chen, Holger Roth, Daguang Xu, Matthew B. Blaschko, Sergio Decherchi, Andrea Cavalli, Alan L. Yuille, Zongwei Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How can we test AI performance? This question seems trivial, but it isn't.
Standard benchmarks often have problems such as in-distribution and small-size
test sets, oversimplified metrics, unfair comparisons, and short-term outcome
pressure. As a consequence, good performance on standard benchmarks does not
guarantee success in real-world scenarios. To address these problems, we
present Touchstone, a large-scale collaborative segmentation benchmark of 9
types of abdominal organs. This benchmark is based on 5,195 training CT scans
from 76 hospitals around the world and 5,903 testing CT scans from 11
additional hospitals. This diverse test set enhances the statistical
significance of benchmark results and rigorously evaluates AI algorithms across
various out-of-distribution scenarios. We invited 14 inventors of 19 AI
algorithms to train their algorithms, while our team, as a third party,
independently evaluated these algorithms on three test sets. In addition, we
also evaluated pre-existing AI frameworks--which, differing from algorithms,
are more flexible and can support different algorithms--including MONAI from
NVIDIA, nnU-Net from DKFZ, and numerous other open-source frameworks. We are
committed to expanding this benchmark to encourage more innovation of AI
algorithms for the medical domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS-2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Stereo Depth Estimation with Multi-Spectral Images Across All
  Lighting Conditions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03638v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03638v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihan Qin, Jialei Xu, Wenbo Zhao, Junjun Jiang, Xianming Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Depth estimation under adverse conditions remains a significant challenge.
Recently, multi-spectral depth estimation, which integrates both visible light
and thermal images, has shown promise in addressing this issue. However,
existing algorithms struggle with precise pixel-level feature matching,
limiting their ability to fully exploit geometric constraints across different
spectra. To address this, we propose a novel framework incorporating stereo
depth estimation to enforce accurate geometric constraints. In particular, we
treat the visible light and thermal images as a stereo pair and utilize a
Cross-modal Feature Matching (CFM) Module to construct a cost volume for
pixel-level matching. To mitigate the effects of poor lighting on stereo
matching, we introduce Degradation Masking, which leverages robust monocular
thermal depth estimation in degraded regions. Our method achieves
state-of-the-art (SOTA) performance on the Multi-Spectral Stereo (MS2) dataset,
with qualitative evaluations demonstrating high-quality depth maps under
varying lighting conditions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Structure Consistent Gaussian Splatting with Matching Prior for Few-shot
  Novel View Synthesis <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03637v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03637v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Peng, Wangze Xu, Luyang Tang, Liwei Liao, Jianbo Jiao, Ronggang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the substantial progress of novel view synthesis, existing methods,
either based on the Neural Radiance Fields (NeRF) or more recently 3D Gaussian
Splatting (3DGS), suffer significant degradation when the input becomes sparse.
Numerous efforts have been introduced to alleviate this problem, but they still
struggle to synthesize satisfactory results efficiently, especially in the
large scene. In this paper, we propose SCGaussian, a Structure Consistent
Gaussian Splatting method using matching priors to learn 3D consistent scene
structure. Considering the high interdependence of Gaussian attributes, we
optimize the scene structure in two folds: rendering geometry and, more
importantly, the position of Gaussian primitives, which is hard to be directly
constrained in the vanilla 3DGS due to the non-structure property. To achieve
this, we present a hybrid Gaussian representation. Besides the ordinary
non-structure Gaussian primitives, our model also consists of ray-based
Gaussian primitives that are bound to matching rays and whose optimization of
their positions is restricted along the ray. Thus, we can utilize the matching
correspondence to directly enforce the position of these Gaussian primitives to
converge to the surface points where rays intersect. Extensive experiments on
forward-facing, surrounding, and complex large scenes show the effectiveness of
our approach with state-of-the-art performance and high efficiency. Code is
available at https://github.com/prstrive/SCGaussian.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 Accepted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ StreamingBench: Assessing the Gap for MLLMs to Achieve Streaming Video
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03628v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03628v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junming Lin, Zheng Fang, Chi Chen, Zihao Wan, Fuwen Luo, Peng Li, Yang Liu, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of Multimodal Large Language Models (MLLMs) has
expanded their capabilities from image comprehension to video understanding.
However, most of these MLLMs focus primarily on offline video comprehension,
necessitating extensive processing of all video frames before any queries can
be made. This presents a significant gap compared to the human ability to
watch, listen, think, and respond to streaming inputs in real time,
highlighting the limitations of current MLLMs. In this paper, we introduce
StreamingBench, the first comprehensive benchmark designed to evaluate the
streaming video understanding capabilities of MLLMs. StreamingBench assesses
three core aspects of streaming video understanding: (1) real-time visual
understanding, (2) omni-source understanding, and (3) contextual understanding.
The benchmark consists of 18 tasks, featuring 900 videos and 4,500
human-curated QA pairs. Each video features five questions presented at
different time points to simulate a continuous streaming scenario. We conduct
experiments on StreamingBench with 13 open-source and proprietary MLLMs and
find that even the most advanced proprietary MLLMs like Gemini 1.5 Pro and
GPT-4o perform significantly below human-level streaming video understanding
capabilities. We hope our work can facilitate further advancements for MLLMs,
empowering them to approach human-level video comprehension and interaction in
more realistic scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross Feature Fusion of Fundus Image and Generated Lesion Map for
  Referable Diabetic Retinopathy Classification <span class="chip">ACCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03618v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03618v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dahyun Mok, Junghyun Bum, Le Duc Tai, Hyunseung Choo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diabetic Retinopathy (DR) is a primary cause of blindness, necessitating
early detection and diagnosis. This paper focuses on referable DR
classification to enhance the applicability of the proposed method in clinical
practice. We develop an advanced cross-learning DR classification method
leveraging transfer learning and cross-attention mechanisms. The proposed
method employs the Swin U-Net architecture to segment lesion maps from DR
fundus images. The Swin U-Net segmentation model, enriched with DR lesion
insights, is transferred to generate a lesion map. Both the fundus image and
its segmented lesion map are used as complementary inputs for the
classification model. A cross-attention mechanism is deployed to improve the
model's ability to capture fine-grained details from the input pairs. Our
experiments, utilizing two public datasets, FGADR and EyePACS, demonstrate a
superior accuracy of 94.6%, surpassing current state-of-the-art methods by
4.4%. To this end, we aim for the proposed method to be seamlessly integrated
into clinical workflows, enhancing accuracy and efficiency in identifying
referable DR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACCV 2024 accepted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ADMIRE: a locally adaptive single-image, non-uniformity correction and
  denoising algorithm: application to uncooled IR camera 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03615v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03615v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yohann Tendero, Jerome Gilles
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new way to correct for the non-uniformity (NU) and the noise in
uncooled infrared-type images. This method works on static images, needs no
registration, no camera motion and no model for the non uniformity. The
proposed method uses an hybrid scheme including an automatic locally-adaptive
contrast adjustment and a state-of-the-art image denoising method. It permits
to correct for a fully non-linear NU and the noise efficiently using only one
image. We compared it with total variation on real raw and simulated NU
infrared images. The strength of this approach lies in its simplicity, low
computational cost. It needs no test-pattern or calibration and produces no
"ghost-artefact".
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LCP-Fusion: A Neural Implicit SLAM with Enhanced Local Constraints and
  Computable Prior <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03610v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03610v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahui Wang, Yinan Deng, Yi Yang, Yufeng Yue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently the dense Simultaneous Localization and Mapping (SLAM) based on
neural implicit representation has shown impressive progress in hole filling
and high-fidelity mapping. Nevertheless, existing methods either heavily rely
on known scene bounds or suffer inconsistent reconstruction due to drift in
potential loop-closure regions, or both, which can be attributed to the
inflexible representation and lack of local constraints. In this paper, we
present LCP-Fusion, a neural implicit SLAM system with enhanced local
constraints and computable prior, which takes the sparse voxel octree structure
containing feature grids and SDF priors as hybrid scene representation,
enabling the scalability and robustness during mapping and tracking. To enhance
the local constraints, we propose a novel sliding window selection strategy
based on visual overlap to address the loop-closure, and a practical warping
loss to constrain relative poses. Moreover, we estimate SDF priors as coarse
initialization for implicit features, which brings additional explicit
constraints and robustness, especially when a light but efficient adaptive
early ending is adopted. Experiments demonstrate that our method achieve better
localization accuracy and reconstruction consistency than existing RGB-D
implicit SLAM, especially in challenging real scenes (ScanNet) as well as
self-captured scenes with unknown scene bounds. The code is available at
https://github.com/laliwang/LCP-Fusion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 2024 IEEE/RSJ International Conference on Intelligent
  Robots and Systems (IROS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hybrid Attention for Robust RGB-T Pedestrian Detection in Real-World
  Conditions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03576v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03576v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arunkumar Rathinam, Leo Pauly, Abd El Rahman Shabayek, Wassim Rharbaoui, Anis Kacem, Vincent Gaudillière, Djamila Aouada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multispectral pedestrian detection has gained significant attention in recent
years, particularly in autonomous driving applications. To address the
challenges posed by adversarial illumination conditions, the combination of
thermal and visible images has demonstrated its advantages. However, existing
fusion methods rely on the critical assumption that the RGB-Thermal (RGB-T)
image pairs are fully overlapping. These assumptions often do not hold in
real-world applications, where only partial overlap between images can occur
due to sensors configuration. Moreover, sensor failure can cause loss of
information in one modality. In this paper, we propose a novel module called
the Hybrid Attention (HA) mechanism as our main contribution to mitigate
performance degradation caused by partial overlap and sensor failure, i.e. when
at least part of the scene is acquired by only one sensor. We propose an
improved RGB-T fusion algorithm, robust against partial overlap and sensor
failure encountered during inference in real-world applications. We also
leverage a mobile-friendly backbone to cope with resource constraints in
embedded systems. We conducted experiments by simulating various partial
overlap and sensor failure scenarios to evaluate the performance of our
proposed method. The results demonstrate that our approach outperforms
state-of-the-art methods, showcasing its superiority in handling real-world
challenges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in IEEE Robotics and Automation Letters,
  October 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Personalized Federated Learning via Comprehensive Knowledge
  Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03569v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03569v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengju Wang, Bochao Liu, Weijia Guo, Yong Li, Shiming Ge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning is a distributed machine learning paradigm designed to
protect data privacy. However, data heterogeneity across various clients
results in catastrophic forgetting, where the model rapidly forgets previous
knowledge while acquiring new knowledge. To address this challenge,
personalized federated learning has emerged to customize a personalized model
for each client. However, the inherent limitation of this mechanism is its
excessive focus on personalization, potentially hindering the generalization of
those models. In this paper, we present a novel personalized federated learning
method that uses global and historical models as teachers and the local model
as the student to facilitate comprehensive knowledge distillation. The
historical model represents the local model from the last round of client
training, containing historical personalized knowledge, while the global model
represents the aggregated model from the last round of server aggregation,
containing global generalized knowledge. By applying knowledge distillation, we
effectively transfer global generalized knowledge and historical personalized
knowledge to the local model, thus mitigating catastrophic forgetting and
enhancing the general performance of personalized models. Extensive
experimental results demonstrate the significant advantages of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE SMC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The American Sign Language Knowledge Graph: Infusing ASL Models with
  Linguistic Knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03568v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03568v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lee Kezar, Nidhi Munikote, Zian Zeng, Zed Sehyr, Naomi Caselli, Jesse Thomason
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models for American Sign Language (ASL) could make language
technologies substantially more accessible to those who sign. To train models
on tasks such as isolated sign recognition (ISR) and ASL-to-English
translation, datasets provide annotated video examples of ASL signs. To
facilitate the generalizability and explainability of these models, we
introduce the American Sign Language Knowledge Graph (ASLKG), compiled from
twelve sources of expert linguistic knowledge. We use the ASLKG to train
neuro-symbolic models for 3 ASL understanding tasks, achieving accuracies of
91% on ISR, 14% for predicting the semantic features of unseen signs, and 36%
for classifying the topic of Youtube-ASL videos.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unfair Alignment: Examining Safety Alignment Across Vision Encoder
  Layers in <span class="highlight-title">Vision-Language</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04291v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04291v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saketh Bachu, Erfan Shayegani, Trishna Chakraborty, Rohit Lal, Arindam Dutta, Chengyu Song, Yue Dong, Nael Abu-Ghazaleh, Amit K. Roy-Chowdhury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models (VLMs) have improved significantly in multi-modal
tasks, but their more complex architecture makes their safety alignment more
challenging than the alignment of large language models (LLMs). In this paper,
we reveal an unfair distribution of safety across the layers of VLM's vision
encoder, with earlier and middle layers being disproportionately vulnerable to
malicious inputs compared to the more robust final layers. This 'cross-layer'
vulnerability stems from the model's inability to generalize its safety
training from the default architectural settings used during training to unseen
or out-of-distribution scenarios, leaving certain layers exposed. We conduct a
comprehensive analysis by projecting activations from various intermediate
layers and demonstrate that these layers are more likely to generate harmful
outputs when exposed to malicious inputs. Our experiments with LLaVA-1.5 and
Llama 3.2 show discrepancies in attack success rates and toxicity scores across
layers, indicating that current safety alignment strategies focused on a single
default layer are insufficient.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint, Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Increasing the scalability of graph convolution for FPGA-implemented
  event-based vision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04269v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04269v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Piotr Wzorek, Kamil Jeziorek, Tomasz Kryjak, Andrea Pinna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event cameras are becoming increasingly popular as an alternative to
traditional frame-based vision sensors, especially in mobile robotics. Taking
full advantage of their high temporal resolution, high dynamic range, low power
consumption and sparsity of event data, which only reflects changes in the
observed scene, requires both an efficient algorithm and a specialised hardware
platform. A recent trend involves using Graph Convolutional Neural Networks
(GCNNs) implemented on a heterogeneous SoC FPGA. In this paper we focus on
optimising hardware modules for graph convolution to allow flexible selection
of the FPGA resource (BlockRAM, DSP and LUT) for their implementation. We
propose a ''two-step convolution'' approach that utilises additional BRAM
buffers in order to reduce up to 94% of LUT usage for multiplications. This
method significantly improves the scalability of GCNNs, enabling the deployment
of models with more layers, larger graphs sizes and their application for more
dynamic scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for the PhD forum during FPT 2024 (International Conference
  on Field Programmable Technology), 10-12 December 2024, Sydney, Australia</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Object Recognition in Human Computer Interaction:- A Comparative
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04263v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04263v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaushik Ranade, Tanmay Khule, Riddhi More
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human-computer interaction (HCI) has been a widely researched area for many
years, with continuous advancements in technology leading to the development of
new techniques that change the way we interact with computers. With the recent
advent of powerful computers, we recognize human actions and interact
accordingly, thus revolutionizing the way we interact with computers. The
purpose of this paper is to provide a comparative analysis of various
algorithms used for recognizing user faces and gestures in the context of
computer vision and HCI. This study aims to explore and evaluate the
performance of different algorithms in terms of accuracy, robustness, and
efficiency. This study aims to provide a comprehensive analysis of algorithms
for face and gesture recognition in the context of computer vision and HCI,
with the goal of improving the design and development of interactive systems
that are more intuitive, efficient, and user-friendly.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pose-Transformation and Radial Distance Clustering for Unsupervised
  Person Re-identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04255v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04255v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddharth Seth, Akash Sonth, Anirban Chakraborty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Person re-identification (re-ID) aims to tackle the problem of matching
identities across non-overlapping cameras. Supervised approaches require
identity information that may be difficult to obtain and are inherently biased
towards the dataset they are trained on, making them unscalable across domains.
To overcome these challenges, we propose an unsupervised approach to the person
re-ID setup. Having zero knowledge of true labels, our proposed method enhances
the discriminating ability of the learned features via a novel two-stage
training strategy. The first stage involves training a deep network on an
expertly designed pose-transformed dataset obtained by generating multiple
perturbations for each original image in the pose space. Next, the network
learns to map similar features closer in the feature space using the proposed
discriminative clustering algorithm. We introduce a novel radial distance loss,
that attends to the fundamental aspects of feature learning - compact clusters
with low intra-cluster and high inter-cluster variation. Extensive experiments
on several large-scale re-ID datasets demonstrate the superiority of our method
compared to state-of-the-art approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PocoLoco: A Point Cloud Diffusion Model of Human Shape in Loose Clothing <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04249v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04249v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddharth Seth, Rishabh Dabral, Diogo Luvizon, Marc Habermann, Ming-Hsuan Yang, Christian Theobalt, Adam Kortylewski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling a human avatar that can plausibly deform to articulations is an
active area of research. We present PocoLoco -- the first template-free,
point-based, pose-conditioned generative model for 3D humans in loose clothing.
We motivate our work by noting that most methods require a parametric model of
the human body to ground pose-dependent deformations. Consequently, they are
restricted to modeling clothing that is topologically similar to the naked body
and do not extend well to loose clothing. The few methods that attempt to model
loose clothing typically require either canonicalization or a
UV-parameterization and need to address the challenging problem of explicitly
estimating correspondences for the deforming clothes. In this work, we
formulate avatar clothing deformation as a conditional point-cloud generation
task within the denoising diffusion framework. Crucially, our framework
operates directly on unordered point clouds, eliminating the need for a
parametric model or a clothing template. This also enables a variety of
practical applications, such as point-cloud completion and pose-based editing
-- important features for virtual human animation. As current datasets for
human avatars in loose clothing are far too small for training diffusion
models, we release a dataset of two subjects performing various poses in loose
clothing with a total of 75K point clouds. By contributing towards tackling the
challenging task of effectively modeling loose clothing and expanding the
available data for training these models, we aim to set the stage for further
innovation in digital humans. The source code is available at
https://github.com/sidsunny/pocoloco .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WACV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WiFlexFormer: Efficient WiFi-Based Person-Centric Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04224v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04224v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian Strohmayer, Matthias Wödlinger, Martin Kampel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose WiFlexFormer, a highly efficient Transformer-based architecture
designed for WiFi Channel State Information (CSI)-based person-centric sensing.
We benchmark WiFlexFormer against state-of-the-art vision and specialized
architectures for processing radio frequency data and demonstrate that it
achieves comparable Human Activity Recognition (HAR) performance while offering
a significantly lower parameter count and faster inference times. With an
inference time of just 10 ms on an Nvidia Jetson Orin Nano, WiFlexFormer is
optimized for real-time inference. Additionally, its low parameter count
contributes to improved cross-domain generalization, where it often outperforms
larger models. Our comprehensive evaluation shows that WiFlexFormer is a
potential solution for efficient, scalable WiFi-based sensing applications. The
PyTorch implementation of WiFlexFormer is publicly available at:
https://github.com/StrohmayerJ/WiFlexFormer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiMSUM: Diffusion Mamba -- A Scalable and Unified Spatial-Frequency
  Method for Image Generation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04168v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04168v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Phung, Quan Dao, Trung Dao, Hoang Phan, Dimitris Metaxas, Anh Tran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel state-space architecture for diffusion models,
effectively harnessing spatial and frequency information to enhance the
inductive bias towards local features in input images for image generation
tasks. While state-space networks, including Mamba, a revolutionary advancement
in recurrent neural networks, typically scan input sequences from left to
right, they face difficulties in designing effective scanning strategies,
especially in the processing of image data. Our method demonstrates that
integrating wavelet transformation into Mamba enhances the local structure
awareness of visual inputs and better captures long-range relations of
frequencies by disentangling them into wavelet subbands, representing both low-
and high-frequency components. These wavelet-based outputs are then processed
and seamlessly fused with the original Mamba outputs through a cross-attention
fusion layer, combining both spatial and frequency information to optimize the
order awareness of state-space models which is essential for the details and
overall quality of image generation. Besides, we introduce a globally-shared
transformer to supercharge the performance of Mamba, harnessing its exceptional
power to capture global relationships. Through extensive experiments on
standard benchmarks, our method demonstrates superior results compared to DiT
and DIFFUSSM, achieving faster training convergence and delivering high-quality
outputs. The codes and pretrained models are released at
https://github.com/VinAIResearch/DiMSUM.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024. Project page:
  https://hao-pt.github.io/dimsum/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ No Train, all Gain: Self-Supervised Gradients Improve Deep Frozen
  Representations <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10964v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10964v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Walter Simoncini, Spyros Gidaris, Andrei Bursuc, Yuki M. Asano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces FUNGI, Features from UNsupervised GradIents, a method
to enhance the features of transformer encoders by leveraging self-supervised
gradients. Our method is simple: given any pretrained model, we first compute
gradients from various self-supervised objectives for each input. These
gradients are projected to a lower dimension and then concatenated with the
model's output embedding. The resulting features are evaluated on k-nearest
neighbor classification over 11 datasets from vision, 5 from natural language
processing, and 2 from audio. Across backbones spanning various sizes and
pretraining strategies, FUNGI features provide consistent performance
improvements over the embeddings. We also show that using FUNGI features can
benefit linear classification, clustering and image retrieval, and that they
significantly improve the retrieval-based in-context scene understanding
abilities of pretrained models, for example improving upon DINO by +17% for
semantic segmentation - without any training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024. Code available at
  https://github.com/WalterSimoncini/fungivision</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeNetDM: Debiasing by Network Depth Modulation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.19863v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.19863v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Silpa Vadakkeeveetil Sreelatha, Adarsh Kappiyath, Abhra Chaudhuri, Anjan Dutta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks trained on biased datasets tend to inadvertently learn
spurious correlations, hindering generalization. We formally prove that (1)
samples that exhibit spurious correlations lie on a lower rank manifold
relative to the ones that do not; and (2) the depth of a network acts as an
implicit regularizer on the rank of the attribute subspace that is encoded in
its representations. Leveraging these insights, we present DeNetDM, a novel
debiasing method that uses network depth modulation as a way of developing
robustness to spurious correlations. Using a training paradigm derived from
Product of Experts, we create both biased and debiased branches with deep and
shallow architectures and then distill knowledge to produce the target debiased
model. Our method requires no bias annotations or explicit data augmentation
while performing on par with approaches that require either or both. We
demonstrate that DeNetDM outperforms existing debiasing techniques on both
synthetic and real-world datasets by 5\%. The project page is available at
https://vssilpa.github.io/denetdm/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-ready version : NeurIPS 2024, * indicates these authors
  contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gaussian Deja-vu: Creating Controllable 3D Gaussian Head-Avatars with
  Enhanced Generalization and Personalization Abilities <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16147v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16147v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peizhi Yan, Rabab Ward, Qiang Tang, Shan Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in 3D Gaussian Splatting (3DGS) have unlocked significant
potential for modeling 3D head avatars, providing greater flexibility than
mesh-based methods and more efficient rendering compared to NeRF-based
approaches. Despite these advancements, the creation of controllable 3DGS-based
head avatars remains time-intensive, often requiring tens of minutes to hours.
To expedite this process, we here introduce the "Gaussian Deja-vu" framework,
which first obtains a generalized model of the head avatar and then
personalizes the result. The generalized model is trained on large 2D
(synthetic and real) image datasets. This model provides a well-initialized 3D
Gaussian head that is further refined using a monocular video to achieve the
personalized head avatar. For personalizing, we propose learnable
expression-aware rectification blendmaps to correct the initial 3D Gaussians,
ensuring rapid convergence without the reliance on neural networks. Experiments
demonstrate that the proposed method meets its objectives. It outperforms
state-of-the-art 3D Gaussian head avatars in terms of photorealistic quality as
well as reduces training time consumption to at least a quarter of the existing
methods, producing the avatar in minutes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, Accepted by WACV 2025 in Round 1</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ bit2bit: 1-bit quanta video reconstruction via self-supervised photon
  prediction <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23247v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23247v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yehe Liu, Alexander Krull, Hector Basevi, Ales Leonardis, Michael W. Jenkins
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quanta image sensors, such as SPAD arrays, are an emerging sensor technology,
producing 1-bit arrays representing photon detection events over exposures as
short as a few nanoseconds. In practice, raw data are post-processed using
heavy spatiotemporal binning to create more useful and interpretable images at
the cost of degrading spatiotemporal resolution. In this work, we propose
bit2bit, a new method for reconstructing high-quality image stacks at the
original spatiotemporal resolution from sparse binary quanta image data.
Inspired by recent work on Poisson denoising, we developed an algorithm that
creates a dense image sequence from sparse binary photon data by predicting the
photon arrival location probability distribution. However, due to the binary
nature of the data, we show that the assumption of a Poisson distribution is
inadequate. Instead, we model the process with a Bernoulli lattice process from
the truncated Poisson. This leads to the proposal of a novel self-supervised
solution based on a masked loss function. We evaluate our method using both
simulated and real data. On simulated data from a conventional video, we
achieve 34.35 mean PSNR with extremely photon-sparse binary input (<0.06
photons per pixel per frame). We also present a novel dataset containing a wide
range of real SPAD high-speed videos under various challenging imaging
conditions. The scenes cover strong/weak ambient light, strong motion,
ultra-fast events, etc., which will be made available to the community, on
which we demonstrate the promise of our approach. Both reconstruction quality
and throughput substantially surpass the state-of-the-art methods (e.g., Quanta
Burst Photography (QBP)). Our approach significantly enhances the visualization
and usability of the data, enabling the application of existing analysis
techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LDTrack: Dynamic People Tracking by Service Robots using Diffusion
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.08774v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.08774v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Angus Fung, Beno Benhabib, Goldie Nejat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tracking of dynamic people in cluttered and crowded human-centered
environments is a challenging robotics problem due to the presence of
intraclass variations including occlusions, pose deformations, and lighting
variations. This paper introduces a novel deep learning architecture, using
conditional latent diffusion models, the Latent Diffusion Track (LDTrack), for
tracking multiple dynamic people under intraclass variations. By uniquely
utilizing conditional latent diffusion models to capture temporal person
embeddings, our architecture can adapt to appearance changes of people over
time. We incorporated a latent feature encoder network which enables the
diffusion process to operate within a high-dimensional latent space to allow
for the extraction and spatial-temporal refinement of such rich features as
person appearance, motion, location, identity, and contextual information.
Extensive experiments demonstrate the effectiveness of LDTrack over other
state-of-the-art tracking methods in cluttered and crowded human-centered
environments under intraclass variations. Namely, the results show our method
outperforms existing deep learning robotic people tracking methods in both
tracking accuracy and tracking precision with statistical significance.
Additionally, a comprehensive multi-object tracking comparison study was
performed against the state-of-the-art methods in urban environments,
demonstrating the generalizability of LDTrack. An ablation study was performed
to validate the design choices of LDTrack.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DMPlug: A Plug-in Method for Solving Inverse Problems with Diffusion
  Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.16749v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.16749v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hengkang Wang, Xu Zhang, Taihui Li, Yuxiang Wan, Tiancong Chen, Ju Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretrained diffusion models (DMs) have recently been popularly used in
solving inverse problems (IPs). The existing methods mostly interleave
iterative steps in the reverse diffusion process and iterative steps to bring
the iterates closer to satisfying the measurement constraint. However, such
interleaving methods struggle to produce final results that look like natural
objects of interest (i.e., manifold feasibility) and fit the measurement (i.e.,
measurement feasibility), especially for nonlinear IPs. Moreover, their
capabilities to deal with noisy IPs with unknown types and levels of
measurement noise are unknown. In this paper, we advocate viewing the reverse
process in DMs as a function and propose a novel plug-in method for solving IPs
using pretrained DMs, dubbed DMPlug. DMPlug addresses the issues of manifold
feasibility and measurement feasibility in a principled manner, and also shows
great potential for being robust to unknown types and levels of noise. Through
extensive experiments across various IP tasks, including two linear and three
nonlinear IPs, we demonstrate that DMPlug consistently outperforms
state-of-the-art methods, often by large margins especially for nonlinear IPs.
The code is available at https://github.com/sun-umn/DMPlug.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in NeurIPS 2024
  (https://openreview.net/forum?id=81IFFsfQUj)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep neural network-based detection of counterfeit products from
  smartphone images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05969v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05969v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hugo Garcia-Cotte, Dorra Mellouli, Abdul Rehman, Li Wang, David G. Stork
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Counterfeit products such as drugs and vaccines as well as luxury items such
as high-fashion handbags, watches, jewelry, garments, and cosmetics, represent
significant direct losses of revenue to legitimate manufacturers and vendors,
as well as indirect costs to societies at large. We present the world's first
purely computer-vision-based system to combat such counterfeiting-one that does
not require special security tags or other alterations to the products or
modifications to supply chain tracking. Our deep neural network system shows
high accuracy on branded garments from our first manufacturer tested (99.71%
after 3.06% rejections) using images captured under natural, weakly controlled
conditions, such as in retail stores, customs checkpoints, warehouses, and
outdoors. Our system, suitably transfer trained on a small number of fake and
genuine articles, should find application in additional product categories as
well, for example fashion accessories, perfume boxes, medicines, and more.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CLIBD: Bridging Vision and Genomics for Biodiversity Monitoring at Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17537v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17537v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        ZeMing Gong, Austin T. Wang, Xiaoliang Huo, Joakim Bruslund Haurum, Scott C. Lowe, Graham W. Taylor, Angel X. Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Measuring biodiversity is crucial for understanding ecosystem health. While
prior works have developed machine learning models for taxonomic classification
of photographic images and DNA separately, in this work, we introduce a
multimodal approach combining both, using CLIP-style contrastive learning to
align images, barcode DNA, and text-based representations of taxonomic labels
in a unified embedding space. This allows for accurate classification of both
known and unknown insect species without task-specific fine-tuning, leveraging
contrastive learning for the first time to fuse DNA and image data. Our method
surpasses previous single-modality approaches in accuracy by over 8% on
zero-shot learning tasks, showcasing its effectiveness in biodiversity studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages with 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BetterDepth: Plug-and-Play Diffusion Refiner for Zero-Shot Monocular
  Depth Estimation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.17952v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.17952v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Zhang, Bingxin Ke, Hayko Riemenschneider, Nando Metzger, Anton Obukhov, Markus Gross, Konrad Schindler, Christopher Schroers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  By training over large-scale datasets, zero-shot monocular depth estimation
(MDE) methods show robust performance in the wild but often suffer from
insufficient detail. Although recent diffusion-based MDE approaches exhibit a
superior ability to extract details, they struggle in geometrically complex
scenes that challenge their geometry prior, trained on less diverse 3D data. To
leverage the complementary merits of both worlds, we propose BetterDepth to
achieve geometrically correct affine-invariant MDE while capturing fine
details. Specifically, BetterDepth is a conditional diffusion-based refiner
that takes the prediction from pre-trained MDE models as depth conditioning, in
which the global depth layout is well-captured, and iteratively refines details
based on the input image. For the training of such a refiner, we propose global
pre-alignment and local patch masking methods to ensure BetterDepth remains
faithful to the depth conditioning while learning to add fine-grained scene
details. With efficient training on small-scale synthetic datasets, BetterDepth
achieves state-of-the-art zero-shot MDE performance on diverse public datasets
and on in-the-wild scenes. Moreover, BetterDepth can improve the performance of
other MDE models in a plug-and-play manner without further re-training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Virchow2: Scaling Self-Supervised Mixed Magnification Models in
  Pathology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.00738v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.00738v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric Zimmermann, Eugene Vorontsov, Julian Viret, Adam Casson, Michal Zelechowski, George Shaikovski, Neil Tenenholtz, James Hall, David Klimstra, Razik Yousfi, Thomas Fuchs, Nicolo Fusi, Siqi Liu, Kristen Severson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models are rapidly being developed for computational pathology
applications. However, it remains an open question which factors are most
important for downstream performance with data scale and diversity, model size,
and training algorithm all playing a role. In this work, we propose algorithmic
modifications, tailored for pathology, and we present the result of scaling
both data and model size, surpassing previous studies in both dimensions. We
introduce three new models: Virchow2, a 632 million parameter vision
transformer, Virchow2G, a 1.9 billion parameter vision transformer, and
Virchow2G Mini, a 22 million parameter distillation of Virchow2G, each trained
with 3.1 million histopathology whole slide images, with diverse tissues,
originating institutions, and stains. We achieve state of the art performance
on 12 tile-level tasks, as compared to the top performing competing models. Our
results suggest that data diversity and domain-specific methods can outperform
models that only scale in the number of parameters, but, on average,
performance benefits from the combination of domain-specific methods, data
scale, and model scale.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Applying Guidance in a Limited Interval Improves Sample and Distribution
  Quality in Diffusion Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.07724v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.07724v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tuomas Kynkäänniemi, Miika Aittala, Tero Karras, Samuli Laine, Timo Aila, Jaakko Lehtinen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Guidance is a crucial technique for extracting the best performance out of
image-generating diffusion models. Traditionally, a constant guidance weight
has been applied throughout the sampling chain of an image. We show that
guidance is clearly harmful toward the beginning of the chain (high noise
levels), largely unnecessary toward the end (low noise levels), and only
beneficial in the middle. We thus restrict it to a specific range of noise
levels, improving both the inference speed and result quality. This limited
guidance interval improves the record FID in ImageNet-512 significantly, from
1.81 to 1.40. We show that it is quantitatively and qualitatively beneficial
across different sampler parameters, network architectures, and datasets,
including the large-scale setting of Stable Diffusion XL. We thus suggest
exposing the guidance interval as a hyperparameter in all diffusion models that
use guidance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-supervised 3D Point Cloud Completion via Multi-view Adversarial
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.09786v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.09786v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lintai Wu, Xianjing Cheng, Yong Xu, Huanqiang Zeng, Junhui Hou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world scenarios, scanned point clouds are often incomplete due to
occlusion issues. The task of self-supervised point cloud completion involves
reconstructing missing regions of these incomplete objects without the
supervision of complete ground truth. Current self-supervised methods either
rely on multiple views of partial observations for supervision or overlook the
intrinsic geometric similarity that can be identified and utilized from the
given partial point clouds. In this paper, we propose MAL-SPC, a framework that
effectively leverages both object-level and category-specific geometric
similarities to complete missing structures. Our MAL-SPC does not require any
3D complete supervision and only necessitates a single partial point cloud for
each object. Specifically, we first introduce a Pattern Retrieval Network to
retrieve similar position and curvature patterns between the partial input and
the predicted shape, then leverage these similarities to densify and refine the
reconstructed results. Additionally, we render the reconstructed complete shape
into multi-view depth maps and design an adversarial learning module to learn
the geometry of the target shape from category-specific single-view depth
images. To achieve anisotropic rendering, we design a density-aware radius
estimation algorithm to improve the quality of the rendered images. Our MAL-SPC
yields the best results compared to current state-of-the-art methods.We will
make the source code publicly available at \url{https://github.com/ltwu6/malspc
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages,10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ChartInsights: Evaluating <span class="highlight-title">Multimodal</span> Large Language Models for Low-Level
  Chart Question Answering <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.07001v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.07001v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Wu, Lutao Yan, Leixian Shen, Yunhai Wang, Nan Tang, Yuyu Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chart question answering (ChartQA) tasks play a critical role in interpreting
and extracting insights from visualization charts. While recent advancements in
multimodal large language models (MLLMs) like GPT-4o have shown promise in
high-level ChartQA tasks, such as chart captioning, their effectiveness in
low-level ChartQA tasks (e.g., identifying correlations) remains underexplored.
In this paper, we address this gap by evaluating MLLMs on low-level ChartQA
using a newly curated dataset, ChartInsights, which consists of 22,347 (chart,
task, query, answer) covering 10 data analysis tasks across 7 chart types. We
systematically evaluate 19 advanced MLLMs, including 12 open-source and 7
closed-source models. The average accuracy rate across these models is 39.8%,
with GPT-4o achieving the highest accuracy at 69.17%. To further explore the
limitations of MLLMs in low-level ChartQA, we conduct experiments that alter
visual elements of charts (e.g., changing color schemes, adding image noise) to
assess their impact on the task effectiveness. Furthermore, we propose a new
textual prompt strategy, Chain-of-Charts, tailored for low-level ChartQA tasks,
which boosts performance by 14.41%, achieving an accuracy of 83.58%. Finally,
incorporating a visual prompt strategy that directs attention to relevant
visual elements further improves accuracy to 84.32%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Conference Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EViT: An Eagle Vision Transformer with Bi-Fovea Self-Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06629v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06629v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yulong Shi, Mingwei Sun, Yongshuai Wang, Jiahao Ma, Zengqiang Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Owing to advancements in deep learning technology, Vision Transformers (ViTs)
have demonstrated impressive performance in various computer vision tasks.
Nonetheless, ViTs still face some challenges, such as high computational
complexity and the absence of desirable inductive biases. To alleviate these
issues, {the potential advantages of combining eagle vision with ViTs are
explored. We summarize a Bi-Fovea Visual Interaction (BFVI) structure inspired
by the unique physiological and visual characteristics of eagle eyes. A novel
Bi-Fovea Self-Attention (BFSA) mechanism and Bi-Fovea Feedforward Network
(BFFN) are proposed based on this structural design approach, which can be used
to mimic the hierarchical and parallel information processing scheme of the
biological visual cortex, enabling networks to learn feature representations of
targets in a coarse-to-fine manner. Furthermore, a Bionic Eagle Vision (BEV)
block is designed as the basic building unit based on the BFSA mechanism and
BFFN. By stacking BEV blocks, a unified and efficient family of pyramid
backbone networks called Eagle Vision Transformers (EViTs) is developed.
Experimental results show that EViTs exhibit highly competitive performance in
various computer vision tasks, such as image classification, object detection
and semantic segmentation. Compared with other approaches, EViTs have
significant advantages, especially in terms of performance and computational
efficiency. Code is available at https://github.com/nkusyl/EViT
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Advantages of Neural Population Coding for Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00393v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00393v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heiko Hoffmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scalar variables, e.g., the orientation of a shape in an image, are commonly
predicted using a single output neuron in a neural network. In contrast, the
mammalian cortex represents variables with a population of neurons. In this
population code, each neuron is most active at its preferred value and shows
partial activity for other values. Here, we investigate the benefit of using a
population code for the output layer of a neural network. We compare population
codes against single-neuron outputs and one-hot vectors. First, we show
theoretically and in experiments with synthetic data that population codes
improve robustness to input noise in networks of stacked linear layers. Second,
we demonstrate the benefit of using population codes to encode ambiguous
outputs, such as the pose of symmetric objects. Using the T-LESS dataset of
feature-less real-world objects, we show that population codes improve the
accuracy of predicting 3D object orientation from image input.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ATM: Improving Model Merging by Alternating <span class="highlight-title">Tuning</span> and Merging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03055v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03055v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Zhou, Daniele Solombrino, Donato Crisostomi, Maria Sofia Bucarelli, Fabrizio Silvestri, Emanuele Rodolà
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model merging has recently emerged as a cost-efficient paradigm for
multi-task learning. Among current approaches, task arithmetic stands out for
its simplicity and effectiveness. In this paper, we motivate the effectiveness
of task vectors by linking them to multi-task gradients. We show that in a
single-epoch scenario, task vectors are mathematically equivalent to the
gradients obtained via gradient descent in a multi-task setting, and still
approximate these gradients in subsequent epochs. Furthermore, we show that
task vectors perform optimally when equality is maintained, and their
effectiveness is largely driven by the first epoch's gradient. Building on this
insight, we propose viewing model merging as a single step in an iterative
process that Alternates between Tuning and Merging (ATM). This method acts as a
bridge between model merging and multi-task gradient descent, achieving
state-of-the-art results with the same data and computational requirements. We
extensively evaluate ATM across diverse settings, achieving up to 20% higher
accuracy in computer vision and NLP tasks, compared to the best baselines.
Finally, we provide both empirical and theoretical support for its
effectiveness, demonstrating increased orthogonality between task vectors and
proving that ATM minimizes an upper bound on the loss obtained by jointly
finetuning all tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Main paper: 10 Pages, 11 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IFAdapter: Instance Feature Control for Grounded Text-to-Image
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08240v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08240v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinwei Wu, Xianpan Zhou, Bing Ma, Xuefeng Su, Kai Ma, Xinchao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Text-to-Image (T2I) diffusion models excel at generating visually
appealing images of individual instances, they struggle to accurately position
and control the features generation of multiple instances. The Layout-to-Image
(L2I) task was introduced to address the positioning challenges by
incorporating bounding boxes as spatial control signals, but it still falls
short in generating precise instance features. In response, we propose the
Instance Feature Generation (IFG) task, which aims to ensure both positional
accuracy and feature fidelity in generated instances. To address the IFG task,
we introduce the Instance Feature Adapter (IFAdapter). The IFAdapter enhances
feature depiction by incorporating additional appearance tokens and utilizing
an Instance Semantic Map to align instance-level features with spatial
locations. The IFAdapter guides the diffusion process as a plug-and-play
module, making it adaptable to various community models. For evaluation, we
contribute an IFG benchmark and develop a verification pipeline to objectively
compare models' abilities to generate instances with accurate positioning and
features. Experimental results demonstrate that IFAdapter outperforms other
models in both quantitative and qualitative evaluations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GVKF: Gaussian Voxel Kernel Functions for Highly Efficient Surface
  Reconstruction in Open Scenes <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01853v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01853v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaochao Song, Chong Cheng, Hao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we present a novel method for efficient and effective 3D
surface reconstruction in open scenes. Existing Neural Radiance Fields (NeRF)
based works typically require extensive training and rendering time due to the
adopted implicit representations. In contrast, 3D Gaussian splatting (3DGS)
uses an explicit and discrete representation, hence the reconstructed surface
is built by the huge number of Gaussian primitives, which leads to excessive
memory consumption and rough surface details in sparse Gaussian areas. To
address these issues, we propose Gaussian Voxel Kernel Functions (GVKF), which
establish a continuous scene representation based on discrete 3DGS through
kernel regression. The GVKF integrates fast 3DGS rasterization and highly
effective scene implicit representations, achieving high-fidelity open scene
surface reconstruction. Experiments on challenging scene datasets demonstrate
the efficiency and effectiveness of our proposed GVKF, featuring with high
reconstruction quality, real-time rendering speed, significant savings in
storage and training memory consumption.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BCDNet: A Fast Residual Neural Network For Invasive Ductal Carcinoma
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.13800v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.13800v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujia Lin, Aiwei Lian, Mingyu Liao, Shuangjie Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is of great significance to diagnose Invasive Ductal Carcinoma (IDC) in
early stage, which is the most common subtype of breast cancer. Although the
powerful models in the Computer-Aided Diagnosis (CAD) systems provide promising
results, it is still difficult to integrate them into other medical devices or
use them without sufficient computation resource. In this paper, we propose
BCDNet, which firstly upsamples the input image by the residual block and use
smaller convolutional block and a special MLP to learn features. BCDNet is
proofed to effectively detect IDC in histopathological RGB images with an
average accuracy of 91.6% and reduce training consumption effectively compared
to ResNet 50 and ViT-B-16.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Classification Done Right for <span class="highlight-title">Vision-Language</span> Pre-Training <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03313v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03313v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zilong Huang, Qinghao Ye, Bingyi Kang, Jiashi Feng, Haoqi Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce SuperClass, a super simple classification method for
vision-language pre-training on image-text data. Unlike its contrastive
counterpart CLIP who contrast with a text encoder, SuperClass directly utilizes
tokenized raw text as supervised classification labels, without the need for
additional text filtering or selection. Due to the absence of the text encoding
as contrastive target, SuperClass does not require a text encoder and does not
need to maintain a large batch size as CLIP does. SuperClass demonstrated
superior performance on various downstream tasks, including classic computer
vision benchmarks and vision language downstream tasks. We further explored the
scaling behavior of SuperClass on model size, training length, or data size,
and reported encouraging results and comparisons to CLIP.
https://github.com/x-cls/superclass
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Concept-Attention Whitening for Interpretable Skin Lesion Diagnosis <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.05997v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.05997v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junlin Hou, Jilan Xu, Hao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The black-box nature of deep learning models has raised concerns about their
interpretability for successful deployment in real-world clinical applications.
To address the concerns, eXplainable Artificial Intelligence (XAI) aims to
provide clear and understandable explanations of the decision-making process.
In the medical domain, concepts such as attributes of lesions or abnormalities
serve as key evidence for deriving diagnostic results. Existing concept-based
models mainly depend on concepts that appear independently and require
fine-grained concept annotations such as bounding boxes. However, a medical
image usually contains multiple concepts, and the fine-grained concept
annotations are difficult to acquire. In this paper, we aim to interpret
representations in deep neural networks by aligning the axes of the latent
space with known concepts of interest. We propose a novel Concept-Attention
Whitening (CAW) framework for interpretable skin lesion diagnosis. CAW is
comprised of a disease diagnosis branch and a concept alignment branch. In the
former branch, we train a convolutional neural network (CNN) with an inserted
CAW layer to perform skin lesion diagnosis. The CAW layer decorrelates features
and aligns image features to conceptual meanings via an orthogonal matrix. In
the latter branch, the orthogonal matrix is calculated under the guidance of
the concept attention mask. We particularly introduce a weakly-supervised
concept mask generator that only leverages coarse concept labels for filtering
local regions that are relevant to certain concepts, improving the optimization
of the orthogonal matrix. Extensive experiments on two public skin lesion
diagnosis datasets demonstrated that CAW not only enhanced interpretability but
also maintained a state-of-the-art diagnostic performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MICCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Degradation Oriented and Regularized Network for Blind Depth
  Super-Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11666v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11666v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengxue Wang, Zhiqiang Yan, Jinshan Pan, Guangwei Gao, Kai Zhang, Jian Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent RGB-guided depth super-resolution methods have achieved impressive
performance under the assumption of fixed and known degradation (e.g., bicubic
downsampling). However, in real-world scenarios, captured depth data often
suffer from unconventional and unknown degradation due to sensor limitations
and complex imaging environments (e.g., low reflective surfaces, varying
illumination). Consequently, the performance of these methods significantly
declines when real-world degradation deviate from their assumptions. In this
paper, we propose the Degradation Oriented and Regularized Network (DORNet), a
novel framework designed to adaptively address unknown degradation in
real-world scenes through implicit degradation representations. Our approach
begins with the development of a self-supervised degradation learning strategy,
which models the degradation representations of low-resolution depth data using
routing selection-based degradation regularization. To facilitate effective
RGB-D fusion, we further introduce a degradation-oriented feature
transformation module that selectively propagates RGB content into the depth
data based on the learned degradation priors. Extensive experimental results on
both real and synthetic datasets demonstrate the superiority of our DORNet in
handling unknown degradation, outperforming existing methods. The code is
available at https://github.com/yanzq95/DORNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cross-Task Affinity Learning for Multitask Dense Scene Predictions <span class="chip">WACV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.11124v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.11124v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dimitrios Sinodinos, Narges Armanfard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multitask learning (MTL) has become prominent for its ability to predict
multiple tasks jointly, achieving better per-task performance with fewer
parameters than single-task learning. Recently, decoder-focused architectures
have significantly improved multitask performance by refining task predictions
using features from related tasks. However, most refinement methods struggle to
efficiently capture both local and long-range dependencies between
task-specific representations and cross-task patterns. In this paper, we
introduce the Cross-Task Affinity Learning (CTAL) module, a lightweight
framework that enhances task refinement in multitask networks. CTAL effectively
captures local and long-range cross-task interactions by optimizing task
affinity matrices for parameter-efficient grouped convolutions without concern
for information loss. Our results demonstrate state-of-the-art MTL performance
for both CNN and transformer backbones, using significantly fewer parameters
than single-task learning. Our code is publicly available at
https://github.com/Armanfard-Lab/EMA-Net.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the IEEE Winter Conference on
  Applications of Computer Vision (WACV) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TFS-NeRF: Template-Free NeRF for Semantic 3D Reconstruction of Dynamic
  Scene <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17459v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17459v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sandika Biswas, Qianyi Wu, Biplab Banerjee, Hamid Rezatofighi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite advancements in Neural Implicit models for 3D surface reconstruction,
handling dynamic environments with arbitrary rigid, non-rigid, or deformable
entities remains challenging. Many template-based methods are entity-specific,
focusing on humans, while generic reconstruction methods adaptable to such
dynamic scenes often require additional inputs like depth or optical flow or
rely on pre-trained image features for reasonable outcomes. These methods
typically use latent codes to capture frame-by-frame deformations. In contrast,
some template-free methods bypass these requirements and adopt traditional LBS
(Linear Blend Skinning) weights for a detailed representation of deformable
object motions, although they involve complex optimizations leading to lengthy
training times. To this end, as a remedy, this paper introduces TFS-NeRF, a
template-free 3D semantic NeRF for dynamic scenes captured from sparse or
single-view RGB videos, featuring interactions among various entities and more
time-efficient than other LBS-based approaches. Our framework uses an
Invertible Neural Network (INN) for LBS prediction, simplifying the training
process. By disentangling the motions of multiple entities and optimizing
per-entity skinning weights, our method efficiently generates accurate,
semantically separable geometries. Extensive experiments demonstrate that our
approach produces high-quality reconstructions of both deformable and
non-deformable objects in complex interactions, with improved training
efficiency compared to existing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeTikZify: Synthesizing Graphics Programs for Scientific Figures and
  Sketches with TikZ <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15306v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15306v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonas Belouadi, Simone Paolo Ponzetto, Steffen Eger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creating high-quality scientific figures can be time-consuming and
challenging, even though sketching ideas on paper is relatively easy.
Furthermore, recreating existing figures that are not stored in formats
preserving semantic information is equally complex. To tackle this problem, we
introduce DeTikZify, a novel multimodal language model that automatically
synthesizes scientific figures as semantics-preserving TikZ graphics programs
based on sketches and existing figures. To achieve this, we create three new
datasets: DaTikZv2, the largest TikZ dataset to date, containing over 360k
human-created TikZ graphics; SketchFig, a dataset that pairs hand-drawn
sketches with their corresponding scientific figures; and MetaFig, a collection
of diverse scientific figures and associated metadata. We train DeTikZify on
MetaFig and DaTikZv2, along with synthetically generated sketches learned from
SketchFig. We also introduce an MCTS-based inference algorithm that enables
DeTikZify to iteratively refine its outputs without the need for additional
training. Through both automatic and human evaluation, we demonstrate that
DeTikZify outperforms commercial Claude 3 and GPT-4V in synthesizing TikZ
programs, with the MCTS algorithm effectively boosting its performance. We make
our code, models, and datasets publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024 (spotlight); Project page:
  https://github.com/potamides/DeTikZify</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OmniGS: Fast Radiance Field Reconstruction using Omnidirectional
  Gaussian Splatting <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.03202v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.03202v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Longwei Li, Huajian Huang, Sai-Kit Yeung, Hui Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Photorealistic reconstruction relying on 3D Gaussian Splatting has shown
promising potential in various domains. However, the current 3D Gaussian
Splatting system only supports radiance field reconstruction using undistorted
perspective images. In this paper, we present OmniGS, a novel omnidirectional
Gaussian splatting system, to take advantage of omnidirectional images for fast
radiance field reconstruction. Specifically, we conduct a theoretical analysis
of spherical camera model derivatives in 3D Gaussian Splatting. According to
the derivatives, we then implement a new GPU-accelerated omnidirectional
rasterizer that directly splats 3D Gaussians onto the equirectangular screen
space for omnidirectional image rendering. We realize differentiable
optimization of the omnidirectional radiance field without the requirement of
cube-map rectification or tangent-plane approximation. Extensive experiments
conducted in egocentric and roaming scenarios demonstrate that our method
achieves state-of-the-art reconstruction quality and high rendering speed using
omnidirectional images. The code will be publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, accepted by WACV 2025, project page:
  https://liquorleaf.github.io/research/OmniGS/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Grid Data: Exploring Graph Neural Networks for Earth Observation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03223v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03223v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shan Zhao, Zhaiyu Chen, Zhitong Xiong, Yilei Shi, Sudipan Saha, Xiao Xiang Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Earth Observation (EO) data analysis has been significantly revolutionized by
deep learning (DL), with applications typically limited to grid-like data
structures. Graph Neural Networks (GNNs) emerge as an important innovation,
propelling DL into the non-Euclidean domain. Naturally, GNNs can effectively
tackle the challenges posed by diverse modalities, multiple sensors, and the
heterogeneous nature of EO data. To introduce GNNs in the related domains, our
review begins by offering fundamental knowledge on GNNs. Then, we summarize the
generic problems in EO, to which GNNs can offer potential solutions. Following
this, we explore a broad spectrum of GNNs' applications to scientific problems
in Earth systems, covering areas such as weather and climate analysis, disaster
management, air quality monitoring, agriculture, land cover classification,
hydrological process modeling, and urban modeling. The rationale behind
adopting GNNs in these fields is explained, alongside methodologies for
organizing graphs and designing favorable architectures for various tasks.
Furthermore, we highlight methodological challenges of implementing GNNs in
these domains and possible solutions that could guide future research. While
acknowledging that GNNs are not a universal solution, we conclude the paper by
comparing them with other popular architectures like transformers and analyzing
their potential synergies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in Geoscience and Remote Sensing Magazine
  (GRSM)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In-Context Translation: Towards Unifying Image Recognition, Processing,
  and Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.09633v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.09633v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Xue, Qianru Sun, Li Song, Wenjun Zhang, Zhiwu Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose In-Context Translation (ICT), a general learning framework to
unify visual recognition (e.g., semantic segmentation), low-level image
processing (e.g., denoising), and conditional image generation (e.g.,
edge-to-image synthesis). Thanks to unification, ICT significantly reduces the
inherent inductive bias that comes with designing models for specific tasks,
and it maximizes mutual enhancement across similar tasks. However, the
unification across a large number of tasks is non-trivial due to various data
formats and training pipelines. To this end, ICT introduces two designs.
Firstly, it standardizes input-output data of different tasks into RGB image
pairs, e.g., semantic segmentation data pairs an RGB image with its
segmentation mask in the same RGB format. This turns different tasks into a
general translation task between two RGB images. Secondly, it standardizes the
training of different tasks into a general in-context learning, where
"in-context" means the input comprises an example input-output pair of the
target task and a query image. The learning objective is to generate the
"missing" data paired with the query. The implicit translation process is thus
between the query and the generated image. In experiments, ICT unifies ten
vision tasks and showcases impressive performance on their respective
benchmarks. Notably, ICT performs well across three major categories of
computer vision tasks, while its two competitors (Painter and PromptDiffusion)
are only effective in at most two of these task categories. In addition,
compared to its competitors, ICT trained on only 4 RTX 3090 GPUs is shown to be
more efficient and less costly in training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Harnessing Webpage UIs for Text-Rich Visual Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13824v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13824v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junpeng Liu, Tianyue Ou, Yifan Song, Yuxiao Qu, Wai Lam, Chenyan Xiong, Wenhu Chen, Graham Neubig, Xiang Yue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-rich visual understanding-the ability to process environments where
dense textual content is integrated with visuals-is crucial for multimodal
large language models (MLLMs) to interact effectively with structured
environments. To enhance this capability, we propose synthesizing general
multimodal instructions from webpage UIs using text-based large language models
(LLMs). Despite lacking direct visual input, text-based LLMs are able to
process structured text representations from webpage accessibility trees. These
instructions are then paired with UI screenshots to train multimodal models. We
introduce MultiUI, a dataset containing 7.3 million samples from 1 million
websites, covering diverse multimodal tasks and UI layouts. Models trained on
MultiUI not only excel in web UI tasks-achieving up to a 48% improvement on
VisualWebBench and a 19.1% boost in element accuracy on a web agent dataset
Mind2Web-but also generalize surprisingly well to non-web UI tasks and even to
non-UI domains, such as document understanding, OCR, and chart interpretation.
These results highlight the broad applicability of web UI data for advancing
text-rich visual understanding across various scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-label Cluster Discrimination for Visual Representation Learning <span class="chip">ECCV2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.17331v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.17331v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang An, Kaicheng Yang, Xiangzi Dai, Ziyong Feng, Jiankang Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive Language Image Pre-training (CLIP) has recently demonstrated
success across various tasks due to superior feature representation empowered
by image-text contrastive learning. However, the instance discrimination method
used by CLIP can hardly encode the semantic structure of training data. To
handle this limitation, cluster discrimination has been proposed through
iterative cluster assignment and classification. Nevertheless, most cluster
discrimination approaches only define a single pseudo-label for each image,
neglecting multi-label signals in the image. In this paper, we propose a novel
Multi-Label Cluster Discrimination method named MLCD to enhance representation
learning. In the clustering step, we first cluster the large-scale LAION-400M
dataset into one million centers based on off-the-shelf embedding features.
Considering that natural images frequently contain multiple visual objects or
attributes, we select the multiple closest centers as auxiliary class labels.
In the discrimination step, we design a novel multi-label classification loss,
which elegantly separates losses from positive classes and negative classes,
and alleviates ambiguity on decision boundary. We validate the proposed
multi-label cluster discrimination method with experiments on different scales
of models and pre-training datasets. Experimental results show that our method
achieves state-of-the-art performance on multiple downstream tasks including
linear probe, zero-shot classification, and image-text retrieval. Code and
models have been released at https://github.com/deepglint/unicom .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CPnP: Consistent Pose Estimator for Perspective-n-Point Problem with
  Bias Elimination 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.05824v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.05824v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangyang Zeng, Shiyu Chen, Biqiang Mu, Guodong Shi, Junfeng Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Perspective-n-Point (PnP) problem has been widely studied in both
computer vision and photogrammetry societies. With the development of feature
extraction techniques, a large number of feature points might be available in a
single shot. It is promising to devise a consistent estimator, i.e., the
estimate can converge to the true camera pose as the number of points
increases. To this end, we propose a consistent PnP solver, named \emph{CPnP},
with bias elimination. Specifically, linear equations are constructed from the
original projection model via measurement model modification and variable
elimination, based on which a closed-form least-squares solution is obtained.
We then analyze and subtract the asymptotic bias of this solution, resulting in
a consistent estimate. Additionally, Gauss-Newton (GN) iterations are executed
to refine the consistent solution. Our proposed estimator is efficient in terms
of computations -- it has $O(n)$ computational complexity. Experimental tests
on both synthetic data and real images show that our proposed estimator is
superior to some well-known ones for images with dense visual features, in
terms of estimation precision and computing time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VHM: Versatile and Honest <span class="highlight-title">Vision Language</span> Model for Remote Sensing Image
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.20213v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.20213v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Pang, Xingxing Weng, Jiang Wu, Jiayu Li, Yi Liu, Jiaxing Sun, Weijia Li, Shuai Wang, Litong Feng, Gui-Song Xia, Conghui He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper develops a Versatile and Honest vision language Model (VHM) for
remote sensing image analysis. VHM is built on a large-scale remote sensing
image-text dataset with rich-content captions (VersaD), and an honest
instruction dataset comprising both factual and deceptive questions (HnstD).
Unlike prevailing remote sensing image-text datasets, in which image captions
focus on a few prominent objects and their relationships, VersaD captions
provide detailed information about image properties, object attributes, and the
overall scene. This comprehensive captioning enables VHM to thoroughly
understand remote sensing images and perform diverse remote sensing tasks.
Moreover, different from existing remote sensing instruction datasets that only
include factual questions, HnstD contains additional deceptive questions
stemming from the non-existence of objects. This feature prevents VHM from
producing affirmative answers to nonsense queries, thereby ensuring its
honesty. In our experiments, VHM significantly outperforms various vision
language models on common tasks of scene classification, visual question
answering, and visual grounding. Additionally, VHM achieves competent
performance on several unexplored tasks, such as building vectorizing,
multi-label classification and honest question answering. We will release the
code, data and model weights at https://github.com/opendatalab/VHM .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Equal contribution: Chao Pang, Xingxing Weng, Jiang Wu; Corresponding
  author: Gui-Song Xia, Conghui He</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FedFMS: Exploring Federated Foundation Models for Medical Image
  Segmentation <span class="chip">MICCAI'2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05408v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05408v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxi Liu, Guibo Luo, Yuesheng Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical image segmentation is crucial for clinical diagnosis. The
Segmentation Anything Model (SAM) serves as a powerful foundation model for
visual segmentation and can be adapted for medical image segmentation. However,
medical imaging data typically contain privacy-sensitive information, making it
challenging to train foundation models with centralized storage and sharing. To
date, there are few foundation models tailored for medical image deployment
within the federated learning framework, and the segmentation performance, as
well as the efficiency of communication and training, remain unexplored. In
response to these issues, we developed Federated Foundation models for Medical
image Segmentation (FedFMS), which includes the Federated SAM (FedSAM) and a
communication and training-efficient Federated SAM with Medical SAM Adapter
(FedMSA). Comprehensive experiments on diverse datasets are conducted to
investigate the performance disparities between centralized training and
federated learning across various configurations of FedFMS. The experiments
revealed that FedFMS could achieve performance comparable to models trained via
centralized training methods while maintaining privacy. Furthermore, FedMSA
demonstrated the potential to enhance communication and training efficiency.
Our model implementation codes are available at
https://github.com/LIU-YUXI/FedFMS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by MICCAI'2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Digi2Real: Bridging the Realism Gap in Synthetic Data Face Recognition
  via Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02188v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02188v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anjith George, Sebastien Marcel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The accuracy of face recognition systems has improved significantly in the
past few years, thanks to the large amount of data collected and the
advancement in neural network architectures. However, these large-scale
datasets are often collected without explicit consent, raising ethical and
privacy concerns. To address this, there have been proposals to use synthetic
datasets for training face recognition models. Yet, such models still rely on
real data to train the generative models and generally exhibit inferior
performance compared to those trained on real datasets. One of these datasets,
DigiFace, uses a graphics pipeline to generate different identities and
different intra-class variations without using real data in training the
models. However, the performance of this approach is poor on face recognition
benchmarks, possibly due to the lack of realism in the images generated from
the graphics pipeline. In this work, we introduce a novel framework for realism
transfer aimed at enhancing the realism of synthetically generated face images.
Our method leverages the large-scale face foundation model, and we adapt the
pipeline for realism enhancement. By integrating the controllable aspects of
the graphics pipeline with our realism enhancement technique, we generate a
large amount of realistic variations-combining the advantages of both
approaches. Our empirical evaluations demonstrate that models trained using our
enhanced dataset significantly improve the performance of face recognition
systems over the baseline. The source code and datasets will be made available
publicly: https://www.idiap.ch/paper/digi2real
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The dataset would be available here:
  https://www.idiap.ch/paper/digi2real</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Data Perspective on Enhanced Identity Preservation for Diffusion
  Personalization <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.04315v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.04315v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingzhe He, Zhiwen Cao, Nicholas Kolkin, Lantao Yu, Kun Wan, Helge Rhodin, Ratheesh Kalarot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large text-to-image models have revolutionized the ability to generate
imagery using natural language. However, particularly unique or personal visual
concepts, such as pets and furniture, will not be captured by the original
model. This has led to interest in how to personalize a text-to-image model.
Despite significant progress, this task remains a formidable challenge,
particularly in preserving the subject's identity. Most researchers attempt to
address this issue by modifying model architectures. These methods are capable
of keeping the subject structure and color but fail to preserve identity
details. Towards this issue, our approach takes a data-centric perspective. We
introduce a novel regularization dataset generation strategy on both the text
and image level. This strategy enables the model to preserve fine details of
the desired subjects, such as text and logos. Our method is
architecture-agnostic and can be flexibly applied on various text-to-image
models. We show on established benchmarks that our data-centric approach forms
the new state of the art in terms of identity preservation and text alignment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WACV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Utilizing Large Language Models in an iterative paradigm with Domain
  feedback for Zero-shot Molecule optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13147v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13147v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khiem Le, Nitesh V. Chawla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Molecule optimization is a critical task in drug discovery to optimize
desired properties of a given molecule through chemical modification. Despite
Large Language Models (LLMs) holding the potential to efficiently simulate this
task by using natural language to direct the optimization, straightforwardly
utilizing shows limited performance. In this work, we facilitate utilizing LLMs
in an iterative paradigm by proposing a simple yet highly effective domain
feedback provider, namely $\text{Re}^3$DF. In detail, $\text{Re}^3$DF harnesses
an external toolkit, RDKit, to handle the molecule hallucination, if the
modified molecule is chemically invalid. Otherwise, its desired properties are
computed and compared to the original one, establishing reliable domain
feedback with correct direction and distance towards the objective, followed by
a retrieved example, to explicitly guide the LLM to refine the modified
molecule. We conduct experiments across both single- and multi-property
objectives with 2 thresholds, where $\text{Re}^3$DF shows significant
improvements. Particularly, for 20 single-property objectives, $\text{Re}^3$DF
enhances Hit ratio by 16.95% and 20.76% under loose and strict thresholds,
respectively. For 32 multi-property objectives, $\text{Re}^3$DF enhances Hit
ratio by 6.04% and 5.25%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalizing Alignment Paradigm of Text-to-Image Generation with
  Preferences through $f$-divergence Minimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09774v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09774v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyuan Sun, Bo Xia, Yongzhe Chang, Xueqian Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Direct Preference Optimization (DPO) has recently expanded its successful
application from aligning large language models (LLMs) to aligning
text-to-image models with human preferences, which has generated considerable
interest within the community. However, we have observed that these approaches
rely solely on minimizing the reverse Kullback-Leibler divergence during
alignment process between the fine-tuned model and the reference model,
neglecting the incorporation of other divergence constraints. In this study, we
focus on extending reverse Kullback-Leibler divergence in the alignment
paradigm of text-to-image models to $f$-divergence, which aims to garner better
alignment performance as well as good generation diversity. We provide the
generalized formula of the alignment paradigm under the $f$-divergence
condition and thoroughly analyze the impact of different divergence constraints
on alignment process from the perspective of gradient fields. We conduct
comprehensive evaluation on image-text alignment performance, human value
alignment performance and generation diversity performance under different
divergence constraints, and the results indicate that alignment based on
Jensen-Shannon divergence achieves the best trade-off among them. The option of
divergence employed for aligning text-to-image models significantly impacts the
trade-off between alignment performance (especially human value alignment) and
generation diversity, which highlights the necessity of selecting an
appropriate divergence for practical applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hierarchical Temporal Context Learning for Camera-based Semantic Scene
  Completion <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.02077v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.02077v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bohan Li, Jiajun Deng, Wenyao Zhang, Zhujin Liang, Dalong Du, Xin Jin, Wenjun Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Camera-based 3D semantic scene completion (SSC) is pivotal for predicting
complicated 3D layouts with limited 2D image observations. The existing
mainstream solutions generally leverage temporal information by roughly
stacking history frames to supplement the current frame, such straightforward
temporal modeling inevitably diminishes valid clues and increases learning
difficulty. To address this problem, we present HTCL, a novel Hierarchical
Temporal Context Learning paradigm for improving camera-based semantic scene
completion. The primary innovation of this work involves decomposing temporal
context learning into two hierarchical steps: (a) cross-frame affinity
measurement and (b) affinity-based dynamic refinement. Firstly, to separate
critical relevant context from redundant information, we introduce the pattern
affinity with scale-aware isolation and multiple independent learners for
fine-grained contextual correspondence modeling. Subsequently, to dynamically
compensate for incomplete observations, we adaptively refine the feature
sampling locations based on initially identified locations with high affinity
and their neighboring relevant regions. Our method ranks $1^{st}$ on the
SemanticKITTI benchmark and even surpasses LiDAR-based methods in terms of mIoU
on the OpenOccupancy benchmark. Our code is available on
https://github.com/Arlo0o/HTCL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TalkMosaic: Interactive PhotoMosaic with <span class="highlight-title">Multi-modal</span> LLM Q&A
  Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13941v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13941v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Li, Fulu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We use images of cars of a wide range of varieties to compose an image of an
animal such as a bird or a lion for the theme of environmental protection to
maximize the information about cars in a single composed image and to raise the
awareness about environmental challenges. We present a novel way of image
interaction with an artistically-composed photomosaic image, in which a simple
operation of "click and display" is used to demonstrate the interactive switch
between a tile image in a photomosaic image and the corresponding original car
image, which will be automatically saved on the Desktop. We build a multimodal
custom GPT named TalkMosaic by incorporating car images information and the
related knowledge to ChatGPT. By uploading the original car image to
TalkMosaic, we can ask questions about the given car image and get the
corresponding answers efficiently and effectively such as where to buy the tire
in the car image that satisfies high environmental standards. We give an
in-depth analysis on how to speed up the inference of multimodal LLM using
sparse attention and quantization techniques with presented probabilistic
FlashAttention (PrFlashAttention) and Staircase Adaptive Quantization (SAQ)
methods. The implemented prototype demonstrates the feasibility and
effectiveness of the presented approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting Surgical Instrument Segmentation Without Human Intervention:
  A Graph Partitioning View <span class="chip">ACM MM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.14789v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.14789v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyu Sheng, Jianan Fan, Dongnan Liu, Ron Kikinis, Weidong Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surgical instrument segmentation (SIS) on endoscopic images stands as a
long-standing and essential task in the context of computer-assisted
interventions for boosting minimally invasive surgery. Given the recent surge
of deep learning methodologies and their data-hungry nature, training a neural
predictive model based on massive expert-curated annotations has been
dominating and served as an off-the-shelf approach in the field, which could,
however, impose prohibitive burden to clinicians for preparing fine-grained
pixel-wise labels corresponding to the collected surgical video frames. In this
work, we propose an unsupervised method by reframing the video frame
segmentation as a graph partitioning problem and regarding image pixels as
graph nodes, which is significantly different from the previous efforts. A
self-supervised pre-trained model is firstly leveraged as a feature extractor
to capture high-level semantic features. Then, Laplacian matrixs are computed
from the features and are eigendecomposed for graph partitioning. On the "deep"
eigenvectors, a surgical video frame is meaningfully segmented into different
modules such as tools and tissues, providing distinguishable semantic
information like locations, classes, and relations. The segmentation problem
can then be naturally tackled by applying clustering or threshold on the
eigenvectors. Extensive experiments are conducted on various datasets (e.g.,
EndoVis2017, EndoVis2018, UCL, etc.) for different clinical endpoints. Across
all the challenging scenarios, our method demonstrates outstanding performance
and robustness higher than unsupervised state-of-the-art (SOTA) methods. The
code is released at https://github.com/MingyuShengSMY/GraphClusteringSIS.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted by The 32nd ACM International Conference on
  Multimedia (ACM MM 2024) Workshop on Multimedia Computing for Health and
  Medicine (MCHM)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Identifying and Solving Conditional Image Leakage in Image-to-Video
  Diffusion Model <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15735v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15735v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Min Zhao, Hongzhou Zhu, Chendong Xiang, Kaiwen Zheng, Chongxuan Li, Jun Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have obtained substantial progress in image-to-video
generation. However, in this paper, we find that these models tend to generate
videos with less motion than expected. We attribute this to the issue called
conditional image leakage, where the image-to-video diffusion models (I2V-DMs)
tend to over-rely on the conditional image at large time steps. We further
address this challenge from both inference and training aspects. First, we
propose to start the generation process from an earlier time step to avoid the
unreliable large-time steps of I2V-DMs, as well as an initial noise
distribution with optimal analytic expressions (Analytic-Init) by minimizing
the KL divergence between it and the actual marginal distribution to bridge the
training-inference gap. Second, we design a time-dependent noise distribution
(TimeNoise) for the conditional image during training, applying higher noise
levels at larger time steps to disrupt it and reduce the model's dependency on
it. We validate these general strategies on various I2V-DMs on our collected
open-domain image benchmark and the UCF101 dataset. Extensive results show that
our methods outperform baselines by producing higher motion scores with lower
errors while maintaining image alignment and temporal consistency, thereby
yielding superior overall performance and enabling more accurate motion
control. The project page: \url{https://cond-image-leak.github.io/}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024. Project page: https://cond-image-leak.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AIWR: Aerial Image Water Resource Dataset for Segmentation Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01797v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01797v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sangdaow Noppitak, Emmanuel Okafor, Olarik Surinta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective water resource management is crucial in agricultural regions like
northeastern Thailand, where limited water retention in sandy soils poses
significant challenges. In response to this issue, the Aerial Image Water
Resource (AIWR) dataset was developed, comprising 800 aerial images focused on
natural and artificial water bodies in this region. The dataset was created
using Bing Maps and follows the standards of the Fundamental Geographic Data
Set (FGDS). It includes ground truth annotations validated by experts in remote
sensing, making it an invaluable resource for researchers in geoinformatics,
computer vision, and artificial intelligence. The AIWR dataset presents
considerable challenges, such as segmentation due to variations in the size,
color, shape, and similarity of water bodies, which often resemble other land
use categories. The objective of the proposed dataset is to explore advanced
AI-driven methods for water body segmentation, addressing the unique challenges
posed by the dataset complexity and limited size. This dataset and related
research contribute to the development of novel algorithms for water
management, supporting sustainable agricultural practices in regions facing
similar challenges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Advancing Efficient Brain Tumor Multi-Class Classification -- New
  Insights from the Vision Mamba Model in Transfer Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21872v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21872v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinyi Lai, Anbo Cao, Yuan Gao, Jiaqi Shang, Zongyu Li, Jia Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Early and accurate diagnosis of brain tumors is crucial for improving patient
survival rates. However, the detection and classification of brain tumors are
challenging due to their diverse types and complex morphological
characteristics. This study investigates the application of pre-trained models
for brain tumor classification, with a particular focus on deploying the Mamba
model. We fine-tuned several mainstream transfer learning models and applied
them to the multi-class classification of brain tumors. By comparing these
models to those trained from scratch, we demonstrated the significant
advantages of transfer learning, especially in the medical imaging field, where
annotated data is often limited. Notably, we introduced the Vision Mamba (Vim),
a novel network architecture, and applied it for the first time in brain tumor
classification, achieving exceptional classification accuracy. Experimental
results indicate that the Vim model achieved 100% classification accuracy on an
independent test set, emphasizing its potential for tumor classification tasks.
These findings underscore the effectiveness of transfer learning in brain tumor
classification and reveal that, compared to existing state-of-the-art models,
the Vim model is lightweight, efficient, and highly accurate, offering a new
perspective for clinical applications. Furthermore, the framework proposed in
this study for brain tumor classification, based on transfer learning and the
Vision Mamba model, is broadly applicable to other medical imaging
classification problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ D2SP: Dynamic Dual-Stage Purification Framework for Dual Noise
  Mitigation in Vision-based Affective Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16473v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16473v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Wang, Xinji Mai, Zeng Tao, Xuan Tong, Junxiong Lin, Yan Wang, Jiawen Yu, Boyang Wang, Shaoqi Yan, Qing Zhao, Ziheng Zhou, Shuyong Gao, Wenqiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The contemporary state-of-the-art of Dynamic Facial Expression Recognition
(DFER) technology facilitates remarkable progress by deriving emotional
mappings of facial expressions from video content, underpinned by training on
voluminous datasets. Yet, the DFER datasets encompass a substantial volume of
noise data. Noise arises from low-quality captures that defy logical labeling,
and instances that suffer from mislabeling due to annotation bias, engendering
two principal types of uncertainty: the uncertainty regarding data usability
and the uncertainty concerning label reliability. Addressing the two types of
uncertainty, we have meticulously crafted a two-stage framework aiming at
\textbf{S}eeking \textbf{C}ertain data \textbf{I}n extensive \textbf{U}ncertain
data (SCIU). This initiative aims to purge the DFER datasets of these
uncertainties, thereby ensuring that only clean, verified data is employed in
training processes. To mitigate the issue of low-quality samples, we introduce
the Coarse-Grained Pruning (CGP) stage, which assesses sample weights and
prunes those deemed unusable due to their low weight. For samples with
incorrect annotations, the Fine-Grained Correction (FGC) stage evaluates
prediction stability to rectify mislabeled data. Moreover, SCIU is conceived as
a universally compatible, plug-and-play framework, tailored to integrate
seamlessly with prevailing DFER methodologies. Rigorous experiments across
prevalent DFER datasets and against numerous benchmark methods substantiates
SCIU's capacity to markedly elevate performance metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Document Parsing Unveiled: Techniques, Challenges, and Prospects for
  Structured Information Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21169v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21169v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qintong Zhang, Victor Shea-Jay Huang, Bin Wang, Junyuan Zhang, Zhengren Wang, Hao Liang, Shawn Wang, Matthieu Lin, Conghui He, Wentao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Document parsing is essential for converting unstructured and semi-structured
documents-such as contracts, academic papers, and invoices-into structured,
machine-readable data. Document parsing extract reliable structured data from
unstructured inputs, providing huge convenience for numerous applications.
Especially with recent achievements in Large Language Models, document parsing
plays an indispensable role in both knowledge base construction and training
data generation. This survey presents a comprehensive review of the current
state of document parsing, covering key methodologies, from modular pipeline
systems to end-to-end models driven by large vision-language models. Core
components such as layout detection, content extraction (including text,
tables, and mathematical expressions), and multi-modal data integration are
examined in detail. Additionally, this paper discusses the challenges faced by
modular document parsing systems and vision-language models in handling complex
layouts, integrating multiple modules, and recognizing high-density text. It
emphasizes the importance of developing larger and more diverse datasets and
outlines future research directions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Task-Specific Strategies for Accelerated MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.12507v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.12507v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihui Wu, Tianwei Yin, Yu Sun, Robert Frost, Andre van der Kouwe, Adrian V. Dalca, Katherine L. Bouman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compressed sensing magnetic resonance imaging (CS-MRI) seeks to recover
visual information from subsampled measurements for diagnostic tasks.
Traditional CS-MRI methods often separately address measurement subsampling,
image reconstruction, and task prediction, resulting in a suboptimal end-to-end
performance. In this work, we propose TACKLE as a unified co-design framework
for jointly optimizing subsampling, reconstruction, and prediction strategies
for the performance on downstream tasks. The na\"ive approach of simply
appending a task prediction module and training with a task-specific loss leads
to suboptimal downstream performance. Instead, we develop a training procedure
where a backbone architecture is first trained for a generic pre-training task
(image reconstruction in our case), and then fine-tuned for different
downstream tasks with a prediction head. Experimental results on multiple
public MRI datasets show that TACKLE achieves an improved performance on
various tasks over traditional CS-MRI methods. We also demonstrate that TACKLE
is robust to distribution shifts by showing that it generalizes to a new
dataset we experimentally collected using different acquisition setups from the
training data. Without additional fine-tuning, TACKLE leads to both numerical
and visual improvements compared to existing baselines. We have further
implemented a learned 4$\times$-accelerated sequence on a Siemens 3T MRI Skyra
scanner. Compared to the fully-sampling scan that takes 335 seconds, our
optimized sequence only takes 84 seconds, achieving a four-fold time reduction
as desired, while maintaining high performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our code is available at https://github.com/zihuiwu/TACKLE. More
  information can be found at http://imaging.cms.caltech.edu/tackle/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficiently Collecting Training Dataset for 2D Object Detection by
  Online Visual Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.04901v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.04901v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takuya Kiyokawa, Naoki Shirakura, Hiroki Katayama, Keita Tomochika, Jun Takamatsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training deep-learning-based vision systems require the manual annotation of
a significant number of images. Such manual annotation is highly time-consuming
and labor-intensive. Although previous studies have attempted to eliminate the
effort required for annotation, the effort required for image collection was
retained. To address this, we propose a human-in-the-loop dataset collection
method that uses a web application. To counterbalance the workload and
performance by encouraging the collection of multi-view object image datasets
in an enjoyable manner, thereby amplifying motivation, we propose three types
of online visual feedback features to track the progress of the collection
status. Our experiments thoroughly investigated the impact of each feature on
collection performance and quality of operation. The results suggested the
feasibility of annotation and object detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Estimating Epistemic and Aleatoric Uncertainty with a Single Model <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.03478v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.03478v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew A. Chan, Maria J. Molina, Christopher A. Metzler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating and disentangling epistemic uncertainty, uncertainty that is
reducible with more training data, and aleatoric uncertainty, uncertainty that
is inherent to the task at hand, is critically important when applying machine
learning to high-stakes applications such as medical imaging and weather
forecasting. Conditional diffusion models' breakthrough ability to accurately
and efficiently sample from the posterior distribution of a dataset now makes
uncertainty estimation conceptually straightforward: One need only train and
sample from a large ensemble of diffusion models. Unfortunately, training such
an ensemble becomes computationally intractable as the complexity of the model
architecture grows. In this work we introduce a new approach to ensembling,
hyper-diffusion models (HyperDM), which allows one to accurately estimate both
epistemic and aleatoric uncertainty with a single model. Unlike existing
single-model uncertainty methods like Monte-Carlo dropout and Bayesian neural
networks, HyperDM offers prediction accuracy on par with, and in some cases
superior to, multi-model ensembles. Furthermore, our proposed approach scales
to modern network architectures such as Attention U-Net and yields more
accurate uncertainty estimates compared to existing methods. We validate our
method on two distinct real-world tasks: x-ray computed tomography
reconstruction and weather temperature forecasting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 11 figures. To be published in Conference on Neural
  Information Processing Systems (NeurIPS) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FocalPose++: Focal Length and Object Pose Estimation via Render and
  Compare 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.02985v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.02985v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martin Cífka, Georgy Ponimatkin, Yann Labbé, Bryan Russell, Mathieu Aubry, Vladimir Petrik, Josef Sivic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce FocalPose++, a neural render-and-compare method for jointly
estimating the camera-object 6D pose and camera focal length given a single RGB
input image depicting a known object. The contributions of this work are
threefold. First, we derive a focal length update rule that extends an existing
state-of-the-art render-and-compare 6D pose estimator to address the joint
estimation task. Second, we investigate several different loss functions for
jointly estimating the object pose and focal length. We find that a combination
of direct focal length regression with a reprojection loss disentangling the
contribution of translation, rotation, and focal length leads to improved
results. Third, we explore the effect of different synthetic training data on
the performance of our method. Specifically, we investigate different
distributions used for sampling object's 6D pose and camera's focal length when
rendering the synthetic images, and show that parametric distribution fitted on
real training data works the best. We show results on three challenging
benchmark datasets that depict known 3D models in uncontrolled settings. We
demonstrate that our focal length and 6D pose estimates have lower error than
the existing state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 22 figures. IEEE TPAMI, 2024. Extended version of the
  conference paper arXiv:2204.05145</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transferable Learned Image Compression-Resistant Adversarial
  Perturbations <span class="chip">BMVC 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03115v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03115v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Sui, Zhuohang Li, Ding Ding, Xiang Pan, Xiaozhong Xu, Shan Liu, Zhenzhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial attacks can readily disrupt the image classification system,
revealing the vulnerability of DNN-based recognition tasks. While existing
adversarial perturbations are primarily applied to uncompressed images or
compressed images by the traditional image compression method, i.e., JPEG,
limited studies have investigated the robustness of models for image
classification in the context of DNN-based image compression. With the rapid
evolution of advanced image compression, DNN-based learned image compression
has emerged as the promising approach for transmitting images in many
security-critical applications, such as cloud-based face recognition and
autonomous driving, due to its superior performance over traditional
compression. Therefore, there is a pressing need to fully investigate the
robustness of a classification system post-processed by learned image
compression. To bridge this research gap, we explore the adversarial attack on
a new pipeline that targets image classification models that utilize learned
image compressors as pre-processing modules. Furthermore, to enhance the
transferability of perturbations across various quality levels and
architectures of learned image compression models, we introduce a saliency
score-based sampling method to enable the fast generation of transferable
perturbation. Extensive experiments with popular attack methods demonstrate the
enhanced transferability of our proposed method when attacking images that have
been post-processed with different learned image compression models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by BMVC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Machine learning approach to brain tumor detection and classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12692v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12692v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alice Oh, Inyoung Noh, Jian Choo, Jihoo Lee, Justin Park, Kate Hwang, Sanghyeon Kim, Soo Min Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Brain tumor detection and classification are critical tasks in medical image
analysis, particularly in early-stage diagnosis, where accurate and timely
detection can significantly improve treatment outcomes. In this study, we apply
various statistical and machine learning models to detect and classify brain
tumors using brain MRI images. We explore a variety of statistical models
including linear, logistic, and Bayesian regressions, and the machine learning
models including decision tree, random forest, single-layer perceptron,
multi-layer perceptron, convolutional neural network (CNN), recurrent neural
network, and long short-term memory. Our findings show that CNN outperforms
other models, achieving the best performance. Additionally, we confirm that the
CNN model can also work for multi-class classification, distinguishing between
four categories of brain MRI images such as normal, glioma, meningioma, and
pituitary tumor images. This study demonstrates that machine learning
approaches are suitable for brain tumor detection and classification,
facilitating real-world medical applications in assisting radiologists with
early and accurate diagnosis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 2 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ContextIQ: A <span class="highlight-title">Multimodal</span> Expert-Based Video Retrieval System for
  Contextual Advertising <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22233v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22233v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashutosh Chaubey, Anoubhav Agarwaal, Sartaki Sinha Roy, Aayush Agrawal, Susmita Ghose
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contextual advertising serves ads that are aligned to the content that the
user is viewing. The rapid growth of video content on social platforms and
streaming services, along with privacy concerns, has increased the need for
contextual advertising. Placing the right ad in the right context creates a
seamless and pleasant ad viewing experience, resulting in higher audience
engagement and, ultimately, better ad monetization. From a technology
standpoint, effective contextual advertising requires a video retrieval system
capable of understanding complex video content at a very granular level.
Current text-to-video retrieval models based on joint multimodal training
demand large datasets and computational resources, limiting their practicality
and lacking the key functionalities required for ad ecosystem integration. We
introduce ContextIQ, a multimodal expert-based video retrieval system designed
specifically for contextual advertising. ContextIQ utilizes modality-specific
experts-video, audio, transcript (captions), and metadata such as objects,
actions, emotion, etc.-to create semantically rich video representations. We
show that our system, without joint training, achieves better or comparable
results to state-of-the-art models and commercial solutions on multiple
text-to-video retrieval benchmarks. Our ablation studies highlight the benefits
of leveraging multiple modalities for enhanced video retrieval accuracy instead
of using a vision-language model alone. Furthermore, we show how video
retrieval systems such as ContextIQ can be used for contextual advertising in
an ad ecosystem while also addressing concerns related to brand safety and
filtering inappropriate content.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at WACV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ INQUIRE: A Natural World Text-to-Image Retrieval Benchmark <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02537v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02537v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edward Vendrow, Omiros Pantazis, Alexander Shepard, Gabriel Brostow, Kate E. Jones, Oisin Mac Aodha, Sara Beery, Grant Van Horn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce INQUIRE, a text-to-image retrieval benchmark designed to
challenge multimodal vision-language models on expert-level queries. INQUIRE
includes iNaturalist 2024 (iNat24), a new dataset of five million natural world
images, along with 250 expert-level retrieval queries. These queries are paired
with all relevant images comprehensively labeled within iNat24, comprising
33,000 total matches. Queries span categories such as species identification,
context, behavior, and appearance, emphasizing tasks that require nuanced image
understanding and domain expertise. Our benchmark evaluates two core retrieval
tasks: (1) INQUIRE-Fullrank, a full dataset ranking task, and (2)
INQUIRE-Rerank, a reranking task for refining top-100 retrievals. Detailed
evaluation of a range of recent multimodal models demonstrates that INQUIRE
poses a significant challenge, with the best models failing to achieve an
mAP@50 above 50%. In addition, we show that reranking with more powerful
multimodal models can enhance retrieval performance, yet there remains a
significant margin for improvement. By focusing on scientifically-motivated
ecological challenges, INQUIRE aims to bridge the gap between AI capabilities
and the needs of real-world scientific inquiry, encouraging the development of
retrieval systems that can assist with accelerating ecological and biodiversity
research. Our dataset and code are available at
https://inquire-benchmark.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in NeurIPS 2024, Datasets and Benchmarks Track</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-11-05T00:00:00Z">2024-11-05</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">106</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MME-Finance: A <span class="highlight-title">Multimodal</span> Finance Benchmark for Expert-level
  Understanding and Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03314v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03314v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziliang Gan, Yu Lu, Dong Zhang, Haohan Li, Che Liu, Jian Liu, Ji Liu, Haipang Wu, Chaoyou Fu, Zenglin Xu, Rongjunchen Zhang, Yong Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, multimodal benchmarks for general domains have guided the
rapid development of multimodal models on general tasks. However, the financial
field has its peculiarities. It features unique graphical images (e.g.,
candlestick charts, technical indicator charts) and possesses a wealth of
specialized financial knowledge (e.g., futures, turnover rate). Therefore,
benchmarks from general fields often fail to measure the performance of
multimodal models in the financial domain, and thus cannot effectively guide
the rapid development of large financial models. To promote the development of
large financial multimodal models, we propose MME-Finance, an bilingual
open-ended and practical usage-oriented Visual Question Answering (VQA)
benchmark. The characteristics of our benchmark are finance and expertise,
which include constructing charts that reflect the actual usage needs of users
(e.g., computer screenshots and mobile photography), creating questions
according to the preferences in financial domain inquiries, and annotating
questions by experts with 10+ years of experience in the financial industry.
Additionally, we have developed a custom-designed financial evaluation system
in which visual information is first introduced in the multi-modal evaluation
process. Extensive experimental evaluations of 19 mainstream MLLMs are
conducted to test their perception, reasoning, and cognition capabilities. The
results indicate that models performing well on general benchmarks cannot do
well on MME-Finance; for instance, the top-performing open-source and
closed-source models obtain 65.69 (Qwen2VL-72B) and 63.18 (GPT-4o),
respectively. Their performance is particularly poor in categories most
relevant to finance, such as candlestick charts and technical indicator charts.
In addition, we propose a Chinese version, which helps compare performance of
MLLMs under a Chinese context.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://hithink-research.github.io/MME-Finance/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLMs for Domain Generation Algorithm Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03307v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03307v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reynier Leyva La O, Carlos A. Catania, Tatiana Parlanti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work analyzes the use of large language models (LLMs) for detecting
domain generation algorithms (DGAs). We perform a detailed evaluation of two
important techniques: In-Context Learning (ICL) and Supervised Fine-Tuning
(SFT), showing how they can improve detection. SFT increases performance by
using domain-specific data, whereas ICL helps the detection model to quickly
adapt to new threats without requiring much retraining. We use Meta's Llama3 8B
model, on a custom dataset with 68 malware families and normal domains,
covering several hard-to-detect schemes, including recent word-based DGAs.
Results proved that LLM-based methods can achieve competitive results in DGA
detection. In particular, the SFT-based LLM DGA detector outperforms
state-of-the-art models using attention layers, achieving 94% accuracy with a
4% false positive rate (FPR) and excelling at detecting word-based DGA domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VERITAS: A Unified Approach to Reliability Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03300v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03300v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rajkumar Ramamurthy, Meghana Arakkal Rajeev, Oliver Molenschot, James Zou, Nazneen Rajani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) often fail to synthesize information from their
context to generate an accurate response. This renders them unreliable in
knowledge intensive settings where reliability of the output is key. A critical
component for reliable LLMs is the integration of a robust fact-checking system
that can detect hallucinations across various formats. While several
open-access fact-checking models are available, their functionality is often
limited to specific tasks, such as grounded question-answering or entailment
verification, and they perform less effectively in conversational settings. On
the other hand, closed-access models like GPT-4 and Claude offer greater
flexibility across different contexts, including grounded dialogue
verification, but are hindered by high costs and latency. In this work, we
introduce VERITAS, a family of hallucination detection models designed to
operate flexibly across diverse contexts while minimizing latency and costs.
VERITAS achieves state-of-the-art results considering average performance on
all major hallucination detection benchmarks, with $10\%$ increase in average
performance when compared to similar-sized models and get close to the
performance of GPT4 turbo with LLM-as-a-judge setting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SMoA: Improving Multi-agent Large Language Models with Sparse
  Mixture-of-Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03284v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03284v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dawei Li, Zhen Tan, Peijia Qian, Yifan Li, Kumar Satvik Chaudhary, Lijie Hu, Jiayi Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While multi-agent systems have been shown to significantly enhance the
performance of Large Language Models (LLMs) across various tasks and
applications, the dense interaction between scaling agents potentially hampers
their efficiency and diversity. To address these challenges, we draw
inspiration from the sparse mixture-of-agents (SMoE) and propose a sparse
mixture-of-agents (SMoA) framework to improve the efficiency and diversity of
multi-agent LLMs. Unlike completely connected structures, SMoA introduces novel
Response Selection and Early Stopping mechanisms to sparsify information flows
among individual LLM agents, striking a balance between performance and
efficiency. Additionally, inspired by the expert diversity principle in SMoE
frameworks for workload balance between experts, we assign distinct role
descriptions to each LLM agent, fostering diverse and divergent thinking.
Extensive experiments on reasoning, alignment, and fairness benchmarks
demonstrate that SMoA achieves performance comparable to traditional
mixture-of-agents approaches but with significantly lower computational costs.
Further analysis reveals that SMoA is more stable, has a greater capacity to
scale, and offers considerable potential through hyper-parameter optimization.
Code and data will be available at: https://github.com/David-Li0406/SMoA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffLM: Controllable Synthetic Data Generation via Diffusion Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03250v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03250v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ying Zhou, Xinyao Wang, Yulei Niu, Yaojie Shen, Lexin Tang, Fan Chen, Ben He, Le Sun, Longyin Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) have significantly
enhanced their knowledge and generative capabilities, leading to a surge of
interest in leveraging LLMs for high-quality data synthesis. However, synthetic
data generation via prompting LLMs remains challenging due to LLMs' limited
understanding of target data distributions and the complexity of prompt
engineering, especially for structured formatted data. To address these issues,
we introduce DiffLM, a controllable data synthesis framework based on
variational autoencoder (VAE), which further (1) leverages diffusion models to
reserve more information of original distribution and format structure in the
learned latent distribution and (2) decouples the learning of target
distribution knowledge from the LLM's generative objectives via a plug-and-play
latent feature injection module. As we observed significant discrepancies
between the VAE's latent representations and the real data distribution, the
latent diffusion module is introduced into our framework to learn a fully
expressive latent distribution. Evaluations on seven real-world datasets with
structured formatted data (i.e., Tabular, Code and Tool data) demonstrate that
DiffLM generates high-quality data, with performance on downstream tasks
surpassing that of real data by 2-7 percent in certain cases. The data and code
will be publicly available upon completion of internal review.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predictor-Corrector Enhanced Transformers with Exponential Moving
  Average Coefficient Learning <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03042v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03042v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bei Li, Tong Zheng, Rui Wang, Jiahao Liu, Qingyan Guo, Junliang Guo, Xu Tan, Tong Xiao, Jingbo Zhu, Jingang Wang, Xunliang Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Residual networks, as discrete approximations of Ordinary Differential
Equations (ODEs), have inspired significant advancements in neural network
design, including multistep methods, high-order methods, and multi-particle
dynamical systems. The precision of the solution to ODEs significantly affects
parameter optimization, thereby impacting model performance. In this work, we
present a series of advanced explorations of Transformer architecture design to
minimize the error compared to the true ``solution.'' First, we introduce a
predictor-corrector learning framework to minimize truncation errors, which
consists of a high-order predictor and a multistep corrector. Second, we
propose an exponential moving average-based coefficient learning method to
strengthen our higher-order predictor. Extensive experiments on large-scale
machine translation, abstractive summarization, language modeling, and natural
language understanding benchmarks demonstrate the superiority of our approach.
On the WMT'14 English-German and English-French tasks, our model achieved BLEU
scores of 30.95 and 44.27, respectively. Furthermore, on the OPUS multilingual
machine translation task, our model surpasses a robust 3.8B DeepNet by an
average of 2.9 SacreBLEU, using only 1/3 parameters. Notably, it also beats
LLama models by 5.7 accuracy points on the LM Harness Evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Compositional Data Augmentation for Scientific Keyphrase Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03039v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03039v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mael Houbre, Florian Boudin, Beatrice Daille, Akiko Aizawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art models for keyphrase generation require large amounts of
training data to achieve good performance. However, obtaining keyphrase-labeled
documents can be challenging and costly. To address this issue, we present a
self-compositional data augmentation method. More specifically, we measure the
relatedness of training documents based on their shared keyphrases, and combine
similar documents to generate synthetic samples. The advantage of our method
lies in its ability to create additional training samples that keep domain
coherence, without relying on external data or resources. Our results on
multiple datasets spanning three different domains, demonstrate that our method
consistently improves keyphrase generation. A qualitative analysis of the
generated keyphrases for the Computer Science domain confirms this improvement
towards their representativity property.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to JCDL 2024 This version is not the final camera ready
  version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Large Language Models in Code Question Answering: Baselines
  and Issues 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03012v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03012v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgy Andryushchenko, Vladimir Ivanov, Vladimir Makharev, Elizaveta Tukhtina, Aidar Valeev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Question answering over source code provides software engineers and project
managers with helpful information about the implemented features of a software
product. This paper presents a work devoted to using large language models for
question answering over source code in Python. The proposed method for
implementing a source code question answering system involves fine-tuning a
large language model on a unified dataset of questions and answers for Python
code. To achieve the highest quality answers, we tested various models trained
on datasets preprocessed in different ways: a dataset without grammar
correction, a dataset with grammar correction, and a dataset augmented with the
generated summaries. The model answers were also analyzed for errors manually.
We report BLEU-4, BERTScore F1, BLEURT, and Exact Match metric values, along
with the conclusions from the manual error analysis. The obtained experimental
results highlight the current problems of the research area, such as poor
quality of the public genuine question-answering datasets. In addition, the
findings include the positive effect of the grammar correction of the training
data on the testing metric values. The addressed findings and issues could be
important for other researchers who attempt to improve the quality of source
code question answering solutions. The training and evaluation code is publicly
available at https://github.com/IU-AES-AI4Code/CodeQuestionAnswering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 3 figures, Accepted to NLP (CCIS) @ AIST'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Growing a Tail: Increasing Output Diversity in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02989v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02989v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michal Shur-Ofry, Bar Horowitz-Amsalem, Adir Rahamim, Yonatan Belinkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How diverse are the outputs of large language models when diversity is
desired? We examine the diversity of responses of various models to questions
with multiple possible answers, comparing them with human responses. Our
findings suggest that models' outputs are highly concentrated, reflecting a
narrow, mainstream 'worldview', in comparison to humans, whose responses
exhibit a much longer-tail. We examine three ways to increase models' output
diversity: 1) increasing generation randomness via temperature sampling; 2)
prompting models to answer from diverse perspectives; 3) aggregating outputs
from several models. A combination of these measures significantly increases
models' output diversity, reaching that of humans. We discuss implications of
these findings for AI policy that wishes to preserve cultural diversity, an
essential building block of a democratic social fabric.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ [Vision Paper] PRObot: Enhancing Patient-Reported Outcome Measures for
  Diabetic Retinopathy using Chatbots and Generative AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02973v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02973v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maren Pielka, Tobias Schneider, Jan Terheyden, Rafet Sifa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present an outline of the first large language model (LLM) based chatbot
application in the context of patient-reported outcome measures (PROMs) for
diabetic retinopathy. By utilizing the capabilities of current LLMs, we enable
patients to provide feedback about their quality of life and treatment progress
via an interactive application. The proposed framework offers significant
advantages over the current approach, which encompasses only qualitative
collection of survey data or a static survey with limited answer options. Using
the PROBot LLM-PROM application, patients will be asked tailored questions
about their individual challenges, and can give more detailed feedback on the
progress of their treatment. Based on this input, we will use machine learning
to infer conventional PROM scores, which can be used by clinicians to evaluate
the treatment status. The goal of the application is to improve adherence to
the healthcare system and treatments, and thus ultimately reduce cases of
subsequent vision impairment. The approach needs to be further validated using
a survey and a clinical study.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Grounding Natural Language to SQL Translation with Data-Based
  Self-Explanations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02948v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02948v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuankai Fan, Tonghui Ren, Can Huang, Zhenying He, X. Sean Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural Language Interfaces for Databases empower non-technical users to
interact with data using natural language (NL). Advanced approaches, utilizing
either neural sequence-to-sequence or more recent sophisticated large-scale
language models, typically implement NL to SQL (NL2SQL) translation in an
end-to-end fashion. However, like humans, these end-to-end translation models
may not always generate the best SQL output on their first try. In this paper,
we propose CycleSQL, an iterative framework designed for end-to-end translation
models to autonomously generate the best output through self-evaluation. The
main idea of CycleSQL is to introduce data-grounded NL explanations of query
results as self-provided feedback, and use the feedback to validate the
correctness of the translation iteratively, hence improving the overall
translation accuracy. Extensive experiments, including quantitative and
qualitative evaluations, are conducted to study CycleSQL by applying it to
seven existing translation models on five widely used benchmarks. The results
show that 1) the feedback loop introduced in CycleSQL can consistently improve
the performance of existing models, and in particular, by applying CycleSQL to
RESDSQL, obtains a translation accuracy of 82.0% (+2.6%) on the validation set,
and 81.6% (+3.2%) on the test set of Spider benchmark; 2) the generated NL
explanations can also provide insightful information for users, aiding in the
comprehension of translation results and consequently enhancing the
interpretability of NL2SQL translation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Capturing research literature attitude towards Sustainable Development
  Goals: an LLM-based topic modeling approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02943v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02943v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Invernici, Francesca Curati, Jelena Jakimov, Amirhossein Samavi, Anna Bernasconi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The world is facing a multitude of challenges that hinder the development of
human civilization and the well-being of humanity on the planet. The
Sustainable Development Goals (SDGs) were formulated by the United Nations in
2015 to address these global challenges by 2030. Natural language processing
techniques can help uncover discussions on SDGs within research literature. We
propose a completely automated pipeline to 1) fetch content from the Scopus
database and prepare datasets dedicated to five groups of SDGs; 2) perform
topic modeling, a statistical technique used to identify topics in large
collections of textual data; and 3) enable topic exploration through
keywords-based search and topic frequency time series extraction. For topic
modeling, we leverage the stack of BERTopic scaled up to be applied on large
corpora of textual documents (we find hundreds of topics on hundreds of
thousands of documents), introducing i) a novel LLM-based embeddings
computation for representing scientific abstracts in the continuous space and
ii) a hyperparameter optimizer to efficiently find the best configuration for
any new big datasets. We additionally produce the visualization of results on
interactive dashboards reporting topics' temporal evolution. Results are made
inspectable and explorable, contributing to the interpretability of the topic
modeling process. Our proposed LLM-based topic modeling pipeline for big-text
datasets allows users to capture insights on the evolution of the attitude
toward SDGs within scientific abstracts in the 2006-2023 time span. All the
results are reproducible by using our system; the workflow can be generalized
to be applied at any point in time to any big corpus of textual documents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 8 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Post-Training Enhanced Optimization Approach for Small Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02939v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02939v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keke Zhai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper delves into the continuous post-training optimization methods for
small language models, and proposes a continuous post-training alignment data
construction method for small language models. The core of this method is based
on the data guidance of large models, optimizing the diversity and accuracy of
alignment data. In addition, to verify the effectiveness of the methods in this
paper, we used Qwen2-0.5B-Instruct model as the baseline model for small
language models, using the alignment dataset constructed by our proposed
method, we trained and compared several groups of experiments, including SFT
(Supervised Fine Tuning) post-training experiment and KTO (Kahneman Tversky
optimization) post-training experiment, as well as SFT-KTO two-stage
post-training experiment and model weight fusion experiment. Finally, we
evaluated and analyzed the performance of post-training models, and confirmed
that the continuous post-training optimization method proposed by us can
significantly improve the performance of small language models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking <span class="highlight-title">Multimodal</span> Retrieval Augmented Generation with Dynamic VQA
  Dataset and Self-adaptive Planning Agent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02937v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02937v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangning Li, Yinghui Li, Xingyu Wang, Yong Jiang, Zhen Zhang, Xinran Zheng, Hui Wang, Hai-Tao Zheng, Philip S. Yu, Fei Huang, Jingren Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Retrieval Augmented Generation (mRAG) plays an important role in
mitigating the "hallucination" issue inherent in multimodal large language
models (MLLMs). Although promising, existing heuristic mRAGs typically
predefined fixed retrieval processes, which causes two issues: (1) Non-adaptive
Retrieval Queries. (2) Overloaded Retrieval Queries. However, these flaws
cannot be adequately reflected by current knowledge-seeking visual question
answering (VQA) datasets, since the most required knowledge can be readily
obtained with a standard two-step retrieval. To bridge the dataset gap, we
first construct Dyn-VQA dataset, consisting of three types of "dynamic"
questions, which require complex knowledge retrieval strategies variable in
query, tool, and time: (1) Questions with rapidly changing answers. (2)
Questions requiring multi-modal knowledge. (3) Multi-hop questions. Experiments
on Dyn-VQA reveal that existing heuristic mRAGs struggle to provide sufficient
and precisely relevant knowledge for dynamic questions due to their rigid
retrieval processes. Hence, we further propose the first self-adaptive planning
agent for multimodal retrieval, OmniSearch. The underlying idea is to emulate
the human behavior in question solution which dynamically decomposes complex
multimodal questions into sub-question chains with retrieval action. Extensive
experiments prove the effectiveness of our OmniSearch, also provide direction
for advancing mRAG. The code and dataset will be open-sourced at
https://github.com/Alibaba-NLP/OmniSearch.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Textual Aesthetics in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02930v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02930v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingjie Jiang, Shaohan Huang, Xun Wu, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image aesthetics is a crucial metric in the field of image generation.
However, textual aesthetics has not been sufficiently explored. With the
widespread application of large language models (LLMs), previous work has
primarily focused on the correctness of content and the helpfulness of
responses. Nonetheless, providing responses with textual aesthetics is also an
important factor for LLMs, which can offer a cleaner layout and ensure greater
consistency and coherence in content. In this work, we introduce a pipeline for
aesthetics polishing and help construct a textual aesthetics dataset named
TexAes. We propose a textual aesthetics-powered fine-tuning method based on
direct preference optimization, termed TAPO, which leverages textual aesthetics
without compromising content correctness. Additionally, we develop two
evaluation methods for textual aesthetics based on text and image analysis,
respectively. Our experiments demonstrate that using textual aesthetics data
and employing the TAPO fine-tuning method not only improves aesthetic scores
but also enhances performance on general evaluation datasets such as
AlpacalEval and Anera-hard.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Membership Inference Attacks against Large <span class="highlight-title">Vision-Language</span> Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02902v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02902v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhan Li, Yongtao Wu, Yihang Chen, Francesco Tonin, Elias Abad Rocamora, Volkan Cevher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large vision-language models (VLLMs) exhibit promising capabilities for
processing multi-modal tasks across various application scenarios. However,
their emergence also raises significant data security concerns, given the
potential inclusion of sensitive information, such as private photos and
medical records, in their training datasets. Detecting inappropriately used
data in VLLMs remains a critical and unresolved issue, mainly due to the lack
of standardized datasets and suitable methodologies. In this study, we
introduce the first membership inference attack (MIA) benchmark tailored for
various VLLMs to facilitate training data detection. Then, we propose a novel
MIA pipeline specifically designed for token-level image detection. Lastly, we
present a new metric called MaxR\'enyi-K%, which is based on the confidence of
the model output and applies to both text and image data. We believe that our
work can deepen the understanding and methodology of MIAs in the context of
VLLMs. Our code and datasets are available at
https://github.com/LIONS-EPFL/VL-MIA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Translation of Circumlocution in Arabic Short Stories into English 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02887v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02887v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dalal Waadallah Shehab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study investigates the translation of circumlocution from Arabic to
English in a corpus of short stories by renowned Arabic authors. By analyzing
the source and target texts, the study aims to identify and categorize
circumlocution instances in Arabic and their corresponding renditions in
English. The study employs Nida's (1964) translation theory as a framework to
assess the appropriateness of the translation strategies employed. It examines
the extent to which translators successfully rendered Arabic circumlocution
into English, identifying potential challenges and limitations in the
translation process. The findings reveal significant similarities between
Arabic circumlocution categories and English metadiscourse categories,
particularly in terms of textual and interpersonal functions. However, the
study also highlights instances where translators encountered difficulties in
accurately conveying the nuances of circumlocution, often resorting to
strategies like addition, subtraction, and alteration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TokenSelect: Efficient Long-Context Inference and Length Extrapolation
  for LLMs via Dynamic Token-Level KV Cache Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02886v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02886v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Wu, Zhuoshi Pan, Chao Wang, Liyi Chen, Yunchu Bai, Kun Fu, Zheng Wang, Hui Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the development of large language models (LLMs), the ability to handle
longer contexts has become a key capability for Web applications such as
cross-document understanding and LLM-powered search systems. However, this
progress faces two major challenges: performance degradation due to sequence
lengths out-of-distribution, and excessively long inference times caused by the
quadratic computational complexity of attention. These issues hinder the
application of LLMs in long-context scenarios. In this paper, we propose
Dynamic Token-Level KV Cache Selection (TokenSelect), a model-agnostic,
training-free method for efficient and accurate long-context inference.
TokenSelect builds upon the observation of non-contiguous attention sparsity,
using Query-Key dot products to measure per-head KV Cache criticality at
token-level. By per-head soft voting mechanism, TokenSelect selectively
involves a small number of critical KV cache tokens in the attention
calculation without sacrificing accuracy. To further accelerate TokenSelect, we
designed the Selection Cache based on observations of consecutive Query
similarity and implemented efficient dot product kernel, significantly reducing
the overhead of token selection. A comprehensive evaluation of TokenSelect
demonstrates up to 23.84x speedup in attention computation and up to 2.28x
acceleration in end-to-end latency, while providing superior performance
compared to state-of-the-art long-context inference methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph-DPEP: Decomposed Plug and Ensemble Play for Few-Shot Document
  Relation Extraction with Graph-of-Thoughts Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02864v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02864v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Zhang, Ning Yan, Masood Mortazavi, Hoang H. Nguyen, Zhongfen Deng, Philip S. Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) pre-trained on massive corpora have demonstrated
impressive few-shot learning capability on many NLP tasks. Recasting an NLP
task into a text-to-text generation task is a common practice so that
generative LLMs can be prompted to resolve it. However, performing
document-level relation extraction (DocRE) tasks with generative LLM models is
still challenging due to the structured output format of DocRE, which
complicates the conversion to plain text. Limited information available in
few-shot samples and prompt instructions induce further difficulties and
challenges in relation extraction for mentioned entities in a document. In this
paper, we represent the structured output as a graph-style triplet rather than
natural language expressions and leverage generative LLMs for the DocRE task.
Our approach, the Graph-DPEP framework is grounded in the reasoning behind
triplet explanation thoughts presented in natural language. In this framework,
we first introduce a ``decomposed-plug" method for performing the generation
from LLMs over prompts with type-space decomposition to alleviate the burden of
distinguishing all relation types. Second, we employ a verifier for calibrating
the generation and identifying overlooked query entity pairs. Third, we develop
"ensemble-play", reapplying generation on the entire type list by leveraging
the reasoning thoughts embedded in a sub-graph associated with the missing
query pair to address the missingness issue. Through extensive comparisons with
existing prompt techniques and alternative Language Models (LLMs), our
framework demonstrates superior performance on publicly available benchmarks in
experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Unify Audio, Visual and Text for Audio-Enhanced Multilingual
  Visual Answer Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02851v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02851v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhibin Wen, Bin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of Multilingual Visual Answer Localization (MVAL) is to locate a
video segment that answers a given multilingual question. Existing methods
either focus solely on visual modality or integrate visual and subtitle
modalities. However, these methods neglect the audio modality in videos,
consequently leading to incomplete input information and poor performance in
the MVAL task. In this paper, we propose a unified Audio-Visual-Textual Span
Localization (AVTSL) method that incorporates audio modality to augment both
visual and textual representations for the MVAL task. Specifically, we
integrate features from three modalities and develop three predictors, each
tailored to the unique contributions of the fused modalities: an audio-visual
predictor, a visual predictor, and a textual predictor. Each predictor
generates predictions based on its respective modality. To maintain consistency
across the predicted results, we introduce an Audio-Visual-Textual Consistency
module. This module utilizes a Dynamic Triangular Loss (DTL) function, allowing
each modality's predictor to dynamically learn from the others. This
collaborative learning ensures that the model generates consistent and
comprehensive answers. Extensive experiments show that our proposed method
outperforms several state-of-the-art (SOTA) methods, which demonstrates the
effectiveness of the audio modality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PersianRAG: A Retrieval-Augmented Generation System for Persian Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02832v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02832v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hossein Hosseini, Mohammad Siobhan Zare, Amir Hossein Mohammadi, Arefeh Kazemi, Zahra Zojaji, Mohammad Ali Nematbakhsh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval augmented generation (RAG) models, which integrate large-scale
pre-trained generative models with external retrieval mechanisms, have shown
significant success in various natural language processing (NLP) tasks.
However, applying RAG models in Persian language as a low-resource language,
poses distinct challenges. These challenges primarily involve the
preprocessing, embedding, retrieval, prompt construction, language modeling,
and response evaluation of the system. In this paper, we address the challenges
towards implementing a real-world RAG system for Persian language called
PersianRAG. We propose novel solutions to overcome these obstacles and evaluate
our approach using several Persian benchmark datasets. Our experimental results
demonstrate the capability of the PersianRAG framework to enhance question
answering task in Persian.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mixtures of In-Context Learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02830v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02830v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giwon Hong, Emile van Krieken, Edoardo Ponti, Nikolay Malkin, Pasquale Minervini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context learning (ICL) adapts LLMs by providing demonstrations without
fine-tuning the model parameters; however, it does not differentiate between
demonstrations and quadratically increases the complexity of Transformer LLMs,
exhausting the memory. As a solution, we propose Mixtures of In-Context
Learners (MoICL), a novel approach to treat subsets of demonstrations as
experts and learn a weighting function to merge their output distributions
based on a training set. In our experiments, we show performance improvements
on 5 out of 7 classification datasets compared to a set of strong baselines (up
to +13\% compared to ICL and LENS). Moreover, we enhance the Pareto frontier of
ICL by reducing the inference time needed to achieve the same performance with
fewer demonstrations. Finally, MoICL is more robust to out-of-domain (up to
+11\%), imbalanced (up to +49\%), or noisy demonstrations (up to +38\%) or can
filter these out from datasets. Overall, MoICL is a more expressive approach to
learning from demonstrations without exhausting the context window or memory.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DroidSpeak: Enhancing Cross-LLM Communication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02820v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02820v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhan Liu, Esha Choukse, Shan Lu, Junchen Jiang, Madan Musuvathi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In multi-agent systems utilizing Large Language Models (LLMs), communication
between agents traditionally relies on natural language. This communication
often includes the full context of the query so far, which can introduce
significant prefill-phase latency, especially with long contexts.
  We introduce DroidSpeak, a novel framework to target this cross-LLM
communication by leveraging the reuse of intermediate data, such as input
embeddings (E-cache) and key-value caches (KV-cache). We efficiently bypass the
need to reprocess entire contexts for fine-tuned versions of the same
foundational model. This approach allows faster context integration while
maintaining the quality of task performance. Experimental evaluations
demonstrate DroidSpeak's ability to significantly accelerate inter-agent
communication, achieving up to a 2.78x speedup in prefill latency with
negligible loss in accuracy. Our findings underscore the potential to create
more efficient and scalable multi-agent systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Evolution of RWKV: Advancements in Efficient Language Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02795v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02795v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akul Datta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper reviews the development of the Receptance Weighted Key Value
(RWKV) architecture, emphasizing its advancements in efficient language
modeling. RWKV combines the training efficiency of Transformers with the
inference efficiency of RNNs through a novel linear attention mechanism. We
examine its core innovations, adaptations across various domains, and
performance advantages over traditional models. The paper also discusses
challenges and future directions for RWKV as a versatile architecture in deep
learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Toward Robust Incomplete <span class="highlight-title">Multimodal</span> Sentiment Analysis via Hierarchical
  Representation Learning <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02793v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02793v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingcheng Li, Dingkang Yang, Yang Liu, Shunli Wang, Jiawei Chen, Shuaibing Wang, Jinjie Wei, Yue Jiang, Qingyao Xu, Xiaolu Hou, Mingyang Sun, Ziyun Qian, Dongliang Kou, Lihua Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Sentiment Analysis (MSA) is an important research area that aims
to understand and recognize human sentiment through multiple modalities. The
complementary information provided by multimodal fusion promotes better
sentiment analysis compared to utilizing only a single modality. Nevertheless,
in real-world applications, many unavoidable factors may lead to situations of
uncertain modality missing, thus hindering the effectiveness of multimodal
modeling and degrading the model's performance. To this end, we propose a
Hierarchical Representation Learning Framework (HRLF) for the MSA task under
uncertain missing modalities. Specifically, we propose a fine-grained
representation factorization module that sufficiently extracts valuable
sentiment information by factorizing modality into sentiment-relevant and
modality-specific representations through crossmodal translation and sentiment
semantic reconstruction. Moreover, a hierarchical mutual information
maximization mechanism is introduced to incrementally maximize the mutual
information between multi-scale representations to align and reconstruct the
high-level semantics in the representations. Ultimately, we propose a
hierarchical adversarial learning mechanism that further aligns and adapts the
latent distribution of sentiment-relevant representations to produce robust
joint multimodal representations. Comprehensive experiments on three datasets
demonstrate that HRLF significantly improves MSA performance under uncertain
modality missing cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Models and Cycle Consistency for Self-Reflective Machine
  Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02791v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02791v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianqiao Wangni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel framework that leverages large language models
(LLMs) for machine translation (MT). We start with one conjecture: an ideal
translation should contain complete and accurate information for a strong
enough LLM to recover the original sentence. We generate multiple translation
candidates from a source language A to a target language B, and subsequently
translate these candidates back to the original language A. By evaluating the
cycle consistency between the original and back-translated sentences using
metrics such as token-level precision and accuracy, we implicitly estimate the
translation quality in language B, without knowing its ground-truth. This also
helps to evaluate the LLM translation capability, only with monolingual
corpora. For each source sentence, we identify the translation candidate with
optimal cycle consistency with the original sentence as the final answer. Our
experiments demonstrate that larger LLMs, or the same LLM with more forward
passes during inference, exhibit increased cycle consistency, aligning with the
LLM model size scaling law and test-time computation scaling law. This work
provide methods for, 1) to implicitly evaluate translation quality of a
sentence in the target language, 2), to evaluate capability of LLM for
any-to-any-language translation, and 3), how to generate a better translation
for a specific LLM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Memory Augmented Cross-encoders for Controllable Personalized Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02790v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02790v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sheshera Mysore, Garima Dhanania, Kishor Patil, Surya Kallumadi, Andrew McCallum, Hamed Zamani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personalized search represents a problem where retrieval models condition on
historical user interaction data in order to improve retrieval results.
However, personalization is commonly perceived as opaque and not amenable to
control by users. Further, personalization necessarily limits the space of
items that users are exposed to. Therefore, prior work notes a tension between
personalization and users' ability for discovering novel items. While discovery
of novel items in personalization setups may be resolved through search result
diversification, these approaches do little to allow user control over
personalization. Therefore, in this paper, we introduce an approach for
controllable personalized search. Our model, CtrlCE presents a novel
cross-encoder model augmented with an editable memory constructed from users
historical items. Our proposed memory augmentation allows cross-encoder models
to condition on large amounts of historical user data and supports interaction
from users permitting control over personalization. Further, controllable
personalization for search must account for queries which don't require
personalization, and in turn user control. For this, we introduce a calibrated
mixing model which determines when personalization is necessary. This allows
system designers using CtrlCE to only obtain user input for control when
necessary. In multiple datasets of personalized search, we show CtrlCE to
result in effective personalization as well as fulfill various key goals for
controllable personalized search.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Novelty-focused R&D landscaping using transformer and local outlier
  factor 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02738v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02738v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaewoong Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While numerous studies have explored the field of research and development
(R&D) landscaping, the preponderance of these investigations has emphasized
predictive analysis based on R&D outcomes, specifically patents, and academic
literature. However, the value of research proposals and novelty analysis has
seldom been addressed. This study proposes a systematic approach to
constructing and navigating the R&D landscape that can be utilized to guide
organizations to respond in a reproducible and timely manner to the challenges
presented by increasing number of research proposals. At the heart of the
proposed approach is the composite use of the transformer-based language model
and the local outlier factor (LOF). The semantic meaning of the research
proposals is captured with our further-trained transformers, thereby
constructing a comprehensive R&D landscape. Subsequently, the novelty of the
newly selected research proposals within the annual landscape is quantified on
a numerical scale utilizing the LOF by assessing the dissimilarity of each
proposal to others preceding and within the same year. A case study examining
research proposals in the energy and resource sector in South Korea is
presented. The systematic process and quantitative outcomes are expected to be
useful decision-support tools, providing future insights regarding R&D planning
and roadmapping.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Natural Language Processing Approach to Support Biomedical Data
  Harmonization: Leveraging Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02730v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02730v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zexu Li, Suraj P. Prabhu, Zachary T. Popp, Shubhi S. Jain, Vijetha Balakundi, Ting Fang Alvin Ang, Rhoda Au, Jinying Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Biomedical research requires large, diverse samples to produce unbiased
results. Automated methods for matching variables across datasets can
accelerate this process. Research in this area has been limited, primarily
focusing on lexical matching and ontology based semantic matching. We aimed to
develop new methods, leveraging large language models (LLM) and ensemble
learning, to automate variable matching. Methods: We utilized data from two
GERAS cohort (European and Japan) studies to develop variable matching methods.
We first manually created a dataset by matching 352 EU variables with 1322
candidate JP variables, where matched variable pairs were positive and
unmatched pairs were negative instances. Using this dataset, we developed and
evaluated two types of natural language processing (NLP) methods, which matched
variables based on variable labels and definitions from data dictionaries: (1)
LLM-based and (2) fuzzy matching. We then developed an ensemble-learning
method, using the Random Forest model, to integrate individual NLP methods. RF
was trained and evaluated on 50 trials. Each trial had a random split (4:1) of
training and test sets, with the model's hyperparameters optimized through
cross-validation on the training set. For each EU variable, 1322 candidate JP
variables were ranked based on NLP-derived similarity scores or RF's
probability scores, denoting their likelihood to match the EU variable. Ranking
performance was measured by top-n hit ratio (HRn) and mean reciprocal rank
(MRR). Results:E5 performed best among individual methods, achieving 0.90 HR-30
and 0.70 MRR. RF performed better than E5 on all metrics over 50 trials (P less
than 0.001) and achieved an average HR 30 of 0.98 and MRR of 0.73. LLM-derived
features contributed most to RF's performance. One major cause of errors in
automatic variable matching was ambiguous variable definitions within data
dictionaries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Multimodal</span> Commonsense Knowledge Distillation for Visual Question
  Answering <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02722v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02722v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuo Yang, Siwen Luo, Soyeon Caren Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing Multimodal Large Language Models (MLLMs) and Visual Language
Pretrained Models (VLPMs) have shown remarkable performances in the general
Visual Question Answering (VQA). However, these models struggle with VQA
questions that require external commonsense knowledge due to the challenges in
generating high-quality prompts and the high computational costs of
fine-tuning. In this work, we propose a novel graph-based multimodal
commonsense knowledge distillation framework that constructs a unified
relational graph over commonsense knowledge, visual objects and questions
through a Graph Convolutional Network (GCN) following a teacher-student
environment. This proposed framework is flexible with any type of teacher and
student models without further fine-tuning, and has achieved competitive
performances on the ScienceQA dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2025 (Accepted, Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Game Plot Design with an LLM-powered Assistant: An Empirical Study with
  Game Designers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02714v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02714v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seyed Hossein Alavi, Weijia Xu, Nebojsa Jojic, Daniel Kennett, Raymond T. Ng, Sudha Rao, Haiyan Zhang, Bill Dolan, Vered Shwartz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce GamePlot, an LLM-powered assistant that supports game designers
in crafting immersive narratives for turn-based games, and allows them to test
these games through a collaborative game play and refine the plot throughout
the process. Our user study with 14 game designers shows high levels of both
satisfaction with the generated game plots and sense of ownership over the
narratives, but also reconfirms that LLM are limited in their ability to
generate complex and truly innovative content. We also show that diverse user
populations have different expectations from AI assistants, and encourage
researchers to study how tailoring assistants to diverse user groups could
potentially lead to increased job satisfaction and greater creativity and
innovation over time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Response Uncertainty in MLLMs: An Empirical Evaluation under
  Misleading Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02708v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02708v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunkai Dang, Mengxi Gao, Yibo Yan, Xin Zou, Yanggan Gu, Aiwei Liu, Xuming Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring that Multimodal Large Language Models (MLLMs) maintain consistency
in their responses is essential for developing trustworthy multimodal
intelligence. However, existing benchmarks include many samples where all MLLMs
\textit{exhibit high response uncertainty when encountering misleading
information}, requiring even 5-15 response attempts per sample to effectively
assess uncertainty. Therefore, we propose a two-stage pipeline: first, we
collect MLLMs' responses without misleading information, and then gather
misleading ones via specific misleading instructions. By calculating the
misleading rate, and capturing both correct-to-incorrect and
incorrect-to-correct shifts between the two sets of responses, we can
effectively metric the model's response uncertainty. Eventually, we establish a
\textbf{\underline{M}}ultimodal \textbf{\underline{U}}ncertainty
\textbf{\underline{B}}enchmark (\textbf{MUB}) that employs both explicit and
implicit misleading instructions to comprehensively assess the vulnerability of
MLLMs across diverse domains. Our experiments reveal that all open-source and
close-source MLLMs are highly susceptible to misleading instructions, with an
average misleading rate exceeding 86\%. To enhance the robustness of MLLMs, we
further fine-tune all open-source MLLMs by incorporating explicit and implicit
misleading data, which demonstrates a significant reduction in misleading
rates. Our code is available at:
\href{https://github.com/Yunkai696/MUB}{https://github.com/Yunkai696/MUB}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RT-Affordance: Affordances are Versatile Intermediate Representations
  for Robot Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02704v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02704v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soroush Nasiriany, Sean Kirmani, Tianli Ding, Laura Smith, Yuke Zhu, Danny Driess, Dorsa Sadigh, Ted Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore how intermediate policy representations can facilitate
generalization by providing guidance on how to perform manipulation tasks.
Existing representations such as language, goal images, and trajectory sketches
have been shown to be helpful, but these representations either do not provide
enough context or provide over-specified context that yields less robust
policies. We propose conditioning policies on affordances, which capture the
pose of the robot at key stages of the task. Affordances offer expressive yet
lightweight abstractions, are easy for users to specify, and facilitate
efficient learning by transferring knowledge from large internet datasets. Our
method, RT-Affordance, is a hierarchical model that first proposes an
affordance plan given the task language, and then conditions the policy on this
affordance plan to perform manipulation. Our model can flexibly bridge
heterogeneous sources of supervision including large web datasets and robot
trajectories. We additionally train our model on cheap-to-collect in-domain
affordance images, allowing us to learn new tasks without collecting any
additional costly robot trajectories. We show on a diverse set of novel tasks
how RT-Affordance exceeds the performance of existing methods by over 50%, and
we empirically demonstrate that affordances are robust to novel settings.
Videos available at https://snasiriany.me/rt-affordance
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ JEL: Applying End-to-End Neural Entity Linking in JPMorgan Chase 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanying Ding, Vinay K. Chaudhri, Naren Chittar, Krishna Konakanchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge Graphs have emerged as a compelling abstraction for capturing key
relationship among the entities of interest to enterprises and for integrating
data from heterogeneous sources. JPMorgan Chase (JPMC) is leading this trend by
leveraging knowledge graphs across the organization for multiple mission
critical applications such as risk assessment, fraud detection, investment
advice, etc. A core problem in leveraging a knowledge graph is to link mentions
(e.g., company names) that are encountered in textual sources to entities in
the knowledge graph. Although several techniques exist for entity linking, they
are tuned for entities that exist in Wikipedia, and fail to generalize for the
entities that are of interest to an enterprise. In this paper, we propose a
novel end-to-end neural entity linking model (JEL) that uses minimal context
information and a margin loss to generate entity embeddings, and a Wide & Deep
Learning model to match character and semantic information respectively. We
show that JEL achieves the state-of-the-art performance to link mentions of
company names in financial news with entities in our knowledge graph. We report
on our efforts to deploy this model in the company-wide system to generate
alerts in response to financial news. The methodology used for JEL is directly
applicable and usable by other enterprises who need entity linking solutions
for data that are unique to their respective situations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures, IAAI-21</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the loss of context-awareness in general instruction <span class="highlight-title">fine-tuning</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02688v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02688v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihan Wang, Andrew Bai, Nanyun Peng, Cho-Jui Hsieh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretrained Large Language Models (LLMs) require post-training methods such as
supervised fine-tuning (SFT) on instruction-response pairs to enable
instruction following. However, this process can potentially harm existing
capabilities learned during pretraining. In this paper, we investigate the loss
of context awareness after SFT, defined as the capability to extract and
understand information from the user-provided context and respond accordingly.
We are the first to identify and show that the loss of context-awareness
appears on instruction-finetuned LLMs when the chat template is applied to the
input prompts. We identify the performance decline is partially caused by the
bias embedded into the chat template to focus less on the user-provided
context. Based on these observations, we propose two methods to mitigate the
loss of context awareness in instruct models: post-hoc attention steering on
user prompts and conditional instruction fine-tuning with a context-dependency
indicator. Empirical experiments on 4 context-dependent downstream tasks and 3
pretrained LLMs of different sizes show that our methods effectively mitigates
the loss of context awareness without compromising the general ability to
follow instructions. Our findings also strongly advocate the necessity to
carefully benchmark context awareness after instruction fine-tuning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Write Rationally: How Information Is Distributed in
  Non-Native Speakers' Essays <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03550v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03550v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixin Tang, Janet G. van Hell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  People tend to distribute information evenly in language production for
better and clearer communication. In this study, we compared essays written by
second language learners with various native language (L1) backgrounds to
investigate how they distribute information in their non-native language (L2)
production. Analyses of surprisal and constancy of entropy rate indicated that
writers with higher L2 proficiency can reduce the expected uncertainty of
language production while still conveying informative content. However, the
uniformity of information distribution showed less variability among different
groups of L2 speakers, suggesting that this feature may be universal in L2
essay writing and less affected by L2 writers' variability in L1 background and
L2 proficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in main of Conference on Empirical Methods in Natural
  Language Processing; EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Benefits of Domain-Pretraining of Generative Large
  Language Models for Chemistry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03542v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03542v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anurag Acharya, Shivam Sharma, Robin Cosbey, Megha Subramanian, Scott Howland, Maria Glenski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A proliferation of Large Language Models (the GPT series, BLOOM, LLaMA, and
more) are driving forward novel development of multipurpose AI for a variety of
tasks, particularly natural language processing (NLP) tasks. These models
demonstrate strong performance on a range of tasks; however, there has been
evidence of brittleness when applied to more niche or narrow domains where
hallucinations or fluent but incorrect responses reduce performance. Given the
complex nature of scientific domains, it is prudent to investigate the
trade-offs of leveraging off-the-shelf versus more targeted foundation models
for scientific domains. In this work, we examine the benefits of in-domain
pre-training for a given scientific domain, chemistry, and compare these to
open-source, off-the-shelf models with zero-shot and few-shot prompting. Our
results show that not only do in-domain base models perform reasonably well on
in-domain tasks in a zero-shot setting but that further adaptation using
instruction fine-tuning yields impressive performance on chemistry-specific
tasks such as named entity recognition and molecular formula generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Long Context RAG Performance of Large Language Models <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03538v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03538v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quinn Leng, Jacob Portes, Sam Havens, Matei Zaharia, Michael Carbin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval Augmented Generation (RAG) has emerged as a crucial technique for
enhancing the accuracy of Large Language Models (LLMs) by incorporating
external information. With the advent of LLMs that support increasingly longer
context lengths, there is a growing interest in understanding how these models
perform in RAG scenarios. Can these new long context models improve RAG
performance? This paper presents a comprehensive study of the impact of
increased context length on RAG performance across 20 popular open source and
commercial LLMs. We ran RAG workflows while varying the total context length
from 2,000 to 128,000 tokens (and 2 million tokens when possible) on three
domain-specific datasets, and report key insights on the benefits and
limitations of long context in RAG applications. Our findings reveal that while
retrieving more documents can improve performance, only a handful of the most
recent state of the art LLMs can maintain consistent accuracy at long context
above 64k tokens. We also identify distinct failure modes in long context
scenarios, suggesting areas for future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2024 NeurIPS workshop on Adaptive Foundation Models: Evolving AI for
  Personalized and Efficient Learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Metric Bias in Minimum Bayes Risk Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03524v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03524v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Geza Kovacs, Daniel Deutsch, Markus Freitag
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Minimum Bayes Risk (MBR) decoding using metrics such as COMET or
MetricX has outperformed traditional decoding methods such as greedy or beam
search, it introduces a challenge we refer to as metric bias. As MBR decoding
aims to produce translations that score highly according to a specific utility
metric, this very process makes it impossible to use the same metric for both
decoding and evaluation, as improvements might simply be due to reward hacking
rather than reflecting real quality improvements. In this work we find that
compared to human ratings, neural metrics not only overestimate the quality of
MBR decoding when the same metric is used as the utility metric, but they also
overestimate the quality of MBR/QE decoding with other neural utility metrics
as well. We also show that the metric bias issue can be mitigated by using an
ensemble of utility metrics during MBR decoding: human evaluations show that
MBR decoding using an ensemble of utility metrics outperforms a single utility
metric.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at WMT2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Change Is the Only Constant: Dynamic LLM Slicing based on Layer
  Redundancy <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03513v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03513v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Razvan-Gabriel Dumitru, Paul-Ioan Clotan, Vikas Yadav, Darius Peteleaza, Mihai Surdeanu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel model compression approach through dynamic
layer-specific pruning in Large Language Models (LLMs), enhancing the
traditional methodology established by SliceGPT. By transitioning from constant
to dynamic slicing, our method leverages the newly proposed Layer Redundancy
(LR) score, which assesses how much change each layer changes its input by
measuring the cosine similarity of the input to the output of the layer. We use
this score to prune parts of individual layers based on redundancy in such a
way that the average pruned percentage for all layers is a fixed value. We
conducted extensive experiments using models like Llama3-8B and Mistral-7B on
multiple datasets, evaluating different slicing bases and percentages to
determine optimal configurations that balance efficiency and performance. Our
findings show that our dynamic slicing approach not only maintains but, in many
cases, enhances model performance compared to the baseline established by
constant slicing methods. For instance, in several settings, we see performance
improvements of up to 5% over the SliceGPT baseline. Additionally, a perplexity
decrease by as much as 7% was observed across multiple benchmarks, validating
the effectiveness of our method. The code, model weights, and datasets are
open-sourced at https://github.com/RazvanDu/DynamicSlicing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP Findings 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty Quantification for Clinical Outcome Predictions with (Large)
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03497v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03497v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zizhang Chen, Peizhao Li, Xiaomeng Dong, Pengyu Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To facilitate healthcare delivery, language models (LMs) have significant
potential for clinical prediction tasks using electronic health records (EHRs).
However, in these high-stakes applications, unreliable decisions can result in
high costs due to compromised patient safety and ethical concerns, thus
increasing the need for good uncertainty modeling of automated clinical
predictions. To address this, we consider the uncertainty quantification of LMs
for EHR tasks in white- and black-box settings. We first quantify uncertainty
in white-box models, where we can access model parameters and output logits. We
show that an effective reduction of model uncertainty can be achieved by using
the proposed multi-tasking and ensemble methods in EHRs. Continuing with this
idea, we extend our approach to black-box settings, including popular
proprietary LMs such as GPT-4. We validate our framework using longitudinal
clinical data from more than 6,000 patients in ten clinical prediction tasks.
Results show that ensembling methods and multi-task prediction prompts reduce
uncertainty across different scenarios. These findings increase the
transparency of the model in white-box and black-box settings, thus advancing
reliable AI healthcare.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic Generation of Question Hints for Mathematics Problems using
  Large Language Models in Educational Technology <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03495v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03495v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junior Cedric Tonga, Benjamin Clement, Pierre-Yves Oudeyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The automatic generation of hints by Large Language Models (LLMs) within
Intelligent Tutoring Systems (ITSs) has shown potential to enhance student
learning. However, generating pedagogically sound hints that address student
misconceptions and adhere to specific educational objectives remains
challenging. This work explores using LLMs (GPT-4o and Llama-3-8B-instruct) as
teachers to generate effective hints for students simulated through LLMs
(GPT-3.5-turbo, Llama-3-8B-Instruct, or Mistral-7B-instruct-v0.3) tackling math
exercises designed for human high-school students, and designed using cognitive
science principles. We present here the study of several dimensions: 1)
identifying error patterns made by simulated students on secondary-level math
exercises; 2) developing various prompts for GPT-4o as a teacher and evaluating
their effectiveness in generating hints that enable simulated students to
self-correct; and 3) testing the best-performing prompts, based on their
ability to produce relevant hints and facilitate error correction, with
Llama-3-8B-Instruct as the teacher, allowing for a performance comparison with
GPT-4o. The results show that model errors increase with higher temperature
settings. Notably, when hints are generated by GPT-4o, the most effective
prompts include prompts tailored to specific errors as well as prompts
providing general hints based on common mathematical errors. Interestingly,
Llama-3-8B-Instruct as a teacher showed better overall performance than GPT-4o.
Also the problem-solving and response revision capabilities of the LLMs as
students, particularly GPT-3.5-turbo, improved significantly after receiving
hints, especially at lower temperature settings. However, models like
Mistral-7B-Instruct demonstrated a decline in performance as the temperature
increased.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024 Workshop on Large Foundation Models for
  Educational Assessment (FM-Assess)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LASER: Attention with Exponential Transformation <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03493v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03493v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sai Surya Duvvuri, Inderjit S. Dhillon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have had tremendous impact for several sequence related tasks,
largely due to their ability to retrieve from any part of the sequence via
softmax based dot-product attention. This mechanism plays a crucial role in
Transformer's performance. We analyze the gradients backpropagated through the
softmax operation in the attention mechanism and observe that these gradients
can often be small. This poor gradient signal backpropagation can lead to
inefficient learning of parameters preceeding the attention operations. To this
end, we introduce a new attention mechanism called LASER, which we analytically
show to admit a larger gradient signal. We show that LASER Attention can be
implemented by making small modifications to existing attention
implementations. We conduct experiments on autoregressive large language models
(LLMs) with upto 2.2 billion parameters where we show upto 3.38% and an average
of ~1% improvement over standard attention on downstream evaluations. Using
LASER gives the following relative improvements in generalization performance
across a variety of tasks (vision, text and speech): 4.67% accuracy in Vision
Transformer (ViT) on Imagenet, 2.25% error rate in Conformer on the Librispeech
speech-to-text and 0.93% fraction of incorrect predictions in BERT with 2.2
billion parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, under review in ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM Generated Distribution-Based Prediction of US Electoral Results,
  Part I 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03486v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03486v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Caleb Bradshaw, Caelen Miller, Sean Warnick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces distribution-based prediction, a novel approach to
using Large Language Models (LLMs) as predictive tools by interpreting output
token probabilities as distributions representing the models' learned
representation of the world. This distribution-based nature offers an
alternative perspective for analyzing algorithmic fidelity, complementing the
approach used in silicon sampling. We demonstrate the use of distribution-based
prediction in the context of recent United States presidential election,
showing that this method can be used to determine task specific bias, prompt
noise, and algorithmic fidelity. This approach has significant implications for
assessing the reliability and increasing transparency of LLM-based predictions
across various domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 10 Figures, Pre-print</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MetRex: A Benchmark for Verilog Code Metric Reasoning Using LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03471v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03471v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manar Abdelatty, Jingxiao Ma, Sherief Reda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have been applied to various hardware design
tasks, including Verilog code generation, EDA tool scripting, and RTL bug
fixing. Despite this extensive exploration, LLMs are yet to be used for the
task of post-synthesis metric reasoning and estimation of HDL designs. In this
paper, we assess the ability of LLMs to reason about post-synthesis metrics of
Verilog designs. We introduce MetRex, a large-scale dataset comprising 25,868
Verilog HDL designs and their corresponding post-synthesis metrics, namely
area, delay, and static power. MetRex incorporates a Chain of Thought (CoT)
template to enhance LLMs' reasoning about these metrics. Extensive experiments
show that Supervised Fine-Tuning (SFT) boosts the LLM's reasoning capabilities
on average by 37.0\%, 25.3\%, and 25.7\% on the area, delay, and static power,
respectively. While SFT improves performance on our benchmark, it remains far
from achieving optimal results, especially on complex problems. Comparing to
state-of-the-art regression models, our approach delivers accurate
post-synthesis predictions for 17.4\% more designs (within a 5\% error margin),
in addition to offering a 1.7x speedup by eliminating the need for
pre-processing. This work lays the groundwork for advancing LLM-based Verilog
code metric reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Solving Trojan Detection Competitions with Linear Weight Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03445v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03445v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Todd Huster, Peter Lin, Razvan Stefanescu, Emmanuel Ekwedike, Ritu Chadha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks can conceal malicious Trojan backdoors that allow a trigger
to covertly change the model behavior. Detecting signs of these backdoors,
particularly without access to any triggered data, is the subject of ongoing
research and open challenges. In one common formulation of the problem, we are
given a set of clean and poisoned models and need to predict whether a given
test model is clean or poisoned. In this paper, we introduce a detector that
works remarkably well across many of the existing datasets and domains. It is
obtained by training a binary classifier on a large number of models' weights
after performing a few different pre-processing steps including feature
selection and standardization, reference model weights subtraction, and model
alignment prior to detection. We evaluate this algorithm on a diverse set of
Trojan detection benchmarks and domains and examine the cases where the
approach is most and least effective.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Usefulness of LLMs as an Author Checklist Assistant for Scientific
  Papers: NeurIPS'24 Experiment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03417v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03417v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Goldberg, Ihsan Ullah, Thanh Gia Hieu Khuong, Benedictus Kent Rachmat, Zhen Xu, Isabelle Guyon, Nihar B. Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) represent a promising, but controversial, tool
in aiding scientific peer review. This study evaluates the usefulness of LLMs
in a conference setting as a tool for vetting paper submissions against
submission standards. We conduct an experiment at the 2024 Neural Information
Processing Systems (NeurIPS) conference, where 234 papers were voluntarily
submitted to an "LLM-based Checklist Assistant." This assistant validates
whether papers adhere to the author checklist used by NeurIPS, which includes
questions to ensure compliance with research and manuscript preparation
standards. Evaluation of the assistant by NeurIPS paper authors suggests that
the LLM-based assistant was generally helpful in verifying checklist
completion. In post-usage surveys, over 70% of authors found the assistant
useful, and 70% indicate that they would revise their papers or checklist
responses based on its feedback. While causal attribution to the assistant is
not definitive, qualitative evidence suggests that the LLM contributed to
improving some submissions. Survey responses and analysis of re-submissions
indicate that authors made substantive revisions to their submissions in
response to specific feedback from the LLM. The experiment also highlights
common issues with LLMs: inaccuracy (20/52) and excessive strictness (14/52)
were the most frequent issues flagged by authors. We also conduct experiments
to understand potential gaming of the system, which reveal that the assistant
could be manipulated to enhance scores through fabricated justifications,
highlighting potential vulnerabilities of automated review tools.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SAUCE: Synchronous and Asynchronous User-Customizable Environment for
  Multi-Agent LLM Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03397v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03397v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shlomo Neuberger, Niv Eckhaus, Uri Berger, Amir Taubenfeld, Gabriel Stanovsky, Ariel Goldstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many human interactions, such as political debates, are carried out in group
settings, where there are arbitrarily many participants, each with different
views and agendas. To explore such complex social settings, we present SAUCE: a
customizable Python platform, allowing researchers to plug-and-play various
LLMs participating in discussions on any topic chosen by the user. Our platform
takes care of instantiating the models, scheduling their responses, managing
the discussion history, and producing a comprehensive output log, all
customizable through configuration files, requiring little to no coding skills.
A novel feature of SAUCE is our asynchronous communication feature, where
models decide when to speak in addition to what to say, thus modeling an
important facet of human communication. We show SAUCE's attractiveness in two
initial experiments, and invite the community to use it in simulating various
group simulations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/Deep-Cognition-Lab/SAUCE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Large Language Models for Specialist-level Oncology Care 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03395v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03395v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anil Palepu, Vikram Dhillon, Polly Niravath, Wei-Hung Weng, Preethi Prasad, Khaled Saab, Ryutaro Tanno, Yong Cheng, Hanh Mai, Ethan Burns, Zainub Ajmal, Kavita Kulkarni, Philip Mansfield, Dale Webster, Joelle Barral, Juraj Gottweis, Mike Schaekermann, S. Sara Mahdavi, Vivek Natarajan, Alan Karthikesalingam, Tao Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown remarkable progress in encoding
clinical knowledge and responding to complex medical queries with appropriate
clinical reasoning. However, their applicability in subspecialist or complex
medical settings remains underexplored. In this work, we probe the performance
of AMIE, a research conversational diagnostic AI system, in the subspecialist
domain of breast oncology care without specific fine-tuning to this challenging
domain. To perform this evaluation, we curated a set of 50 synthetic breast
cancer vignettes representing a range of treatment-naive and
treatment-refractory cases and mirroring the key information available to a
multidisciplinary tumor board for decision-making (openly released with this
work). We developed a detailed clinical rubric for evaluating management plans,
including axes such as the quality of case summarization, safety of the
proposed care plan, and recommendations for chemotherapy, radiotherapy, surgery
and hormonal therapy. To improve performance, we enhanced AMIE with the
inference-time ability to perform web search retrieval to gather relevant and
up-to-date clinical knowledge and refine its responses with a multi-stage
self-critique pipeline. We compare response quality of AMIE with internal
medicine trainees, oncology fellows, and general oncology attendings under both
automated and specialist clinician evaluations. In our evaluations, AMIE
outperformed trainees and fellows demonstrating the potential of the system in
this challenging and important domain. We further demonstrate through
qualitative examples, how systems such as AMIE might facilitate conversational
interactions to assist clinicians in their decision making. However, AMIE's
performance was overall inferior to attending oncologists suggesting that
further research is needed prior to consideration of prospective uses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GraphReader: Building Graph-based Agent to Enhance Long-Context
  Abilities of Large Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14550v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14550v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shilong Li, Yancheng He, Hangyu Guo, Xingyuan Bu, Ge Bai, Jie Liu, Jiaheng Liu, Xingwei Qu, Yangguang Li, Wanli Ouyang, Wenbo Su, Bo Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-context capabilities are essential for large language models (LLMs) to
tackle complex and long-input tasks. Despite numerous efforts made to optimize
LLMs for long contexts, challenges persist in robustly processing long inputs.
In this paper, we introduce GraphReader, a graph-based agent system designed to
handle long texts by structuring them into a graph and employing an agent to
explore this graph autonomously. Upon receiving a question, the agent first
undertakes a step-by-step analysis and devises a rational plan. It then invokes
a set of predefined functions to read node content and neighbors, facilitating
a coarse-to-fine exploration of the graph. Throughout the exploration, the
agent continuously records new insights and reflects on current circumstances
to optimize the process until it has gathered sufficient information to
generate an answer. Experimental results on the LV-Eval dataset reveal that
GraphReader, using a 4k context window, consistently outperforms GPT-4-128k
across context lengths from 16k to 256k by a large margin. Additionally, our
approach demonstrates superior performance on four challenging single-hop and
multi-hop benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>[EMNLP 2024] The first four authors contributed equally, 29 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Parallelizing Linear Transformers with the Delta Rule over Sequence
  Length <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.06484v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.06484v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, Yoon Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers with linear attention (i.e., linear transformers) and
state-space models have recently been suggested as a viable linear-time
alternative to transformers with softmax attention. However, these models still
underperform transformers especially on tasks that require in-context
retrieval. While more expressive variants of linear transformers which replace
the additive update in linear transformers with the delta rule (DeltaNet) have
been found to be more effective at associative recall, existing algorithms for
training such models do not parallelize over sequence length and are thus
inefficient to train on modern hardware. This work describes a
hardware-efficient algorithm for training linear transformers with the delta
rule, which exploits a memory-efficient representation for computing products
of Householder matrices. This algorithm allows us to scale up DeltaNet to
standard language modeling settings. We train a 1.3B model for 100B tokens and
find that it outperforms recent linear-time baselines such as Mamba and GLA in
terms of perplexity and zero-shot performance on downstream tasks. We also
experiment with two hybrid models which combine DeltaNet layers with (1)
sliding-window attention layers every other layer or (2) two global attention
layers, and find that these hybrids outperform strong transformer baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 camera ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RepLiQA: A Question-Answering Dataset for Benchmarking LLMs on Unseen
  Reference Content 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11811v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11811v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joao Monteiro, Pierre-Andre Noel, Etienne Marcotte, Sai Rajeswar, Valentina Zantedeschi, David Vazquez, Nicolas Chapados, Christopher Pal, Perouz Taslakian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are trained on vast amounts of data, most of
which is automatically scraped from the internet. This data includes
encyclopedic documents that harbor a vast amount of general knowledge (e.g.,
Wikipedia) but also potentially overlap with benchmark datasets used for
evaluating LLMs. Consequently, evaluating models on test splits that might have
leaked into the training set is prone to misleading conclusions. To foster
sound evaluation of language models, we introduce a new test dataset named
RepLiQA, suited for question-answering and topic retrieval tasks. RepLiQA is a
collection of five splits of test sets, four of which have not been released to
the internet or exposed to LLM APIs prior to this publication. Each sample in
RepLiQA comprises (1) a reference document crafted by a human annotator and
depicting an imaginary scenario (e.g., a news article) absent from the
internet; (2) a question about the document's topic; (3) a ground-truth answer
derived directly from the information in the document; and (4) the paragraph
extracted from the reference document containing the answer. As such, accurate
answers can only be generated if a model can find relevant content within the
provided document. We run a large-scale benchmark comprising several
state-of-the-art LLMs to uncover differences in performance across models of
various types and sizes in a context-conditional language modeling setting.
Released splits of RepLiQA can be found here:
https://huggingface.co/datasets/ServiceNow/repliqa.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language
  Models in Multi-Turn Dialogues <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14762v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14762v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ge Bai, Jie Liu, Xingyuan Bu, Yancheng He, Jiaheng Liu, Zhanhui Zhou, Zhuoran Lin, Wenbo Su, Tiezheng Ge, Bo Zheng, Wanli Ouyang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of Large Language Models (LLMs) has drastically enhanced dialogue
systems. However, comprehensively evaluating the dialogue abilities of LLMs
remains a challenge. Previous benchmarks have primarily focused on single-turn
dialogues or provided coarse-grained and incomplete assessments of multi-turn
dialogues, overlooking the complexity and fine-grained nuances of real-life
dialogues. To address this issue, we introduce MT-Bench-101, specifically
designed to evaluate the fine-grained abilities of LLMs in multi-turn
dialogues. By conducting a detailed analysis of real multi-turn dialogue data,
we construct a three-tier hierarchical ability taxonomy comprising 4208 turns
across 1388 multi-turn dialogues in 13 distinct tasks. We then evaluate 21
popular LLMs based on MT-Bench-101, conducting comprehensive analyses from both
ability and task perspectives and observing differing trends in LLMs
performance across dialogue turns within various tasks. Further analysis
indicates that neither utilizing common alignment techniques nor chat-specific
designs has led to obvious enhancements in the multi-turn abilities of LLMs.
Extensive case studies suggest that our designed tasks accurately assess the
corresponding multi-turn abilities. The data and code are available at
\url{https://github.com/mtbench101/mt-bench-101}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>[ACL 2024] The first three authors contribute equally, 34 pages, repo
  at https://github.com/mtbench101/mt-bench-101</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contextual Knowledge Pursuit for Faithful Visual Synthesis <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.17898v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.17898v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinqi Luo, Kwan Ho Ryan Chan, Dimitris Dimos, René Vidal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern text-to-vision generative models often hallucinate when the prompt
describing the scene to be generated is underspecified. In large language
models (LLMs), a prevalent strategy to reduce hallucinations is to retrieve
factual knowledge from an external database. While such retrieval augmentation
strategies have great potential to enhance text-to-vision generators, existing
static top-K retrieval methods explore the knowledge pool once, missing the
broader context necessary for high-quality generation. Furthermore, LLMs
internally possess rich world knowledge learned during large-scale training
(parametric knowledge) that could mitigate the need for external data
retrieval. This paper proposes Contextual Knowledge Pursuit (CKPT), a framework
that leverages the complementary strengths of external and parametric knowledge
to help generators produce reliable visual content. Instead of the one-time
retrieval of facts from an external database to improve a given prompt, CKPT
uses (1) an LLM to decide whether to seek external knowledge or to self-elicit
descriptions from LLM parametric knowledge, (2) a knowledge pursuit process to
contextually seek and sequentially gather most relevant facts, (3) a knowledge
aggregator for prompt enhancement with the gathered fact context, and (4) a
filtered fine-tuning objective to improve visual synthesis with richer prompts.
We evaluate CKPT across multiple text-driven generative tasks (image, 3D
rendering, and video) on datasets of rare objects and daily scenarios. Our
results show that CKPT is capable of generating faithful and semantically rich
content across diverse visual domains, offering a promising data source for
zero-shot synthesis and filtered fine-tuning of text-to-vision generative
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ECCV 2024 SDCV Workshop. GitHub repository at
  https://github.com/peterljq/Contextual-Knowledge-Pursuit</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficacy of Various Large Language Models in Generating Smart Contracts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11019v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11019v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddhartha Chatterjee, Bina Ramamurthy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study analyzes the application of code-generating Large Language Models
in the creation of immutable Solidity smart contracts on the Ethereum
Blockchain. Other works have previously analyzed Artificial Intelligence code
generation abilities. This paper aims to expand this to a larger scope to
include programs where security and efficiency are of utmost priority such as
smart contracts. The hypothesis leading into the study was that LLMs in general
would have difficulty in rigorously implementing security details in the code,
which was shown through our results, but surprisingly generally succeeded in
many common types of contracts. We also discovered a novel way of generating
smart contracts through new prompting strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, accepted for presentation at 8th annual Future of
  Information and Communication Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Kun: Answer Polishment for Chinese Self-Alignment with Instruction
  Back-Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.06477v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.06477v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyu Zheng, Shuyue Guo, Xingwei Qu, Jiawei Guo, Xinrun Du, Qi Jia, Chenghua Lin, Wenhao Huang, Jie Fu, Ge Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce Kun, a novel approach for creating high-quality
instruction-tuning datasets for large language models (LLMs) without relying on
manual annotations. Adapting a self-training algorithm based on instruction
back-translation and answer polishment, Kun leverages unlabelled data from
diverse sources such as Wudao, Wanjuan, and SkyPile to generate a substantial
dataset of over a million Chinese instructional data points. This approach
significantly deviates from traditional methods by using a self-curation
process to refine and select the most effective instruction-output pairs. Our
experiments with the 6B-parameter Yi model across various benchmarks
demonstrate Kun's robustness and scalability. Our method's core contributions
lie in its algorithmic advancement, which enhances data retention and clarity,
and its innovative data generation approach that substantially reduces the
reliance on costly and time-consuming manual annotations. This methodology
presents a scalable and efficient solution for improving the
instruction-following capabilities of LLMs, with significant implications for
their application across diverse fields. The code and dataset can be found at
https://github.com/Zheng0428/COIG-Kun
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Teaching Models to Improve on Tape 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01483v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01483v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liat Bezalel, Eyal Orgad, Amir Globerson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) often struggle when prompted to generate content
under specific constraints. However, in such cases it is often easy to check
whether these constraints are satisfied or violated. Recent works have shown
that LLMs can benefit from such ``corrective feedback''. Here we claim that
this skill of LLMs can be significantly enhanced via training. We introduce an
RL framework for teaching models to use such rewards, by simulating interaction
sessions, and rewarding the model according to its ability to satisfy the
constraints. We refer to our method as CORGI (Controlled Generation with RL for
Guided Interaction), and evaluate it on a variety of controlled generation
tasks using unlabeled training data. We find that CORGI consistently
outperforms the baseline reinforcement learning method that does not
incorporate conversational feedback. Furthermore, CORGI's interactive framework
enables meta-learning, allowing the LLM to generalize better to guided
interaction in new tasks. Our results clearly show that conversational
optimization, when combined with reinforcement learning, significantly improves
the effectiveness of LLMs in controlled generation contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PaCE: Parsimonious Concept Engineering for Large Language Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04331v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04331v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinqi Luo, Tianjiao Ding, Kwan Ho Ryan Chan, Darshan Thaker, Aditya Chattopadhyay, Chris Callison-Burch, René Vidal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are being used for a wide variety of tasks.
While they are capable of generating human-like responses, they can also
produce undesirable output including potentially harmful information, racist or
sexist language, and hallucinations. Alignment methods are designed to reduce
such undesirable outputs via techniques such as fine-tuning, prompt
engineering, and representation engineering. However, existing methods face
several challenges: some require costly fine-tuning for every alignment task;
some do not adequately remove undesirable concepts, failing alignment; some
remove benign concepts, lowering the linguistic capabilities of LLMs. To
address these issues, we propose Parsimonious Concept Engineering (PaCE), a
novel activation engineering framework for alignment. First, to sufficiently
model the concepts, we construct a large-scale concept dictionary in the
activation space, in which each atom corresponds to a semantic concept. Given
any alignment task, we instruct a concept partitioner to efficiently annotate
the concepts as benign or undesirable. Then, at inference time, we decompose
the LLM activations along the concept dictionary via sparse coding, to
accurately represent the activations as linear combinations of benign and
undesirable components. By removing the latter ones from the activations, we
reorient the behavior of the LLM towards the alignment goal. We conduct
experiments on tasks such as response detoxification, faithfulness enhancement,
and sentiment revising, and show that PaCE achieves state-of-the-art alignment
performance while maintaining linguistic capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in NeurIPS 2024. GitHub repository at
  https://github.com/peterljq/Parsimonious-Concept-Engineering</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Privacy Risks of Speculative Decoding in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01076v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01076v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiankun Wei, Abdulrahman Abdulrazzag, Tianchen Zhang, Adel Muursepp, Gururaj Saileshwar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speculative decoding in large language models (LLMs) accelerates token
generation by speculatively predicting multiple tokens cheaply and verifying
them in parallel, and has been widely deployed. In this paper, we provide the
first study demonstrating the privacy risks of speculative decoding. We observe
that input-dependent patterns of correct and incorrect predictions can be
leaked out to an adversary monitoring token generation times and packet sizes,
leading to privacy breaches. By observing the pattern of correctly and
incorrectly speculated tokens, we show that a malicious adversary can
fingerprint queries and learn private user inputs with more than $90\%$
accuracy across three different speculative decoding techniques - REST (almost
$100\%$ accuracy), LADE (up to $92\%$ accuracy), and BiLD (up to $95\%$
accuracy). We show that an adversary can also leak out confidential
intellectual property used to design these techniques, such as data from
data-stores used for prediction (in REST) at a rate of more than $25$ tokens
per second, or even hyper-parameters used for prediction (in LADE). We also
discuss mitigation strategies, such as aggregating tokens across multiple
iterations and padding packets with additional bytes, to avoid such privacy or
confidentiality breaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mind Your Step (by Step): Chain-of-Thought can Reduce Performance on
  Tasks where Thinking Makes Humans Worse 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21333v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21333v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan Liu, Jiayi Geng, Addison J. Wu, Ilia Sucholutsky, Tania Lombrozo, Thomas L. Griffiths
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chain-of-thought (CoT) prompting has become a widely used strategy for
working with large language and multimodal models. While CoT has been shown to
improve performance across many tasks, determining the settings in which it is
effective remains an ongoing effort. In particular, it is still an open
question in what settings CoT systematically reduces model performance. In this
paper, we seek to identify the characteristics of tasks where CoT reduces
performance by drawing inspiration from cognitive psychology, looking at cases
where (i) verbal thinking or deliberation hurts performance in humans, and (ii)
the constraints governing human performance generalize to language models.
Three such cases are implicit statistical learning, visual recognition, and
classifying with patterns containing exceptions. In extensive experiments
across all three settings, we find that a diverse collection of
state-of-the-art models exhibit significant drop-offs in performance (e.g., up
to 36.3% absolute accuracy for OpenAI o1-preview compared to GPT-4o) when using
inference-time reasoning compared to zero-shot counterparts. We also identify
three tasks that satisfy condition (i) but not (ii), and find that while verbal
thinking reduces human performance in these tasks, CoT retains or increases
model performance. Overall, our results show that while there is not an exact
parallel between the cognitive processes of models and those of humans,
considering cases where thinking has negative consequences for human
performance can help us identify settings where it negatively impacts models.
By connecting the literature on human deliberation with evaluations of CoT, we
offer a new tool that can be used in understanding the impact of prompt choices
and inference-time reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Programming Language Sandbox for LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23074v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23074v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shihan Dou, Jiazheng Zhang, Jianxiang Zang, Yunbo Tao, Weikang Zhou, Haoxiang Jia, Shichun Liu, Yuming Yang, Zhiheng Xi, Shenxi Wu, Shaoqing Zhang, Muling Wu, Changze Lv, Limao Xiong, Wenyu Zhan, Lin Zhang, Rongxiang Weng, Jingang Wang, Xunliang Cai, Yueming Wu, Ming Wen, Rui Zheng, Tao Ji, Yixin Cao, Tao Gui, Xipeng Qiu, Qi Zhang, Xuanjing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce MPLSandbox, an out-of-the-box multi-programming language sandbox
designed to provide unified and comprehensive feedback from compiler and
analysis tools for Large Language Models (LLMs). It can automatically identify
the programming language of the code, compiling and executing it within an
isolated sub-sandbox to ensure safety and stability. In addition, MPLSandbox
also integrates both traditional and LLM-based code analysis tools, providing a
comprehensive analysis of generated code. MPLSandbox can be effortlessly
integrated into the training and deployment of LLMs to improve the quality and
correctness of their generated code. It also helps researchers streamline their
workflows for various LLM-based code-related tasks, reducing the development
cost. To validate the effectiveness of MPLSandbox, we integrate it into
training and deployment approaches, and also employ it to optimize workflows
for a wide range of real-world code-related tasks. Our goal is to enhance
researcher productivity on LLM-based code-related tasks by simplifying and
automating workflows through delegation to MPLSandbox.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Natural Language Processing-Based Classification and Mode-Based
  Ranking of Musculoskeletal Disorder Risk Factors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.11517v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.11517v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Abrar Jahin, Subrata Talapatra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research delves into Musculoskeletal Disorder (MSD) risk factors, using
a blend of Natural Language Processing (NLP) and mode-based ranking. The aim is
to refine understanding, classification, and prioritization for focused
prevention and treatment. Eight NLP models are evaluated, combining pre-trained
transformers, cosine similarity, and distance metrics to categorize factors
into personal, biomechanical, workplace, psychological, and organizational
classes. BERT with cosine similarity achieves 28% accuracy; sentence
transformer with Euclidean, Bray-Curtis, and Minkowski distances scores 100%.
With 10-fold cross-validation, statistical tests ensure robust results. Survey
data and mode-based ranking determine severity hierarchy, aligning with the
literature. "Working posture" is the most severe, highlighting posture's role.
Survey insights emphasize "Job insecurity," "Effort reward imbalance," and
"Poor employee facility" as significant contributors. Rankings offer actionable
insights for MSD prevention. The study suggests targeted interventions,
workplace improvements, and future research directions. This integrated NLP and
ranking approach enhances MSD comprehension and informs occupational health
strategies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Text Steganography with Large Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.10229v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.10229v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxuan Wu, Zhengxian Wu, Yiming Xue, Juan Wen, Wanli Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in large language models (LLMs) have blurred the boundary of
high-quality text generation between humans and machines, which is favorable
for generative text steganography. While, current advanced steganographic
mapping is not suitable for LLMs since most users are restricted to accessing
only the black-box API or user interface of the LLMs, thereby lacking access to
the training vocabulary and its sampling probabilities. In this paper, we
explore a black-box generative text steganographic method based on the user
interfaces of large language models, which is called LLM-Stega. The main goal
of LLM-Stega is that the secure covert communication between Alice (sender) and
Bob (receiver) is conducted by using the user interfaces of LLMs. Specifically,
We first construct a keyword set and design a new encrypted steganographic
mapping to embed secret messages. Furthermore, to guarantee accurate extraction
of secret messages and rich semantics of generated stego texts, an optimization
mechanism based on reject sampling is proposed. Comprehensive experiments
demonstrate that the proposed LLM-Stega outperforms current state-of-the-art
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures, accepted at ACM Multimedia 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In-Context Former: Lightning-fast Compressing Context for Large Language
  Model <span class="chip">EMNLP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13618v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13618v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangfeng Wang, Zaiyi Chen, Zheyong Xie, Tong Xu, Yongyi He, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rising popularity of Transformer-based large language models (LLMs),
reducing their high inference costs has become a significant research focus.
One effective approach is to compress the long input contexts. Existing methods
typically leverage the self-attention mechanism of the LLM itself for context
compression. While these methods have achieved notable results, the compression
process still involves quadratic time complexity, which limits their
applicability. To mitigate this limitation, we propose the In-Context Former
(IC-Former). Unlike previous methods, IC-Former does not depend on the target
LLMs. Instead, it leverages the cross-attention mechanism and a small number of
learnable digest tokens to directly condense information from the contextual
word embeddings. This approach significantly reduces inference time, which
achieves linear growth in time complexity within the compression range.
Experimental results indicate that our method requires only 1/32 of the
floating-point operations of the baseline during compression and improves
processing speed by 68 to 112 times while achieving over 90% of the baseline
performance on evaluation metrics. Overall, our model effectively reduces
compression costs and makes real-time compression scenarios feasible.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP2024(Findings)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ForecastBench: A Dynamic Benchmark of AI Forecasting Capabilities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19839v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19839v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ezra Karger, Houtan Bastani, Chen Yueh-Han, Zachary Jacobs, Danny Halawi, Fred Zhang, Philip E. Tetlock
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Forecasts of future events are essential inputs into informed
decision-making. Machine learning (ML) systems have the potential to deliver
forecasts at scale, but there is no framework for evaluating the accuracy of ML
systems on a standardized set of forecasting questions. To address this gap, we
introduce ForecastBench: a dynamic benchmark that evaluates the accuracy of ML
systems on an automatically generated and regularly updated set of 1,000
forecasting questions. To avoid any possibility of data leakage, ForecastBench
is comprised solely of questions about future events that have no known answer
at the time of submission. We quantify the capabilities of current ML systems
by collecting forecasts from expert (human) forecasters, the general public,
and LLMs on a random subset of questions from the benchmark ($N=200$). While
LLMs have achieved super-human performance on many benchmarks, they perform
less well here: expert forecasters outperform the top-performing LLM (p-value
$=0.01$). We display system and human scores in a public leaderboard at
www.forecastbench.org.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dialog2Flow: Pre-training Soft-Contrastive Action-Driven Sentence
  Embeddings for Automatic Dialog Flow Extraction <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18481v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18481v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sergio Burdisso, Srikanth Madikeri, Petr Motlicek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficiently deriving structured workflows from unannotated dialogs remains an
underexplored and formidable challenge in computational linguistics. Automating
this process could significantly accelerate the manual design of workflows in
new domains and enable the grounding of large language models in
domain-specific flowcharts, enhancing transparency and controllability. In this
paper, we introduce Dialog2Flow (D2F) embeddings, which differ from
conventional sentence embeddings by mapping utterances to a latent space where
they are grouped according to their communicative and informative functions
(i.e., the actions they represent). D2F allows for modeling dialogs as
continuous trajectories in a latent space with distinct action-related regions.
By clustering D2F embeddings, the latent space is quantized, and dialogs can be
converted into sequences of region/action IDs, facilitating the extraction of
the underlying workflow. To pre-train D2F, we build a comprehensive dataset by
unifying twenty task-oriented dialog datasets with normalized per-turn action
annotations. We also introduce a novel soft contrastive loss that leverages the
semantic information of these actions to guide the representation learning
process, showing superior performance compared to standard supervised
contrastive loss. Evaluation against various sentence embeddings, including
dialog-specific ones, demonstrates that D2F yields superior qualitative and
quantitative results across diverse domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DEM: Distribution Edited Model for Training with Mixed Data
  Distributions <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15570v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15570v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dhananjay Ram, Aditya Rawal, Momchil Hardalov, Nikolaos Pappas, Sheng Zha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training with mixed data distributions is a common and important part of
creating multi-task and instruction-following models. The diversity of the data
distributions and cost of joint training makes the optimization procedure
extremely challenging. Data mixing methods partially address this problem,
albeit having a sub-optimal performance across data sources and require
multiple expensive training runs. In this paper, we propose a simple and
efficient alternative for better optimization of the data sources by combining
models individually trained on each data source with the base model using basic
element-wise vector operations. The resulting model, namely Distribution Edited
Model (DEM), is 11x cheaper than standard data mixing and outperforms strong
baselines on a variety of benchmarks, yielding upto 6.2% improvement on MMLU,
11.5% on BBH, 16.1% on DROP, 6% on MathQA, and 9.3% on HELM with models of size
3B to 13B. Notably, DEM does not require full re-training when modifying a
single data-source, thus making it very flexible and scalable for training with
diverse data sources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 (Main Conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ocean-omni: To Understand the World with Omni-modality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08565v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08565v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yadong Li, Haoze Sun, Mingan Lin, Tianpeng Li, Guosheng Dong, Tao Zhang, Bowen Ding, Wei Song, Zhenglin Cheng, Yuqi Huo, Song Chen, Xu Li, Da Pan, Shusen Zhang, Xin Wu, Zheng Liang, Jun Liu, Tao Zhang, Keer Lu, Yaqi Zhao, Yanjun Shen, Fan Yang, Kaicheng Yu, Tao Lin, Jianhua Xu, Zenan Zhou, Weipeng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The salient multimodal capabilities and interactive experience of GPT-4o
highlight its critical role in practical applications, yet it lacks a
high-performing open-source counterpart. In this paper, we introduce
Ocean-omni, the first open-source 7B Multimodal Large Language Model (MLLM)
adept at concurrently processing and analyzing modalities of image, video,
audio, and text, while delivering an advanced multimodal interactive experience
and strong performance. We propose an effective multimodal training schema
starting with 7B model and proceeding through two stages of multimodal
alignment and multitask fine-tuning across audio, image, video, and text modal.
This approach equips the language model with the ability to handle visual and
audio data effectively. Demonstrating strong performance across various
omni-modal and multimodal benchmarks, we aim for this contribution to serve as
a competitive baseline for the open-source community in advancing multimodal
understanding and real-time interaction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating <span class="highlight-title">Fine-Tuning</span> Efficiency of Human-Inspired Learning Strategies
  in Medical Question Answering <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.07888v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.07888v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yushi Yang, Andrew M. Bean, Robert McCraith, Adam Mahdi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning Large Language Models (LLMs) incurs considerable training costs,
driving the need for data-efficient training with optimised data ordering.
Human-inspired strategies offer a solution by organising data based on human
learning practices. This study evaluates the fine-tuning efficiency of five
human-inspired strategies across four language models, three datasets, and both
human- and LLM-labelled data in the context of medical question answering.
These strategies achieve the best accuracy gain of 1.81% and an average gain of
1.02% across datasets, with interleaved strategies delivering the best average
results. However, the best strategy varies across model-dataset combinations,
limiting the generalisability of the effects of any single strategy.
Additionally, LLM-defined question difficulty outperforms human-defined labels
in curriculum-based learning, showing the potential of model-generated data as
a cost-effective alternative for optimising fine-tuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 Workshop on Fine-Tuning in Modern Machine Learning:
  Principles and Scalability (FITML)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Synthetic SQL Column Descriptions and Their Impact on Text-to-SQL
  Performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.04691v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.04691v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niklas Wretblad, Oskar Holmström, Erik Larsson, Axel Wiksäter, Oscar Söderlund, Hjalmar Öhman, Ture Pontén, Martin Forsberg, Martin Sörme, Fredrik Heintz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Relational databases often suffer from uninformative descriptors of table
contents, such as ambiguous columns and hard-to-interpret values, impacting
both human users and text-to-SQL models. In this paper, we explore the use of
large language models (LLMs) to automatically generate detailed natural
language descriptions for SQL database columns, aiming to improve text-to-SQL
performance and automate metadata creation. We create a dataset of gold column
descriptions based on the BIRD-Bench benchmark, manually refining its column
descriptions and creating a taxonomy for categorizing column difficulty. We
then evaluate several different LLMs in generating column descriptions across
the columns and different difficulties in the dataset, finding that models
unsurprisingly struggle with columns that exhibit inherent ambiguity,
highlighting the need for manual expert input. We also find that incorporating
such generated column descriptions consistently enhances text-to-SQL model
performance, particularly for larger models like GPT-4o, Qwen2 72B and Mixtral
22Bx8. Notably, Qwen2-generated descriptions, containing by annotators deemed
superfluous information, outperform manually curated gold descriptions,
suggesting that models benefit from more detailed metadata than humans expect.
Future work will investigate the specific features of these high-performing
descriptions and explore other types of metadata, such as numerical reasoning
and synonyms, to further improve text-to-SQL systems. The dataset, annotations
and code will all be made available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding Transformers via N-gram Statistics <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12034v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12034v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timothy Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer based large-language models (LLMs) display extreme proficiency
with language yet a precise understanding of how they work remains elusive. One
way of demystifying transformer predictions would be to describe how they
depend on their context in terms of simple template functions. This paper takes
a first step in this direction by considering families of functions (i.e.
rules) formed out of simple N-gram based statistics of the training data. By
studying how well these rulesets approximate transformer predictions, we obtain
a variety of novel discoveries: a simple method to detect overfitting during
training without using a holdout set, a quantitative measure of how
transformers progress from learning simple to more complex statistical rules
over the course of training, a model-variance criterion governing when
transformer predictions tend to be described by N-gram rules, and insights into
how well transformers can be approximated by N-gram rulesets in the limit where
these rulesets become increasingly complex. In this latter direction, we find
that for 79% and 68% of LLM next-token distributions on TinyStories and
Wikipedia, respectively, their top-1 predictions agree with those provided by
our N-gram rulesets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024. Datasets and N-gram statistics open-sourced:
  https://github.com/google-deepmind/transformer_ngrams</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bias in the Mirror: Are LLMs opinions robust to their own adversarial
  attacks ? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13517v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13517v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Virgile Rennard, Christos Xypolopoulos, Michalis Vazirgiannis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) inherit biases from their training data and
alignment processes, influencing their responses in subtle ways. While many
studies have examined these biases, little work has explored their robustness
during interactions. In this paper, we introduce a novel approach where two
instances of an LLM engage in self-debate, arguing opposing viewpoints to
persuade a neutral version of the model. Through this, we evaluate how firmly
biases hold and whether models are susceptible to reinforcing misinformation or
shifting to harmful viewpoints. Our experiments span multiple LLMs of varying
sizes, origins, and languages, providing deeper insights into bias persistence
and flexibility across linguistic and cultural contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned
  Judge Model is not a General Substitute for GPT-4 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02839v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02839v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Huang, Yingqi Qu, Xingyuan Bu, Hongli Zhou, Jing Liu, Muyun Yang, Bing Xu, Tiejun Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, there has been a growing trend of utilizing Large Language Model
(LLM) to evaluate the quality of other LLMs. Many studies have employed
proprietary close-sourced models, especially GPT-4, as the evaluator.
Alternatively, other works have fine-tuned judge models based on open-source
LLMs as the evaluator. While the fine-tuned judge models are claimed to achieve
comparable evaluation capability with GPT-4, in this work, we conduct an
empirical study of judge models. Our findings indicate that although the
fine-tuned judge models achieve high performance on in-domain test sets, even
surpassing GPT-4, they underperform GPT-4 across several dimensions, including
generalizability, fairness, aspect-specific evaluation, and scalability. We
also reveal that the fine-tuned judge model inherently operates as a
task-specific classifier, consequently imposing the limitations. Finally, we
introduce a integrated method, leveraging GPT-4 to compensate for the
limitations and improve the fine-tuned judges. Experiment results show our
method achieves accuracy on par with GPT-4 with only 50% of the API expense.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Jailbreaking Large Language Models with Symbolic Mathematics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11445v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11445v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emet Bethany, Mazal Bethany, Juan Arturo Nolazco Flores, Sumit Kumar Jha, Peyman Najafirad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in AI safety have led to increased efforts in training
and red-teaming large language models (LLMs) to mitigate unsafe content
generation. However, these safety mechanisms may not be comprehensive, leaving
potential vulnerabilities unexplored. This paper introduces MathPrompt, a novel
jailbreaking technique that exploits LLMs' advanced capabilities in symbolic
mathematics to bypass their safety mechanisms. By encoding harmful natural
language prompts into mathematical problems, we demonstrate a critical
vulnerability in current AI safety measures. Our experiments across 13
state-of-the-art LLMs reveal an average attack success rate of 73.6\%,
highlighting the inability of existing safety training mechanisms to generalize
to mathematically encoded inputs. Analysis of embedding vectors shows a
substantial semantic shift between original and encoded prompts, helping
explain the attack's success. This work emphasizes the importance of a holistic
approach to AI safety, calling for expanded red-teaming efforts to develop
robust safeguards across all potential input types and their associated risks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Advancements and limitations of LLMs in replicating human color-word
  associations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02116v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02116v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Makoto Fukushima, Shusuke Eshita, Hiroshige Fukuhara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Color-word associations play a fundamental role in human cognition and design
applications. Large Language Models (LLMs) have become widely available and
demonstrated intelligent behaviors in various benchmarks with natural
conversation skills. However, their ability to replicate human color-word
associations remains understudied. We compared multiple generations of LLMs
(from GPT-3 to GPT-4o) against human color-word associations using data
collected from over 10,000 Japanese participants, involving 17 colors and words
from eight categories in Japanese. Our findings reveal a clear progression in
LLM performance across generations, with GPT-4o achieving the highest accuracy
in predicting the best voted word for each color and category. However, the
highest median performance was approximately 50% even for GPT-4o with visual
inputs (chance level is 10%), and the performance levels varied significantly
across word categories and colors, indicating a failure to fully replicate
human color-word associations. On the other hand, color discrimination ability
estimated from our color-word association data showed that LLMs demonstrated
high correlation with human color discrimination patterns, similarly to
previous studies. Our study highlights both the advancements in LLM
capabilities and their persistent limitations, suggesting differences in
semantic memory structures between humans and LLMs in representing color-word
associations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 7 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DAPE: Data-Adaptive Positional Encoding for Length Extrapolation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14722v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14722v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuanyang Zheng, Yihang Gao, Han Shi, Minbin Huang, Jingyao Li, Jing Xiong, Xiaozhe Ren, Michael Ng, Xin Jiang, Zhenguo Li, Yu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Positional encoding plays a crucial role in transformers, significantly
impacting model performance and length generalization. Prior research has
introduced absolute positional encoding (APE) and relative positional encoding
(RPE) to distinguish token positions in given sequences. However, both APE and
RPE remain fixed after model training regardless of input data, limiting their
adaptability and flexibility. Hence, we expect that the desired positional
encoding should be data-adaptive and can be dynamically adjusted with the given
attention. In this paper, we propose a Data-Adaptive Positional Encoding (DAPE)
method, which dynamically and semantically adjusts based on input context and
learned fixed priors. Experimental validation on real-world datasets (Arxiv,
Books3, and CHE) demonstrates that DAPE enhances model performances in terms of
trained length and length generalization, where the improvements are
statistically significant. The model visualization suggests that our model can
keep both local and anti-local information. Finally, we successfully train the
model on sequence length 128 and achieve better performance at evaluation
sequence length 8192, compared with other static positional encoding methods,
revealing the benefit of the adaptive positional encoding method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SeTAR: Out-of-Distribution Detection with Selective Low-Rank
  Approximation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12629v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12629v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixia Li, Boya Xiong, Guanhua Chen, Yun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Out-of-distribution (OOD) detection is crucial for the safe deployment of
neural networks. Existing CLIP-based approaches perform OOD detection by
devising novel scoring functions or sophisticated fine-tuning methods. In this
work, we propose SeTAR, a novel, training-free OOD detection method that
leverages selective low-rank approximation of weight matrices in
vision-language and vision-only models. SeTAR enhances OOD detection via
post-hoc modification of the model's weight matrices using a simple greedy
search algorithm. Based on SeTAR, we further propose SeTAR+FT, a fine-tuning
extension optimizing model performance for OOD detection tasks. Extensive
evaluations on ImageNet1K and Pascal-VOC benchmarks show SeTAR's superior
performance, reducing the relatively false positive rate by up to 18.95% and
36.80% compared to zero-shot and fine-tuning baselines. Ablation studies
further validate SeTAR's effectiveness, robustness, and generalizability across
different model backbones. Our work offers a scalable, efficient solution for
OOD detection, setting a new state-of-the-art in this area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024. Project page is live at
  https://SeTAR-OOD.github.io. Code are available at
  https://github.com/X1AOX1A/SeTAR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Limits of Transformer Language Models on Learning to Compose Algorithms <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.05785v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.05785v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Thomm, Giacomo Camposampiero, Aleksandar Terzic, Michael Hersche, Bernhard Schölkopf, Abbas Rahimi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We analyze the capabilities of Transformer language models in learning
compositional discrete tasks. To this end, we evaluate training LLaMA models
and prompting GPT-4 and Gemini on four tasks demanding to learn a composition
of several discrete sub-tasks. In particular, we measure how well these models
can reuse primitives observable in the sub-tasks to learn the composition task.
Our results indicate that compositional learning in state-of-the-art
Transformer language models is highly sample inefficient: LLaMA requires more
data samples than relearning all sub-tasks from scratch to learn the
compositional task; in-context prompting with few samples is unreliable and
fails at executing the sub-tasks or correcting the errors in multi-round code
generation. Further, by leveraging complexity theory, we support these findings
with a theoretical analysis focused on the sample inefficiency of gradient
descent in memorizing feedforward models. We open source our code at
https://github.com/IBM/limitations-lm-algorithmic-compositional-learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluation and Improvement of Fault Detection for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.14419v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.14419v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiang Hu, Jin Wen, Maxime Cordy, Yuheng Huang, Wei Ma, Xiaofei Xie, Lei Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have recently achieved significant success
across various application domains, garnering substantial attention from
different communities. Unfortunately, even for the best LLM, many
\textit{faults} still exist that LLM cannot properly predict. Such faults will
harm the usability of LLMs in general and could introduce safety issues in
reliability-critical systems such as autonomous driving systems. How to quickly
reveal these faults in real-world datasets that LLM could face is important,
but challenging. The major reason is that the ground truth is necessary but the
data labeling process is heavy considering the time and human effort. To handle
this problem, in the conventional deep learning testing field, test selection
methods have been proposed for efficiently evaluating deep learning models by
prioritizing faults. However, despite their importance, the usefulness of these
methods on LLMs is unclear, and lack of exploration. In this paper, we conduct
the first empirical study to investigate the effectiveness of existing fault
detection methods for LLMs. Experimental results on four different
tasks~(including both code tasks and natural language processing tasks) and
four LLMs~(e.g., LLaMA3 and GPT4) demonstrated that simple methods such as
Margin perform well on LLMs but there is still a big room for improvement.
Based on the study, we further propose \textbf{MuCS}, a prompt
\textbf{Mu}tation-based prediction \textbf{C}onfidence \textbf{S}moothing
framework to boost the fault detection capability of existing methods.
Concretely, multiple prompt mutation techniques have been proposed to help
collect more diverse outputs for confidence smoothing. The results show that
our proposed framework significantly enhances existing methods with the
improvement of test relative coverage by up to 70.53\%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Multi-modal</span> Preference Alignment Remedies Degradation of Visual
  Instruction <span class="highlight-title">Tuning</span> on Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.10884v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.10884v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengzhi Li, Rongyu Lin, Shichao Pei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal large language models (MLLMs) are expected to support multi-turn
queries of interchanging image and text modalities in production. However, the
current MLLMs trained with visual-question-answering (VQA) datasets could
suffer from degradation, as VQA datasets lack the diversity and complexity of
the original text instruction datasets with which the underlying language model
was trained. To address this degradation, we first collect a lightweight,
5k-sample VQA preference dataset where answers were annotated by Gemini for
five quality metrics in a granular fashion and investigate standard Supervised
Fine-tuning, rejection sampling, Direct Preference Optimization (DPO) and
SteerLM algorithms. Our findings indicate that with DPO, we can surpass the
instruction-following capabilities of the language model, achieving a 6.73
score on MT-Bench, compared to Vicuna's 6.57 and LLaVA's 5.99. This enhancement
in textual instruction-following capability correlates with boosted visual
instruction performance (+4.9\% on MM-Vet, +6\% on LLaVA-Bench), with minimal
alignment tax on visual knowledge benchmarks compared to the previous RLHF
approach. In conclusion, we propose a distillation-based multi-modal alignment
model with fine-grained annotations on a small dataset that restores and boosts
MLLM's language capability after visual instruction tuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project code, model and data: https://github.com/findalexli/mllm-dpo</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Layer-wise Importance Matters: Less Memory for Better Performance in
  Parameter-efficient <span class="highlight-title">Fine-tuning</span> of Large Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11772v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11772v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Yao, Penglei Gao, Lichun Li, Yuan Zhao, Xiaofeng Wang, Wei Wang, Jianke Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parameter-Efficient Fine-Tuning (PEFT) methods have gained significant
popularity for adapting pre-trained Large Language Models (LLMs) to downstream
tasks, primarily due to their potential to significantly reduce memory and
computational overheads. However, a common limitation in most PEFT approaches
is their application of a uniform architectural design across all layers. This
uniformity involves identical trainable modules and ignores the varying
importance of each layer, leading to sub-optimal fine-tuning results. To
overcome the above limitation and obtain better performance, we develop a novel
approach, Importance-aware Sparse Tuning (IST), to fully utilize the inherent
sparsity and select the most important subset of full layers with effective
layer-wise importance scoring. The proposed IST is a versatile and
plug-and-play technique compatible with various PEFT methods that operate on a
per-layer basis. By leveraging the estimated importance scores, IST dynamically
updates these selected layers in PEFT modules, leading to reduced memory
demands. We further provide theoretical proof of convergence and empirical
evidence of superior performance to demonstrate the advantages of IST over
uniform updating strategies. Extensive experiments on a range of LLMs, PEFTs,
and downstream tasks substantiate the effectiveness of our proposed method,
showcasing IST's capacity to enhance existing layer-based PEFT methods. Our
code is available at https://github.com/Kaiseem/IST.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Aligning to Thousands of Preferences via System Message Generalization <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17977v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17977v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seongyun Lee, Sue Hyun Park, Seungone Kim, Minjoon Seo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although humans inherently have diverse values, current large language model
(LLM) alignment methods often assume that aligning LLMs with the general
public's preferences is optimal. A major challenge in adopting a more
individualized approach to LLM alignment is its lack of scalability, as it
involves repeatedly acquiring preference data and training new reward models
and LLMs for each individual's preferences. To address these challenges, we
propose a new paradigm where users specify what they value most within the
system message, steering the LLM's generation behavior to better align with the
user's intentions. However, a naive application of such an approach is
non-trivial since LLMs are typically trained on a uniform system message (e.g.,
"You are a helpful assistant") which limits their ability to generalize to
diverse, unseen system messages. To improve this generalization, we create the
Multifaceted Collection, a preference dataset with 192k combinations of values
beyond generic helpfulness and harmlessness, spanning 65k user instructions.
Using this dataset, we train a 7B LLM called Janus and test it on 921 prompts
from 5 benchmarks (AlpacaEval 2.0, FLASK, Koala, MT-Bench, and Self-Instruct)
by adding various unseen system messages that reflect user preferences. Janus
achieves tie+win rate of 75.2%, 72.4%, and 66.4% against Mistral 7B Instruct
v0.2, GPT-3.5 Turbo, and GPT-4, respectively. Unexpectedly, on three benchmarks
focused on response helpfulness (AlpacaEval 2.0, MT-Bench, Arena Hard Auto
v0.1), Janus also outperforms LLaMA 3 8B Instruct by a +4.0%, +0.1%, +3.0%
margin, underscoring that training with a vast array of system messages could
also enhance alignment to the general public's preference as well. Our code,
dataset, benchmark, and models are available at
https://github.com/kaistAI/Janus.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Birdie: Advancing State Space Models with Reward-Driven Objectives and
  Curricula <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01030v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01030v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sam Blouir, Jimmy T. H. Smith, Antonios Anastasopoulos, Amarda Shehu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient state space models (SSMs), such as linear recurrent neural networks
and linear attention variants, offer computational advantages over Transformers
but struggle with tasks requiring long-range in-context retrieval-like text
copying, associative recall, and question answering over long contexts.
Previous efforts to address these challenges have focused on architectural
modifications, often reintroducing computational inefficiencies. In this paper,
we propose a novel training procedure, Birdie, that significantly enhances the
in-context retrieval capabilities of SSMs without altering their architecture.
Our approach combines bidirectional input processing with dynamic mixtures of
specialized pre-training objectives, optimized via reinforcement learning. We
introduce a new bidirectional SSM architecture that seamlessly transitions from
bidirectional context processing to causal generation. Experimental evaluations
demonstrate that Birdie markedly improves performance on retrieval-intensive
tasks such as multi-number phone book lookup, long paragraph
question-answering, and infilling. This narrows the performance gap with
Transformers, while retaining computational efficiency. Our findings highlight
the importance of training procedures in leveraging the fixed-state capacity of
SSMs, offering a new direction to advance their capabilities. All code and
pre-trained models are available at https://www.github.com/samblouir/birdie,
with support for JAX and PyTorch.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 (Main Conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated
  Parameters by Tencent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02265v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02265v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingwu Sun, Yanfeng Chen, Yiqing Huang, Ruobing Xie, Jiaqi Zhu, Kai Zhang, Shuaipeng Li, Zhen Yang, Jonny Han, Xiaobo Shu, Jiahao Bu, Zhongzhi Chen, Xuemeng Huang, Fengzong Lian, Saiyong Yang, Jianfeng Yan, Yuyuan Zeng, Xiaoqin Ren, Chao Yu, Lulu Wu, Yue Mao, Jun Xia, Tao Yang, Suncong Zheng, Kan Wu, Dian Jiao, Jinbao Xue, Xipeng Zhang, Decheng Wu, Kai Liu, Dengpeng Wu, Guanghui Xu, Shaohua Chen, Shuang Chen, Xiao Feng, Yigeng Hong, Junqiang Zheng, Chengcheng Xu, Zongwei Li, Xiong Kuang, Jianglu Hu, Yiqi Chen, Yuchi Deng, Guiyang Li, Ao Liu, Chenchen Zhang, Shihui Hu, Zilong Zhao, Zifan Wu, Yao Ding, Weichao Wang, Han Liu, Roberts Wang, Hao Fei, Peijie She, Ze Zhao, Xun Cao, Hai Wang, Fusheng Xiang, Mengyuan Huang, Zhiyuan Xiong, Bin Hu, Xuebin Hou, Lei Jiang, Jiajia Wu, Yaping Deng, Yi Shen, Qian Wang, Weijie Liu, Jie Liu, Meng Chen, Liang Dong, Weiwen Jia, Hu Chen, Feifei Liu, Rui Yuan, Huilin Xu, Zhenxiang Yan, Tengfei Cao, Zhichao Hu, Xinhua Feng, Dong Du, Tinghao She, Yangyu Tao, Feng Zhang, Jianchen Zhu, Chengzhong Xu, Xirui Li, Chong Zha, Wen Ouyang, Yinben Xia, Xiang Li, Zekun He, Rongpeng Chen, Jiawei Song, Ruibin Chen, Fan Jiang, Chongqing Zhao, Bo Wang, Hao Gong, Rong Gan, Winston Hu, Zhanhui Kang, Yong Yang, Yuhong Liu, Di Wang, Jie Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce Hunyuan-Large, which is currently the largest
open-source Transformer-based mixture of experts model, with a total of 389
billion parameters and 52 billion activation parameters, capable of handling up
to 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior
performance across various benchmarks including language understanding and
generation, logical reasoning, mathematical problem-solving, coding,
long-context, and aggregated tasks, where it outperforms LLama3.1-70B and
exhibits comparable performance when compared to the significantly larger
LLama3.1-405B model. Key practice of Hunyuan-Large include large-scale
synthetic data that is orders larger than in previous literature, a mixed
expert routing strategy, a key-value cache compression technique, and an
expert-specific learning rate strategy. Additionally, we also investigate the
scaling laws and learning rate schedule of mixture of experts models, providing
valuable insights and guidances for future model development and optimization.
The code and checkpoints of Hunyuan-Large are released to facilitate future
innovations and applications.
  Codes: https://github.com/Tencent/Hunyuan-Large
  Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 4 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Investigation of Warning Erroneous Chat Translations in Cross-lingual
  Communication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.15543v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.15543v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunmeng Li, Jun Suzuki, Makoto Morishita, Kaori Abe, Kentaro Inui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine translation models are still inappropriate for translating chats,
despite the popularity of translation software and plug-in applications. The
complexity of dialogues poses significant challenges and can hinder
crosslingual communication. Instead of pursuing a flawless translation system,
a more practical approach would be to issue warning messages about potential
mistranslations to reduce confusion. However, it is still unclear how
individuals perceive these warning messages and whether they benefit the crowd.
This paper tackles to investigate this question and demonstrates the warning
messages' contribution to making chat translation systems effective.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pearl: Personalizing Large Language Model Writing Assistants with
  Generation-Calibrated Retrievers <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09180v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09180v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sheshera Mysore, Zhuoran Lu, Mengting Wan, Longqi Yang, Bahareh Sarrafzadeh, Steve Menezes, Tina Baghaee, Emmanuel Barajas Gonzalez, Jennifer Neville, Tara Safavi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Powerful large language models have facilitated the development of writing
assistants that promise to significantly improve the quality and efficiency of
composition and communication. However, a barrier to effective assistance is
the lack of personalization in LLM outputs to the author's communication style,
specialized knowledge, and values. In this paper, we address this challenge by
proposing Pearl, a LLM writing assistant personalized with a retriever that is
trained to be generation-calibrated for personalization. Generation calibration
ensures that our retriever selects historic user authored documents to augment
an LLM prompt such that they are likely to help an LLM generation better adhere
to a users' preferences. We propose two key novelties for training such a
retriever: (1) A training data selection method that identifies user requests
likely to benefit from personalization and documents that provide that benefit;
and (2) A scale-calibrating KL-divergence objective that ensures that our
retriever scores remain proportional to the downstream generation quality from
using the document for personalized generation. In a series of holistic
evaluations, we demonstrate the effectiveness of Pearl in generating long-form
texts on multiple social media datasets. Finally, we demonstrate how a
generation-calibrated retriever can double as a performance predictor --
detecting low quality retrieval, and improving potentially under-performing
outputs via revision with LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Workshop on Customizable NLP at EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unveiling the Potential of LLM-Based ASR on Chinese Open-Source Datasets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.02132v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.02132v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuelong Geng, Tianyi Xu, Kun Wei, Bingshen Mu, Hongfei Xue, He Wang, Yangze Li, Pengcheng Guo, Yuhang Dai, Longhao Li, Mingchen Shao, Lei Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated unparalleled effectiveness in
various NLP tasks, and integrating LLMs with automatic speech recognition (ASR)
is becoming a mainstream paradigm. Building upon this momentum, our research
delves into an in-depth examination of this paradigm on a large open-source
Chinese dataset. Specifically, our research aims to evaluate the impact of
various configurations of speech encoders, LLMs, and projector modules in the
context of the speech foundation encoder-LLM ASR paradigm. Furthermore, we
introduce a three-stage training approach, expressly developed to enhance the
model's ability to align auditory and textual information. The implementation
of this approach, alongside the strategic integration of ASR components,
enabled us to achieve the SOTA performance on the AISHELL-1, Test_Net, and
Test_Meeting test sets. Our analysis presents an empirical foundation for
future research in LLM-based ASR systems and offers insights into optimizing
performance using Chinese datasets. We will publicly release all scripts used
for data preparation, training, inference, and scoring, as well as pre-trained
models and training logs to promote reproducible research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLMs and the Madness of Crowds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01539v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01539v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William F. Bradley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the patterns of incorrect answers produced by large language
models (LLMs) during evaluation. These errors exhibit highly non-intuitive
behaviors unique to each model. By analyzing these patterns, we measure the
similarities between LLMs and construct a taxonomy that categorizes them based
on their error correlations. Our findings reveal that the incorrect responses
are not randomly distributed but systematically correlated across models,
providing new insights into the underlying structures and relationships among
LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing LLM Evaluations: The Garbling Trick 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01533v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01533v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William F. Bradley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) become increasingly powerful, traditional
evaluation metrics tend to saturate, making it challenging to distinguish
between models based on their performance. We propose a general method to
transform existing LLM evaluations into a series of progressively more
difficult tasks. These enhanced evaluations emphasize reasoning capabilities
and can reveal relative performance differences that are not apparent in the
original assessments.
  To demonstrate the effectiveness of our approach, we create a new
multiple-choice test corpus, extend it into a family of evaluations, and assess
a collection of LLMs. Our results offer insights into the comparative reasoning
abilities of these models, particularly highlighting distinctions between
OpenAI's o1-preview and Google's gemini-pro-1.5-002.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Safe Unlearning: A Surprisingly Effective and Generalizable Solution to
  Defend Against Jailbreak Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.02855v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.02855v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhexin Zhang, Junxiao Yang, Pei Ke, Shiyao Cui, Chujie Zheng, Hongning Wang, Minlie Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLMs are known to be vulnerable to jailbreak attacks, even after safety
alignment. An important observation is that, while different types of jailbreak
attacks can generate significantly different queries, they mostly result in
similar responses that are rooted in the same harmful knowledge (e.g., detailed
steps to make a bomb). Therefore, we conjecture that directly unlearn the
harmful knowledge in the LLM can be a more effective way to defend against
jailbreak attacks than the mainstream supervised fine-tuning (SFT) approaches.
Our extensive experiments demonstrate the surprising generalizability of our
unlearning-based approach: using only 20 raw harmful questions without any
jailbreak prompt during training, our solution reduced the Attack Success Rate
(ASR) in Vicuna-7B from 82.6% to 7.7% on out-of-distribution (OOD) harmful
questions wrapped with various complex jailbreak prompts . This significantly
outperforms Llama2-7B-Chat, which is fine-tuned on about 0.1M safety alignment
samples but still has an ASR of 21.9% even under the help of an additional
safety system prompt. Further analysis reveals that the generalization ability
of our solution may stem from the intrinsic relatedness among harmful responses
across harmful questions (e.g., response patterns, shared steps and actions in
response, and similarity among their learned representations in the LLM). Our
code is available at \url{https://github.com/thu-coai/SafeUnlearning}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking Misalignment in <span class="highlight-title">Vision-Language</span> Model Adaptation from a
  Causal Perspective <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12816v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12816v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanan Zhang, Jiangmeng Li, Lixiang Liu, Wenwen Qiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundational Vision-Language models such as CLIP have exhibited impressive
generalization in downstream tasks. However, CLIP suffers from a two-level
misalignment issue, i.e., task misalignment and data misalignment, when
adapting to specific tasks. Soft prompt tuning has mitigated the task
misalignment, yet the data misalignment remains a challenge. To analyze the
impacts of the data misalignment, we revisit the pre-training and adaptation
processes of CLIP and develop a structural causal model. We discover that while
we expect to capture task-relevant information for downstream tasks accurately,
the task-irrelevant knowledge impacts the prediction results and hampers the
modeling of the true relationships between the images and the predicted
classes. As task-irrelevant knowledge is unobservable, we leverage the
front-door adjustment and propose Causality-Guided Semantic Decoupling and
Classification (CDC) to mitigate the interference of task-irrelevant knowledge.
Specifically, we decouple semantics contained in the data of downstream tasks
and perform classification based on each semantic. Furthermore, we employ the
Dempster-Shafer evidence theory to evaluate the uncertainty of each prediction
generated by diverse semantics. Experiments conducted in multiple different
settings have consistently demonstrated the effectiveness of CDC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Seeing the Image: Prioritizing Visual Correlation by Contrastive
  Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17871v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17871v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Xiao, Bohong Wu, Jiacong Wang, Chunyuan Li, Xun Zhou, Haoyuan Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing image-text modality alignment in Vision Language Models (VLMs)
treats each text token equally in an autoregressive manner. Despite being
simple and effective, this method results in sub-optimal cross-modal alignment
by over-emphasizing the text tokens that are less correlated with or even
contradictory with the input images. In this paper, we advocate for assigning
distinct contributions for each text token based on its visual correlation.
Specifically, we present by contrasting image inputs, the difference in
prediction logits on each text token provides strong guidance of visual
correlation. We therefore introduce Contrastive ALignment (CAL), a simple yet
effective re-weighting strategy that prioritizes training visually correlated
tokens. Our experimental results demonstrate that CAL consistently improves
different types of VLMs across different resolutions and model sizes on various
benchmark datasets. Importantly, our method incurs minimal additional
computational overhead, rendering it highly efficient compared to alternative
data scaling strategies. Codes are available at
https://github.com/foundation-multimodal-models/CAL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurlPS 2024, Camera ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.16725v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.16725v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhifei Xie, Changqiao Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in language models have achieved significant progress.
GPT-4o, as a new milestone, has enabled real-time conversations with humans,
demonstrating near-human natural fluency. Such human-computer interaction
necessitates models with the capability to perform reasoning directly with the
audio modality and generate output in streaming. However, this remains beyond
the reach of current academic models, as they typically depend on extra TTS
systems for speech synthesis, resulting in undesirable latency. This paper
introduces the Mini-Omni, an audio-based end-to-end conversational model,
capable of real-time speech interaction. To achieve this capability, we propose
a text-instructed speech generation method, along with batch-parallel
strategies during inference to further boost the performance. Our method also
helps to retain the original model's language capabilities with minimal
degradation, enabling other works to establish real-time interaction
capabilities. We call this training method "Any Model Can Talk". We also
introduce the VoiceAssistant-400K dataset to fine-tune models optimized for
speech output. To our best knowledge, Mini-Omni is the first fully end-to-end,
open-source model for real-time speech interaction, offering valuable potential
for future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report, work in progress. Demo and code:
  https://github.com/gpt-omni/mini-omni</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AraTrust: An Evaluation of Trustworthiness for LLMs in Arabic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09017v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09017v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emad A. Alghamdi, Reem I. Masoud, Deema Alnuhait, Afnan Y. Alomairi, Ahmed Ashraf, Mohamed Zaytoon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The swift progress and widespread acceptance of artificial intelligence (AI)
systems highlight a pressing requirement to comprehend both the capabilities
and potential risks associated with AI. Given the linguistic complexity,
cultural richness, and underrepresented status of Arabic in AI research, there
is a pressing need to focus on Large Language Models (LLMs) performance and
safety for Arabic-related tasks. Despite some progress in their development,
there is a lack of comprehensive trustworthiness evaluation benchmarks, which
presents a major challenge in accurately assessing and improving the safety of
LLMs when prompted in Arabic. In this paper, we introduce AraTrust, the first
comprehensive trustworthiness benchmark for LLMs in Arabic. AraTrust comprises
522 human-written multiple-choice questions addressing diverse dimensions
related to truthfulness, ethics, safety, physical health, mental health,
unfairness, illegal activities, privacy, and offensive language. We evaluated a
set of LLMs against our benchmark to assess their trustworthiness. GPT-4 was
the most trustworthy LLM, while open-source models, particularly AceGPT 7B and
Jais 13B, struggled to achieve a score of 60% in our benchmark.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ShieldLM: Empowering LLMs as Aligned, Customizable and Explainable
  Safety Detectors <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16444v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16444v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhexin Zhang, Yida Lu, Jingyuan Ma, Di Zhang, Rui Li, Pei Ke, Hao Sun, Lei Sha, Zhifang Sui, Hongning Wang, Minlie Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The safety of Large Language Models (LLMs) has gained increasing attention in
recent years, but there still lacks a comprehensive approach for detecting
safety issues within LLMs' responses in an aligned, customizable and
explainable manner. In this paper, we propose ShieldLM, an LLM-based safety
detector, which aligns with common safety standards, supports customizable
detection rules, and provides explanations for its decisions. To train
ShieldLM, we compile a large bilingual dataset comprising 14,387 query-response
pairs, annotating the safety of responses based on various safety standards.
Through extensive experiments, we demonstrate that ShieldLM surpasses strong
baselines across four test sets, showcasing remarkable customizability and
explainability. Besides performing well on standard detection datasets,
ShieldLM has also been shown to be effective as a safety evaluator for advanced
LLMs. ShieldLM is released at \url{https://github.com/thu-coai/ShieldLM} to
support accurate and explainable safety detection under various safety
standards.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages. Camera ready version of EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scaling Laws for Reward Model Overoptimization in Direct Alignment
  Algorithms <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.02900v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.02900v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rafael Rafailov, Yaswanth Chittepu, Ryan Park, Harshit Sikchi, Joey Hejna, Bradley Knox, Chelsea Finn, Scott Niekum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning from Human Feedback (RLHF) has been crucial to the
recent success of Large Language Models (LLMs), however, it is often a complex
and brittle process. In the classical RLHF framework, a reward model is first
trained to represent human preferences, which is in turn used by an online
reinforcement learning (RL) algorithm to optimize the LLM. A prominent issue
with such methods is reward over-optimization or reward hacking, where
performance as measured by the learned proxy reward model increases, but true
quality plateaus or even deteriorates. Direct Alignment Algorithms (DDAs) like
Direct Preference Optimization have emerged as alternatives to the classical
RLHF pipeline by circumventing the reward modeling phase. However, although
DAAs do not use a separate proxy reward model, they still commonly deteriorate
from over-optimization. While the so-called reward hacking phenomenon is not
well-defined for DAAs, we still uncover similar trends: at higher KL budgets,
DAA algorithms exhibit similar degradation patterns to their classic RLHF
counterparts. In particular, we find that DAA methods deteriorate not only
across a wide range of KL budgets but also often before even a single epoch of
the dataset is completed. Through extensive empirical experimentation, this
work formulates and formalizes the reward over-optimization or hacking problem
for DAAs and explores its consequences across objectives, training regimes, and
model scales.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 38th Conference on Neural Information Processing Systems
  (NeurIPS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Evaluating Explanation Utility for Human-AI Decision Making in NLP <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.03545v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.03545v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fateme Hashemi Chaleshtori, Atreya Ghosal, Alexander Gill, Purbid Bambroo, Ana Marasović
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Is explainability a false promise? This debate has emerged from the
insufficient evidence that explanations help people in situations they are
introduced for. More human-centered, application-grounded evaluations of
explanations are needed to settle this. Yet, with no established guidelines for
such studies in NLP, researchers accustomed to standardized proxy evaluations
must discover appropriate measurements, tasks, datasets, and sensible models
for human-AI teams in their studies.
  To aid with this, we first review existing metrics suitable for
application-grounded evaluation. We then establish criteria to select
appropriate datasets, and using them, we find that only 4 out of over 50
datasets available for explainability research in NLP meet them. We then
demonstrate the importance of reassessing the state of the art to form and
study human-AI teams: teaming people with models for certain tasks might only
now start to make sense, and for others, it remains unsound. Finally, we
present the exemplar studies of human-AI decision-making for one of the
identified tasks -- verifying the correctness of a legal claim given a
contract. Our results show that providing AI predictions, with or without
explanations, does not cause decision makers to speed up their work without
compromising performance. We argue for revisiting the setup of human-AI teams
and improving automatic deferral of instances to AI, where explanations could
play a useful role.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP Findings 2024; 10 pages main, 7 pages references, 32 pages
  appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hidden Persuaders: LLMs' Political Leaning and Their Influence on Voters <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24190v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24190v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujin Potter, Shiyang Lai, Junsol Kim, James Evans, Dawn Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How could LLMs influence our democracy? We investigate LLMs' political
leanings and the potential influence of LLMs on voters by conducting multiple
experiments in a U.S. presidential election context. Through a voting
simulation, we first demonstrate 18 open- and closed-weight LLMs' political
preference for a Democratic nominee over a Republican nominee. We show how this
leaning towards the Democratic nominee becomes more pronounced in
instruction-tuned models compared to their base versions by analyzing their
responses to candidate-policy related questions. We further explore the
potential impact of LLMs on voter choice by conducting an experiment with 935
U.S. registered voters. During the experiments, participants interacted with
LLMs (Claude-3, Llama-3, and GPT-4) over five exchanges. The experiment results
show a shift in voter choices towards the Democratic nominee following LLM
interaction, widening the voting margin from 0.7% to 4.6%, even though LLMs
were not asked to persuade users to support the Democratic nominee during the
discourse. This effect is larger than many previous studies on the
persuasiveness of political campaigns, which have shown minimal effects in
presidential elections. Many users also expressed a desire for further
political interaction with LLMs. Which aspects of LLM interactions drove these
shifts in voter choice requires further study. Lastly, we explore how a safety
method can make LLMs more politically neutral, while leaving some open
questions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LADDER: Language Driven Slice Discovery and Error Rectification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.07832v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.07832v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shantanu Ghosh, Rayan Syed, Chenyu Wang, Clare B. Poynton, Shyam Visweswaran, Kayhan Batmanghelich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Error slice discovery associates structured patterns with model errors.
Existing methods discover error slices by clustering the error-prone samples
with similar patterns or assigning discrete attributes to each sample for
post-hoc analysis. While these methods aim for interpretability and easier
mitigation through reweighting or rebalancing, they may not capture the full
complexity of error patterns due to incomplete or missing attributes. Contrary
to the existing approach, this paper utilizes the reasoning capabilities of the
Large Language Model (LLM) to analyze complex error patterns and generate
testable hypotheses. This paper proposes LADDER: Language Driven slice
Discovery and Error Rectification. It first projects the model's representation
into a language-aligned feature space (eg CLIP) to preserve semantics in the
original model feature space. This ensures the accurate retrieval of sentences
that highlight the model's errors. Next, the LLM utilizes the sentences and
generates hypotheses to discover error slices. Finally, we mitigate the error
by fine-tuning the classification head by creating a group-balanced dataset
using the hypotheses. Our entire method does not require any attribute
annotation, either explicitly or through external tagging models. We validate
our method with \textbf{five} image classification datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NutriBench: A Dataset for Evaluating Large Language Models in
  Carbohydrate Estimation from Meal Descriptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12843v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12843v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andong Hua, Mehak Preet Dhaliwal, Ryan Burke, Laya Pullela, Yao Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate nutrition estimation helps people make informed dietary choices and
is essential in the prevention of serious health complications. We present
NutriBench, the first publicly available natural language meal description
nutrition benchmark. NutriBench consists of 11,857 meal descriptions generated
from real-world global dietary intake data. The data is human-verified and
annotated with macro-nutrient labels, including carbohydrates, proteins, fats,
and calories. We conduct an extensive evaluation of NutriBench on the task of
carbohydrate estimation, testing twelve leading Large Language Models (LLMs),
including GPT-4o, Llama3.1, Qwen2, Gemma2, and OpenBioLLM models, using
standard, Chain-of-Thought and Retrieval-Augmented Generation strategies.
Additionally, we present a study involving professional nutritionists, finding
that LLMs can provide more accurate and faster estimates. Finally, we perform a
real-world risk assessment by simulating the effect of carbohydrate predictions
on the blood glucose levels of individuals with diabetes. Our work highlights
the opportunities and challenges of using LLMs for nutrition estimation,
demonstrating their potential to aid professionals and laypersons and improve
health outcomes. Our benchmark is publicly available at:
https://mehak126.github.io/nutribench.html
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Building on Efficient Foundations: Effectively Training LLMs with
  Structured Feedforward Layers <span class="chip">NeurIPS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16450v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16450v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiuying Wei, Skander Moalla, Razvan Pascanu, Caglar Gulcehre
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art results in large language models (LLMs) often rely on scale,
which becomes computationally expensive. This has sparked a research agenda to
reduce these models' parameter counts and computational costs without
significantly impacting their performance. Our study focuses on
transformer-based LLMs, specifically targeting the computationally intensive
feedforward networks (FFNs), which are less studied than attention blocks. We
consider three structured linear parameterizations of the FFN using efficient
low-rank and block-diagonal matrices. In contrast to many previous works that
examined these approximations, our study i) explores these structures from a
training-from-scratch perspective, ii) scales up to 1.3B parameters, and iii)
is conducted within recent Transformer-based LLMs rather than convolutional
architectures. We demonstrate that these structures can lead to actual
computational gains in various scenarios, including online decoding when using
a pre-merge technique. Additionally, we propose a novel training regime, called
\textit{self-guided training}, aimed at improving the poor training dynamics
that these approximations exhibit when used from initialization. Interestingly,
the scaling performance of structured matrices is explored, revealing steeper
curves in scaling training FLOPs, along with a favorable scaling trend in the
overtraining regime. Specifically, we show that wide and structured networks
can utilize training FLOPs more efficiently, with fewer parameters and lower
loss than dense models at their optimal trade-off. Our code is available at
\url{https://github.com/CLAIRE-Labo/StructuredFFN/tree/main}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tree-Averaging Algorithms for Ensemble-Based Unsupervised Discontinuous
  Constituency Parsing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00143v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00143v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Behzad Shayegh, Yuqiao Wen, Lili Mou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address unsupervised discontinuous constituency parsing, where we observe
a high variance in the performance of the only previous model in the
literature. We propose to build an ensemble of different runs of the existing
discontinuous parser by averaging the predicted trees, to stabilize and boost
performance. To begin with, we provide comprehensive computational complexity
analysis (in terms of P and NP-complete) for tree averaging under different
setups of binarity and continuity. We then develop an efficient exact algorithm
to tackle the task, which runs in a reasonable time for all samples in our
experiments. Results on three datasets show our method outperforms all
baselines in all metrics; we also provide in-depth analyses of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AutoKaggle: A Multi-Agent Framework for Autonomous Data Science
  Competitions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20424v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20424v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziming Li, Qianbo Zang, David Ma, Jiawei Guo, Tuney Zheng, Minghao Liu, Xinyao Niu, Yue Wang, Jian Yang, Jiaheng Liu, Wanjun Zhong, Wangchunshu Zhou, Wenhao Huang, Ge Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data science tasks involving tabular data present complex challenges that
require sophisticated problem-solving approaches. We propose AutoKaggle, a
powerful and user-centric framework that assists data scientists in completing
daily data pipelines through a collaborative multi-agent system. AutoKaggle
implements an iterative development process that combines code execution,
debugging, and comprehensive unit testing to ensure code correctness and logic
consistency. The framework offers highly customizable workflows, allowing users
to intervene at each phase, thus integrating automated intelligence with human
expertise. Our universal data science toolkit, comprising validated functions
for data cleaning, feature engineering, and modeling, forms the foundation of
this solution, enhancing productivity by streamlining common tasks. We selected
8 Kaggle competitions to simulate data processing workflows in real-world
application scenarios. Evaluation results demonstrate that AutoKaggle achieves
a validation submission rate of 0.85 and a comprehensive score of 0.82 in
typical data science pipelines, fully proving its effectiveness and
practicality in handling complex data science tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>44 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Learning Based Amharic Chatbot for FAQs in Universities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.01720v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.01720v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Goitom Ybrah Hailu, Hadush Hailu, Shishay Welay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  University students often spend a considerable amount of time seeking answers
to common questions from administrators or teachers. This can become tedious
for both parties, leading to a need for a solution. In response, this paper
proposes a chatbot model that utilizes natural language processing and deep
learning techniques to answer frequently asked questions (FAQs) in the Amharic
language. Chatbots are computer programs that simulate human conversation
through the use of artificial intelligence (AI), acting as a virtual assistant
to handle questions and other tasks. The proposed chatbot program employs
tokenization, normalization, stop word removal, and stemming to analyze and
categorize Amharic input sentences. Three machine learning model algorithms
were used to classify tokens and retrieve appropriate responses: Support Vector
Machine (SVM), Multinomial Na\"ive Bayes, and deep neural networks implemented
through TensorFlow, Keras, and NLTK. The deep learning model achieved the best
results with 91.55% accuracy and a validation loss of 0.3548 using an Adam
optimizer and SoftMax activation function. The chatbot model was integrated
with Facebook Messenger and deployed on a Heroku server for 24-hour
accessibility. The experimental results demonstrate that the chatbot framework
achieved its objectives and effectively addressed challenges such as Amharic
Fidel variation, morphological variation, and lexical gaps. Future research
could explore the integration of Amharic WordNet to narrow the lexical gap and
support more complex questions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LiveMind: Low-latency Large Language Models with Simultaneous Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14319v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14319v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuangtao Chen, Grace Li Zhang, Xunzhao Yin, Cheng Zhuo, Ulf Schlichtmann, Bing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce LiveMind, a novel low-latency inference framework
for large language model (LLM) inference which enables LLMs to perform
inferences with incomplete user input. By reallocating computational processes
to the input phase, a substantial reduction in latency is achieved, thereby
significantly enhancing the interactive experience for users of LLMs. The
framework adeptly manages the visibility of the streaming input to the model,
allowing it to infer from incomplete user input or await additional content.
Compared with traditional inference methods on complete user input, our
approach demonstrates an average reduction in response latency of 84.0% on the
MMLU dataset and 71.6% on the MMLU-Pro dataset, while maintaining comparable
accuracy. Additionally, our framework facilitates collaborative inference and
output across different models. By employing an large LLM for inference and a
small LLM for output, we achieve an average 37% reduction in response latency,
alongside a 4.30% improvement in accuracy on the MMLU-Pro dataset compared with
the baseline. The proposed LiveMind framework advances the field of human-AI
interaction by enabling more responsive and efficient communication between
users and AI systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adapting Language Models via Token Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00593v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00593v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhili Feng, Tanya Marwah, Nicolo Fusi, David Alvarez-Melis, Lester Mackey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern large language models use a fixed tokenizer to effectively compress
text drawn from a source domain. However, applying the same tokenizer to a new
target domain often leads to inferior compression, more costly inference, and
reduced semantic alignment. To address this deficiency, we introduce Sparse
Sinkhorn Token Translation (S2T2). S2T2 trains a tailored tokenizer for the
target domain and learns to translate between target and source tokens,
enabling more effective reuse of the pre-trained next-source-token predictor.
In our experiments with finetuned English language models, S2T2 improves both
the perplexity and the compression of out-of-domain protein sequences,
outperforming direct finetuning with either the source or target tokenizer. In
addition, we find that token translations learned for smaller, less expensive
models can be directly transferred to larger, more powerful models to reap the
benefits of S2T2 at lower cost.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">125</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MME-Finance: A <span class="highlight-title">Multimodal</span> Finance Benchmark for Expert-level
  Understanding and Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03314v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03314v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziliang Gan, Yu Lu, Dong Zhang, Haohan Li, Che Liu, Jian Liu, Ji Liu, Haipang Wu, Chaoyou Fu, Zenglin Xu, Rongjunchen Zhang, Yong Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, multimodal benchmarks for general domains have guided the
rapid development of multimodal models on general tasks. However, the financial
field has its peculiarities. It features unique graphical images (e.g.,
candlestick charts, technical indicator charts) and possesses a wealth of
specialized financial knowledge (e.g., futures, turnover rate). Therefore,
benchmarks from general fields often fail to measure the performance of
multimodal models in the financial domain, and thus cannot effectively guide
the rapid development of large financial models. To promote the development of
large financial multimodal models, we propose MME-Finance, an bilingual
open-ended and practical usage-oriented Visual Question Answering (VQA)
benchmark. The characteristics of our benchmark are finance and expertise,
which include constructing charts that reflect the actual usage needs of users
(e.g., computer screenshots and mobile photography), creating questions
according to the preferences in financial domain inquiries, and annotating
questions by experts with 10+ years of experience in the financial industry.
Additionally, we have developed a custom-designed financial evaluation system
in which visual information is first introduced in the multi-modal evaluation
process. Extensive experimental evaluations of 19 mainstream MLLMs are
conducted to test their perception, reasoning, and cognition capabilities. The
results indicate that models performing well on general benchmarks cannot do
well on MME-Finance; for instance, the top-performing open-source and
closed-source models obtain 65.69 (Qwen2VL-72B) and 63.18 (GPT-4o),
respectively. Their performance is particularly poor in categories most
relevant to finance, such as candlestick charts and technical indicator charts.
In addition, we propose a Chinese version, which helps compare performance of
MLLMs under a Chinese context.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://hithink-research.github.io/MME-Finance/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Classification Done Right for <span class="highlight-title">Vision-Language</span> Pre-Training <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03313v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03313v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huang Zilong, Ye Qinghao, Kang Bingyi, Feng Jiashi, Fan Haoqi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce SuperClass, a super simple classification method for
vision-language pre-training on image-text data. Unlike its contrastive
counterpart CLIP who contrast with a text encoder, SuperClass directly utilizes
tokenized raw text as supervised classification labels, without the need for
additional text filtering or selection. Due to the absence of the text encoding
as contrastive target, SuperClass does not require a text encoder and does not
need to maintain a large batch size as CLIP does. SuperClass demonstrated
superior performance on various downstream tasks, including classic computer
vision benchmarks and vision language downstream tasks. We further explored the
scaling behavior of SuperClass on model size, training length, or data size,
and reported encouraging results and comparisons to CLIP.
https://github.com/x-cls/superclass
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inference Optimal VLMs Need Only One Visual Token but Larger Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03312v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03312v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Y. Li, Sachin Goyal, Joao D. Semedo, J. Zico Kolter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Language Models (VLMs) have demonstrated strong capabilities across
various visual understanding and reasoning tasks. However, their real-world
deployment is often constrained by high latency during inference due to
substantial compute required to process the large number of input tokens
(predominantly from the image) by the LLM. To reduce inference costs, one can
either downsize the LLM or reduce the number of input image-tokens, the latter
of which has been the focus of many recent works around token compression.
However, it is unclear what the optimal trade-off is, as both the factors
directly affect the VLM performance. We first characterize this optimal
trade-off between the number of visual tokens and LLM parameters by
establishing scaling laws that capture variations in performance with these two
factors. Our results reveal a surprising trend: for visual reasoning tasks, the
inference-optimal behavior in VLMs, i.e., minimum downstream error at any given
fixed inference compute, is achieved when using the largest LLM that fits
within the inference budget while minimizing visual token count - often to a
single token. While the token reduction literature has mainly focused on
maintaining base model performance by modestly reducing the token count (e.g.,
$5-10\times$), our results indicate that the compute-optimal inference regime
requires operating under even higher token compression ratios. Based on these
insights, we take some initial steps towards building approaches tailored for
high token compression settings. Code is available at
https://github.com/locuslab/llava-token-compression.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiT4Edit: Diffusion Transformer for Image Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03286v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03286v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunyu Feng, Yue Ma, Bingyuan Wang, Chenyang Qi, Haozhe Chen, Qifeng Chen, Zeyu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent advances in UNet-based image editing, methods for shape-aware
object editing in high-resolution images are still lacking. Compared to UNet,
Diffusion Transformers (DiT) demonstrate superior capabilities to effectively
capture the long-range dependencies among patches, leading to higher-quality
image generation. In this paper, we propose DiT4Edit, the first Diffusion
Transformer-based image editing framework. Specifically, DiT4Edit uses the
DPM-Solver inversion algorithm to obtain the inverted latents, reducing the
number of steps compared to the DDIM inversion algorithm commonly used in
UNet-based frameworks. Additionally, we design unified attention control and
patches merging, tailored for transformer computation streams. This integration
allows our framework to generate higher-quality edited images faster. Our
design leverages the advantages of DiT, enabling it to surpass UNet structures
in image editing, especially in high-resolution and arbitrary-size images.
Extensive experiments demonstrate the strong performance of DiT4Edit across
various editing scenarios, highlighting the potential of Diffusion Transformers
in supporting image editing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ShadowMamba: State-Space Model with Boundary-Region Selective Scan for
  Shadow Removal 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03260v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03260v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiujin Zhu, Chee-Onn Chow, Joon Huang Chuah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image shadow removal is a typical low-level vision problem, where the
presence of shadows leads to abrupt changes in brightness in certain regions,
affecting the accuracy of upstream tasks. Current shadow removal methods still
face challenges such as residual boundary artifacts, and capturing feature
information at shadow boundaries is crucial for removing shadows and
eliminating residual boundary artifacts. Recently, Mamba has achieved
remarkable success in computer vision by globally modeling long-sequence
information with linear complexity. However, when applied to image shadow
removal, the original Mamba scanning method overlooks the semantic continuity
of shadow boundaries as well as the continuity of semantics within the same
region. Based on the unique characteristics of shadow images, this paper
proposes a novel selective scanning method called boundary-region selective
scanning. This method scans boundary regions, shadow regions, and non-shadow
regions independently, bringing pixels of the same region type closer together
in the long sequence, especially focusing on the local information at the
boundaries, which is crucial for shadow removal. This method combines with
global scanning and channel scanning to jointly accomplish the shadow removal.
We name our model ShadowMamba, the first Mamba-based model for shadow removal.
Extensive experimental results show that our method outperforms current
state-of-the-art models across most metrics on multiple datasets. The code for
ShadowMamba is available at (Code will be released upon acceptance).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decoupling Fine Detail and Global Geometry for Compressed Depth Map
  Super-Resolution <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03239v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03239v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huan Zheng, Wencheng Han, Jianbing Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recovering high-quality depth maps from compressed sources has gained
significant attention due to the limitations of consumer-grade depth cameras
and the bandwidth restrictions during data transmission. However, current
methods still suffer from two challenges. First, bit-depth compression produces
a uniform depth representation in regions with subtle variations, hindering the
recovery of detailed information. Second, densely distributed random noise
reduces the accuracy of estimating the global geometric structure of the scene.
To address these challenges, we propose a novel framework, termed
geometry-decoupled network (GDNet), for compressed depth map super-resolution
that decouples the high-quality depth map reconstruction process by handling
global and detailed geometric features separately. To be specific, we propose
the fine geometry detail encoder (FGDE), which is designed to aggregate fine
geometry details in high-resolution low-level image features while
simultaneously enriching them with complementary information from
low-resolution context-level image features. In addition, we develop the global
geometry encoder (GGE) that aims at suppressing noise and extracting global
geometric information effectively via constructing compact feature
representation in a low-rank space. We conduct experiments on multiple
benchmark datasets, demonstrating that our GDNet significantly outperforms
current methods in terms of geometric consistency and detail recovery. In the
ECCV 2024 AIM Compressed Depth Upsampling Challenge, our solution won the 1st
place award. Our codes will be available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The 1st solution for the ECCV 2024 AIM Compressed Depth Upsampling
  Challenge</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Topograph: An efficient Graph-Based Framework for Strictly Topology
  Preserving Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03228v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03228v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laurin Lux, Alexander H. Berger, Alexander Weers, Nico Stucki, Daniel Rueckert, Ulrich Bauer, Johannes C. Paetzold
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topological correctness plays a critical role in many image segmentation
tasks, yet most networks are trained using pixel-wise loss functions, such as
Dice, neglecting topological accuracy. Existing topology-aware methods often
lack robust topological guarantees, are limited to specific use cases, or
impose high computational costs. In this work, we propose a novel, graph-based
framework for topologically accurate image segmentation that is both
computationally efficient and generally applicable. Our method constructs a
component graph that fully encodes the topological information of both the
prediction and ground truth, allowing us to efficiently identify topologically
critical regions and aggregate a loss based on local neighborhood information.
Furthermore, we introduce a strict topological metric capturing the homotopy
equivalence between the union and intersection of prediction-label pairs. We
formally prove the topological guarantees of our approach and empirically
validate its effectiveness on binary and multi-class datasets. Our loss
demonstrates state-of-the-art performance with up to fivefold faster loss
computation compared to persistent homology methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Kernel Orthogonality does not necessarily imply a Decrease in Feature
  Map Redundancy in CNNs: Convolutional Similarity Minimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03226v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03226v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zakariae Belmekki, Jun Li, Patrick Reuter, David Antonio Gómez Jáuregui, Karl Jenkins
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional Neural Networks (CNNs) have been heavily used in Deep Learning
due to their success in various tasks. Nonetheless, it has been observed that
CNNs suffer from redundancy in feature maps, leading to inefficient capacity
utilization. Efforts to mitigate and solve this problem led to the emergence of
multiple methods, amongst which is kernel orthogonality through variant means.
In this work, we challenge the common belief that kernel orthogonality leads to
a decrease in feature map redundancy, which is, supposedly, the ultimate
objective behind kernel orthogonality. We prove, theoretically and empirically,
that kernel orthogonality has an unpredictable effect on feature map similarity
and does not necessarily decrease it. Based on our theoretical result, we
propose an effective method to reduce feature map similarity independently of
the input of the CNN. This is done by minimizing a novel loss function we call
Convolutional Similarity. Empirical results show that minimizing the
Convolutional Similarity increases the performance of classification models and
can accelerate their convergence. Furthermore, using our proposed method pushes
towards a more efficient use of the capacity of models, allowing the use of
significantly smaller models to achieve the same levels of performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge Graphs of Driving Scenes to Empower the Emerging Capabilities
  of Neurosymbolic AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03225v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03225v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruwan Wickramarachchi, Cory Henson, Amit Sheth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the era of Generative AI, Neurosymbolic AI is emerging as a powerful
approach for tasks spanning from perception to cognition. The use of
Neurosymbolic AI has been shown to achieve enhanced capabilities, including
improved grounding, alignment, explainability, and reliability. However, due to
its nascent stage, there is a lack of widely available real-world benchmark
datasets tailored to Neurosymbolic AI tasks. To address this gap and support
the evaluation of current and future methods, we introduce DSceneKG -- a suite
of knowledge graphs of driving scenes built from real-world, high-quality
scenes from multiple open autonomous driving datasets. In this article, we
detail the construction process of DSceneKG and highlight its application in
seven different tasks. DSceneKG is publicly accessible at:
https://github.com/ruwantw/DSceneKG
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Grid Data: Exploring Graph Neural Networks for Earth Observation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03223v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03223v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shan Zhao, Zhaiyu Chen, Zhitong Xiong, Yilei Shi, Sudipan Saha, Xiao Xiang Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Earth Observation (EO) data analysis has been significantly revolutionized by
deep learning (DL), with applications typically limited to grid-like data
structures. Graph Neural Networks (GNNs) emerge as an important innovation,
propelling DL into the non-Euclidean domain. Naturally, GNNs can effectively
tackle the challenges posed by diverse modalities, multiple sensors, and the
heterogeneous nature of EO data. To introduce GNNs in the related domains, our
review begins by offering fundamental knowledge on GNNs. Then, we summarize the
generic problems in EO, to which GNNs can offer potential solutions. Following
this, we explore a broad spectrum of GNNs' applications to scientific problems
in Earth systems, covering areas such as weather and climate analysis, disaster
management, air quality monitoring, agriculture, land cover classification,
hydrological process modeling, and urban modeling. The rationale behind
adopting GNNs in these fields is explained, alongside methodologies for
organizing graphs and designing favorable architectures for various tasks.
Furthermore, we highlight methodological challenges of implementing GNNs in
these domains and possible solutions that could guide future research. While
acknowledging that GNNs are not a universal solution, we conclude the paper by
comparing them with other popular architectures like transformers and analyzing
their potential synergies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in Geoscience and Remote Sensing Magazine
  (GRSM)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Improved Conditioning Mechanisms and Pre-training Strategies for
  Diffusion Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03177v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03177v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tariq Berrada Ifriqi, Pietro Astolfi, Melissa Hall, Reyhane Askari-Hemmat, Yohann Benchetrit, Marton Havasi, Matthew Muckley, Karteek Alahari, Adriana Romero-Soriano, Jakob Verbeek, Michal Drozdzal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale training of latent diffusion models (LDMs) has enabled
unprecedented quality in image generation. However, the key components of the
best performing LDM training recipes are oftentimes not available to the
research community, preventing apple-to-apple comparisons and hindering the
validation of progress in the field. In this work, we perform an in-depth study
of LDM training recipes focusing on the performance of models and their
training efficiency. To ensure apple-to-apple comparisons, we re-implement five
previously published models with their corresponding recipes. Through our
study, we explore the effects of (i)~the mechanisms used to condition the
generative model on semantic information (e.g., text prompt) and control
metadata (e.g., crop size, random flip flag, etc.) on the model performance,
and (ii)~the transfer of the representations learned on smaller and
lower-resolution datasets to larger ones on the training efficiency and model
performance. We then propose a novel conditioning mechanism that disentangles
semantic and control metadata conditionings and sets a new state-of-the-art in
class-conditional generation on the ImageNet-1k dataset -- with FID
improvements of 7% on 256 and 8% on 512 resolutions -- as well as text-to-image
generation on the CC12M dataset -- with FID improvements of 8% on 256 and 23%
on 512 resolution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a conference paper (poster) for NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pre-trained Visual Dynamics Representations for Efficient Policy
  Learning <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03169v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03169v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Luo, Bohan Zhou, Zongqing Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-training for Reinforcement Learning (RL) with purely video data is a
valuable yet challenging problem. Although in-the-wild videos are readily
available and inhere a vast amount of prior world knowledge, the absence of
action annotations and the common domain gap with downstream tasks hinder
utilizing videos for RL pre-training. To address the challenge of pre-training
with videos, we propose Pre-trained Visual Dynamics Representations (PVDR) to
bridge the domain gap between videos and downstream tasks for efficient policy
learning. By adopting video prediction as a pre-training task, we use a
Transformer-based Conditional Variational Autoencoder (CVAE) to learn visual
dynamics representations. The pre-trained visual dynamics representations
capture the visual dynamics prior knowledge in the videos. This abstract prior
knowledge can be readily adapted to downstream tasks and aligned with
executable actions through online adaptation. We conduct experiments on a
series of robotics visual control tasks and verify that PVDR is an effective
form for pre-training with videos to promote policy learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MA^2: A Self-Supervised and Motion Augmenting Autoencoder for Gait-Based
  Automatic Disease Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03129v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03129v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiqun Liu, Ke Zhang, Yin Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ground reaction force (GRF) is the force exerted by the ground on a body in
contact with it. GRF-based automatic disease detection (ADD) has become an
emerging medical diagnosis method, which aims to learn and identify disease
patterns corresponding to different gait pressures based on deep learning
methods. Although existing ADD methods can save doctors time in making
diagnoses, training deep models still struggles with the cost caused by the
labeling engineering for a large number of gait diagnostic data for subjects.
On the other hand, the accuracy of the deep model under the unified benchmark
GRF dataset and the generalization ability on scalable gait datasets need to be
further improved. To address these issues, we propose MA2, a GRF-based
self-supervised and motion augmenting auto-encoder, which models the ADD task
as an encoder-decoder paradigm. In the encoder, we introduce an embedding block
including the 3-layer 1D convolution for extracting the token and a mask
generator to randomly mask out the sequence of tokens to maximize the model's
potential to capture high-level, discriminative, intrinsic representations.
whereafter, the decoder utilizes this information to reconstruct the pixel
sequence of the origin input and calculate the reconstruction loss to optimize
the network. Moreover, the backbone of an auto-encoder is multi-head
self-attention that can consider the global information of the token from the
input, not just the local neighborhood. This allows the model to capture
generalized contextual information. Extensive experiments demonstrate MA2 has
SOTA performance of 90.91% accuracy on 1% limited pathological GRF samples with
labels, and good generalization ability of 78.57% accuracy on scalable
Parkinson disease dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 11 figures, article</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Investigating the Applicability of a Snapshot Computed Tomography
  Imaging Spectrometer for the Prediction of Brix and pH of Grapes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03114v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03114v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mads Svanborg Peters, Mads Juul Ahlebæk, Mads Toudal Frandsen, Bjarke Jørgensen, Christian Hald Jessen, Andreas Krogh Carlsen, Wei-Chih Huang, René Lynge Eriksen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, a recently developed snapshot hyperspectral imaging (HSI)
system based on Computed Tomography Imaging Spectroscopy (CTIS) is utilized to
determine Brix and pH values in Sheegene 20 table grapes through Partial Least
Squares Regression (PLSR) modeling. The performance of the CTIS system is
compared with that of a state-of-the-art line scan HSI system by imaging 100
grapes across both platforms. Reference measurements of Brix and pH values are
obtained directly using a refractometer and a pH meter, as these parameters are
essential for assessing the quality of table and wine grapes. The findings
indicate that the spectra captured by the CTIS camera correlate well with the
reference measurements, despite the system's narrower spectral range. The CTIS
camera's advantages, including its lower cost, portability, and reduced
susceptibility to motion errors, highlight its potential for promising in-field
applications in grape quality assessment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Local Lesion Generation is Effective for Capsule Endoscopy Image Data
  Augmentation in a Limited Data Setting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03098v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03098v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adrian B. Chłopowiec, Adam R. Chłopowiec, Krzysztof Galus, Wojciech Cebula, Martin Tabakov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Limited medical imaging datasets challenge deep learning models by increasing
risks of overfitting and reduced generalization, particularly in Generative
Adversarial Networks (GANs), where discriminators may overfit, leading to
training divergence. This constraint also impairs classification models trained
on small datasets. Generative Data Augmentation (GDA) addresses this by
expanding training datasets with synthetic data, although it requires training
a generative model. We propose and evaluate two local lesion generation
approaches to address the challenge of augmenting small medical image datasets.
The first approach employs the Poisson Image Editing algorithm, a classical
image processing technique, to create realistic image composites that
outperform current state-of-the-art methods. The second approach introduces a
novel generative method, leveraging a fine-tuned Image Inpainting GAN to
synthesize realistic lesions within specified regions of real training images.
A comprehensive comparison of the two proposed methods demonstrates that
effective local lesion generation in a data-constrained setting allows for
reaching new state-of-the-art results in capsule endoscopy lesion
classification. Combination of our techniques achieves a macro F1-score of
33.07%, surpassing the previous best result by 7.84 percentage points (p.p.) on
the highly imbalanced Kvasir Capsule Dataset, a benchmark for capsule
endoscopy. To the best of our knowledge, this work is the first to apply a
fine-tuned Image Inpainting GAN for GDA in medical imaging, demonstrating that
an image-conditional GAN can be adapted effectively to limited datasets to
generate high-quality examples, facilitating effective data augmentation.
Additionally, we show that combining this GAN-based approach with classical
image processing techniques further enhances the results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>45 pages, 27 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HFGaussian: Learning Generalizable Gaussian Human with Integrated Human
  Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03086v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03086v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arnab Dey, Cheng-You Lu, Andrew I. Comport, Srinath Sridhar, Chin-Teng Lin, Jean Martinet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in radiance field rendering show promising results in 3D
scene representation, where Gaussian splatting-based techniques emerge as
state-of-the-art due to their quality and efficiency. Gaussian splatting is
widely used for various applications, including 3D human representation.
However, previous 3D Gaussian splatting methods either use parametric body
models as additional information or fail to provide any underlying structure,
like human biomechanical features, which are essential for different
applications. In this paper, we present a novel approach called HFGaussian that
can estimate novel views and human features, such as the 3D skeleton, 3D key
points, and dense pose, from sparse input images in real time at 25 FPS. The
proposed method leverages generalizable Gaussian splatting technique to
represent the human subject and its associated features, enabling efficient and
generalizable reconstruction. By incorporating a pose regression network and
the feature splatting technique with Gaussian splatting, HFGaussian
demonstrates improved capabilities over existing 3D human methods, showcasing
the potential of 3D human representations with integrated biomechanics. We
thoroughly evaluate our HFGaussian method against the latest state-of-the-art
techniques in human Gaussian splatting and pose estimation, demonstrating its
real-time, state-of-the-art performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-supervised cross-modality learning for uncertainty-aware object
  detection and recognition in applications which lack pre-labelled training
  data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03082v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03082v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Irum Mehboob, Li Sun, Alireza Astegarpanah, Rustam Stolkin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper shows how an uncertainty-aware, deep neural network can be trained
to detect, recognise and localise objects in 2D RGB images, in applications
lacking annotated train-ng datasets. We propose a self-supervising
teacher-student pipeline, in which a relatively simple teacher classifier,
trained with only a few labelled 2D thumbnails, automatically processes a
larger body of unlabelled RGB-D data to teach a student network based on a
modified YOLOv3 architecture. Firstly, 3D object detection with back projection
is used to automatically extract and teach 2D detection and localisation
information to the student network. Secondly, a weakly supervised 2D thumbnail
classifier, with minimal training on a small number of hand-labelled images, is
used to teach object category recognition. Thirdly, we use a Gaussian Process
GP to encode and teach a robust uncertainty estimation functionality, so that
the student can output confidence scores with each categorization. The
resulting student significantly outperforms the same YOLO architecture trained
directly on the same amount of labelled data. Our GP-based approach yields
robust and meaningful uncertainty estimations for complex industrial object
classifications. The end-to-end network is also capable of real-time
processing, needed for robotics applications. Our method can be applied to many
important industrial tasks, where labelled datasets are typically unavailable.
In this paper, we demonstrate an example of detection, localisation, and object
category recognition of nuclear mixed-waste materials in highly cluttered and
unstructured scenes. This is critical for robotic sorting and handling of
legacy nuclear waste, which poses complex environmental remediation challenges
in many nuclearised nations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploiting the Segment Anything Model (SAM) for Lung Segmentation in
  Chest X-ray Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03064v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03064v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriel Bellon de Carvalho, Jurandy Almeida
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Segment Anything Model (SAM), a new AI model from Meta AI released in April
2023, is an ambitious tool designed to identify and separate individual objects
within a given image through semantic interpretation. The advanced capabilities
of SAM are the result of its training with millions of images and masks, and a
few days after its release, several researchers began testing the model on
medical images to evaluate its performance in this domain. With this
perspective in focus -- i.e., optimizing work in the healthcare field -- this
work proposes the use of this new technology to evaluate and study chest X-ray
images. The approach adopted for this work, with the aim of improving the
model's performance for lung segmentation, involved a transfer learning
process, specifically the fine-tuning technique. After applying this
adjustment, a substantial improvement was observed in the evaluation metrics
used to assess SAM's performance compared to the masks provided by the
datasets. The results obtained by the model after the adjustments were
satisfactory and similar to cutting-edge neural networks, such as U-Net.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ATM: Improving Model Merging by Alternating <span class="highlight-title">Tuning</span> and Merging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03055v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03055v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Zhou, Daniele Solombrino, Donato Crisostomi, Maria Sofia Bucarelli, Fabrizio Silvestri, Emanuele Rodolà
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model merging has recently emerged as a cost-efficient paradigm for
multi-task learning. Among current approaches, task arithmetic stands out for
its simplicity and effectiveness. In this paper, we motivate the effectiveness
of task vectors by linking them to multi-task gradients. We show that in a
single-epoch scenario, task vectors are mathematically equivalent to the
gradients obtained via gradient descent in a multi-task setting, and still
approximate these gradients in subsequent epochs. Furthermore, we show that
task vectors perform optimally when equality is maintained, and their
effectiveness is largely driven by the first epoch's gradient. Building on this
insight, we propose viewing model merging as a single step in an iterative
process that Alternates between Tuning and Merging (ATM). This method acts as a
bridge between model merging and multi-task gradient descent, achieving
state-of-the-art results with the same data and computational requirements. We
extensively evaluate ATM across diverse settings, achieving up to 20% higher
accuracy in computer vision and NLP tasks, compared to the best
baselines.Finally, we provide both empirical and theoretical support for its
effectiveness, demonstrating increased orthogonality between task vectors and
proving that ATM minimizes an upper bound on the loss obtained by jointly
finetuning all tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Main paper: 10 Pages, 11 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gradient-Guided Conditional Diffusion Models for Private Image
  Reconstruction: Analyzing Adversarial Impacts of Differential Privacy and
  Denoising 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03053v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03053v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Huang, Jiayang Meng, Hong Chen, Guolong Zheng, Xu Yang, Xun Yi, Hua Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the construction of gradient-guided conditional diffusion
models for reconstructing private images, focusing on the adversarial interplay
between differential privacy noise and the denoising capabilities of diffusion
models. While current gradient-based reconstruction methods struggle with
high-resolution images due to computational complexity and prior knowledge
requirements, we propose two novel methods that require minimal modifications
to the diffusion model's generation process and eliminate the need for prior
knowledge. Our approach leverages the strong image generation capabilities of
diffusion models to reconstruct private images starting from randomly generated
noise, even when a small amount of differentially private noise has been added
to the gradients. We also conduct a comprehensive theoretical analysis of the
impact of differential privacy noise on the quality of reconstructed images,
revealing the relationship among noise magnitude, the architecture of attacked
models, and the attacker's reconstruction capability. Additionally, extensive
experiments validate the effectiveness of our proposed methods and the accuracy
of our theoretical findings, suggesting new directions for privacy risk
auditing using conditional diffusion models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GarVerseLOD: High-Fidelity 3D Garment Reconstruction from a Single
  In-the-Wild Image using a Dataset with Levels of Details 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03047v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03047v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongjin Luo, Haolin Liu, Chenghong Li, Wanghao Du, Zirong Jin, Wanhu Sun, Yinyu Nie, Weikai Chen, Xiaoguang Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural implicit functions have brought impressive advances to the
state-of-the-art of clothed human digitization from multiple or even single
images. However, despite the progress, current arts still have difficulty
generalizing to unseen images with complex cloth deformation and body poses. In
this work, we present GarVerseLOD, a new dataset and framework that paves the
way to achieving unprecedented robustness in high-fidelity 3D garment
reconstruction from a single unconstrained image. Inspired by the recent
success of large generative models, we believe that one key to addressing the
generalization challenge lies in the quantity and quality of 3D garment data.
Towards this end, GarVerseLOD collects 6,000 high-quality cloth models with
fine-grained geometry details manually created by professional artists. In
addition to the scale of training data, we observe that having disentangled
granularities of geometry can play an important role in boosting the
generalization capability and inference accuracy of the learned model. We hence
craft GarVerseLOD as a hierarchical dataset with levels of details (LOD),
spanning from detail-free stylized shape to pose-blended garment with
pixel-aligned details. This allows us to make this highly under-constrained
problem tractable by factorizing the inference into easier tasks, each narrowed
down with smaller searching space. To ensure GarVerseLOD can generalize well to
in-the-wild images, we propose a novel labeling paradigm based on conditional
diffusion models to generate extensive paired images for each garment model
with high photorealism. We evaluate our method on a massive amount of
in-the-wild images. Experimental results demonstrate that GarVerseLOD can
generate standalone garment pieces with significantly better quality than prior
approaches. Project page: https://garverselod.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://garverselod.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluation of handwriting kinematics and pressure for differential
  diagnosis of Parkinson's disease 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03044v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03044v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Drotár, Jiří Mekyska, Irena Rektorová, Lucia Masarová, Zdeněk Smékal, Marcos Faundez-Zanuy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Objective: We present the PaHaW Parkinson's disease handwriting database,
consisting of handwriting samples from Parkinson's disease (PD) patients and
healthy controls. Our goal is to show that kinematic features and pressure
features in handwriting can be used for the differential diagnosis of PD.
Methods and Material: The database contains records from 37 PD patients and 38
healthy controls performing eight different handwriting tasks. The tasks
include drawing an Archimedean spiral, repetitively writing orthographically
simple syllables and words, and writing of a sentence. In addition to the
conventional kinematic features related to the dynamics of handwriting, we
investigated new pressure features based on the pressure exerted on the writing
surface. To discriminate between PD patients and healthy subjects, three
different classifiers were compared: K-nearest neighbors (K-NN), ensemble
AdaBoost classifier, and support vector machines (SVM). Results: For predicting
PD based on kinematic and pressure features of handwriting, the best performing
model was SVM with classification accuracy of Pacc = 81.3% (sensitivity Psen =
87.4% and specificity of Pspe = 80.9%). When evaluated separately, pressure
features proved to be relevant for PD diagnosis, yielding Pacc = 82.5% compared
to Pacc = 75.4% using kinematic features. Conclusion: Experimental results
showed that an analysis of kinematic and pressure features during handwriting
can help assess subtle characteristics of handwriting and discriminate between
PD patients and healthy controls.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Judge Like a Real Doctor: Dual Teacher Sample Consistency Framework for
  Semi-supervised Medical Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03041v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03041v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhang Qixiang, Yang Yuxiang, Zu Chen, Zhang Jianjia, Wu Xi, Zhou Jiliu, Wang Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised learning (SSL) is a popular solution to alleviate the high
annotation cost in medical image classification. As a main branch of SSL,
consistency regularization engages in imposing consensus between the
predictions of a single sample from different views, termed as Absolute
Location consistency (AL-c). However, only AL-c may be insufficient. Just like
when diagnosing a case in practice, besides the case itself, the doctor usually
refers to certain related trustworthy cases to make more reliable
decisions.Therefore, we argue that solely relying on AL-c may ignore the
relative differences across samples, which we interpret as relative locations,
and only exploit limited information from one perspective. To address this
issue, we propose a Sample Consistency Mean Teacher (SCMT) which not only
incorporates AL c but also additionally enforces consistency between the
samples' relative similarities to its related samples, called Relative Location
consistency (RL c). AL c and RL c conduct consistency regularization from two
different perspectives, jointly extracting more diverse semantic information
for classification. On the other hand, due to the highly similar structures in
medical images, the sample distribution could be overly dense in feature space,
making their relative locations susceptible to noise. To tackle this problem,
we further develop a Sample Scatter Mean Teacher (SSMT) by utilizing
contrastive learning to sparsify the sample distribution and obtain robust and
effective relative locations. Extensive experiments on different datasets
demonstrate the superiority of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Emerging Topics in Computational
  Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Decoders for Transformer-based Semantic Segmentation:
  Compression is All You Need <span class="chip">NeurIPS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03033v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03033v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qishuai Wen, Chun-Guang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art methods for Transformer-based semantic segmentation
typically adopt Transformer decoders that are used to extract additional
embeddings from image embeddings via cross-attention, refine either or both
types of embeddings via self-attention, and project image embeddings onto the
additional embeddings via dot-product. Despite their remarkable success, these
empirical designs still lack theoretical justifications or interpretations,
thus hindering potentially principled improvements. In this paper, we argue
that there are fundamental connections between semantic segmentation and
compression, especially between the Transformer decoders and Principal
Component Analysis (PCA). From such a perspective, we derive a white-box, fully
attentional DEcoder for PrIncipled semantiC segemenTation (DEPICT), with the
interpretations as follows: 1) the self-attention operator refines image
embeddings to construct an ideal principal subspace that aligns with the
supervision and retains most information; 2) the cross-attention operator seeks
to find a low-rank approximation of the refined image embeddings, which is
expected to be a set of orthonormal bases of the principal subspace and
corresponds to the predefined classes; 3) the dot-product operation yields
compact representation for image embeddings as segmentation masks. Experiments
conducted on dataset ADE20K find that DEPICT consistently outperforms its
black-box counterpart, Segmenter, and it is light weight and more robust.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS2024. Code:https://github.com/QishuaiWen/DEPICT/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FEDLAD: Federated Evaluation of Deep Leakage Attacks and Defenses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03019v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03019v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isaac Baglin, Xiatian Zhu, Simon Hadfield
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning is a privacy preserving decentralized machine learning
paradigm designed to collaboratively train models across multiple clients by
exchanging gradients to the server and keeping private data local.
Nevertheless, recent research has revealed that the security of Federated
Learning is compromised, as private ground truth data can be recovered through
a gradient inversion technique known as Deep Leakage. While these attacks are
crafted with a focus on applications in Federated Learning, they generally are
not evaluated in realistic scenarios. This paper introduces the FEDLAD
Framework (Federated Evaluation of Deep Leakage Attacks and Defenses), a
comprehensive benchmark for evaluating Deep Leakage attacks and defenses within
a realistic Federated context. By implementing a unified benchmark that
encompasses multiple state-of-the-art Deep Leakage techniques and various
defense strategies, our framework facilitates the evaluation and comparison of
the efficacy of these methods across different datasets and training states.
This work highlights a crucial trade-off between privacy and model accuracy in
Federated Learning and aims to advance the understanding of security challenges
in decentralized machine learning systems, stimulate future research, and
enhance reproducibility in evaluating Deep Leakage attacks and defenses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CRT-Fusion: Camera, Radar, Temporal Fusion Using Motion Information for
  3D Object Detection <span class="chip">NeurIPS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03013v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03013v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jisong Kim, Minjae Seong, Jun Won Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate and robust 3D object detection is a critical component in autonomous
vehicles and robotics. While recent radar-camera fusion methods have made
significant progress by fusing information in the bird's-eye view (BEV)
representation, they often struggle to effectively capture the motion of
dynamic objects, leading to limited performance in real-world scenarios. In
this paper, we introduce CRT-Fusion, a novel framework that integrates temporal
information into radar-camera fusion to address this challenge. Our approach
comprises three key modules: Multi-View Fusion (MVF), Motion Feature Estimator
(MFE), and Motion Guided Temporal Fusion (MGTF). The MVF module fuses radar and
image features within both the camera view and bird's-eye view, thereby
generating a more precise unified BEV representation. The MFE module conducts
two simultaneous tasks: estimation of pixel-wise velocity information and BEV
segmentation. Based on the velocity and the occupancy score map obtained from
the MFE module, the MGTF module aligns and fuses feature maps across multiple
timestamps in a recurrent manner. By considering the motion of dynamic objects,
CRT-Fusion can produce robust BEV feature maps, thereby improving detection
accuracy and robustness. Extensive evaluations on the challenging nuScenes
dataset demonstrate that CRT-Fusion achieves state-of-the-art performance for
radar-camera-based 3D object detection. Our approach outperforms the previous
best method in terms of NDS by +1.7%, while also surpassing the leading
approach in mAP by +1.4%. These significant improvements in both metrics
showcase the effectiveness of our proposed fusion strategy in enhancing the
reliability and accuracy of 3D object detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Precise Drive with VLM: First Prize Solution for PRCV 2024 Drive LM
  challenge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02999v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02999v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin Huang, Siyu Wang, Yuanpeng Chen, Yidan Wu, Hui Song, Zifan Ding, Jing Leng, Chengpeng Liang, Peng Xue, Junliang Zhang, Tiankun Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This technical report outlines the methodologies we applied for the PRCV
Challenge, focusing on cognition and decision-making in driving scenarios. We
employed InternVL-2.0, a pioneering open-source multi-modal model, and enhanced
it by refining both the model input and training methodologies. For the input
data, we strategically concatenated and formatted the multi-view images. It is
worth mentioning that we utilized the coordinates of the original images
without transformation. In terms of model training, we initially pre-trained
the model on publicly available autonomous driving scenario datasets to bolster
its alignment capabilities of the challenge tasks, followed by fine-tuning on
the DriveLM-nuscenes Dataset. During the fine-tuning phase, we innovatively
modified the loss function to enhance the model's precision in predicting
coordinate values. These approaches ensure that our model possesses advanced
cognitive and decision-making capabilities in driving scenarios. Consequently,
our model achieved a score of 0.6064, securing the first prize on the
competition's final results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PV-faultNet: Optimized CNN Architecture to detect defects resulting
  efficient PV production 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02997v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02997v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eiffat E Zaman, Rahima Khanam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The global shift towards renewable energy has pushed PV cell manufacturing as
a pivotal point as they are the fundamental building block of green energy.
However, the manufacturing process is complex enough to lose its purpose due to
probable defects experienced during the time impacting the overall efficiency.
However, at the moment, manual inspection is being conducted to detect the
defects that can cause bias, leading to time and cost inefficiency. Even if
automated solutions have also been proposed, most of them are
resource-intensive, proving ineffective in production environments. In that
context, this study presents PV-faultNet, a lightweight Convolutional Neural
Network (CNN) architecture optimized for efficient and real-time defect
detection in photovoltaic (PV) cells, designed to be deployable on
resource-limited production devices. Addressing computational challenges in
industrial PV manufacturing environments, the model includes only 2.92 million
parameters, significantly reducing processing demands without sacrificing
accuracy. Comprehensive data augmentation techniques were implemented to tackle
data scarcity, thus enhancing model generalization and maintaining a balance
between precision and recall. The proposed model achieved high performance with
91\% precision, 89\% recall, and a 90\% F1 score, demonstrating its
effectiveness for scalable quality control in PV production.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient and Effective Adaptation of <span class="highlight-title">Multimodal</span> Foundation Models in
  Sequential Recommendation <span class="chip">SIGIR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02992v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02992v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junchen Fu, Xuri Ge, Xin Xin, Alexandros Karatzoglou, Ioannis Arapakis, Kaiwen Zheng, Yongxin Ni, Joemon M. Jose
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal foundation models (MFMs) have revolutionized sequential
recommender systems through advanced representation learning. While
Parameter-efficient Fine-tuning (PEFT) is commonly used to adapt these models,
studies often prioritize parameter efficiency, neglecting GPU memory and
training speed. To address this, we introduced the IISAN framework,
significantly enhancing efficiency. However, IISAN was limited to symmetrical
MFMs and identical text and image encoders, preventing the use of
state-of-the-art Large Language Models. To overcome this, we developed
IISAN-Versa, a versatile plug-and-play architecture compatible with both
symmetrical and asymmetrical MFMs. IISAN-Versa employs a Decoupled PEFT
structure and utilizes both intra- and inter-modal adaptation. It effectively
handles asymmetry through a simple yet effective combination of group
layer-dropping and dimension transformation alignment. Our research
demonstrates that IISAN-Versa effectively adapts large text encoders, and we
further identify a scaling effect where larger encoders generally perform
better. IISAN-Versa also demonstrates strong versatility in our defined
multimodal scenarios, which include raw titles and captions generated from
images and videos. Additionally, IISAN-Versa achieved state-of-the-art
performance on the Microlens public benchmark. We will release our code and
datasets to support future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The extension of IISAN in SIGIR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CAD-NeRF: Learning NeRFs from Uncalibrated Few-view Images by CAD Model
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02979v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02979v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Wen, Xuening Zhu, Renjiao Yi, Zhifeng Wang, Chenyang Zhu, Kai Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing from multi-view images is a longstanding problem in 3D vision,
where neural radiance fields (NeRFs) have shown great potential and get
realistic rendered images of novel views. Currently, most NeRF methods either
require accurate camera poses or a large number of input images, or even both.
Reconstructing NeRF from few-view images without poses is challenging and
highly ill-posed. To address this problem, we propose CAD-NeRF, a method
reconstructed from less than 10 images without any known poses. Specifically,
we build a mini library of several CAD models from ShapeNet and render them
from many random views. Given sparse-view input images, we run a model and pose
retrieval from the library, to get a model with similar shapes, serving as the
density supervision and pose initializations. Here we propose a multi-view pose
retrieval method to avoid pose conflicts among views, which is a new and unseen
problem in uncalibrated NeRF methods. Then, the geometry of the object is
trained by the CAD guidance. The deformation of the density field and camera
poses are optimized jointly. Then texture and density are trained and
fine-tuned as well. All training phases are in self-supervised manners.
Comprehensive evaluations of synthetic and real images show that CAD-NeRF
successfully learns accurate densities with a large deformation from retrieved
CAD models, showing the generalization abilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The article has been accepted by Frontiers of Computer Science (FCS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Region-Guided Attack on the Segment Anything Model (SAM) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02974v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02974v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoliang Liu, Furao Shen, Jian Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Segment Anything Model (SAM) is a cornerstone of image segmentation,
demonstrating exceptional performance across various applications, particularly
in autonomous driving and medical imaging, where precise segmentation is
crucial. However, SAM is vulnerable to adversarial attacks that can
significantly impair its functionality through minor input perturbations.
Traditional techniques, such as FGSM and PGD, are often ineffective in
segmentation tasks due to their reliance on global perturbations that overlook
spatial nuances. Recent methods like Attack-SAM-K and UAD have begun to address
these challenges, but they frequently depend on external cues and do not fully
leverage the structural interdependencies within segmentation processes. This
limitation underscores the need for a novel adversarial strategy that exploits
the unique characteristics of segmentation tasks. In response, we introduce the
Region-Guided Attack (RGA), designed specifically for SAM. RGA utilizes a
Region-Guided Map (RGM) to manipulate segmented regions, enabling targeted
perturbations that fragment large segments and expand smaller ones, resulting
in erroneous outputs from SAM. Our experiments demonstrate that RGA achieves
high success rates in both white-box and black-box scenarios, emphasizing the
need for robust defenses against such sophisticated attacks. RGA not only
reveals SAM's vulnerabilities but also lays the groundwork for developing more
resilient defenses against adversarial threats in image segmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Seasonal Variability in the Context of Neural Radiance Fields
  for 3D Reconstruction on Satellite Imagery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02972v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02972v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liv Kåreborn, Erica Ingerstad, Amanda Berg, Justus Karlsson, Leif Haglund
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, the seasonal predictive capabilities of Neural Radiance Fields
(NeRF) applied to satellite images are investigated. Focusing on the
utilization of satellite data, the study explores how Sat-NeRF, a novel
approach in computer vision, performs in predicting seasonal variations across
different months. Through comprehensive analysis and visualization, the study
examines the model's ability to capture and predict seasonal changes,
highlighting specific challenges and strengths. Results showcase the impact of
the sun direction on predictions, revealing nuanced details in seasonal
transitions, such as snow cover, color accuracy, and texture representation in
different landscapes. Given these results, we propose Planet-NeRF, an extension
to Sat-NeRF capable of incorporating seasonal variability through a set of
month embedding vectors. Comparative evaluations reveal that Planet-NeRF
outperforms prior models in the case where seasonal changes are present. The
extensive evaluation combined with the proposed method offers promising avenues
for future research in this domain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Multi-modal</span> NeRF Self-Supervision for LiDAR Semantic Segmentation <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02969v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02969v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xavier Timoneda, Markus Herb, Fabian Duerr, Daniel Goehring, Fisher Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LiDAR Semantic Segmentation is a fundamental task in autonomous driving
perception consisting of associating each LiDAR point to a semantic label.
Fully-supervised models have widely tackled this task, but they require labels
for each scan, which either limits their domain or requires impractical amounts
of expensive annotations. Camera images, which are generally recorded alongside
LiDAR pointclouds, can be processed by the widely available 2D foundation
models, which are generic and dataset-agnostic. However, distilling knowledge
from 2D data to improve LiDAR perception raises domain adaptation challenges.
For example, the classical perspective projection suffers from the parallax
effect produced by the position shift between both sensors at their respective
capture times. We propose a Semi-Supervised Learning setup to leverage
unlabeled LiDAR pointclouds alongside distilled knowledge from the camera
images. To self-supervise our model on the unlabeled scans, we add an auxiliary
NeRF head and cast rays from the camera viewpoint over the unlabeled voxel
features. The NeRF head predicts densities and semantic logits at each sampled
ray location which are used for rendering pixel semantics. Concurrently, we
query the Segment-Anything (SAM) foundation model with the camera image to
generate a set of unlabeled generic masks. We fuse the masks with the rendered
pixel semantics from LiDAR to produce pseudo-labels that supervise the pixel
predictions. During inference, we drop the NeRF head and run our model with
only LiDAR. We show the effectiveness of our approach in three public LiDAR
Semantic Segmentation benchmarks: nuScenes, SemanticKITTI and ScribbleKITTI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE/RSJ International Conference on Intelligent Robots and Systems
  (IROS) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LDPM: Towards undersampled MRI reconstruction with MR-VAE and Latent
  Diffusion Prior 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02951v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02951v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingjian Tang, Jingwei Guan, Linge Li, Youmei Zhang, Mengye Lyu, Li Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion model, as a powerful generative model, has found a wide range of
applications including MRI reconstruction. However, most existing diffusion
model-based MRI reconstruction methods operate directly in pixel space, which
makes their optimization and inference computationally expensive. Latent
diffusion models were introduced to address this problem in natural image
processing, but directly applying them to MRI reconstruction still faces many
challenges, including the lack of control over the generated results, the
adaptability of Variational AutoEncoder (VAE) to MRI, and the exploration of
applicable data consistency in latent space. To address these challenges, a
Latent Diffusion Prior based undersampled MRI reconstruction (LDPM) method is
proposed. A sketcher module is utilized to provide appropriate control and
balance the quality and fidelity of the reconstructed MR images. A VAE adapted
for MRI tasks (MR-VAE) is explored, which can serve as the backbone for future
MR-related tasks. Furthermore, a variation of the DDIM sampler, called the
Dual-Stage Sampler, is proposed to achieve high-fidelity reconstruction in the
latent space. The proposed method achieves competitive results on fastMRI
datasets, and the effectiveness of each module is demonstrated in ablation
experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mapping Africa Settlements: High Resolution Urban and Rural Map by Deep
  Learning and Satellite Imagery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02935v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02935v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Kakooei, James Bailie, Albin Söderberg, Albin Becevic, Adel Daoud
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate Land Use and Land Cover (LULC) maps are essential for understanding
the drivers of sustainable development, in terms of its complex
interrelationships between human activities and natural resources. However,
existing LULC maps often lack precise urban and rural classifications,
particularly in diverse regions like Africa. This study presents a novel
construction of a high-resolution rural-urban map using deep learning
techniques and satellite imagery. We developed a deep learning model based on
the DeepLabV3 architecture, which was trained on satellite imagery from
Landsat-8 and the ESRI LULC dataset, augmented with human settlement data from
the GHS-SMOD. The model utilizes semantic segmentation to classify land into
detailed categories, including urban and rural areas, at a 10-meter resolution.
Our findings demonstrate that incorporating LULC along with urban and rural
classifications significantly enhances the model's ability to accurately
distinguish between urban, rural, and non-human settlement areas. Therefore,
our maps can support more informed decision-making for policymakers,
researchers, and stakeholders. We release a continent wide urban-rural map,
covering the period 2016 and 2022.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Domain Expansion and Boundary Growth for Open-Set Single-Source Domain
  Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02920v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02920v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengkun Jiao, Na Zhao, Jingjing Chen, Yu-Gang Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-set single-source domain generalization aims to use a single-source
domain to learn a robust model that can be generalized to unknown target
domains with both domain shifts and label shifts. The scarcity of the source
domain and the unknown data distribution of the target domain pose a great
challenge for domain-invariant feature learning and unknown class recognition.
In this paper, we propose a novel learning approach based on domain expansion
and boundary growth to expand the scarce source samples and enlarge the
boundaries across the known classes that indirectly broaden the boundary
between the known and unknown classes. Specifically, we achieve domain
expansion by employing both background suppression and style augmentation on
the source data to synthesize new samples. Then we force the model to distill
consistent knowledge from the synthesized samples so that the model can learn
domain-invariant information. Furthermore, we realize boundary growth across
classes by using edge maps as an additional modality of samples when training
multi-binary classifiers. In this way, it enlarges the boundary between the
inliers and outliers, and consequently improves the unknown class recognition
during open-set generalization. Extensive experiments show that our approach
can achieve significant improvements and reach state-of-the-art performance on
several cross-domain image classification datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>TMM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Membership Inference Attacks against Large <span class="highlight-title">Vision-Language</span> Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02902v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02902v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhan Li, Yongtao Wu, Yihang Chen, Francesco Tonin, Elias Abad Rocamora, Volkan Cevher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large vision-language models (VLLMs) exhibit promising capabilities for
processing multi-modal tasks across various application scenarios. However,
their emergence also raises significant data security concerns, given the
potential inclusion of sensitive information, such as private photos and
medical records, in their training datasets. Detecting inappropriately used
data in VLLMs remains a critical and unresolved issue, mainly due to the lack
of standardized datasets and suitable methodologies. In this study, we
introduce the first membership inference attack (MIA) benchmark tailored for
various VLLMs to facilitate training data detection. Then, we propose a novel
MIA pipeline specifically designed for token-level image detection. Lastly, we
present a new metric called MaxR\'enyi-K%, which is based on the confidence of
the model output and applies to both text and image data. We believe that our
work can deepen the understanding and methodology of MIAs in the context of
VLLMs. Our code and datasets are available at
https://github.com/LIONS-EPFL/VL-MIA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fried deconvolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02890v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02890v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jerome Gilles, Stanley Osher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we present a new approach to deblur the effect of atmospheric
turbulence in the case of long range imaging. Our method is based on an
analytical formulation, the Fried kernel, of the atmosphere modulation transfer
function (MTF) and a framelet based deconvolution algorithm. An important
parameter is the refractive index structure which requires specific
measurements to be known. Then we propose a method which provides a good
estimation of this parameter from the input blurred image. The final algorithms
are very easy to implement and show very good results on both simulated blur
and real images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Turbulence stabilization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02889v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02889v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Mao, Jerome Gilles
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We recently developed a new approach to get a stabilized image from a
sequence of frames acquired through atmospheric turbulence. The goal of this
algorihtm is to remove the geometric distortions due by the atmosphere
movements. This method is based on a variational formulation and is efficiently
solved by the use of Bregman iterations and the operator splitting method. In
this paper we propose to study the influence of the choice of the regularizing
term in the model. Then we proposed to experiment some of the most used
regularization constraints available in the litterature.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Symmetric Dynamic Learning Framework for Diffeomorphic Medical Image
  Registration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02888v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02888v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinqiu Deng, Ke Chen, Mingke Li, Daoping Zhang, Chong Chen, Alejandro F. Frangi, Jianping Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffeomorphic image registration is crucial for various medical imaging
applications because it can preserve the topology of the transformation. This
study introduces DCCNN-LSTM-Reg, a learning framework that evolves dynamically
and learns a symmetrical registration path by satisfying a specified control
increment system. This framework aims to obtain symmetric diffeomorphic
deformations between moving and fixed images. To achieve this, we combine deep
learning networks with diffeomorphic mathematical mechanisms to create a
continuous and dynamic registration architecture, which consists of multiple
Symmetric Registration (SR) modules cascaded on five different scales.
Specifically, our method first uses two U-nets with shared parameters to
extract multiscale feature pyramids from the images. We then develop an
SR-module comprising a sequential CNN-LSTM architecture to progressively
correct the forward and reverse multiscale deformation fields using control
increment learning and the homotopy continuation technique. Through extensive
experiments on three 3D registration tasks, we demonstrate that our method
outperforms existing approaches in both quantitative and qualitative
evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages,7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Adversarial Robustness via Uncertainty-Aware Distributional
  Adversarial Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02871v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02871v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junhao Dong, Xinghua Qu, Z. Jane Wang, Yew-Soon Ong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite remarkable achievements in deep learning across various domains, its
inherent vulnerability to adversarial examples still remains a critical concern
for practical deployment. Adversarial training has emerged as one of the most
effective defensive techniques for improving model robustness against such
malicious inputs. However, existing adversarial training schemes often lead to
limited generalization ability against underlying adversaries with diversity
due to their overreliance on a point-by-point augmentation strategy by mapping
each clean example to its adversarial counterpart during training. In addition,
adversarial examples can induce significant disruptions in the statistical
information w.r.t. the target model, thereby introducing substantial
uncertainty and challenges to modeling the distribution of adversarial
examples. To circumvent these issues, in this paper, we propose a novel
uncertainty-aware distributional adversarial training method, which enforces
adversary modeling by leveraging both the statistical information of
adversarial examples and its corresponding uncertainty estimation, with the
goal of augmenting the diversity of adversaries. Considering the potentially
negative impact induced by aligning adversaries to misclassified clean
examples, we also refine the alignment reference based on the statistical
proximity to clean examples during adversarial training, thereby reframing
adversarial training within a distribution-to-distribution matching framework
interacted between the clean and adversarial domains. Furthermore, we design an
introspective gradient alignment approach via matching input gradients between
these domains without introducing external models. Extensive experiments across
four benchmark datasets and various network architectures demonstrate that our
approach achieves state-of-the-art adversarial robustness and maintains natural
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AtlasSeg: Atlas Prior Guided Dual-U-Net for Cortical Segmentation in
  Fetal Brain MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02867v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02867v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoan Xu, Tianshu Zheng, Xinyi Xu, Yao Shen, Jiwei Sun, Cong Sun, Guangbin Wang, Dan Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate tissue segmentation in fetal brain MRI remains challenging due to
the dynamically changing anatomical anatomy and contrast during fetal
development. To enhance segmentation accuracy throughout gestation, we
introduced AtlasSeg, a dual-U-shape convolution network incorporating
gestational age (GA) specific information as guidance. By providing a publicly
available fetal brain atlas with segmentation label at the corresponding GA,
AtlasSeg effectively extracted the contextual features of age-specific patterns
in atlas branch and generated tissue segmentation in segmentation branch.
Multi-scale attentive atlas feature fusions were constructed in all stages
during encoding and decoding, giving rise to a dual-U-shape network to assist
feature flow and information interactions between two branches. AtlasSeg
outperformed six well-known segmentation networks in both our internal fetal
brain MRI dataset and the external FeTA dataset. Ablation experiments
demonstrate the efficiency of atlas guidance and the attention mechanism. The
proposed AtlasSeg demonstrated superior segmentation performance against other
convolution networks with higher segmentation accuracy, and may facilitate
fetal brain MRI analysis in large-scale fetal brain studies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Centerness-based Instance-aware Knowledge Distillation with Task-wise
  Mutual Lifting for Object Detection on Drone Imagery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02861v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02861v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowei Du, Zhixuan Liao, Yanan Zhang, Zhi Cai, Jiaxin Chen, Di Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing accurate and efficient detectors for drone imagery is challenging
due to the inherent complexity of aerial scenes. While some existing methods
aim to achieve high accuracy by utilizing larger models, their computational
cost is prohibitive for drones. Recently, Knowledge Distillation (KD) has shown
promising potential for maintaining satisfactory accuracy while significantly
compressing models in general object detection. Considering the advantages of
KD, this paper presents the first attempt to adapt it to object detection on
drone imagery and addresses two intrinsic issues: (1) low foreground-background
ratio and (2) small instances and complex backgrounds, which lead to inadequate
training, resulting insufficient distillation. Therefore, we propose a
task-wise Lightweight Mutual Lifting (Light-ML) module with a Centerness-based
Instance-aware Distillation (CID) strategy. The Light-ML module mutually
harmonizes the classification and localization branches by channel shuffling
and convolution, integrating teacher supervision across different tasks during
back-propagation, thus facilitating training the student model. The CID
strategy extracts valuable regions surrounding instances through the centerness
of proposals, enhancing distillation efficacy. Experiments on the VisDrone,
UAVDT, and COCO benchmarks demonstrate that the proposed approach promotes the
accuracies of existing state-of-the-art KD methods with comparable
computational requirements. Codes will be available upon acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continual Audio-Visual Sound Separation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02860v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02860v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiguo Pian, Yiyang Nan, Shijian Deng, Shentong Mo, Yunhui Guo, Yapeng Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a novel continual audio-visual sound separation
task, aiming to continuously separate sound sources for new classes while
preserving performance on previously learned classes, with the aid of visual
guidance. This problem is crucial for practical visually guided auditory
perception as it can significantly enhance the adaptability and robustness of
audio-visual sound separation models, making them more applicable for
real-world scenarios where encountering new sound sources is commonplace. The
task is inherently challenging as our models must not only effectively utilize
information from both modalities in current tasks but also preserve their
cross-modal association in old tasks to mitigate catastrophic forgetting during
audio-visual continual learning. To address these challenges, we propose a
novel approach named ContAV-Sep (\textbf{Cont}inual
\textbf{A}udio-\textbf{V}isual Sound \textbf{Sep}aration). ContAV-Sep presents
a novel Cross-modal Similarity Distillation Constraint (CrossSDC) to uphold the
cross-modal semantic similarity through incremental tasks and retain previously
acquired knowledge of semantic similarity in old models, mitigating the risk of
catastrophic forgetting. The CrossSDC can seamlessly integrate into the
training process of different audio-visual sound separation frameworks.
Experiments demonstrate that ContAV-Sep can effectively mitigate catastrophic
forgetting and achieve significantly better performance compared to other
continual learning baselines for audio-visual sound separation. Code is
available at: \url{https://github.com/weiguoPian/ContAV-Sep_NeurIPS2024}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OLAF: A Plug-and-Play Framework for Enhanced Multi-object Multi-part
  Scene Parsing <span class="chip">ECCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02858v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02858v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pranav Gupta, Rishubh Singh, Pradeep Shenoy, Ravikiran Sarvadevabhatla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-object multi-part scene segmentation is a challenging task whose
complexity scales exponentially with part granularity and number of scene
objects. To address the task, we propose a plug-and-play approach termed OLAF.
First, we augment the input (RGB) with channels containing object-based
structural cues (fg/bg mask, boundary edge mask). We propose a weight
adaptation technique which enables regular (RGB) pre-trained models to process
the augmented (5-channel) input in a stable manner during optimization. In
addition, we introduce an encoder module termed LDF to provide low-level dense
feature guidance. This assists segmentation, particularly for smaller parts.
OLAF enables significant mIoU gains of $\mathbf{3.3}$ (Pascal-Parts-58),
$\mathbf{3.5}$ (Pascal-Parts-108) over the SOTA model. On the most challenging
variant (Pascal-Parts-201), the gain is $\mathbf{4.0}$. Experimentally, we show
that OLAF's broad applicability enables gains across multiple architectures
(CNN, U-Net, Transformer) and datasets. The code is available at
olafseg.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in The European Conference on Computer Vision (ECCV) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analyzing Poverty through Intra-Annual Time-Series: A Wavelet Transform
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02855v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02855v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Kakooei, Klaudia Solska, Adel Daoud
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reducing global poverty is a key objective of the Sustainable Development
Goals (SDGs). Achieving this requires high-frequency, granular data to capture
neighborhood-level changes, particularly in data scarce regions such as low-
and middle-income countries. To fill in the data gaps, recent computer vision
methods combining machine learning (ML) with earth observation (EO) data to
improve poverty estimation. However, while much progress have been made, they
often omit intra-annual variations, which are crucial for estimating poverty in
agriculturally dependent countries. We explored the impact of integrating
intra-annual NDVI information with annual multi-spectral data on model
accuracy. To evaluate our method, we created a simulated dataset using Landsat
imagery and nighttime light data to evaluate EO-ML methods that use
intra-annual EO data. Additionally, we evaluated our method against the
Demographic and Health Survey (DHS) dataset across Africa. Our results indicate
that integrating specific NDVI-derived features with multi-spectral data
provides valuable insights for poverty analysis, emphasizing the importance of
retaining intra-annual information.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Correlation of Object Detection Performance with Visual Saliency and
  Depth Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02844v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02844v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthias Bartolo, Dylan Seychell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As object detection techniques continue to evolve, understanding their
relationships with complementary visual tasks becomes crucial for optimising
model architectures and computational resources. This paper investigates the
correlations between object detection accuracy and two fundamental visual
tasks: depth prediction and visual saliency prediction. Through comprehensive
experiments using state-of-the-art models (DeepGaze IIE, Depth Anything,
DPT-Large, and Itti's model) on COCO and Pascal VOC datasets, we find that
visual saliency shows consistently stronger correlations with object detection
accuracy (mA$\rho$ up to 0.459 on Pascal VOC) compared to depth prediction
(mA$\rho$ up to 0.283). Our analysis reveals significant variations in these
correlations across object categories, with larger objects showing correlation
values up to three times higher than smaller objects. These findings suggest
incorporating visual saliency features into object detection architectures
could be more beneficial than depth information, particularly for specific
object categories. The observed category-specific variations also provide
insights for targeted feature engineering and dataset design improvements,
potentially leading to more efficient and accurate object detection systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code Available at:
  https://github.com/mbar0075/Object-Detection-Correlation-Saliency-vs-Depth</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advances in Photoacoustic Imaging Reconstruction and Quantitative
  Analysis for Biomedical Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02843v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02843v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Wang, Weiming Zeng, Kai Long, Rongfeng Lan, Li Liu, Wai Ting Siok, Nizhuan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Photoacoustic imaging (PAI) represents an innovative biomedical imaging
modality that harnesses the advantages of optical resolution and acoustic
penetration depth while ensuring enhanced safety. Despite its promising
potential across a diverse array of preclinical and clinical applications, the
clinical implementation of PAI faces significant challenges, including the
trade-off between penetration depth and spatial resolution, as well as the
demand for faster imaging speeds. This paper explores the fundamental
principles underlying PAI, with a particular emphasis on three primary
implementations: photoacoustic computed tomography (PACT), photoacoustic
microscopy (PAM), and photoacoustic endoscopy (PAE). We undertake a critical
assessment of their respective strengths and practical limitations.
Furthermore, recent developments in utilizing conventional or deep learning
(DL) methodologies for image reconstruction and artefact mitigation across
PACT, PAM, and PAE are outlined, demonstrating considerable potential to
enhance image quality and accelerate imaging processes. Furthermore, this paper
examines the recent developments in quantitative analysis within PAI, including
the quantification of haemoglobin concentration, oxygen saturation, and other
physiological parameters within tissues. Finally, our discussion encompasses
current trends and future directions in PAI research while emphasizing the
transformative impact of deep learning on advancing PAI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Test-Time Dynamic Image Fusion <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02840v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02840v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bing Cao, Yinan Xia, Yi Ding, Changqing Zhang, Qinghua Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The inherent challenge of image fusion lies in capturing the correlation of
multi-source images and comprehensively integrating effective information from
different sources. Most existing techniques fail to perform dynamic image
fusion while notably lacking theoretical guarantees, leading to potential
deployment risks in this field. Is it possible to conduct dynamic image fusion
with a clear theoretical justification? In this paper, we give our solution
from a generalization perspective. We proceed to reveal the generalized form of
image fusion and derive a new test-time dynamic image fusion paradigm. It
provably reduces the upper bound of generalization error. Specifically, we
decompose the fused image into multiple components corresponding to its source
data. The decomposed components represent the effective information from the
source data, thus the gap between them reflects the Relative Dominability (RD)
of the uni-source data in constructing the fusion image. Theoretically, we
prove that the key to reducing generalization error hinges on the negative
correlation between the RD-based fusion weight and the uni-source
reconstruction loss. Intuitively, RD dynamically highlights the dominant
regions of each source and can be naturally converted to the corresponding
fusion weight, achieving robust results. Extensive experiments and discussions
with in-depth analysis on multiple benchmarks confirm our findings and
superiority. Our code is available at https://github.com/Yinan-Xia/TTD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lost in Context: The Influence of Context on Feature Attribution Methods
  for Object Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02833v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02833v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sayanta Adhikari, Rishav Kumar, Konda Reddy Mopuri, Rajalakshmi Pachamuthu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contextual information plays a critical role in object recognition models
within computer vision, where changes in context can significantly affect
accuracy, underscoring models' dependence on contextual cues. This study
investigates how context manipulation influences both model accuracy and
feature attribution, providing insights into the reliance of object recognition
models on contextual information as understood through the lens of feature
attribution methods.
  We employ a range of feature attribution techniques to decipher the reliance
of deep neural networks on context in object recognition tasks. Using the
ImageNet-9 and our curated ImageNet-CS datasets, we conduct experiments to
evaluate the impact of contextual variations, analyzed through feature
attribution methods. Our findings reveal several key insights: (a) Correctly
classified images predominantly emphasize object volume attribution over
context volume attribution. (b) The dependence on context remains relatively
stable across different context modifications, irrespective of classification
accuracy. (c) Context change exerts a more pronounced effect on model
performance than Context perturbations. (d) Surprisingly, context attribution
in `no-information' scenarios is non-trivial. Our research moves beyond
traditional methods by assessing the implications of broad-level modifications
on object recognition, either in the object or its context.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in ICVGIP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Estimating Ego-Body Pose from Doubly Sparse Egocentric Video Data <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03561v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03561v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seunggeun Chi, Pin-Hao Huang, Enna Sachdeva, Hengbo Ma, Karthik Ramani, Kwonjoon Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of estimating the body movements of a camera wearer from
egocentric videos. Current methods for ego-body pose estimation rely on
temporally dense sensor data, such as IMU measurements from spatially sparse
body parts like the head and hands. However, we propose that even temporally
sparse observations, such as hand poses captured intermittently from egocentric
videos during natural or periodic hand movements, can effectively constrain
overall body motion. Naively applying diffusion models to generate full-body
pose from head pose and sparse hand pose leads to suboptimal results. To
overcome this, we develop a two-stage approach that decomposes the problem into
temporal completion and spatial completion. First, our method employs masked
autoencoders to impute hand trajectories by leveraging the spatiotemporal
correlations between the head pose sequence and intermittent hand poses,
providing uncertainty estimates. Subsequently, we employ conditional diffusion
models to generate plausible full-body motions based on these temporally dense
trajectories of the head and hands, guided by the uncertainty estimates from
the imputation. The effectiveness of our method was rigorously tested and
validated through comprehensive experiments conducted on various HMD setup with
AMASS and Ego-Exo4D datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Object and Contact Point Tracking in Demonstrations Using 3D Gaussian
  Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03555v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03555v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Büttner, Jonathan Francis, Helge Rhodin, Andrew Melnik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a method to enhance Interactive Imitation Learning
(IIL) by extracting touch interaction points and tracking object movement from
video demonstrations. The approach extends current IIL systems by providing
robots with detailed knowledge of both where and how to interact with objects,
particularly complex articulated ones like doors and drawers. By leveraging
cutting-edge techniques such as 3D Gaussian Splatting and FoundationPose for
tracking, this method allows robots to better understand and manipulate objects
in dynamic environments. The research lays the foundation for more effective
task learning and execution in autonomous robotic systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CoRL 2024, Workshop on Lifelong Learning for Home Robots, Munich,
  Germany</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking <span class="highlight-title">Vision Language</span> Model Unlearning via Fictitious Facial
  Identity Dataset 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03554v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03554v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingzi Ma, Jiongxiao Wang, Fei Wang, Siyuan Ma, Jiazhao Li, Xiujun Li, Furong Huang, Lichao Sun, Bo Li, Yejin Choi, Muhao Chen, Chaowei Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine unlearning has emerged as an effective strategy for forgetting
specific information in the training data. However, with the increasing
integration of visual data, privacy concerns in Vision Language Models (VLMs)
remain underexplored. To address this, we introduce Facial Identity Unlearning
Benchmark (FIUBench), a novel VLM unlearning benchmark designed to robustly
evaluate the effectiveness of unlearning algorithms under the Right to be
Forgotten setting. Specifically, we formulate the VLM unlearning task via
constructing the Fictitious Facial Identity VQA dataset and apply a two-stage
evaluation pipeline that is designed to precisely control the sources of
information and their exposure levels. In terms of evaluation, since VLM
supports various forms of ways to ask questions with the same semantic meaning,
we also provide robust evaluation metrics including membership inference
attacks and carefully designed adversarial privacy attacks to evaluate the
performance of algorithms. Through the evaluation of four baseline VLM
unlearning algorithms within FIUBench, we find that all methods remain limited
in their unlearning performance, with significant trade-offs between model
utility and forget quality. Furthermore, our findings also highlight the
importance of privacy attacks for robust evaluations. We hope FIUBench will
drive progress in developing more effective VLM unlearning algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Weakly Supervised Semantic Segmentation for Fibrosis via
  Controllable Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03551v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03551v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiling Yue, Yingying Fang, Liutao Yang, Nikhil Baid, Simon Walsh, Guang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fibrotic Lung Disease (FLD) is a severe condition marked by lung stiffening
and scarring, leading to respiratory decline. High-resolution computed
tomography (HRCT) is critical for diagnosing and monitoring FLD; however,
fibrosis appears as irregular, diffuse patterns with unclear boundaries,
leading to high inter-observer variability and time-intensive manual
annotation. To tackle this challenge, we propose DiffSeg, a novel weakly
supervised semantic segmentation (WSSS) method that uses image-level
annotations to generate pixel-level fibrosis segmentation, reducing the need
for fine-grained manual labeling. Additionally, our DiffSeg incorporates a
diffusion-based generative model to synthesize HRCT images with different
levels of fibrosis from healthy slices, enabling the generation of the
fibrosis-injected slices and their paired fibrosis location. Experiments
indicate that our method significantly improves the accuracy of pseudo masks
generated by existing WSSS methods, greatly reducing the complexity of manual
labeling and enhancing the consistency of the generated masks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Personalized Video Summarization by <span class="highlight-title">Multimodal</span> Video Understanding <span class="chip">CIKM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03531v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03531v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brian Chen, Xiangyuan Zhao, Yingnan Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video summarization techniques have been proven to improve the overall user
experience when it comes to accessing and comprehending video content. If the
user's preference is known, video summarization can identify significant
information or relevant content from an input video, aiding them in obtaining
the necessary information or determining their interest in watching the
original video. Adapting video summarization to various types of video and user
preferences requires significant training data and expensive human labeling. To
facilitate such research, we proposed a new benchmark for video summarization
that captures various user preferences. Also, we present a pipeline called
Video Summarization with Language (VSL) for user-preferred video summarization
that is based on pre-trained visual language models (VLMs) to avoid the need to
train a video summarization system on a large training dataset. The pipeline
takes both video and closed captioning as input and performs semantic analysis
at the scene level by converting video frames into text. Subsequently, the
user's genre preference was used as the basis for selecting the pertinent
textual scenes. The experimental results demonstrate that our proposed pipeline
outperforms current state-of-the-art unsupervised video summarization models.
We show that our method is more adaptable across different datasets compared to
supervised query-based video summarization models. In the end, the runtime
analysis demonstrates that our pipeline is more suitable for practical use when
scaling up the number of user preferences and videos.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings of CIKM 2024 Applied Research Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Complete Shapes: A quantitative Evaluation of 3D Shape Matching
  Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03511v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03511v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Viktoria Ehm, Nafie El Amrani, Yizheng Xie, Lennart Bastian, Maolin Gao, Weikang Wang, Lu Sang, Dongliang Cao, Zorah Lähner, Daniel Cremers, Florian Bernard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Finding correspondences between 3D shapes is an important and long-standing
problem in computer vision, graphics and beyond. While approaches based on
machine learning dominate modern 3D shape matching, almost all existing
(learning-based) methods require that at least one of the involved shapes is
complete. In contrast, the most challenging and arguably most practically
relevant setting of matching partially observed shapes, is currently
underexplored. One important factor is that existing datasets contain only a
small number of shapes (typically below 100), which are unable to serve
data-hungry machine learning approaches, particularly in the unsupervised
regime. In addition, the type of partiality present in existing datasets is
often artificial and far from realistic. To address these limitations and to
encourage research on these relevant settings, we provide a generic and
flexible framework for the procedural generation of challenging partial shape
matching scenarios. Our framework allows for a virtually infinite generation of
partial shape matching instances from a finite set of shapes with complete
geometry. Further, we manually create cross-dataset correspondences between
seven existing (complete geometry) shape matching datasets, leading to a total
of 2543 shapes. Based on this, we propose several challenging partial benchmark
settings, for which we evaluate respective state-of-the-art methods as
baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SynthSet: Generative Diffusion Model for Semantic Segmentation in
  Precision Agriculture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03505v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03505v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Heschl, Mauricio Murillo, Keyhan Najafian, Farhad Maleki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a methodology for generating synthetic annotated data
to address data scarcity in semantic segmentation tasks within the precision
agriculture domain. Utilizing Denoising Diffusion Probabilistic Models (DDPMs)
and Generative Adversarial Networks (GANs), we propose a dual diffusion model
architecture for synthesizing realistic annotated agricultural data, without
any human intervention. We employ super-resolution to enhance the phenotypic
characteristics of the synthesized images and their coherence with the
corresponding generated masks. We showcase the utility of the proposed method
for wheat head segmentation. The high quality of synthesized data underscores
the effectiveness of the proposed methodology in generating image-mask pairs.
Furthermore, models trained on our generated data exhibit promising performance
when tested on an external, diverse dataset of real wheat fields. The results
show the efficacy of the proposed methodology for addressing data scarcity for
semantic segmentation tasks. Moreover, the proposed approach can be readily
adapted for various segmentation tasks in precision agriculture and beyond.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Application-Agnostic Automatic Target Recognition System Using Vision
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03491v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03491v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anthony Palladino, Dana Gajewski, Abigail Aronica, Patryk Deptula, Alexander Hamme, Seiyoung C. Lee, Jeff Muri, Todd Nelling, Michael A. Riley, Brian Wong, Margaret Duff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel Automatic Target Recognition (ATR) system using
open-vocabulary object detection and classification models. A primary advantage
of this approach is that target classes can be defined just before runtime by a
non-technical end user, using either a few natural language text descriptions
of the target, or a few image exemplars, or both. Nuances in the desired
targets can be expressed in natural language, which is useful for unique
targets with little or no training data. We also implemented a novel
combination of several techniques to improve performance, such as leveraging
the additional information in the sequence of overlapping frames to perform
tubelet identification (i.e., sequential bounding box matching), bounding box
re-scoring, and tubelet linking. Additionally, we developed a technique to
visualize the aggregate output of many overlapping frames as a mosaic of the
area scanned during the aerial surveillance or reconnaissance, and a kernel
density estimate (or heatmap) of the detected targets. We initially applied
this ATR system to the use case of detecting and clearing unexploded ordinance
on airfield runways and we are currently extending our research to other
real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the Thirty-Seventh Annual Conference on Innovative
  Applications of Artificial Intelligence (IAAI-25)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rainfall regression from C-band Synthetic Aperture Radar using
  Multi-Task Generative Adversarial Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03480v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03480v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aurélien Colin, Romain Husson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a data-driven approach to estimate precipitation rates
from Synthetic Aperture Radar (SAR) at a spatial resolution of 200 meters per
pixel. It addresses previous challenges related to the collocation of SAR and
weather radar data, specifically the misalignment in collocations and the
scarcity of rainfall examples under strong wind. To tackle these challenges,
the paper proposes a multi-objective formulation, introducing patch-level
components and an adversarial component. It exploits the full NEXRAD archive to
look for potential co-locations with Sentinel-1 data. With additional
enhancements to the training procedure and the incorporation of additional
inputs, the resulting model demonstrates improved accuracy in rainfall
estimates and the ability to extend its performance to scenarios up to 15 m/s.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self Supervised Networks for Learning Latent Space Representations of
  Human Body Scans and Motions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03475v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03475v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emmanuel Hartman, Nicolas Charon, Martin Bauer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces self-supervised neural network models to tackle several
fundamental problems in the field of 3D human body analysis and processing.
First, we propose VariShaPE (Varifold Shape Parameter Estimator), a novel
architecture for the retrieval of latent space representations of body shapes
and poses. This network offers a fast and robust method to estimate the
embedding of arbitrary unregistered meshes into the latent space. Second, we
complement the estimation of latent codes with MoGeN (Motion Geometry Network)
a framework that learns the geometry on the latent space itself. This is
achieved by lifting the body pose parameter space into a higher dimensional
Euclidean space in which body motion mini-sequences from a training set of 4D
data can be approximated by simple linear interpolation. Using the SMPL latent
space representation we illustrate how the combination of these network models,
once trained, can be used to perform a variety of tasks with very limited
computational cost. This includes operations such as motion interpolation,
extrapolation and transfer as well as random shape and pose generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 11 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TopoTxR: A topology-guided deep convolutional network for breast
  parenchyma learning on DCE-MRIs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03464v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03464v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Wang, Zhilin Zou, Nicole Sakla, Luke Partyka, Nil Rawal, Gagandeep Singh, Wei Zhao, Haibin Ling, Chuan Huang, Prateek Prasanna, Chao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Characterization of breast parenchyma in dynamic contrast-enhanced magnetic
resonance imaging (DCE-MRI) is a challenging task owing to the complexity of
underlying tissue structures. Existing quantitative approaches, like radiomics
and deep learning models, lack explicit quantification of intricate and subtle
parenchymal structures, including fibroglandular tissue. To address this, we
propose a novel topological approach that explicitly extracts multi-scale
topological structures to better approximate breast parenchymal structures, and
then incorporates these structures into a deep-learning-based prediction model
via an attention mechanism. Our topology-informed deep learning model,
\emph{TopoTxR}, leverages topology to provide enhanced insights into tissues
critical for disease pathophysiology and treatment response. We empirically
validate \emph{TopoTxR} using the VICTRE phantom breast dataset, showing that
the topological structures extracted by our model effectively approximate the
breast parenchymal structures. We further demonstrate \emph{TopoTxR}'s efficacy
in predicting response to neoadjuvant chemotherapy. Our qualitative and
quantitative analyses suggest differential topological behavior of breast
tissue in treatment-na\"ive imaging, in patients who respond favorably to
therapy as achieving pathological complete response (pCR) versus those who do
not. In a comparative analysis with several baselines on the publicly available
I-SPY 1 dataset (N=161, including 47 patients with pCR and 114 without) and the
Rutgers proprietary dataset (N=120, with 69 patients achieving pCR and 51 not),
\emph{TopoTxR} demonstrates a notable improvement, achieving a 2.6\% increase
in accuracy and a 4.6\% enhancement in AUC compared to the state-of-the-art
method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 8 figures, 8 tables, accepted by Medical Image Analysis (
  https://www.sciencedirect.com/science/article/abs/pii/S1361841524002986 )</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BOston Neonatal Brain Injury Data for Hypoxic Ischemic Encephalopathy
  (BONBID-HIE): II. 2-year Neurocognitive Outcome and NICU Outcome <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03456v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03456v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rina Bao, Yangming Ou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hypoxic Ischemic Encephalopathy (HIE) affects approximately 1-5/1000 newborns
globally and leads to adverse neurocognitive outcomes in 30% to 50% of cases by
two years of age. Despite therapeutic advances with Therapeutic Hypothermia
(TH), prognosis remains challenging, highlighting the need for improved
biomarkers. This paper introduces the second release of the Boston Neonatal
Brain Injury Dataset for Hypoxic-Ischemic Encephalopathy (BONBID-HIE), an
open-source, comprehensive MRI and clinical dataset featuring 237 patients,
including NICU outcomes and 2-year neurocognitive outcomes from Massachusetts
General Hospital and Boston Children's Hospital.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Data description for BONBID-HIE 2024 Challenge on MICCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Solving Trojan Detection Competitions with Linear Weight Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03445v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03445v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Todd Huster, Peter Lin, Razvan Stefanescu, Emmanuel Ekwedike, Ritu Chadha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks can conceal malicious Trojan backdoors that allow a trigger
to covertly change the model behavior. Detecting signs of these backdoors,
particularly without access to any triggered data, is the subject of ongoing
research and open challenges. In one common formulation of the problem, we are
given a set of clean and poisoned models and need to predict whether a given
test model is clean or poisoned. In this paper, we introduce a detector that
works remarkably well across many of the existing datasets and domains. It is
obtained by training a binary classifier on a large number of models' weights
after performing a few different pre-processing steps including feature
selection and standardization, reference model weights subtraction, and model
alignment prior to detection. We evaluate this algorithm on a diverse set of
Trojan detection benchmarks and domains and examine the cases where the
approach is most and least effective.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fine-Grained Spatial and Verbal Losses for 3D Visual Grounding <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03405v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03405v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sombit Dey, Ozan Unal, Christos Sakaridis, Luc Van Gool
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D visual grounding consists of identifying the instance in a 3D scene which
is referred by an accompanying language description. While several
architectures have been proposed within the commonly employed
grounding-by-selection framework, the utilized losses are comparatively
under-explored. In particular, most methods rely on a basic supervised
cross-entropy loss on the predicted distribution over candidate instances,
which fails to model both spatial relations between instances and the internal
fine-grained word-level structure of the verbal referral. Sparse attempts to
additionally supervise verbal embeddings globally by learning the class of the
referred instance from the description or employing verbo-visual contrast to
better separate instance embeddings do not fundamentally lift the
aforementioned limitations. Responding to these shortcomings, we introduce two
novel losses for 3D visual grounding: a visual-level offset loss on regressed
vector offsets from each instance to the ground-truth referred instance and a
language-related span loss on predictions for the word-level span of the
referred instance in the description. In addition, we equip the verbo-visual
fusion module of our new 3D visual grounding architecture AsphaltNet with a
top-down bidirectional attentive fusion block, which enables the supervisory
signals from our two losses to propagate to the respective converse branches of
the network and thus aid the latter to learn context-aware instance embeddings
and grounding-aware verbal embeddings. AsphaltNet proposes novel auxiliary
losses to aid 3D visual grounding with competitive results compared to the
state-of-the-art on the ReferIt3D benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at WACV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Maritime Situational Awareness through End-to-End Onboard Raw
  Data Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03403v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03403v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roberto Del Prete, Manuel Salvoldi, Domenico Barretta, Nicolas Longépé, Gabriele Meoni, Arnon Karnieli, Maria Daniela Graziano, Alfredo Renga
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Satellite-based onboard data processing is crucial for time-sensitive
applications requiring timely and efficient rapid response. Advances in edge
artificial intelligence are shifting computational power from ground-based
centers to on-orbit platforms, transforming the
"sensing-communication-decision-feedback" cycle and reducing latency from
acquisition to delivery. The current research presents a framework addressing
the strict bandwidth, energy, and latency constraints of small satellites,
focusing on maritime monitoring. The study contributes three main innovations.
Firstly, it investigates the application of deep learning techniques for direct
ship detection and classification from raw satellite imagery. By simplifying
the onboard processing chain, our approach facilitates direct analyses without
requiring computationally intensive steps such as calibration and
ortho-rectification. Secondly, to address the scarcity of raw satellite data,
we introduce two novel datasets, VDS2Raw and VDV2Raw, which are derived from
raw data from Sentinel-2 and Vegetation and Environment Monitoring New Micro
Satellite (VENuS) missions, respectively, and enriched with Automatic
Identification System (AIS) records. Thirdly, we characterize the tasks'
optimal single and multiple spectral band combinations through statistical and
feature-based analyses validated on both datasets. In sum, we demonstrate the
feasibility of the proposed method through a proof-of-concept on CubeSat-like
hardware, confirming the models' potential for operational satellite-based
maritime monitoring.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DAAL: Density-Aware Adaptive Line Margin Loss for <span class="highlight-title">Multi-Modal</span> Deep
  Metric Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05438v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05438v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hadush Hailu Gebrerufael, Anil Kumar Tiwari, Gaurav Neupane, Goitom Ybrah Hailu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal deep metric learning is crucial for effectively capturing diverse
representations in tasks such as face verification, fine-grained object
recognition, and product search. Traditional approaches to metric learning,
whether based on distance or margin metrics, primarily emphasize class
separation, often overlooking the intra-class distribution essential for
multi-modal feature learning. In this context, we propose a novel loss function
called Density-Aware Adaptive Margin Loss(DAAL), which preserves the density
distribution of embeddings while encouraging the formation of adaptive
sub-clusters within each class. By employing an adaptive line strategy, DAAL
not only enhances intra-class variance but also ensures robust inter-class
separation, facilitating effective multi-modal representation. Comprehensive
experiments on benchmark fine-grained datasets demonstrate the superior
performance of DAAL, underscoring its potential in advancing retrieval
applications and multi-modal deep metric learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 4 fugues, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cognitive Planning for Object Goal Navigation using Generative AI Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.00318v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.00318v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arjun P S, Andrew Melnik, Gora Chand Nandi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Generative AI, particularly in Large Language Models
(LLMs) and Large Vision-Language Models (LVLMs), offer new possibilities for
integrating cognitive planning into robotic systems. In this work, we present a
novel framework for solving the object goal navigation problem that generates
efficient exploration strategies. Our approach enables a robot to navigate
unfamiliar environments by leveraging LLMs and LVLMs to understand the semantic
structure of the scene. To address the challenge of representing complex
environments without overwhelming the system, we propose a 3D modular scene
representation, enriched with semantic descriptions. This representation is
dynamically pruned using an LLM-based mechanism, which filters irrelevant
information and focuses on task-specific data. By combining these elements, our
system generates high-level sub-goals that guide the exploration of the robot
toward the target object. We validate our approach in simulated environments,
demonstrating its ability to enhance object search efficiency while maintaining
scalability in complex settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Digi2Real: Bridging the Realism Gap in Synthetic Data Face Recognition
  via Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02188v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02188v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anjith George, Sebastien Marcel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The accuracy of face recognition systems has improved significantly in the
past few years, thanks to the large amount of data collected and the
advancement in neural network architectures. However, these large-scale
datasets are often collected without explicit consent, raising ethical and
privacy concerns. To address this, there have been proposals to use synthetic
datasets for training face recognition models. Yet, such models still rely on
real data to train the generative models and generally exhibit inferior
performance compared to those trained on real datasets. One of these datasets,
DigiFace, uses a graphics pipeline to generate different identities and
different intra-class variations without using real data in training the
models. However, the performance of this approach is poor on face recognition
benchmarks, possibly due to the lack of realism in the images generated from
the graphics pipeline. In this work, we introduce a novel framework for realism
transfer aimed at enhancing the realism of synthetically generated face images.
Our method leverages the large-scale face foundation model, and we adapt the
pipeline for realism enhancement. By integrating the controllable aspects of
the graphics pipeline with our realism enhancement technique, we generate a
large amount of realistic variations-combining the advantages of both
approaches. Our empirical evaluations demonstrate that models trained using our
enhanced dataset significantly improve the performance of face recognition
systems over the baseline. The source code and datasets will be made available
publicly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decoupled Pseudo-labeling for Semi-Supervised Monocular 3D Object
  Detection <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17387v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17387v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiacheng Zhang, Jiaming Li, Xiangru Lin, Wei Zhang, Xiao Tan, Junyu Han, Errui Ding, Jingdong Wang, Guanbin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We delve into pseudo-labeling for semi-supervised monocular 3D object
detection (SSM3OD) and discover two primary issues: a misalignment between the
prediction quality of 3D and 2D attributes and the tendency of depth
supervision derived from pseudo-labels to be noisy, leading to significant
optimization conflicts with other reliable forms of supervision. We introduce a
novel decoupled pseudo-labeling (DPL) approach for SSM3OD. Our approach
features a Decoupled Pseudo-label Generation (DPG) module, designed to
efficiently generate pseudo-labels by separately processing 2D and 3D
attributes. This module incorporates a unique homography-based method for
identifying dependable pseudo-labels in BEV space, specifically for 3D
attributes. Additionally, we present a DepthGradient Projection (DGP) module to
mitigate optimization conflicts caused by noisy depth supervision of
pseudo-labels, effectively decoupling the depth gradient and removing
conflicting gradients. This dual decoupling strategy-at both the pseudo-label
generation and gradient levels-significantly improves the utilization of
pseudo-labels in SSM3OD. Our comprehensive experiments on the KITTI benchmark
demonstrate the superiority of our method over existing approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted to CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contextual Knowledge Pursuit for Faithful Visual Synthesis <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.17898v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.17898v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinqi Luo, Kwan Ho Ryan Chan, Dimitris Dimos, René Vidal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern text-to-vision generative models often hallucinate when the prompt
describing the scene to be generated is underspecified. In large language
models (LLMs), a prevalent strategy to reduce hallucinations is to retrieve
factual knowledge from an external database. While such retrieval augmentation
strategies have great potential to enhance text-to-vision generators, existing
static top-K retrieval methods explore the knowledge pool once, missing the
broader context necessary for high-quality generation. Furthermore, LLMs
internally possess rich world knowledge learned during large-scale training
(parametric knowledge) that could mitigate the need for external data
retrieval. This paper proposes Contextual Knowledge Pursuit (CKPT), a framework
that leverages the complementary strengths of external and parametric knowledge
to help generators produce reliable visual content. Instead of the one-time
retrieval of facts from an external database to improve a given prompt, CKPT
uses (1) an LLM to decide whether to seek external knowledge or to self-elicit
descriptions from LLM parametric knowledge, (2) a knowledge pursuit process to
contextually seek and sequentially gather most relevant facts, (3) a knowledge
aggregator for prompt enhancement with the gathered fact context, and (4) a
filtered fine-tuning objective to improve visual synthesis with richer prompts.
We evaluate CKPT across multiple text-driven generative tasks (image, 3D
rendering, and video) on datasets of rare objects and daily scenarios. Our
results show that CKPT is capable of generating faithful and semantically rich
content across diverse visual domains, offering a promising data source for
zero-shot synthesis and filtered fine-tuning of text-to-vision generative
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ECCV 2024 SDCV Workshop. GitHub repository at
  https://github.com/peterljq/Contextual-Knowledge-Pursuit</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeBaRA: Denoising-Based 3D Room Arrangement Generation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18336v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18336v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Léopold Maillard, Nicolas Sereyjol-Garros, Tom Durand, Maks Ovsjanikov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating realistic and diverse layouts of furnished indoor 3D scenes
unlocks multiple interactive applications impacting a wide range of industries.
The inherent complexity of object interactions, the limited amount of available
data and the requirement to fulfill spatial constraints all make generative
modeling for 3D scene synthesis and arrangement challenging. Current methods
address these challenges autoregressively or by using off-the-shelf diffusion
objectives by simultaneously predicting all attributes without 3D reasoning
considerations. In this paper, we introduce DeBaRA, a score-based model
specifically tailored for precise, controllable and flexible arrangement
generation in a bounded environment. We argue that the most critical component
of a scene synthesis system is to accurately establish the size and position of
various objects within a restricted area. Based on this insight, we propose a
lightweight conditional score-based model designed with 3D spatial awareness at
its core. We demonstrate that by focusing on spatial attributes of objects, a
single trained DeBaRA model can be leveraged at test time to perform several
downstream applications such as scene synthesis, completion and re-arrangement.
Further, we introduce a novel Self Score Evaluation procedure so it can be
optimally employed alongside external LLM models. We evaluate our approach
through extensive experiments and demonstrate significant improvement upon
state-of-the-art approaches in a range of scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Seeing Eye to AI: Comparing Human Gaze and Model Attention in Video
  Memorability <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.16484v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.16484v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prajneya Kumar, Eshika Khandelwal, Makarand Tapaswi, Vishnu Sreekumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding what makes a video memorable has important applications in
advertising or education technology. Towards this goal, we investigate
spatio-temporal attention mechanisms underlying video memorability. Different
from previous works that fuse multiple features, we adopt a simple
CNN+Transformer architecture that enables analysis of spatio-temporal attention
while matching state-of-the-art (SoTA) performance on video memorability
prediction. We compare model attention against human gaze fixations collected
through a small-scale eye-tracking study where humans perform the video memory
task. We uncover the following insights: (i) Quantitative saliency metrics show
that our model, trained only to predict a memorability score, exhibits similar
spatial attention patterns to human gaze, especially for more memorable videos.
(ii) The model assigns greater importance to initial frames in a video,
mimicking human attention patterns. (iii) Panoptic segmentation reveals that
both (model and humans) assign a greater share of attention to things and less
attention to stuff as compared to their occurrence probability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to WACV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Re-assembling the past: The RePAIR dataset and benchmark for real world
  2D and 3D puzzle solving <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24010v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24010v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Theodore Tsesmelis, Luca Palmieri, Marina Khoroshiltseva, Adeela Islam, Gur Elkin, Ofir Itzhak Shahar, Gianluca Scarpellini, Stefano Fiorini, Yaniv Ohayon, Nadav Alali, Sinem Aslan, Pietro Morerio, Sebastiano Vascon, Elena Gravina, Maria Cristina Napolitano, Giuseppe Scarpati, Gabriel Zuchtriegel, Alexandra Spühler, Michel E. Fuchs, Stuart James, Ohad Ben-Shahar, Marcello Pelillo, Alessio Del Bue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes the RePAIR dataset that represents a challenging
benchmark to test modern computational and data driven methods for
puzzle-solving and reassembly tasks. Our dataset has unique properties that are
uncommon to current benchmarks for 2D and 3D puzzle solving. The fragments and
fractures are realistic, caused by a collapse of a fresco during a World War II
bombing at the Pompeii archaeological park. The fragments are also eroded and
have missing pieces with irregular shapes and different dimensions, challenging
further the reassembly algorithms. The dataset is multi-modal providing high
resolution images with characteristic pictorial elements, detailed 3D scans of
the fragments and meta-data annotated by the archaeologists. Ground truth has
been generated through several years of unceasing fieldwork, including the
excavation and cleaning of each fragment, followed by manual puzzle solving by
archaeologists of a subset of approx. 1000 pieces among the 16000 available.
After digitizing all the fragments in 3D, a benchmark was prepared to challenge
current reassembly and puzzle-solving methods that often solve more simplistic
synthetic scenarios. The tested baselines show that there clearly exists a gap
to fill in solving this computationally complex problem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024, Track Datasets and Benchmarks, 10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attention-based Class-Conditioned Alignment for Multi-Source Domain
  Adaptation of Object Detectors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09918v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09918v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atif Belal, Akhil Meethal, Francisco Perdigon Romero, Marco Pedersoli, Eric Granger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain adaptation methods for object detection (OD) strive to mitigate the
impact of distribution shifts by promoting feature alignment across source and
target domains. Multi-source domain adaptation (MSDA) allows leveraging
multiple annotated source datasets and unlabeled target data to improve the
accuracy and robustness of the detection model. Most state-of-the-art MSDA
methods for OD perform feature alignment in a class-agnostic manner. This is
challenging since the objects have unique modal information due to variations
in object appearance across domains. A recent prototype-based approach proposed
a class-wise alignment, yet it suffers from error accumulation due to noisy
pseudo-labels that can negatively affect adaptation with imbalanced data. To
overcome these limitations, we propose an attention-based class-conditioned
alignment method for MSDA that aligns instances of each object category across
domains. In particular, an attention module coupled with an adversarial domain
classifier allows learning domain-invariant and class-specific instance
representations. Experimental results on multiple benchmarking MSDA datasets
indicate that our method outperforms the state-of-the-art methods and is robust
to class imbalance using a conceptually simple class-conditioning method. Our
code is available at https://github.com/imatif17/ACIA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2309.14950</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GlobalDoc: A Cross-Modal <span class="highlight-title">Vision-Language</span> Framework for Real-World
  Document Image Retrieval and Classification <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.05756v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.05756v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Souhail Bakkali, Sanket Biswas, Zuheng Ming, Mickaël Coustaty, Marçal Rusiñol, Oriol Ramos Terrades, Josep Lladós
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual document understanding (VDU) has rapidly advanced with the development
of powerful multi-modal language models. However, these models typically
require extensive document pre-training data to learn intermediate
representations and often suffer a significant performance drop in real-world
online industrial settings. A primary issue is their heavy reliance on OCR
engines to extract local positional information within document pages, which
limits the models' ability to capture global information and hinders their
generalizability, flexibility, and robustness. In this paper, we introduce
GlobalDoc, a cross-modal transformer-based architecture pre-trained in a
self-supervised manner using three novel pretext objective tasks. GlobalDoc
improves the learning of richer semantic concepts by unifying language and
visual representations, resulting in more transferable models. For proper
evaluation, we also propose two novel document-level downstream VDU tasks,
Few-Shot Document Image Classification (DIC) and Content-based Document Image
Retrieval (DIR), designed to simulate industrial scenarios more closely.
Extensive experimentation has been conducted to demonstrate GlobalDoc's
effectiveness in practical settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at WACV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SynCo: Synthetic Hard Negatives in Contrastive Learning for Better
  Unsupervised Visual Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02401v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02401v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolaos Giakoumoglou, Tania Stathaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive learning has become a dominant approach in self-supervised visual
representation learning. Hard negatives - samples closely resembling the anchor
- are key to enhancing learned representations' discriminative power. However,
efficiently leveraging hard negatives remains challenging. We introduce SynCo
(Synthetic Negatives in Contrastive learning), a novel approach that improves
model performance by generating synthetic hard negatives on the representation
space. Building on the MoCo framework, SynCo introduces six strategies for
creating diverse synthetic hard negatives on-the-fly with minimal computational
overhead. SynCo achieves faster training and better representation learning,
reaching 67.9% top-1 accuracy on ImageNet ILSVRC-2012 linear evaluation after
200 pretraining epochs, surpassing MoCo's 67.5% using the same ResNet-50
encoder. It also transfers more effectively to detection tasks: on PASCAL VOC,
it outperforms both the supervised baseline and MoCo with 82.5% AP; on COCO, it
sets new benchmarks with 40.9% AP for bounding box detection and 35.5% AP for
instance segmentation. Our synthetic hard negative generation approach
significantly enhances visual representations learned through self-supervised
contrastive learning. Code is available at
https://github.com/giakoumoglou/synco.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tencent Hunyuan3D-1.0: A Unified Framework for Text-to-3D and
  Image-to-3D Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02293v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02293v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianghui Yang, Huiwen Shi, Bowen Zhang, Fan Yang, Jiacheng Wang, Hongxu Zhao, Xinhai Liu, Xinzhou Wang, Qingxiang Lin, Jiaao Yu, Lifu Wang, Zhuo Chen, Sicong Liu, Yuhong Liu, Yong Yang, Di Wang, Jie Jiang, Chunchao Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While 3D generative models have greatly improved artists' workflows, the
existing diffusion models for 3D generation suffer from slow generation and
poor generalization. To address this issue, we propose a two-stage approach
named Hunyuan3D-1.0 including a lite version and a standard version, that both
support text- and image-conditioned generation. In the first stage, we employ a
multi-view diffusion model that efficiently generates multi-view RGB in
approximately 4 seconds. These multi-view images capture rich details of the 3D
asset from different viewpoints, relaxing the tasks from single-view to
multi-view reconstruction. In the second stage, we introduce a feed-forward
reconstruction model that rapidly and faithfully reconstructs the 3D asset
given the generated multi-view images in approximately 7 seconds. The
reconstruction network learns to handle noises and in-consistency introduced by
the multi-view diffusion and leverages the available information from the
condition image to efficiently recover the 3D structure. Our framework involves
the text-to-image model, i.e., Hunyuan-DiT, making it a unified framework to
support both text- and image-conditioned 3D generation. Our standard version
has 3x more parameters than our lite and other existing model. Our
Hunyuan3D-1.0 achieves an impressive balance between speed and quality,
significantly reducing generation time while maintaining the quality and
diversity of the produced assets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report; 3D Generation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Framework for Real-Time Volcano-Seismic Event Recognition Based on
  Multi-Station Seismograms and Semantic Segmentation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20595v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20595v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Camilo Espinosa-Curilem, Millaray Curilem, Daniel Basualto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In volcano monitoring, effective recognition of seismic events is essential
for understanding volcanic activity and raising timely warning alerts.
Traditional methods rely on manual analysis, which can be subjective and
labor-intensive. Furthermore, current automatic approaches often tackle
detection and classification separately, mostly rely on single station
information and generally require tailored preprocessing and representations to
perform predictions. These limitations often hinder their application to
real-time monitoring and utilization across different volcano conditions. This
study introduces a novel approach that utilizes Semantic Segmentation models to
automate seismic event recognition by applying a straight forward
transformation of multi-channel 1D signals into 2D representations, enabling
their use as images. Our framework employs a data-driven, end-to-end design
that integrates multi-station seismic data with minimal preprocessing,
performing both detection and classification simultaneously for five seismic
event classes. We evaluated four state-of-the-art segmentation models (UNet,
UNet++, DeepLabV3+ and SwinUNet) on approximately 25.000 seismic events
recorded at four different Chilean volcanoes: Nevados del Chill\'an Volcanic
Complex, Laguna del Maule, Villarrica and Puyehue-Cord\'on Caulle. Among these
models, the UNet architecture was identified as the most effective model,
achieving mean F1 and Intersection over Union (IoU) scores of up to 0.91 and
0.88, respectively, and demonstrating superior noise robustness and model
flexibility to unseen volcano datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 9 figures. This is a pre-print, it is currently under
  review for publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Better, Not Just More: Data-Centric Machine Learning for Earth
  Observation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.05327v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.05327v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ribana Roscher, Marc Rußwurm, Caroline Gevaert, Michael Kampffmeyer, Jefersson A. dos Santos, Maria Vakalopoulou, Ronny Hänsch, Stine Hansen, Keiller Nogueira, Jonathan Prexl, Devis Tuia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent developments and research in modern machine learning have led to
substantial improvements in the geospatial field. Although numerous deep
learning architectures and models have been proposed, the majority of them have
been solely developed on benchmark datasets that lack strong real-world
relevance. Furthermore, the performance of many methods has already saturated
on these datasets. We argue that a shift from a model-centric view to a
complementary data-centric perspective is necessary for further improvements in
accuracy, generalization ability, and real impact on end-user applications.
Furthermore, considering the entire machine learning cycle-from problem
definition to model deployment with feedback-is crucial for enhancing machine
learning models that can be reliable in unforeseen situations. This work
presents a definition as well as a precise categorization and overview of
automated data-centric learning approaches for geospatial data. It highlights
the complementary role of data-centric learning with respect to model-centric
in the larger machine learning deployment cycle. We review papers across the
entire geospatial field and categorize them into different groups. A set of
representative experiments shows concrete implementation examples. These
examples provide concrete steps to act on geospatial data with data-centric
machine learning approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Geoscience and Remote Sensing Magazine</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robustly overfitting latents for flexible neural image compression <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.17789v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.17789v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yura Perugachi-Diaz, Arwin Gansekoele, Sandjai Bhulai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural image compression has made a great deal of progress. State-of-the-art
models are based on variational autoencoders and are outperforming classical
models. Neural compression models learn to encode an image into a quantized
latent representation that can be efficiently sent to the decoder, which
decodes the quantized latent into a reconstructed image. While these models
have proven successful in practice, they lead to sub-optimal results due to
imperfect optimization and limitations in the encoder and decoder capacity.
Recent work shows how to use stochastic Gumbel annealing (SGA) to refine the
latents of pre-trained neural image compression models. We extend this idea by
introducing SGA+, which contains three different methods that build upon SGA.
We show how our method improves the overall compression performance in terms of
the R-D trade-off, compared to its predecessors. Additionally, we show how
refinement of the latents with our best-performing method improves the
compression performance on both the Tecnick and CLIC dataset. Our method is
deployed for a pre-trained hyperprior and for a more flexible model. Further,
we give a detailed analysis of our proposed methods and show that they are less
sensitive to hyperparameter choices. Finally, we show how each method can be
extended to three- instead of two-class rounding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Neural Information Processing Systems (NeurIPS) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Priors for Video Quality Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22566v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22566v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddharath Narayan Shakya, Parimala Kancharla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we designed a completely blind video quality assessment
algorithm using the deep video prior. This work mainly explores the utility of
deep video prior in estimating the visual quality of the video. In our work, we
have used a single distorted video and a reference video pair to learn the deep
video prior. At inference time, the learned deep prior is used to restore the
original videos from the distorted videos. The ability of learned deep video
prior to restore the original video from the distorted video is measured to
quantify distortion in the video. Our hypothesis is that the learned deep video
prior fails in restoring the highly distorted videos. The restoring ability of
deep video prior is proportional to the distortion present in the video.
Therefore, we propose to use the distance between the distorted video and the
restored video as the perceptual quality of the video. Our algorithm is trained
using a single video pair and it does not need any labelled data. We show that
our proposed algorithm outperforms the existing unsupervised video quality
assessment algorithms in terms of LCC and SROCC on a synthetically distorted
video quality assessment dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Indian Conference on Computer Vision, Graphics and Image Processing
  (ICVGIP) 2024 conference tinny paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Asynchronous Perception Machine For Efficient Test-Time-Training <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20535v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20535v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rajat Modi, Yogesh Singh Rawat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we propose Asynchronous Perception Machine (APM), a
computationally-efficient architecture for test-time-training (TTT). APM can
process patches of an image one at a time in any order asymmetrically and still
encode semantic-awareness in the net. We demonstrate APM's ability to recognize
out-of-distribution images without dataset-specific pre-training, augmentation
or any-pretext task. APM offers competitive performance over existing TTT
approaches. To perform TTT, APM just distills test sample's representation
once. APM possesses a unique property: it can learn using just this single
representation and starts predicting semantically-aware features.
  APM demostrates potential applications beyond test-time-training: APM can
scale up to a dataset of 2D images and yield semantic-clusterings in a single
forward pass. APM also provides first empirical evidence towards validating
GLOM's insight, i.e. input percept is a field. Therefore, APM helps us converge
towards an implementation which can do both interpolation and perception on a
shared-connectionist hardware. Our code is publicly available at this link:
https://rajatmodi62.github.io/apm_project_page/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024 Main Track. APM is a step to getting
  Geoffrey Hinton's GLOM working</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic Typography: Bringing Text to Life via Video Diffusion Prior 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.11614v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.11614v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zichen Liu, Yihao Meng, Hao Ouyang, Yue Yu, Bolin Zhao, Daniel Cohen-Or, Huamin Qu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text animation serves as an expressive medium, transforming static
communication into dynamic experiences by infusing words with motion to evoke
emotions, emphasize meanings, and construct compelling narratives. Crafting
animations that are semantically aware poses significant challenges, demanding
expertise in graphic design and animation. We present an automated text
animation scheme, termed "Dynamic Typography", which combines two challenging
tasks. It deforms letters to convey semantic meaning and infuses them with
vibrant movements based on user prompts. Our technique harnesses vector
graphics representations and an end-to-end optimization-based framework. This
framework employs neural displacement fields to convert letters into base
shapes and applies per-frame motion, encouraging coherence with the intended
textual concept. Shape preservation techniques and perceptual loss
regularization are employed to maintain legibility and structural integrity
throughout the animation process. We demonstrate the generalizability of our
approach across various text-to-video models and highlight the superiority of
our end-to-end methodology over baseline methods, which might comprise separate
tasks. Through quantitative and qualitative evaluations, we demonstrate the
effectiveness of our framework in generating coherent text animations that
faithfully interpret user prompts while maintaining readability. Our code is
available at: https://animate-your-word.github.io/demo/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our demo and code is available at:
  https://animate-your-word.github.io/demo/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FakeShield: Explainable Image Forgery Detection and Localization via
  <span class="highlight-title">Multi-modal</span> Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02761v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02761v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhipei Xu, Xuanyu Zhang, Runyi Li, Zecheng Tang, Qing Huang, Jian Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of generative AI is a double-edged sword, which not
only facilitates content creation but also makes image manipulation easier and
more difficult to detect. Although current image forgery detection and
localization (IFDL) methods are generally effective, they tend to face two
challenges: \textbf{1)} black-box nature with unknown detection principle,
\textbf{2)} limited generalization across diverse tampering methods (e.g.,
Photoshop, DeepFake, AIGC-Editing). To address these issues, we propose the
explainable IFDL task and design FakeShield, a multi-modal framework capable of
evaluating image authenticity, generating tampered region masks, and providing
a judgment basis based on pixel-level and image-level tampering clues.
Additionally, we leverage GPT-4o to enhance existing IFDL datasets, creating
the Multi-Modal Tamper Description dataSet (MMTD-Set) for training FakeShield's
tampering analysis capabilities. Meanwhile, we incorporate a Domain Tag-guided
Explainable Forgery Detection Module (DTE-FDM) and a Multi-modal Forgery
Localization Module (MFLM) to address various types of tamper detection
interpretation and achieve forgery localization guided by detailed textual
descriptions. Extensive experiments demonstrate that FakeShield effectively
detects and localizes various tampering techniques, offering an explainable and
superior solution compared to previous IFDL methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ACE: All-round Creator and Editor Following Instructions via Diffusion
  Transformer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00086v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00086v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Han, Zeyinzi Jiang, Yulin Pan, Jingfeng Zhang, Chaojie Mao, Chenwei Xie, Yu Liu, Jingren Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have emerged as a powerful generative technology and have
been found to be applicable in various scenarios. Most existing foundational
diffusion models are primarily designed for text-guided visual generation and
do not support multi-modal conditions, which are essential for many visual
editing tasks. This limitation prevents these foundational diffusion models
from serving as a unified model in the field of visual generation, like GPT-4
in the natural language processing field. In this work, we propose ACE, an
All-round Creator and Editor, which achieves comparable performance compared to
those expert models in a wide range of visual generation tasks. To achieve this
goal, we first introduce a unified condition format termed Long-context
Condition Unit (LCU), and propose a novel Transformer-based diffusion model
that uses LCU as input, aiming for joint training across various generation and
editing tasks. Furthermore, we propose an efficient data collection approach to
address the issue of the absence of available training data. It involves
acquiring pairwise images with synthesis-based or clustering-based pipelines
and supplying these pairs with accurate textual instructions by leveraging a
fine-tuned multi-modal large language model. To comprehensively evaluate the
performance of our model, we establish a benchmark of manually annotated pairs
data across a variety of visual generation tasks. The extensive experimental
results demonstrate the superiority of our model in visual generation fields.
Thanks to the all-in-one capabilities of our model, we can easily build a
multi-modal chat system that responds to any interactive request for image
creation using a single model to serve as the backend, avoiding the cumbersome
pipeline typically employed in visual agents. Code and models will be available
on the project page: https://ali-vilab.github.io/ace-page/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Blind Image Restoration via Fast Diffusion Inversion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19572v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19572v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamadi Chihaoui, Abdelhak Lemkhenter, Paolo Favaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image Restoration (IR) methods based on a pre-trained diffusion model have
demonstrated state-of-the-art performance. However, they have two fundamental
limitations: 1) they often assume that the degradation operator is completely
known and 2) they alter the diffusion sampling process, which may result in
restored images that do not lie onto the data manifold. To address these
issues, we propose Blind Image Restoration via fast Diffusion inversion (BIRD)
a blind IR method that jointly optimizes for the degradation model parameters
and the restored image. To ensure that the restored images lie onto the data
manifold, we propose a novel sampling technique on a pre-trained diffusion
model. A key idea in our method is not to modify the reverse sampling, i.e, not
to alter all the intermediate latents, once an initial noise is sampled. This
is ultimately equivalent to casting the IR task as an optimization problem in
the space of the input noise. Moreover, to mitigate the computational cost
associated with inverting a fully unrolled diffusion model, we leverage the
inherent capability of these models to skip ahead in the forward diffusion
process using large time steps. We experimentally validate BIRD on several
image restoration tasks and show that it achieves state of the art performance
on all of them. Our code is available at
https://github.com/hamadichihaoui/BIRD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Neurips 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sampling Strategies in Bayesian Inversion: A Study of RTO and Langevin
  Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16658v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16658v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Remi Laumont, Yiqiu Dong, Martin Skovgaard Andersen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies two classes of sampling methods for the solution of
inverse problems, namely Randomize-Then-Optimize (RTO), which is rooted in
sensitivity analysis, and Langevin methods, which are rooted in the Bayesian
framework. The two classes of methods correspond to different assumptions and
yield samples from different target distributions. We highlight the main
conceptual and theoretical differences between the two approaches and compare
them from a practical point of view by tackling two classical inverse problems
in imaging: deblurring and inpainting. We show that the choice of the sampling
method has a significant impact on the quality of the reconstruction and that
the RTO method is more robust to the choice of the parameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 3DGS.zip: A survey on 3D Gaussian Splatting Compression Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.09510v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.09510v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Milena T. Bagdasarian, Paul Knoll, Yi-Hsin Li, Florian Barthel, Anna Hilsmann, Peter Eisert, Wieland Morgenstern
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian Splatting (3DGS) has emerged as a cutting-edge technique for
real-time radiance field rendering, offering state-of-the-art performance in
terms of both quality and speed. 3DGS models a scene as a collection of
three-dimensional Gaussians, or splats, with additional attributes optimized to
conform to the scene's geometric and visual properties. Despite its advantages
in rendering speed and image fidelity, 3DGS is limited by its significant
storage and memory demands. These high demands make 3DGS impractical for mobile
devices or headsets, reducing its applicability in important areas of computer
graphics. To address these challenges and advance the practicality of 3DGS,
this survey provides a comprehensive and detailed examination of compression
and compaction techniques developed to make 3DGS more efficient. We categorize
current approaches into compression techniques, which aim at achieving the
highest quality at minimal data size, and compaction techniques, which aim for
optimal quality with the fewest Gaussians. We introduce the basic mathematical
concepts underlying the analyzed methods, as well as key implementation details
and design choices. Our report thoroughly discusses similarities and
differences among the methods, as well as their respective advantages and
disadvantages. We establish a consistent standard for comparing these methods
based on key performance metrics and datasets. Specifically, since these
methods have been developed in parallel and over a short period of time,
currently, no comprehensive comparison exists. This survey, for the first time,
presents a unified standard to evaluate 3DGS compression techniques. To
facilitate the continuous monitoring of emerging methodologies, we maintain a
dedicated website that will be regularly updated with new techniques and
revisions of existing findings https://w-m.github.io/3dgs-compression-survey/ .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>3D Gaussian Splatting compression survey; 3DGS compression; new
  approaches added</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Target Detection of Safety Protective Gear Using the Improved YOLOv5 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.05964v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.05964v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Liu, Xue Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In high-risk railway construction, personal protective equipment monitoring
is critical but challenging due to small and frequently obstructed targets. We
propose YOLO-EA, an innovative model that enhances safety measure detection by
integrating ECA into its backbone's convolutional layers, improving discernment
of minuscule objects like hardhats. YOLO-EA further refines target recognition
under occlusion by replacing GIoU with EIoU loss. YOLO-EA's effectiveness was
empirically substantiated using a dataset derived from real-world railway
construction site surveillance footage. It outperforms YOLOv5, achieving 98.9%
precision and 94.7% recall, up 2.5% and 0.5% respectively, while maintaining
real-time performance at 70.774 fps. This highly efficient and precise YOLO-EA
holds great promise for practical application in intricate construction
scenarios, enforcing stringent safety compliance during complex railway
construction projects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ocean-omni: To Understand the World with Omni-modality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08565v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08565v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yadong Li, Haoze Sun, Mingan Lin, Tianpeng Li, Guosheng Dong, Tao Zhang, Bowen Ding, Wei Song, Zhenglin Cheng, Yuqi Huo, Song Chen, Xu Li, Da Pan, Shusen Zhang, Xin Wu, Zheng Liang, Jun Liu, Tao Zhang, Keer Lu, Yaqi Zhao, Yanjun Shen, Fan Yang, Kaicheng Yu, Tao Lin, Jianhua Xu, Zenan Zhou, Weipeng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The salient multimodal capabilities and interactive experience of GPT-4o
highlight its critical role in practical applications, yet it lacks a
high-performing open-source counterpart. In this paper, we introduce
Ocean-omni, the first open-source 7B Multimodal Large Language Model (MLLM)
adept at concurrently processing and analyzing modalities of image, video,
audio, and text, while delivering an advanced multimodal interactive experience
and strong performance. We propose an effective multimodal training schema
starting with 7B model and proceeding through two stages of multimodal
alignment and multitask fine-tuning across audio, image, video, and text modal.
This approach equips the language model with the ability to handle visual and
audio data effectively. Demonstrating strong performance across various
omni-modal and multimodal benchmarks, we aim for this contribution to serve as
a competitive baseline for the open-source community in advancing multimodal
understanding and real-time interaction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FASTER: A Font-Agnostic Scene Text Editing and Rendering Framework <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.02905v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.02905v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alloy Das, Sanket Biswas, Prasun Roy, Subhankar Ghosh, Umapada Pal, Michael Blumenstein, Josep Lladós, Saumik Bhattacharya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scene Text Editing (STE) is a challenging research problem, that primarily
aims towards modifying existing texts in an image while preserving the
background and the font style of the original text. Despite its utility in
numerous real-world applications, existing style-transfer-based approaches have
shown sub-par editing performance due to (1) complex image backgrounds, (2)
diverse font attributes, and (3) varying word lengths within the text. To
address such limitations, in this paper, we propose a novel font-agnostic scene
text editing and rendering framework, named FASTER, for simultaneously
generating text in arbitrary styles and locations while preserving a natural
and realistic appearance and structure. A combined fusion of target mask
generation and style transfer units, with a cascaded self-attention mechanism
has been proposed to focus on multi-level text region edits to handle varying
word lengths. Extensive evaluation on a real-world database with further
subjective human evaluation study indicates the superiority of FASTER in both
scene text editing and rendering tasks, in terms of model performance and
efficiency. Our code will be released upon acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in WACV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diversifying Deep Ensembles: A Saliency Map Approach for Enhanced OOD
  Detection, Calibration, and Accuracy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.11616v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.11616v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stanislav Dereka, Ivan Karpukhin, Maksim Zhdanov, Sergey Kolesnikov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep ensembles are capable of achieving state-of-the-art results in
classification and out-of-distribution (OOD) detection. However, their
effectiveness is limited due to the homogeneity of learned patterns within
ensembles. To overcome this issue, our study introduces Saliency Diversified
Deep Ensemble (SDDE), a novel approach that promotes diversity among ensemble
members by leveraging saliency maps. Through incorporating saliency map
diversification, our method outperforms conventional ensemble techniques and
improves calibration in multiple classification and OOD detection tasks. In
particular, the proposed method achieves state-of-the-art OOD detection
quality, calibration, and accuracy on multiple benchmarks, including
CIFAR10/100 and large-scale ImageNet datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Identity Curvature Laplace Approximation for Improved
  Out-of-Distribution Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10464v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10464v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maksim Zhdanov, Stanislav Dereka, Sergey Kolesnikov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Uncertainty estimation is crucial in safety-critical applications, where
robust out-of-distribution (OOD) detection is essential. Traditional Bayesian
methods, though effective, are often hindered by high computational demands. As
an alternative, Laplace approximation offers a more practical and efficient
approach to uncertainty estimation. In this paper, we introduce the Identity
Curvature Laplace Approximation (ICLA), a novel method that challenges the
conventional posterior covariance formulation by using identity curvature and
optimizing prior precision. This innovative design significantly enhances OOD
detection performance on well-known datasets such as CIFAR-10, CIFAR-100, and
ImageNet, while maintaining calibration scores. We attribute this improvement
to the alignment issues between typical feature embeddings and curvature as
measured by the Fisher information matrix. Our findings are further supported
by demonstrating that incorporating Fisher penalty or sharpness-aware
minimization techniques can greatly enhance the uncertainty estimation
capabilities of standard Laplace approximation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Not Just Object, But State: Compositional Incremental Learning without
  Forgetting <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01739v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01739v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanyi Zhang, Binglin Qiu, Qi Jia, Yu Liu, Ran He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most incremental learners excessively prioritize coarse classes of objects
while neglecting various kinds of states (e.g. color and material) attached to
the objects. As a result, they are limited in the ability to reason
fine-grained compositionality of state-object pairs. To remedy this limitation,
we propose a novel task called Compositional Incremental Learning
(composition-IL), enabling the model to recognize state-object compositions as
a whole in an incremental learning fashion. Since the lack of suitable
benchmarks, we re-organize two existing datasets and make them tailored for
composition-IL. Then, we propose a prompt-based Composition Incremental Learner
(CompILer), to overcome the ambiguous composition boundary problem which
challenges composition-IL largely. Specifically, we exploit multi-pool prompt
learning, which is regularized by inter-pool prompt discrepancy and intra-pool
prompt diversity. Besides, we devise object-injected state prompting by using
object prompts to guide the selection of state prompts. Furthermore, we fuse
the selected prompts by a generalized-mean strategy, to eliminate irrelevant
information learned in the prompts. Extensive experiments on two datasets
exhibit state-of-the-art performance achieved by CompILer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Extrapolating Prospective Glaucoma Fundus Images through Diffusion Model
  in Irregular Longitudinal Sequences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21130v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21130v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihao Zhao, Junjie Yang, Shahrooz Faghihroohi, Yinzheng Zhao, Daniel Zapp, Kai Huang, Nassir Navab, M. Ali Nasseri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The utilization of longitudinal datasets for glaucoma progression prediction
offers a compelling approach to support early therapeutic interventions.
Predominant methodologies in this domain have primarily focused on the direct
prediction of glaucoma stage labels from longitudinal datasets. However, such
methods may not adequately encapsulate the nuanced developmental trajectory of
the disease. To enhance the diagnostic acumen of medical practitioners, we
propose a novel diffusion-based model to predict prospective images by
extrapolating from existing longitudinal fundus images of patients. The
methodology delineated in this study distinctively leverages sequences of
images as inputs. Subsequently, a time-aligned mask is employed to select a
specific year for image generation. During the training phase, the time-aligned
mask resolves the issue of irregular temporal intervals in longitudinal image
sequence sampling. Additionally, we utilize a strategy of randomly masking a
frame in the sequence to establish the ground truth. This methodology aids the
network in continuously acquiring knowledge regarding the internal
relationships among the sequences throughout the learning phase. Moreover, the
introduction of textual labels is instrumental in categorizing images generated
within the sequence. The empirical findings from the conducted experiments
indicate that our proposed model not only effectively generates longitudinal
data but also significantly improves the precision of downstream classification
tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at BIBM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Shape2.5D: A Dataset of Texture-less Surfaces for Depth and Normals
  Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15831v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15831v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Saif Ullah Khan, Sankalp Sinha, Didier Stricker, Marcus Liwicki, Muhammad Zeshan Afzal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing texture-less surfaces poses unique challenges in computer
vision, primarily due to the lack of specialized datasets that cater to the
nuanced needs of depth and normals estimation in the absence of textural
information. We introduce "Shape2.5D," a novel, large-scale dataset designed to
address this gap. Comprising 1.17 million frames spanning over 39,772 3D models
and 48 unique objects, our dataset provides depth and surface normal maps for
texture-less object reconstruction. The proposed dataset includes synthetic
images rendered with 3D modeling software to simulate various lighting
conditions and viewing angles. It also includes a real-world subset comprising
4,672 frames captured with a depth camera. Our comprehensive benchmarks
demonstrate the dataset's ability to support the development of algorithms that
robustly estimate depth and normals from RGB images and perform voxel
reconstruction. Our open-source data generation pipeline allows the dataset to
be extended and adapted for future research. The dataset is publicly available
at https://github.com/saifkhichi96/Shape25D.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in IEEE Access</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UniVST: A Unified Framework for Training-free Localized Video Style
  Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20084v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20084v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quanjian Song, Mingbao Lin, Wengyi Zhan, Shuicheng Yan, Liujuan Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents UniVST, a unified framework for localized video style
transfer. It operates without the need for training, offering a distinct
advantage over existing methods that transfer style across entire videos. The
endeavors of this paper comprise: (1) A point-matching mask propagation
strategy that leverages feature maps from the DDIM inversion. This streamlines
the model's architecture by obviating the need for tracking models. (2) An
AdaIN-guided style transfer mechanism that operates at both the latent and
attention levels. This balances content fidelity and style richness, mitigating
the loss of localized details commonly associated with direct video
stylization. (3) A sliding window smoothing strategy that harnesses optical
flow within the pixel representation and refines predicted noise to update the
latent space. This significantly enhances temporal consistency and diminishes
artifacts in video outputs. Our proposed UniVST has been validated to be
superior to existing methods in quantitative and qualitative metrics. It
adeptly addresses the challenges of preserving the primary object's style while
ensuring temporal consistency and detail preservation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages not including reference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stable-Pose: Leveraging Transformers for Pose-Guided Text-to-Image
  Generation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.02485v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.02485v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiajun Wang, Morteza Ghahremani, Yitong Li, Björn Ommer, Christian Wachinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Controllable text-to-image (T2I) diffusion models have shown impressive
performance in generating high-quality visual content through the incorporation
of various conditions. Current methods, however, exhibit limited performance
when guided by skeleton human poses, especially in complex pose conditions such
as side or rear perspectives of human figures. To address this issue, we
present Stable-Pose, a novel adapter model that introduces a coarse-to-fine
attention masking strategy into a vision Transformer (ViT) to gain accurate
pose guidance for T2I models. Stable-Pose is designed to adeptly handle pose
conditions within pre-trained Stable Diffusion, providing a refined and
efficient way of aligning pose representation during image synthesis. We
leverage the query-key self-attention mechanism of ViTs to explore the
interconnections among different anatomical parts in human pose skeletons.
Masked pose images are used to smoothly refine the attention maps based on
target pose-related features in a hierarchical manner, transitioning from
coarse to fine levels. Additionally, our loss function is formulated to
allocate increased emphasis to the pose region, thereby augmenting the model's
precision in capturing intricate pose details. We assessed the performance of
Stable-Pose across five public datasets under a wide range of indoor and
outdoor human pose scenarios. Stable-Pose achieved an AP score of 57.1 in the
LAION-Human dataset, marking around 13% improvement over the established
technique ControlNet. The project link and code is available at
https://github.com/ai-med/StablePose.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PPLLaVA: Varied Video Sequence Understanding With <span class="highlight-title">Prompt</span> Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02327v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02327v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruyang Liu, Haoran Tang, Haibo Liu, Yixiao Ge, Ying Shan, Chen Li, Jiankun Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The past year has witnessed the significant advancement of video-based large
language models. However, the challenge of developing a unified model for both
short and long video understanding remains unresolved. Most existing video LLMs
cannot handle hour-long videos, while methods custom for long videos tend to be
ineffective for shorter videos and images. In this paper, we identify the key
issue as the redundant content in videos. To address this, we propose a novel
pooling strategy that simultaneously achieves token compression and
instruction-aware visual feature aggregation. Our model is termed Prompt-guided
Pooling LLaVA, or PPLLaVA for short. Specifically, PPLLaVA consists of three
core components: the CLIP-based visual-prompt alignment that extracts visual
information relevant to the user's instructions, the prompt-guided pooling that
compresses the visual sequence to arbitrary scales using convolution-style
pooling, and the clip context extension designed for lengthy prompt common in
visual dialogue. Moreover, our codebase also integrates the most advanced video
Direct Preference Optimization (DPO) and visual interleave training. Extensive
experiments have validated the performance of our model. With superior
throughput and only 1024 visual context, PPLLaVA achieves better results on
image benchmarks as a video LLM, while achieving state-of-the-art performance
across various video benchmarks, excelling in tasks ranging from caption
generation to multiple-choice questions, and handling video lengths from
seconds to hours. Codes have been available at
https://github.com/farewellthree/PPLLaVA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DenoiseRep: Denoising Model for Representation Learning <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.08773v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.08773v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengrui Xu, Guan'an Wang, Xiaowen Huang, Jitao Sang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The denoising model has been proven a powerful generative model but has
little exploration of discriminative tasks. Representation learning is
important in discriminative tasks, which is defined as "learning
representations (or features) of the data that make it easier to extract useful
information when building classifiers or other predictors". In this paper, we
propose a novel Denoising Model for Representation Learning (DenoiseRep) to
improve feature discrimination with joint feature extraction and denoising.
DenoiseRep views each embedding layer in a backbone as a denoising layer,
processing the cascaded embedding layers as if we are recursively denoise
features step-by-step. This unifies the frameworks of feature extraction and
denoising, where the former progressively embeds features from low-level to
high-level, and the latter recursively denoises features step-by-step. After
that, DenoiseRep fuses the parameters of feature extraction and denoising
layers, and theoretically demonstrates its equivalence before and after the
fusion, thus making feature denoising computation-free. DenoiseRep is a
label-free algorithm that incrementally improves features but also
complementary to the label if available. Experimental results on various
discriminative vision tasks, including re-identification (Market-1501,
DukeMTMC-reID, MSMT17, CUHK-03, vehicleID), image classification (ImageNet,
UB200, Oxford-Pet, Flowers), object detection (COCO), image segmentation
(ADE20K) show stability and impressive improvements. We also validate its
effectiveness on the CNN (ResNet) and Transformer (ViT, Swin, Vmamda)
architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024 (Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging generative models to characterize the failure conditions of
  image classifiers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12814v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12814v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adrien LeCoz, Stéphane Herbin, Faouzi Adjed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address in this work the question of identifying the failure conditions of
a given image classifier. To do so, we exploit the capacity of producing
controllable distributions of high quality image data made available by recent
Generative Adversarial Networks (StyleGAN2): the failure conditions are
expressed as directions of strong performance degradation in the generative
model latent space. This strategy of analysis is used to discover corner cases
that combine multiple sources of corruption, and to compare in more details the
behavior of different classifiers. The directions of degradation can also be
rendered visually by generating data for better interpretability. Some
degradations such as image quality can affect all classes, whereas other ones
such as shape are more class-specific. The approach is demonstrated on the
MNIST dataset that has been completed by two sources of corruption: noise and
blur, and shows a promising way to better understand and control the risks of
exploiting Artificial Intelligence components for safety-critical applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Latent Representation Matters: Human-like Sketches in One-shot Drawing
  Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.06079v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.06079v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Victor Boutin, Rishav Mukherji, Aditya Agrawal, Sabine Muzellec, Thomas Fel, Thomas Serre, Rufin VanRullen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans can effortlessly draw new categories from a single exemplar, a feat
that has long posed a challenge for generative models. However, this gap has
started to close with recent advances in diffusion models. This one-shot
drawing task requires powerful inductive biases that have not been
systematically investigated. Here, we study how different inductive biases
shape the latent space of Latent Diffusion Models (LDMs). Along with standard
LDM regularizers (KL and vector quantization), we explore supervised
regularizations (including classification and prototype-based representation)
and contrastive inductive biases (using SimCLR and redundancy reduction
objectives). We demonstrate that LDMs with redundancy reduction and
prototype-based regularizations produce near-human-like drawings (regarding
both samples' recognizability and originality) -- better mimicking human
perception (as evaluated psychophysically). Overall, our results suggest that
the gap between humans and machines in one-shot drawings is almost closed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explaining an image classifier with a generative model conditioned by
  uncertainty 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13871v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13871v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adrien LeCoz, Stéphane Herbin, Faouzi Adjed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose to condition a generative model by a given image classifier
uncertainty in order to analyze and explain its behavior. Preliminary
experiments on synthetic data and a corrupted version of MNIST dataset
illustrate the idea.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Advancements and limitations of LLMs in replicating human color-word
  associations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02116v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02116v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Makoto Fukushima, Shusuke Eshita, Hiroshige Fukuhara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Color-word associations play a fundamental role in human cognition and design
applications. Large Language Models (LLMs) have become widely available and
demonstrated intelligent behaviors in various benchmarks with natural
conversation skills. However, their ability to replicate human color-word
associations remains understudied. We compared multiple generations of LLMs
(from GPT-3 to GPT-4o) against human color-word associations using data
collected from over 10,000 Japanese participants, involving 17 colors and words
from eight categories in Japanese. Our findings reveal a clear progression in
LLM performance across generations, with GPT-4o achieving the highest accuracy
in predicting the best voted word for each color and category. However, the
highest median performance was approximately 50% even for GPT-4o with visual
inputs (chance level is 10%), and the performance levels varied significantly
across word categories and colors, indicating a failure to fully replicate
human color-word associations. On the other hand, color discrimination ability
estimated from our color-word association data showed that LLMs demonstrated
high correlation with human color discrimination patterns, similarly to
previous studies. Our study highlights both the advancements in LLM
capabilities and their persistent limitations, suggesting differences in
semantic memory structures between humans and LLMs in representing color-word
associations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 7 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Predictive Dynamic Fusion <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04802v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04802v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bing Cao, Yinan Xia, Yi Ding, Changqing Zhang, Qinghua Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal fusion is crucial in joint decision-making systems for rendering
holistic judgments. Since multimodal data changes in open environments, dynamic
fusion has emerged and achieved remarkable progress in numerous applications.
However, most existing dynamic multimodal fusion methods lack theoretical
guarantees and easily fall into suboptimal problems, yielding unreliability and
instability. To address this issue, we propose a Predictive Dynamic Fusion
(PDF) framework for multimodal learning. We proceed to reveal the multimodal
fusion from a generalization perspective and theoretically derive the
predictable Collaborative Belief (Co-Belief) with Mono- and Holo-Confidence,
which provably reduces the upper bound of generalization error. Accordingly, we
further propose a relative calibration strategy to calibrate the predicted
Co-Belief for potential uncertainty. Extensive experiments on multiple
benchmarks confirm our superiority. Our code is available at
https://github.com/Yinan-Xia/PDF.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICML 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EchoSight: Advancing Visual-Language Models with Wiki Knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12735v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12735v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yibin Yan, Weidi Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge-based Visual Question Answering (KVQA) tasks require answering
questions about images using extensive background knowledge. Despite
significant advancements, generative models often struggle with these tasks due
to the limited integration of external knowledge. In this paper, we introduce
EchoSight, a novel multimodal Retrieval-Augmented Generation (RAG) framework
that enables large language models (LLMs) to answer visual questions requiring
fine-grained encyclopedic knowledge. To strive for high-performing retrieval,
EchoSight first searches wiki articles by using visual-only information,
subsequently, these candidate articles are further reranked according to their
relevance to the combined text-image query. This approach significantly
improves the integration of multimodal knowledge, leading to enhanced retrieval
outcomes and more accurate VQA responses. Our experimental results on the
Encyclopedic VQA and InfoSeek datasets demonstrate that EchoSight establishes
new state-of-the-art results in knowledge-based VQA, achieving an accuracy of
41.8% on Encyclopedic VQA and 31.3% on InfoSeek.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report; Project Page: https://go2heart.github.io/echosight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MSTA3D: Multi-scale Twin-attention for 3D Instance Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01781v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01781v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Duc Dang Trung Tran, Byeongkeun Kang, Yeejin Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, transformer-based techniques incorporating superpoints have become
prevalent in 3D instance segmentation. However, they often encounter an
over-segmentation problem, especially noticeable with large objects.
Additionally, unreliable mask predictions stemming from superpoint mask
prediction further compound this issue. To address these challenges, we propose
a novel framework called MSTA3D. It leverages multi-scale feature
representation and introduces a twin-attention mechanism to effectively capture
them. Furthermore, MSTA3D integrates a box query with a box regularizer,
offering a complementary spatial constraint alongside semantic queries.
Experimental evaluations on ScanNetV2, ScanNet200 and S3DIS datasets
demonstrate that our approach surpasses state-of-the-art 3D instance
segmentation methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 9 figures, 7 tables, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diversity-Driven Synthesis: Enhancing Dataset Distillation through
  Directed Weight Adjustment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17612v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17612v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Du, Xin Zhang, Juncheng Hu, Wenxin Huang, Joey Tianyi Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The sharp increase in data-related expenses has motivated research into
condensing datasets while retaining the most informative features. Dataset
distillation has thus recently come to the fore. This paradigm generates
synthetic datasets that are representative enough to replace the original
dataset in training a neural network. To avoid redundancy in these synthetic
datasets, it is crucial that each element contains unique features and remains
diverse from others during the synthesis stage. In this paper, we provide a
thorough theoretical and empirical analysis of diversity within synthesized
datasets. We argue that enhancing diversity can improve the parallelizable yet
isolated synthesizing approach. Specifically, we introduce a novel method that
employs dynamic and directed weight adjustment techniques to modulate the
synthesis process, thereby maximizing the representativeness and diversity of
each synthetic instance. Our method ensures that each batch of synthetic data
mirrors the characteristics of a large, varying subset of the original dataset.
Extensive experiments across multiple datasets, including CIFAR, Tiny-ImageNet,
and ImageNet-1K, demonstrate the superior performance of our method,
highlighting its effectiveness in producing diverse and representative
synthetic datasets with minimal computational expense. Our code is available at
https://github.com/AngusDujw/Diversity-Driven-Synthesis.https://github.com/AngusDujw/Diversity-Driven-Synthesis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In-Context LoRA for Diffusion Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23775v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23775v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lianghua Huang, Wei Wang, Zhi-Fan Wu, Yupeng Shi, Huanzhang Dou, Chen Liang, Yutong Feng, Yu Liu, Jingren Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research arXiv:2410.15027 has explored the use of diffusion
transformers (DiTs) for task-agnostic image generation by simply concatenating
attention tokens across images. However, despite substantial computational
resources, the fidelity of the generated images remains suboptimal. In this
study, we reevaluate and streamline this framework by hypothesizing that
text-to-image DiTs inherently possess in-context generation capabilities,
requiring only minimal tuning to activate them. Through diverse task
experiments, we qualitatively demonstrate that existing text-to-image DiTs can
effectively perform in-context generation without any tuning. Building on this
insight, we propose a remarkably simple pipeline to leverage the in-context
abilities of DiTs: (1) concatenate images instead of tokens, (2) perform joint
captioning of multiple images, and (3) apply task-specific LoRA tuning using
small datasets (e.g., 20~100 samples) instead of full-parameter tuning with
large datasets. We name our models In-Context LoRA (IC-LoRA). This approach
requires no modifications to the original DiT models, only changes to the
training data. Remarkably, our pipeline generates high-fidelity image sets that
better adhere to prompts. While task-specific in terms of tuning data, our
framework remains task-agnostic in architecture and pipeline, offering a
powerful tool for the community and providing valuable insights for further
research on product-level task-agnostic generation systems. We release our
code, data, and models at https://github.com/ali-vilab/In-Context-LoRA
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tech report. Project page:
  https://ali-vilab.github.io/In-Context-LoRA-Page/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Edge AI-Enabled Chicken Health Detection Based on Enhanced FCOS-Lite and
  Knowledge Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.09562v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.09562v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiang Tong, Jinrui Wang, Wenshuang Yang, Songtao Wu, Wenqi Zhang, Chen Sun, Kuanhong Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The utilization of AIoT technology has become a crucial trend in modern
poultry management, offering the potential to optimize farming operations and
reduce human workloads. This paper presents a real-time and compact edge-AI
enabled detector designed to identify chickens and their healthy statuses using
frames captured by a lightweight and intelligent camera equipped with an
edge-AI enabled CMOS sensor. To ensure efficient deployment of the proposed
compact detector within the memory-constrained edge-AI enabled CMOS sensor, we
employ a FCOS-Lite detector leveraging MobileNet as the backbone. To mitigate
the issue of reduced accuracy in compact edge-AI detectors without incurring
additional inference costs, we propose a gradient weighting loss function as
classification loss and introduce CIOU loss function as localization loss.
Additionally, we propose a knowledge distillation scheme to transfer valuable
information from a large teacher detector to the proposed FCOS-Lite detector,
thereby enhancing its performance while preserving a compact model size.
Experimental results demonstrate the proposed edge-AI enabled detector achieves
commendable performance metrics, including a mean average precision (mAP) of
95.1$\%$ and an F1-score of 94.2$\%$, etc. Notably, the proposed detector can
be efficiently deployed and operates at a speed exceeding 20 FPS on the edge-AI
enabled CMOS sensor, achieved through int8 quantization. That meets practical
demands for automated poultry health monitoring using lightweight intelligent
cameras with low power consumption and minimal bandwidth costs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SeTAR: Out-of-Distribution Detection with Selective Low-Rank
  Approximation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12629v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12629v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixia Li, Boya Xiong, Guanhua Chen, Yun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Out-of-distribution (OOD) detection is crucial for the safe deployment of
neural networks. Existing CLIP-based approaches perform OOD detection by
devising novel scoring functions or sophisticated fine-tuning methods. In this
work, we propose SeTAR, a novel, training-free OOD detection method that
leverages selective low-rank approximation of weight matrices in
vision-language and vision-only models. SeTAR enhances OOD detection via
post-hoc modification of the model's weight matrices using a simple greedy
search algorithm. Based on SeTAR, we further propose SeTAR+FT, a fine-tuning
extension optimizing model performance for OOD detection tasks. Extensive
evaluations on ImageNet1K and Pascal-VOC benchmarks show SeTAR's superior
performance, reducing the relatively false positive rate by up to 18.95% and
36.80% compared to zero-shot and fine-tuning baselines. Ablation studies
further validate SeTAR's effectiveness, robustness, and generalizability across
different model backbones. Our work offers a scalable, efficient solution for
OOD detection, setting a new state-of-the-art in this area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024. Project page is live at
  https://SeTAR-OOD.github.io. Code are available at
  https://github.com/X1AOX1A/SeTAR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TaCOS: Task-Specific Camera Optimization with Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.11031v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.11031v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengyang Yan, Donald G. Dansereau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The performance of perception tasks is heavily influenced by imaging systems.
However, designing cameras with high task performance is costly, requiring
extensive camera knowledge and experimentation with physical hardware.
Additionally, cameras and perception tasks are mostly designed in isolation,
whereas recent methods that jointly design cameras and tasks have shown
improved performance. Therefore, we present a novel end-to-end optimization
approach that co-designs cameras with specific vision tasks. This method
combines derivative-free and gradient-based optimizers to support both
continuous and discrete camera parameters within manufacturing constraints. We
leverage recent computer graphics techniques and physical camera
characteristics to simulate the cameras in virtual environments, making the
design process cost-effective. We validate our simulations against physical
cameras and provide a procedurally generated virtual environment. Our
experiments demonstrate that our method designs cameras that outperform common
off-the-shelf options, and more efficiently compared to the state-of-the-art
approach, requiring only 2 minutes to design a camera on an example experiment
compared with 67 minutes for the competing method. Designed to support the
development of cameras under manufacturing constraints, multiple cameras, and
unconventional cameras, we believe this approach can advance the fully
automated design of cameras.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Passive Non-Line-of-Sight Imaging with Light Transport Modulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.16014v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.16014v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiarui Zhang, Ruixu Geng, Xiaolong Du, Yan Chen, Houqiang Li, Yang Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Passive non-line-of-sight (NLOS) imaging has witnessed rapid development in
recent years, due to its ability to image objects that are out of sight. The
light transport condition plays an important role in this task since changing
the conditions will lead to different imaging models. Existing learning-based
NLOS methods usually train independent models for different light transport
conditions, which is computationally inefficient and impairs the practicality
of the models. In this work, we propose NLOS-LTM, a novel passive NLOS imaging
method that effectively handles multiple light transport conditions with a
single network. We achieve this by inferring a latent light transport
representation from the projection image and using this representation to
modulate the network that reconstructs the hidden image from the projection
image. We train a light transport encoder together with a vector quantizer to
obtain the light transport representation. To further regulate this
representation, we jointly learn both the reconstruction network and the
reprojection network during training. A set of light transport modulation
blocks is used to modulate the two jointly trained networks in a multi-scale
way. Extensive experiments on a large-scale passive NLOS dataset demonstrate
the superiority of the proposed method. The code is available at
https://github.com/JerryOctopus/NLOS-LTM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GenXD: Generating Any 3D and 4D Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02319v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02319v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuyang Zhao, Chung-Ching Lin, Kevin Lin, Zhiwen Yan, Linjie Li, Zhengyuan Yang, Jianfeng Wang, Gim Hee Lee, Lijuan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent developments in 2D visual generation have been remarkably successful.
However, 3D and 4D generation remain challenging in real-world applications due
to the lack of large-scale 4D data and effective model design. In this paper,
we propose to jointly investigate general 3D and 4D generation by leveraging
camera and object movements commonly observed in daily life. Due to the lack of
real-world 4D data in the community, we first propose a data curation pipeline
to obtain camera poses and object motion strength from videos. Based on this
pipeline, we introduce a large-scale real-world 4D scene dataset: CamVid-30K.
By leveraging all the 3D and 4D data, we develop our framework, GenXD, which
allows us to produce any 3D or 4D scene. We propose multiview-temporal modules,
which disentangle camera and object movements, to seamlessly learn from both 3D
and 4D data. Additionally, GenXD employs masked latent conditions to support a
variety of conditioning views. GenXD can generate videos that follow the camera
trajectory as well as consistent 3D views that can be lifted into 3D
representations. We perform extensive evaluations across various real-world and
synthetic datasets, demonstrating GenXD's effectiveness and versatility
compared to previous methods in 3D and 4D generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-supervised Auxiliary Learning for Texture and Model-based Hybrid
  Robust and Fair Featuring in Face Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19582v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19582v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shukesh Reddy, Nishit Poddar, Srijan Das, Abhijit Das
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we explore Self-supervised Learning (SSL) as an auxiliary task
to blend the texture-based local descriptors into feature modelling for
efficient face analysis. Combining a primary task and a self-supervised
auxiliary task is beneficial for robust representation. Therefore, we used the
SSL task of mask auto-encoder (MAE) as an auxiliary task to reconstruct texture
features such as local patterns along with the primary task for robust and
unbiased face analysis. We experimented with our hypothesis on three major
paradigms of face analysis: face attribute and face-based emotion analysis, and
deepfake detection. Our experiment results exhibit that better feature
representation can be gleaned from our proposed model for fair and bias-less
face analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LADDER: Language Driven Slice Discovery and Error Rectification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.07832v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.07832v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shantanu Ghosh, Rayan Syed, Chenyu Wang, Clare B. Poynton, Shyam Visweswaran, Kayhan Batmanghelich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Error slice discovery associates structured patterns with model errors.
Existing methods discover error slices by clustering the error-prone samples
with similar patterns or assigning discrete attributes to each sample for
post-hoc analysis. While these methods aim for interpretability and easier
mitigation through reweighting or rebalancing, they may not capture the full
complexity of error patterns due to incomplete or missing attributes. Contrary
to the existing approach, this paper utilizes the reasoning capabilities of the
Large Language Model (LLM) to analyze complex error patterns and generate
testable hypotheses. This paper proposes LADDER: Language Driven slice
Discovery and Error Rectification. It first projects the model's representation
into a language-aligned feature space (eg CLIP) to preserve semantics in the
original model feature space. This ensures the accurate retrieval of sentences
that highlight the model's errors. Next, the LLM utilizes the sentences and
generates hypotheses to discover error slices. Finally, we mitigate the error
by fine-tuning the classification head by creating a group-balanced dataset
using the hypotheses. Our entire method does not require any attribute
annotation, either explicitly or through external tagging models. We validate
our method with \textbf{five} image classification datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LifelongMemory: Leveraging LLMs for Answering Queries in Long-form
  Egocentric Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.05269v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.05269v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ying Wang, Yanlai Yang, Mengye Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we introduce LifelongMemory, a new framework for accessing
long-form egocentric videographic memory through natural language question
answering and retrieval. LifelongMemory generates concise video activity
descriptions of the camera wearer and leverages the zero-shot capabilities of
pretrained large language models to perform reasoning over long-form video
context. Furthermore, LifelongMemory uses a confidence and explanation module
to produce confident, high-quality, and interpretable answers. Our approach
achieves state-of-the-art performance on the EgoSchema benchmark for question
answering and is highly competitive on the natural language query (NLQ)
challenge of Ego4D. Code is available at
https://github.com/agentic-learning-ai-lab/lifelong-memory.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ D3: Data Diversity Design for Systematic Generalization in Visual
  Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.08798v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.08798v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir Rahimi, Vanessa D'Amario, Moyuru Yamada, Kentaro Takemoto, Tomotake Sasaki, Xavier Boix
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Systematic generalization is a crucial aspect of intelligence, which refers
to the ability to generalize to novel tasks by combining known subtasks and
concepts. One critical factor that has been shown to influence systematic
generalization is the diversity of training data. However, diversity can be
defined in various ways, as data have many factors of variation. A more
granular understanding of how different aspects of data diversity affect
systematic generalization is lacking. We present new evidence in the problem of
Visual Question Answering (VQA) that reveals that the diversity of simple tasks
(i.e. tasks formed by a few subtasks and concepts) plays a key role in
achieving systematic generalization. This implies that it may not be essential
to gather a large and varied number of complex tasks, which could be costly to
obtain. We demonstrate that this result is independent of the similarity
between the training and testing data and applies to well-known families of
neural network architectures for VQA (i.e. monolithic architectures and neural
module networks). Additionally, we observe that neural module networks leverage
all forms of data diversity we evaluated, while monolithic architectures
require more extensive amounts of data to do so. These findings provide a first
step towards understanding the interactions between data diversity design,
neural network architectures, and systematic generalization capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>TMLR (https://openreview.net/forum?id=ZAin13msOp)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interpretable Lightweight Transformer via Unrolling of Learned Graph
  Smoothness Priors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04090v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04090v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tam Thuc Do, Parham Eftekhar, Seyed Alireza Hosseini, Gene Cheung, Philip Chou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We build interpretable and lightweight transformer-like neural networks by
unrolling iterative optimization algorithms that minimize graph smoothness
priors -- the quadratic graph Laplacian regularizer (GLR) and the $\ell_1$-norm
graph total variation (GTV) -- subject to an interpolation constraint. The
crucial insight is that a normalized signal-dependent graph learning module
amounts to a variant of the basic self-attention mechanism in conventional
transformers. Unlike "black-box" transformers that require learning of large
key, query and value matrices to compute scaled dot products as affinities and
subsequent output embeddings, resulting in huge parameter sets, our unrolled
networks employ shallow CNNs to learn low-dimensional features per node to
establish pairwise Mahalanobis distances and construct sparse similarity
graphs. At each layer, given a learned graph, the target interpolated signal is
simply a low-pass filtered output derived from the minimization of an assumed
graph smoothness prior, leading to a dramatic reduction in parameter count.
Experiments for two image interpolation applications verify the restoration
performance, parameter efficiency and robustness to covariate shift of our
graph-based unrolled networks compared to conventional transformers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GD doesn't make the cut: Three ways that non-differentiability affects
  neural network training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.08426v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.08426v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddharth Krishna Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the distinctions between gradient methods applied to
non-differentiable functions (NGDMs) and classical gradient descents (GDs)
designed for differentiable functions. First, we demonstrate significant
differences in the convergence properties of NGDMs compared to GDs, challenging
the applicability of the extensive neural network convergence literature based
on $L-smoothness$ to non-smooth neural networks. Next, we demonstrate the
paradoxical nature of NGDM solutions for $L_{1}$-regularized problems, showing
that increasing the regularization penalty leads to an increase in the $L_{1}$
norm of optimal solutions in NGDMs. Consequently, we show that widely adopted
$L_{1}$ penalization-based techniques for network pruning do not yield expected
results. Additionally, we dispel the common belief that optimization algorithms
like Adam and RMSProp perform similarly in non-differentiable contexts.
Finally, we explore the Edge of Stability phenomenon, indicating its
inapplicability even to Lipschitz continuous convex differentiable functions,
leaving its relevance to non-convex non-differentiable neural networks
inconclusive. Our analysis exposes misguided interpretations of NGDMs in widely
referenced papers and texts due to an overreliance on strong smoothness
assumptions, emphasizing the necessity for a nuanced understanding of
foundational assumptions in the analysis of these systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Anatomical Foundation Models for Brain MRIs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.07079v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.07079v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlo Alberto Barbano, Matteo Brunello, Benoit Dufumier, Marco Grangetto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Learning (DL) in neuroimaging has become increasingly relevant for
detecting neurological conditions and neurodegenerative disorders. One of the
most predominant biomarkers in neuroimaging is represented by brain age, which
has been shown to be a good indicator for different conditions, such as
Alzheimer's Disease. Using brain age for pretraining DL models in transfer
learning settings has also recently shown promising results, especially when
dealing with data scarcity of different conditions. On the other hand,
anatomical information of brain MRIs (e.g. cortical thickness) can provide
important information for learning good representations that can be transferred
to many downstream tasks. In this work, we propose AnatCL, an anatomical
foundation model for brain MRIs that i.) leverages anatomical information with
a weakly contrastive learning approach and ii.) achieves state-of-the-art
performances in many different downstream tasks. To validate our approach we
consider 12 different downstream tasks for diagnosis classification, and
prediction of 10 different clinical assessment scores. Pretrained models can be
found at https://github.com/EIDOSLAB/AnatCL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages; added source url</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FewViewGS: Gaussian Splatting with Few View Matching and Multi-stage
  Training <span class="chip">NeurIPS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02229v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02229v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruihong Yin, Vladimir Yugay, Yue Li, Sezer Karaoglu, Theo Gevers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of novel view synthesis from images has seen rapid advancements
with the introduction of Neural Radiance Fields (NeRF) and more recently with
3D Gaussian Splatting. Gaussian Splatting became widely adopted due to its
efficiency and ability to render novel views accurately. While Gaussian
Splatting performs well when a sufficient amount of training images are
available, its unstructured explicit representation tends to overfit in
scenarios with sparse input images, resulting in poor rendering performance. To
address this, we present a 3D Gaussian-based novel view synthesis method using
sparse input images that can accurately render the scene from the viewpoints
not covered by the training images. We propose a multi-stage training scheme
with matching-based consistency constraints imposed on the novel views without
relying on pre-trained depth estimation or diffusion models. This is achieved
by using the matches of the available training images to supervise the
generation of the novel views sampled between the training frames with color,
geometry, and semantic losses. In addition, we introduce a locality preserving
regularization for 3D Gaussians which removes rendering artifacts by preserving
the local color structure of the scene. Evaluation on synthetic and real-world
datasets demonstrates competitive or superior performance of our method in
few-shot novel view synthesis compared to existing state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SLVideo: A Sign Language Video Moment Retrieval Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15668v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15668v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gonçalo Vinagre Martins, João Magalhães, Afonso Quinaz, Carla Viegas, Sofia Cavaco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  SLVideo is a video moment retrieval system for Sign Language videos that
incorporates facial expressions, addressing this gap in existing technology.
The system extracts embedding representations for the hand and face signs from
video frames to capture the signs in their entirety, enabling users to search
for a specific sign language video segment with text queries. A collection of
eight hours of annotated Portuguese Sign Language videos is used as the
dataset, and a CLIP model is used to generate the embeddings. The initial
results are promising in a zero-shot setting. In addition, SLVideo incorporates
a thesaurus that enables users to search for similar signs to those retrieved,
using the video segment embeddings, and also supports the edition and creation
of video sign language annotations. Project web page:
https://novasearch.github.io/SLVideo/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 1 figure, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DC-Gaussian: Improving 3D Gaussian Splatting for Reflective Dash Cam
  Videos <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17705v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17705v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linhan Wang, Kai Cheng, Shuo Lei, Shengkun Wang, Wei Yin, Chenyang Lei, Xiaoxiao Long, Chang-Tien Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present DC-Gaussian, a new method for generating novel views from
in-vehicle dash cam videos. While neural rendering techniques have made
significant strides in driving scenarios, existing methods are primarily
designed for videos collected by autonomous vehicles. However, these videos are
limited in both quantity and diversity compared to dash cam videos, which are
more widely used across various types of vehicles and capture a broader range
of scenarios. Dash cam videos often suffer from severe obstructions such as
reflections and occlusions on the windshields, which significantly impede the
application of neural rendering techniques. To address this challenge, we
develop DC-Gaussian based on the recent real-time neural rendering technique 3D
Gaussian Splatting (3DGS). Our approach includes an adaptive image
decomposition module to model reflections and occlusions in a unified manner.
Additionally, we introduce illumination-aware obstruction modeling to manage
reflections and occlusions under varying lighting conditions. Lastly, we employ
a geometry-guided Gaussian enhancement strategy to improve rendering details by
incorporating additional geometry priors. Experiments on self-captured and
public dash cam videos show that our method not only achieves state-of-the-art
performance in novel view synthesis, but also accurately reconstructing
captured scenes getting rid of obstructions. See the project page for code,
data: https://linhanwang.github.io/dcgaussian/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages,7 figures;project page:
  https://linhanwang.github.io/dcgaussian/; Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ODGEN: Domain-specific Object Detection Data Generation with Diffusion
  Models <span class="chip">NeurIPS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15199v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15199v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyuan Zhu, Shiyu Li, Yuxuan Liu, Ping Huang, Jiulong Shan, Huimin Ma, Jian Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern diffusion-based image generative models have made significant progress
and become promising to enrich training data for the object detection task.
However, the generation quality and the controllability for complex scenes
containing multi-class objects and dense objects with occlusions remain
limited. This paper presents ODGEN, a novel method to generate high-quality
images conditioned on bounding boxes, thereby facilitating data synthesis for
object detection. Given a domain-specific object detection dataset, we first
fine-tune a pre-trained diffusion model on both cropped foreground objects and
entire images to fit target distributions. Then we propose to control the
diffusion model using synthesized visual prompts with spatial constraints and
object-wise textual descriptions. ODGEN exhibits robustness in handling complex
scenes and specific domains. Further, we design a dataset synthesis pipeline to
evaluate ODGEN on 7 domain-specific benchmarks to demonstrate its
effectiveness. Adding training data generated by ODGEN improves up to 25.3%
mAP@.50:.95 with object detectors like YOLOv5 and YOLOv7, outperforming prior
controllable generative methods. In addition, we design an evaluation protocol
based on COCO-2014 to validate ODGEN in general domains and observe an
advantage up to 5.6% in mAP@.50:.95 against existing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-11-04T00:00:00Z">2024-11-04</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">116</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Prompt</span>ing with Phonemes: Enhancing LLM Multilinguality for non-Latin
  Script Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02398v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02398v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hoang Nguyen, Khyati Mahajan, Vikas Yadav, Philip S. Yu, Masoud Hashemi, Rishabh Maheshwary
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multilingual LLMs have achieved remarkable benchmark performance, but we find
they continue to underperform on non-Latin script languages across contemporary
LLM families. This discrepancy arises from the fact that LLMs are pretrained
with orthographic scripts, which are dominated by Latin characters that obscure
their shared phonology with non-Latin scripts. We propose leveraging phonemic
transcriptions as complementary signals to induce script-invariant
representations. Our study demonstrates that integrating phonemic signals
improves performance across both non-Latin and Latin languages, with a
particularly significant impact on closing the performance gap between the two.
Through detailed experiments, we show that phonemic and orthographic scripts
retrieve distinct examples for in-context learning (ICL). This motivates our
proposed Mixed-ICL retrieval strategy, where further aggregation leads to our
significant performance improvements for both Latin script languages (up to
12.6%) and non-Latin script languages (up to 15.1%) compared to randomized ICL
retrieval.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attacking <span class="highlight-title">Vision-Language</span> Computer Agents via Pop-ups 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02391v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02391v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanzhe Zhang, Tao Yu, Diyi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous agents powered by large vision and language models (VLM) have
demonstrated significant potential in completing daily computer tasks, such as
browsing the web to book travel and operating desktop software, which requires
agents to understand these interfaces. Despite such visual inputs becoming more
integrated into agentic applications, what types of risks and attacks exist
around them still remain unclear. In this work, we demonstrate that VLM agents
can be easily attacked by a set of carefully designed adversarial pop-ups,
which human users would typically recognize and ignore. This distraction leads
agents to click these pop-ups instead of performing the tasks as usual.
Integrating these pop-ups into existing agent testing environments like OSWorld
and VisualWebArena leads to an attack success rate (the frequency of the agent
clicking the pop-ups) of 86% on average and decreases the task success rate by
47%. Basic defense techniques such as asking the agent to ignore pop-ups or
including an advertisement notice, are ineffective against the attack.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Scientific Hypothesis Generation with Knowledge Grounded Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02382v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02382v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangzhi Xiong, Eric Xie, Amir Hassan Shariatmadari, Sikun Guo, Stefan Bekiranov, Aidong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated remarkable capabilities in
various scientific domains, from natural language processing to complex
problem-solving tasks. Their ability to understand and generate human-like text
has opened up new possibilities for advancing scientific research, enabling
tasks such as data analysis, literature review, and even experimental design.
One of the most promising applications of LLMs in this context is hypothesis
generation, where they can identify novel research directions by analyzing
existing knowledge. However, despite their potential, LLMs are prone to
generating ``hallucinations'', outputs that are plausible-sounding but
factually incorrect. Such a problem presents significant challenges in
scientific fields that demand rigorous accuracy and verifiability, potentially
leading to erroneous or misleading conclusions. To overcome these challenges,
we propose KG-CoI (Knowledge Grounded Chain of Ideas), a novel system that
enhances LLM hypothesis generation by integrating external, structured
knowledge from knowledge graphs (KGs). KG-CoI guides LLMs through a structured
reasoning process, organizing their output as a chain of ideas (CoI), and
includes a KG-supported module for the detection of hallucinations. With
experiments on our newly constructed hypothesis generation dataset, we
demonstrate that KG-CoI not only improves the accuracy of LLM-generated
hypotheses but also reduces the hallucination in their reasoning chains,
highlighting its effectiveness in advancing real-world scientific research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Large Language Models generalize analogy solving like people can? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02348v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02348v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Claire E. Stevenson, Alexandra Pafford, Han L. J. van der Maas, Melanie Mitchell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When we solve an analogy we transfer information from a known context to a
new one through abstract rules and relational similarity. In people, the
ability to solve analogies such as "body : feet :: table : ?" emerges in
childhood, and appears to transfer easily to other domains, such as the visual
domain "( : ) :: < : ?". Recent research shows that large language models
(LLMs) can solve various forms of analogies. However, can LLMs generalize
analogy solving to new domains like people can? To investigate this, we had
children, adults, and LLMs solve a series of letter-string analogies (e.g., a b
: a c :: j k : ?) in the Latin alphabet, in a near transfer domain (Greek
alphabet), and a far transfer domain (list of symbols). As expected, children
and adults easily generalized their knowledge to unfamiliar domains, whereas
LLMs did not. This key difference between human and AI performance is evidence
that these LLMs still struggle with robust human-like analogical transfer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Seq-VCR: Preventing Collapse in Intermediate Transformer Representations
  for Enhanced Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02344v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02344v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Rifat Arefin, Gopeshh Subbaraj, Nicolas Gontier, <span class="highlight-author">Yann LeCun</span>, Irina Rish, Ravid Shwartz-Ziv, Christopher Pal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decoder-only Transformers often struggle with complex reasoning tasks,
particularly arithmetic reasoning requiring multiple sequential operations. In
this work, we identify representation collapse in the model's intermediate
layers as a key factor limiting their reasoning capabilities. To address this,
we propose Sequential Variance-Covariance Regularization (Seq-VCR), which
enhances the entropy of intermediate representations and prevents collapse.
Combined with dummy pause tokens as substitutes for chain-of-thought (CoT)
tokens, our method significantly improves performance in arithmetic reasoning
problems. In the challenging $5 \times 5$ integer multiplication task, our
approach achieves $99.5\%$ exact match accuracy, outperforming models of the
same size (which yield $0\%$ accuracy) and GPT-4 with five-shot CoT prompting
($44\%$). We also demonstrate superior results on arithmetic expression and
longest increasing subsequence (LIS) datasets. Our findings highlight the
importance of preventing intermediate layer representation collapse to enhance
the reasoning capabilities of Transformers and show that Seq-VCR offers an
effective solution without requiring explicit CoT supervision.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02337v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02337v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zehan Qi, Xiao Liu, Iat Long Iong, Hanyu Lai, Xueqiao Sun, Xinyue Yang, Jiadai Sun, Yu Yang, Shuntian Yao, Tianjie Zhang, Wei Xu, Jie Tang, Yuxiao Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown remarkable potential as autonomous
agents, particularly in web-based tasks. However, existing LLM web agents
heavily rely on expensive proprietary LLM APIs, while open LLMs lack the
necessary decision-making capabilities. This paper introduces WebRL, a
self-evolving online curriculum reinforcement learning framework designed to
train high-performance web agents using open LLMs. WebRL addresses three key
challenges in building LLM web agents, including the scarcity of training
tasks, sparse feedback signals, and policy distribution drift in online
learning. Specifically, WebRL incorporates 1) a self-evolving curriculum that
generates new tasks from unsuccessful attempts, 2) a robust outcome-supervised
reward model (ORM), and 3) adaptive reinforcement learning strategies to ensure
consistent improvements. We apply WebRL to transform open Llama-3.1 and GLM-4
models into proficient web agents. On WebArena-Lite, WebRL improves the success
rate of Llama-3.1-8B from 4.8% to 42.4%, and from 6.1% to 43% for GLM-4-9B.
These open models significantly surpass the performance of GPT-4-Turbo (17.6%)
and GPT-4o (13.9%) and outperform previous state-of-the-art web agents trained
on open LLMs (AutoWebGLM, 18.2%). Our findings demonstrate WebRL's
effectiveness in bridging the gap between open and proprietary LLM-based web
agents, paving the way for more accessible and powerful autonomous web
interaction systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sparsing Law: Towards Large Language Models with Greater Activation
  Sparsity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02335v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02335v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqi Luo, Chenyang Song, Xu Han, Yingfa Chen, Chaojun Xiao, Zhiyuan Liu, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Activation sparsity denotes the existence of substantial weakly-contributed
elements within activation outputs that can be eliminated, benefiting many
important applications concerned with large language models (LLMs). Although
promoting greater activation sparsity within LLMs deserves deep studies,
existing works lack comprehensive and quantitative research on the correlation
between activation sparsity and potentially influential factors. In this paper,
we present a comprehensive study on the quantitative scaling properties and
influential factors of the activation sparsity within decoder-only
Transformer-based LLMs. Specifically, we propose PPL-$p\%$ sparsity, a precise
and performance-aware activation sparsity metric that is applicable to any
activation function. Through extensive experiments, we find several important
phenomena. Firstly, different activation functions exhibit comparable
performance but opposite training-time sparsity trends. The activation ratio
(i.e., $1-\mathrm{sparsity\ ratio}$) evolves as a convergent increasing
power-law and decreasing logspace power-law with the amount of training data
for SiLU-activated and ReLU-activated LLMs, respectively. These demonstrate
that ReLU is more efficient as the activation function than SiLU and can
leverage more training data to improve activation sparsity. Secondly, the
activation ratio linearly increases with the width-depth ratio below a certain
bottleneck point, indicating the potential advantage of a deeper architecture
at a fixed parameter scale. Finally, at similar width-depth ratios, we
surprisingly find that the limit value of activation sparsity varies weakly
with the parameter scale, i.e., the activation patterns within LLMs are
insensitive to the parameter scale. These empirical laws towards LLMs with
greater activation sparsity have important implications for making LLMs more
efficient and interpretable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 13 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Creative Short Story Generation in Humans and Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02316v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02316v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mete Ismayilzada, Claire Stevenson, Lonneke van der Plas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Storytelling is a fundamental aspect of human communication, relying heavily
on creativity to produce narratives that are novel, appropriate, and
surprising. While large language models (LLMs) have recently demonstrated the
ability to generate high-quality stories, their creative capabilities remain
underexplored. Previous research has either focused on creativity tests
requiring short responses or primarily compared model performance in story
generation to that of professional writers. However, the question of whether
LLMs exhibit creativity in writing short stories on par with the average human
remains unanswered. In this work, we conduct a systematic analysis of
creativity in short story generation across LLMs and everyday people. Using a
five-sentence creative story task, commonly employed in psychology to assess
human creativity, we automatically evaluate model- and human-generated stories
across several dimensions of creativity, including novelty, surprise, and
diversity. Our findings reveal that while LLMs can generate stylistically
complex stories, they tend to fall short in terms of creativity when compared
to average human writers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MdEval: Massively Multilingual Code Debugging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02310v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02310v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shukai Liu, Linzheng Chai, Jian Yang, Jiajun Shi, He Zhu, Liran Wang, Ke Jin, Wei Zhang, Hualei Zhu, Shuyue Guo, Tao Sun, Jiaheng Liu, Yunlong Duan, Yu Hao, Liqun Yang, Guanglin Niu, Ge Zhang, Zhoujun Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Code large language models (LLMs) have made significant progress in code
debugging by directly generating the correct code based on the buggy code
snippet. Programming benchmarks, typically consisting of buggy code snippet and
their associated test cases, are used to assess the debugging capabilities of
LLMs. However, many existing benchmarks primarily focus on Python and are often
limited in terms of language diversity (e.g., DebugBench and DebugEval). To
advance the field of multilingual debugging with LLMs, we propose the first
massively multilingual debugging benchmark, which includes 3.6K test samples of
18 programming languages and covers the automated program repair (APR) task,
the code review (CR) task, and the bug identification (BI) task. Further, we
introduce the debugging instruction corpora MDEVAL-INSTRUCT by injecting bugs
into the correct multilingual queries and solutions (xDebugGen). Further, a
multilingual debugger xDebugCoder trained on MDEVAL-INSTRUCT as a strong
baseline specifically to handle the bugs of a wide range of programming
languages (e.g. "Missing Mut" in language Rust and "Misused Macro Definition"
in language C). Our extensive experiments on MDEVAL reveal a notable
performance gap between open-source models and closed-source LLMs (e.g., GPT
and Claude series), highlighting huge room for improvement in multilingual code
debugging scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CRMArena: Understanding the Capacity of LLM Agents to Perform
  Professional CRM Tasks in Realistic Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02305v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02305v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kung-Hsiang Huang, Akshara Prabhakar, Sidharth Dhawan, Yixin Mao, Huan Wang, Silvio Savarese, Caiming Xiong, Philippe Laban, Chien-Sheng Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Customer Relationship Management (CRM) systems are vital for modern
enterprises, providing a foundation for managing customer interactions and
data. Integrating AI agents into CRM systems can automate routine processes and
enhance personalized service. However, deploying and evaluating these agents is
challenging due to the lack of realistic benchmarks that reflect the complexity
of real-world CRM tasks. To address this issue, we introduce CRMArena, a novel
benchmark designed to evaluate AI agents on realistic tasks grounded in
professional work environments. Following guidance from CRM experts and
industry best practices, we designed CRMArena with nine customer service tasks
distributed across three personas: service agent, analyst, and manager. The
benchmark includes 16 commonly used industrial objects (e.g., account, order,
knowledge article, case) with high interconnectivity, along with latent
variables (e.g., complaint habits, policy violations) to simulate realistic
data distributions. Experimental results reveal that state-of-the-art LLM
agents succeed in less than 40% of the tasks with ReAct prompting, and less
than 55% even with function-calling abilities. Our findings highlight the need
for enhanced agent capabilities in function-calling and rule-following to be
deployed in real-world work environments. CRMArena is an open challenge to the
community: systems that can reliably complete tasks showcase direct business
value in a popular work environment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The LLM Language Network: A Neuroscientific Approach for Identifying
  Causally Task-Relevant Units 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02280v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02280v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Badr AlKhamissi, Greta Tuckute, Antoine Bosselut, Martin Schrimpf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) exhibit remarkable capabilities on not just
language tasks, but also various tasks that are not linguistic in nature, such
as logical reasoning and social inference. In the human brain, neuroscience has
identified a core language system that selectively and causally supports
language processing. We here ask whether similar specialization for language
emerges in LLMs. We identify language-selective units within 18 popular LLMs,
using the same localization approach that is used in neuroscience. We then
establish the causal role of these units by demonstrating that ablating LLM
language-selective units -- but not random units -- leads to drastic deficits
in language tasks. Correspondingly, language-selective LLM units are more
aligned to brain recordings from the human language system than random units.
Finally, we investigate whether our localization method extends to other
cognitive domains: while we find specialized networks in some LLMs for
reasoning and social capabilities, there are substantial differences among
models. These findings provide functional and causal evidence for
specialization in large language models, and highlight parallels with the
functional organization in the brain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Combining Induction and Transduction for Abstract Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02272v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02272v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wen-Ding Li, Keya Hu, Carter Larsen, Yuqing Wu, Simon Alford, Caleb Woo, Spencer M. Dunn, Hao Tang, Michelangelo Naim, Dat Nguyen, Wei-Long Zheng, Zenna Tavares, Yewen Pu, Kevin Ellis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When learning an input-output mapping from very few examples, is it better to
first infer a latent function that explains the examples, or is it better to
directly predict new test outputs, e.g. using a neural network? We study this
question on ARC, a highly diverse dataset of abstract reasoning tasks. We train
neural models for induction (inferring latent functions) and transduction
(directly predicting the test output for a given test input). Our models are
trained on synthetic data generated by prompting LLMs to produce Python code
specifying a function to be inferred, plus a stochastic subroutine for
generating inputs to that function. We find inductive and transductive models
solve very different problems, despite training on the same problems, and
despite sharing the same neural architecture.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated
  Parameters by Tencent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02265v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02265v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingwu Sun, Yanfeng Chen, Yiqing Huang, Ruobing Xie, Jiaqi Zhu, Kai Zhang, Shuaipeng Li, Zhen Yang, Jonny Han, Xiaobo Shu, Jiahao Bu, Zhongzhi Chen, Xuemeng Huang, Fengzong Lian, Saiyong Yang, Jianfeng Yan, Yuyuan Zeng, Xiaoqin Ren, Chao Yu, Lulu Wu, Yue Mao, Tao Yang, Suncong Zheng, Kan Wu, Dian Jiao, Jinbao Xue, Xipeng Zhang, Decheng Wu, Kai Liu, Dengpeng Wu, Guanghui Xu, Shaohua Chen, Shuang Chen, Xiao Feng, Yigeng Hong, Junqiang Zheng, Chengcheng Xu, Zongwei Li, Xiong Kuang, Jianglu Hu, Yiqi Chen, Yuchi Deng, Guiyang Li, Ao Liu, Chenchen Zhang, Shihui Hu, Zilong Zhao, Zifan Wu, Yao Ding, Weichao Wang, Han Liu, Roberts Wang, Hao Fei, Peijie She, Ze Zhao, Xun Cao, Hai Wang, Fusheng Xiang, Mengyuan Huang, Zhiyuan Xiong, Bin Hu, Xuebin Hou, Lei Jiang, Jiajia Wu, Yaping Deng, Yi Shen, Qian Wang, Weijie Liu, Jie Liu, Meng Chen, Liang Dong, Weiwen Jia, Hu Chen, Feifei Liu, Rui Yuan, Huilin Xu, Zhenxiang Yan, Tengfei Cao, Zhichao Hu, Xinhua Feng, Dong Du, Tinghao She, Yangyu Tao, Feng Zhang, Jianchen Zhu, Chengzhong Xu, Xirui Li, Chong Zha, Wen Ouyang, Yinben Xia, Xiang Li, Zekun He, Rongpeng Chen, Jiawei Song, Ruibin Chen, Fan Jiang, Chongqing Zhao, Bo Wang, Hao Gong, Rong Gan, Winston Hu, Zhanhui Kang, Yong Yang, Yuhong Liu, Di Wang, Jie Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce Hunyuan-Large, which is currently the largest
open-source Transformer-based mixture of experts model, with a total of 389
billion parameters and 52 billion activation parameters, capable of handling up
to 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior
performance across various benchmarks including language understanding and
generation, logical reasoning, mathematical problem-solving, coding,
long-context, and aggregated tasks, where it outperforms LLama3.1-70B and
exhibits comparable performance when compared to the significantly larger
LLama3.1-405B model. Key practice of Hunyuan-Large include large-scale
synthetic data that is orders larger than in previous literature, a mixed
expert routing strategy, a key-value cache compression technique, and an
expert-specific learning rate strategy. Additionally, we also investigate the
scaling laws and learning rate schedule of mixture of experts models, providing
valuable insights and guidances for future model development and optimization.
The code and checkpoints of Hunyuan-Large are released to facilitate future
innovations and applications.
  Codes: https://github.com/Tencent/Hunyuan-Large
  Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 4 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Positive Experience Reflection for Agents in Interactive Text
  Environments <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02223v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02223v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philip Lippmann, Matthijs T. J. Spaan, Jie Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intelligent agents designed for interactive environments face significant
challenges in text-based games, a domain that demands complex reasoning and
adaptability. While agents based on large language models (LLMs) using
self-reflection have shown promise, they struggle when initially successful and
exhibit reduced effectiveness when using smaller LLMs. We introduce Sweet&Sour,
a novel approach that addresses these limitations in existing reflection
methods by incorporating positive experiences and managed memory to enrich the
context available to the agent at decision time. Our comprehensive analysis
spans both closed- and open-source LLMs and demonstrates the effectiveness of
Sweet&Sour in improving agent performance, particularly in scenarios where
previous approaches fall short.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at NeurIPS 2024 Language Gamification workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Role of DevOps in Enhancing Enterprise Software Delivery Success
  through R&D Efficiency and Source Code Management 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02209v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02209v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study examines the impact of DevOps practices on enterprise software
delivery success, focusing on enhancing R&D efficiency and source code
management (SCM). Using a qualitative methodology, data were collected from
case studies of large-scale enterprises implementing DevOps to explore how
these practices streamline software development processes. Findings reveal that
DevOps significantly improves R&D productivity by fostering cross-functional
collaboration, reducing development cycle times, and enhancing software quality
through effective SCM practices, such as version control and continuous
integration. Additionally, SCM tools within DevOps enable precise change
tracking and reliable code maintenance, further supporting faster, more robust
software delivery. However, the study identifies challenges, including cultural
resistance and tool integration issues, that can hinder DevOps implementation.
Additionally, This research contributes to the growing body of DevOps
literature by highlighting the role of R&D efficiency and SCM as crucial
factors for software delivery success. Future studies should investigate these
factors across diverse industries to validate findings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Steering Vectors by Targeting Sparse Autoencoder Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02193v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02193v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sviatoslav Chalnev, Matthew Siu, Arthur Conmy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To control the behavior of language models, steering methods attempt to
ensure that outputs of the model satisfy specific pre-defined properties.
Adding steering vectors to the model is a promising method of model control
that is easier than finetuning, and may be more robust than prompting. However,
it can be difficult to anticipate the effects of steering vectors produced by
almost all existing methods, such as CAA (Panickssery et al., 2024) or the
direct use of SAE latents (Templeton et al., 2024). In our work, we address
this issue by using SAEs to measure the effects of steering vectors, giving us
a method that can be used to understand the causal effect of any steering
vector intervention. We use this method for measuring causal effects to develop
an improved steering method, SAE-Targeted Steering (SAE-TS), which finds
steering vectors to target specific SAE features while minimizing unintended
side effects. We show that overall, SAE-TS balances steering effects with
coherence better than CAA and SAE feature steering, when evaluated on a range
of tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 maintext pages and 9 appendix pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Grounding Emotional Descriptions to Electrovibration Haptic Signals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02118v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02118v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guimin Hu, Zirui Zhao, Lukas Heilmann, Yasemin Vardar, Hasti Seifi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designing and displaying haptic signals with sensory and emotional attributes
can improve the user experience in various applications. Free-form user
language provides rich sensory and emotional information for haptic design
(e.g., ``This signal feels smooth and exciting''), but little work exists on
linking user descriptions to haptic signals (i.e., language grounding). To
address this gap, we conducted a study where 12 users described the feel of 32
signals perceived on a surface haptics (i.e., electrovibration) display. We
developed a computational pipeline using natural language processing (NLP)
techniques, such as GPT-3.5 Turbo and word embedding methods, to extract
sensory and emotional keywords and group them into semantic clusters (i.e.,
concepts). We linked the keyword clusters to haptic signal features (e.g.,
pulse count) using correlation analysis. The proposed pipeline demonstrates the
viability of a computational approach to analyzing haptic experiences. We
discuss our future plans for creating a predictive model of haptic experience.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AVSS: Layer Importance Evaluation in Large Language Models via
  Activation Variance-Sparsity Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02117v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02117v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zichen Song, Yuxin Wu, Sitan Huang, Zhongfeng Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evaluation of layer importance in deep learning has been an active area
of research, with significant implications for model optimization and
interpretability. Recently, large language models (LLMs) have gained prominence
across various domains, yet limited studies have explored the functional
importance and performance contributions of individual layers within LLMs,
especially from the perspective of activation distribution. In this work, we
propose the Activation Variance-Sparsity Score (AVSS), a novel metric combining
normalized activation variance and sparsity to assess each layer's contribution
to model performance. By identifying and removing approximately the lowest 25%
of layers based on AVSS, we achieve over 90% of original model performance
across tasks such as question answering, language modeling, and sentiment
classification, indicating that these layers may be non-essential. Our approach
provides a systematic method for identifying less critical layers, contributing
to efficient large language model architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancements and limitations of LLMs in replicating human color-word
  associations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02116v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02116v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Makoto Fukushima, Shusuke Eshita, Hiroshige Fukuhara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Color-word associations play a fundamental role in human cognition and design
applications. Large Language Models (LLMs) have become widely available and
demonstrated intelligent behaviors in various benchmarks with natural
conversation skills. However, their ability to replicate human color-word
associations remains understudied. We compared multiple generations of LLMs
(from GPT-3 to GPT- 4o) against human color-word associations using data
collected from over 10,000 Japanese participants, involving 17 colors and words
from eight categories in Japanese. Our findings reveal a clear progression in
LLM performance across generations, with GPT-4o achieving the highest accuracy
in predicting the best voted word for each color and category, particularly
when using visual inputs rather than text-based color codes. However, the
highest median performance was approximately 50% even for GPT4-o with visual
inputs (chance level is 10%), and the performance levels varied significantly
across word categories and colors, indicating a failure to fully replicate
human color-word associations. On the other hand, color discrimination ability
estimated from our color-word association data showed that LLMs demonstrated
high correlation with human color discrimination patterns, similarly to
previous studies. Our study highlights both the advancements in LLM
capabilities and their persistent limitations, suggesting differences in
semantic memory structures between humans and LLMs in representing color-word
associations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 7 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Regress, Don't Guess -- A Regression-like Loss on Number Tokens for
  Language Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02083v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02083v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonas Zausinger, Lars Pennig, Kacper Chlodny, Vincent Limbach, Anna Ketteler, Thorben Prein, Vishwa Mohan Singh, Michael Morris Danziger, Jannis Born
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While language models have exceptional capabilities at text generation, they
lack a natural inductive bias for emitting numbers and thus struggle in tasks
involving reasoning over quantities, especially arithmetics. This has
particular relevance in scientific datasets where combinations of text and
numerical data are abundant. One fundamental limitation is the nature of the CE
loss, which assumes a nominal (categorical) scale and thus cannot convey
proximity between generated number tokens. As a remedy, we here present two
versions of a number token loss. The first is based on an $L_p$ loss between
the ground truth token value and the weighted sum of the predicted class
probabilities. The second loss minimizes the Wasserstein-1 distance between the
distribution of the predicted output probabilities and the ground truth
distribution. These regression-like losses can easily be added to any language
model and extend the CE objective during training. We compare the proposed
schemes on a mathematics dataset against existing tokenization, encoding, and
decoding schemes for improving number representation in language models. Our
results reveal a significant improvement in numerical accuracy when equipping a
standard T5 model with the proposed loss schemes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5-page version for NeurIPS 2024 (MathAI workshop)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable Efficient Training of Large Language Models with
  Low-dimensional Projected Attention <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02063v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02063v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingtai Lv, Ning Ding, Kaiyan Zhang, Ermo Hua, Ganqu Cui, Bowen Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Improving the effectiveness and efficiency of large language models (LLMs)
simultaneously is a critical yet challenging research goal. In this paper, we
find that low-rank pre-training, normally considered as efficient methods that
will compromise performance, can be scalably effective when reduced parameters
are precisely targeted. Specifically, applying the low-dimensional module only
to the attention layer -- resolves this issue and enhances both effectiveness
and efficiency. We refer to this structure as Low-dimensional Projected
Attention (LPA) and provide an explanatory analysis. Through extensive
experimentation at parameter scales of 130M, 370M, and scaling up to 3B, we
have validated the effectiveness and scalability of LPA. Our results show that
LPA model can save up to 12.4% in time while achieving an approximate 5%
improvement in test perplexity (ppl) and on downstream tasks compared with the
vanilla Transformer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 (Main Conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explainable cognitive decline detection in free dialogues with a Machine
  Learning approach based on pre-trained Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02036v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02036v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francisco de Arriba-Pérez, Silvia García-Méndez, Javier Otero-Mosquera, Francisco J. González-Castaño
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cognitive and neurological impairments are very common, but only a small
proportion of affected individuals are diagnosed and treated, partly because of
the high costs associated with frequent screening. Detecting pre-illness stages
and analyzing the progression of neurological disorders through effective and
efficient intelligent systems can be beneficial for timely diagnosis and early
intervention. We propose using Large Language Models to extract features from
free dialogues to detect cognitive decline. These features comprise high-level
reasoning content-independent features (such as comprehension, decreased
awareness, increased distraction, and memory problems). Our solution comprises
(i) preprocessing, (ii) feature engineering via Natural Language Processing
techniques and prompt engineering, (iii) feature analysis and selection to
optimize performance, and (iv) classification, supported by automatic
explainability. We also explore how to improve Chatgpt's direct cognitive
impairment prediction capabilities using the best features in our models.
Evaluation metrics obtained endorse the effectiveness of a mixed approach
combining feature extraction with Chatgpt and a specialized Machine Learning
model to detect cognitive decline within free-form conversational dialogues
with older adults. Ultimately, our work may facilitate the development of an
inexpensive, non-invasive, and rapid means of detecting and explaining
cognitive decline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Shortcut Learning in In-Context Learning: A Survey 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02018v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02018v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Song, Yingji Li, Fausto Giunchiglia, Hao Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Shortcut learning refers to the phenomenon where models employ simple,
non-robust decision rules in practical tasks, which hinders their
generalization and robustness. With the rapid development of large language
models (LLMs) in recent years, an increasing number of studies have shown the
impact of shortcut learning on LLMs. This paper provides a novel perspective to
review relevant research on shortcut learning in In-Context Learning (ICL). It
conducts a detailed exploration of the types of shortcuts in ICL tasks, their
causes, available benchmarks, and strategies for mitigating shortcuts. Based on
corresponding observations, it summarizes the unresolved issues in existing
research and attempts to outline the future research landscape of shortcut
learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Culinary Class Wars: Evaluating LLMs using ASH in Cuisine Transfer Task 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01996v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01996v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hoonick Lee, Mogan Gim, Donghyeon Park, Donghee Choi, Jaewoo Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of Large Language Models (LLMs) have shown promise in various
creative domains, including culinary arts. However, many LLMs still struggle to
deliver the desired level of culinary creativity, especially when tasked with
adapting recipes to meet specific cultural requirements. This study focuses on
cuisine transfer-applying elements of one cuisine to another-to assess LLMs'
culinary creativity. We employ a diverse set of LLMs to generate and evaluate
culturally adapted recipes, comparing their evaluations against LLM and human
judgments. We introduce the ASH (authenticity, sensitivity, harmony) benchmark
to evaluate LLMs' recipe generation abilities in the cuisine transfer task,
assessing their cultural accuracy and creativity in the culinary domain. Our
findings reveal crucial insights into both generative and evaluative
capabilities of LLMs in the culinary domain, highlighting strengths and
limitations in understanding and applying cultural nuances in recipe creation.
The code and dataset used in this project will be openly available in
\url{http://github.com/dmis-lab/CulinaryASH}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Language Models Learn to Skip Steps? <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01855v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01855v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tengxiao Liu, Qipeng Guo, Xiangkun Hu, Cheng Jiayang, Yue Zhang, Xipeng Qiu, Zheng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trained on vast corpora of human language, language models demonstrate
emergent human-like reasoning abilities. Yet they are still far from true
intelligence, which opens up intriguing opportunities to explore the parallels
of humans and model behaviors. In this work, we study the ability to skip steps
in reasoning - a hallmark of human expertise developed through practice. Unlike
humans, who may skip steps to enhance efficiency or to reduce cognitive load,
models do not inherently possess such motivations to minimize reasoning steps.
To address this, we introduce a controlled framework that stimulates
step-skipping behavior by iteratively refining models to generate shorter and
accurate reasoning paths. Empirical results indicate that models can develop
the step skipping ability under our guidance. Moreover, after fine-tuning on
expanded datasets that include both complete and skipped reasoning sequences,
the models can not only resolve tasks with increased efficiency without
sacrificing accuracy, but also exhibit comparable and even enhanced
generalization capabilities in out-of-domain scenarios. Our work presents the
first exploration into human-like step-skipping ability and provides fresh
perspectives on how such cognitive abilities can benefit AI models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Label Semantics and Meta-Label Refinement for Multi-Label
  Question Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01841v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01841v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shi Dong, Xiaobei Niu, Rui Zhong, Zhifeng Wang, Mingzhang Zuo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate annotation of educational resources is critical in the rapidly
advancing field of online education due to the complexity and volume of
content. Existing classification methods face challenges with semantic overlap
and distribution imbalance of labels in the multi-label context, which impedes
effective personalized learning and resource recommendation. This paper
introduces RR2QC, a novel Retrieval Reranking method To multi-label Question
Classification by leveraging label semantics and meta-label refinement.
Firstly, RR2QC leverages semantic relationships within and across label groups
to enhance pre-training strategie in multi-label context. Next, a class center
learning task is introduced, integrating label texts into downstream training
to ensure questions consistently align with label semantics, retrieving the
most relevant label sequences. Finally, this method decomposes labels into
meta-labels and trains a meta-label classifier to rerank the retrieved label
sequences. In doing so, RR2QC enhances the understanding and prediction
capability of long-tail labels by learning from meta-labels frequently
appearing in other labels. Addtionally, a Math LLM is used to generate
solutions for questions, extracting latent information to further refine the
model's insights. Experimental results demonstrate that RR2QC outperforms
existing classification methods in Precision@k and F1 scores across multiple
educational datasets, establishing it as a potent enhancement for online
educational content utilization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TriG-NER: Triplet-Grid Framework for Discontinuous Named Entity
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01839v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01839v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rina Carines Cabral, Soyeon Caren Han, Areej Alhassan, Riza Batista-Navarro, Goran Nenadic, Josiah Poon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discontinuous Named Entity Recognition (DNER) presents a challenging problem
where entities may be scattered across multiple non-adjacent tokens, making
traditional sequence labelling approaches inadequate. Existing methods
predominantly rely on custom tagging schemes to handle these discontinuous
entities, resulting in models tightly coupled to specific tagging strategies
and lacking generalisability across diverse datasets. To address these
challenges, we propose TriG-NER, a novel Triplet-Grid Framework that introduces
a generalisable approach to learning robust token-level representations for
discontinuous entity extraction. Our framework applies triplet loss at the
token level, where similarity is defined by word pairs existing within the same
entity, effectively pulling together similar and pushing apart dissimilar ones.
This approach enhances entity boundary detection and reduces the dependency on
specific tagging schemes by focusing on word-pair relationships within a
flexible grid structure. We evaluate TriG-NER on three benchmark DNER datasets
and demonstrate significant improvements over existing grid-based
architectures. These results underscore our framework's effectiveness in
capturing complex entity structures and its adaptability to various tagging
schemes, setting a new benchmark for discontinuous entity extraction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code will be made available upon publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Align-SLM: Textless Spoken Language Models with Reinforcement Learning
  from AI Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01834v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01834v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guan-Ting Lin, Prashanth Gurunath Shivakumar, Aditya Gourav, Yile Gu, Ankur Gandhe, Hung-yi Lee, Ivan Bulyko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While textless Spoken Language Models (SLMs) have shown potential in
end-to-end speech-to-speech modeling, they still lag behind text-based Large
Language Models (LLMs) in terms of semantic coherence and relevance. This work
introduces the Align-SLM framework, which leverages preference optimization
inspired by Reinforcement Learning with AI Feedback (RLAIF) to enhance the
semantic understanding of SLMs. Our approach generates multiple speech
continuations from a given prompt and uses semantic metrics to create
preference data for Direct Preference Optimization (DPO). We evaluate the
framework using ZeroSpeech 2021 benchmarks for lexical and syntactic modeling,
the spoken version of the StoryCloze dataset for semantic coherence, and other
speech generation metrics, including the GPT4-o score and human evaluation.
Experimental results show that our method achieves state-of-the-art performance
for SLMs on most benchmarks, highlighting the importance of preference
optimization to improve the semantics of SLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Pedagogical LLMs with Supervised Fine <span class="highlight-title">Tuning</span> for Computing
  Education 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01765v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01765v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandra Vassar, Jake Renzella, Emily Ross, Andrew Taylor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates supervised fine-tuning of large language models
(LLMs) to improve their pedagogical alignment in computing education,
addressing concerns that LLMs may hinder learning outcomes. The project
utilised a proprietary dataset of 2,500 high quality question/answer pairs from
programming course forums, and explores two research questions: the suitability
of university course forums in contributing to fine-tuning datasets, and how
supervised fine-tuning can improve LLMs' alignment with educational principles
such as constructivism. Initial findings suggest benefits in pedagogical
alignment of LLMs, with deeper evaluations required.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>3 pages, 1 table, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAGViz: Diagnose and Visualize Retrieval-Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01751v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01751v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tevin Wang, Jingyuan He, Chenyan Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) combines knowledge from domain-specific
sources into large language models to ground answer generation. Current RAG
systems lack customizable visibility on the context documents and the model's
attentiveness towards such documents. We propose RAGViz, a RAG diagnosis tool
that visualizes the attentiveness of the generated tokens in retrieved
documents. With a built-in user interface, retrieval index, and Large Language
Model (LLM) backbone, RAGViz provides two main functionalities: (1) token and
document-level attention visualization, and (2) generation comparison upon
context document addition and removal. As an open-source toolkit, RAGViz can be
easily hosted with a custom embedding model and HuggingFace-supported LLM
backbone. Using a hybrid ANN (Approximate Nearest Neighbor) index,
memory-efficient LLM inference tool, and custom context snippet method, RAGViz
operates efficiently with a median query time of about 5 seconds on a moderate
GPU node. Our code is available at https://github.com/cxcscmu/RAGViz. A demo
video of RAGViz can be found at https://youtu.be/cTAbuTu6ur4.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DynaSaur: Large Language Agents Beyond Predefined Actions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01747v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01747v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dang Nguyen, Viet Dac Lai, Seunghyun Yoon, Ryan A. Rossi, Handong Zhao, Ruiyi Zhang, Puneet Mathur, Nedim Lipka, Yu Wang, Trung Bui, Franck Dernoncourt, Tianyi Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing LLM agent systems typically select actions from a fixed and
predefined set at every step. While this approach is effective in closed,
narrowly-scoped environments, we argue that it presents two major challenges
when deploying LLM agents in real-world scenarios: (1) selecting from a fixed
set of actions significantly restricts the planning and acting capabilities of
LLM agents, and (2) this approach requires substantial human effort to
enumerate and implement all possible actions, which becomes impractical in
complex environments with a vast number of potential actions. In this work, we
propose an LLM agent framework that enables the dynamic creation and
composition of actions in an online manner. In this framework, the agent
interacts with the environment by generating and executing programs written in
a general-purpose programming language at each step. Furthermore, generated
actions are accumulated over time for future reuse. Our extensive experiments
on the GAIA benchmark demonstrate that this framework offers significantly
greater flexibility and outperforms previous methods. Notably, it allows an LLM
agent to recover in scenarios where no relevant action exists in the predefined
set or when existing actions fail due to unforeseen edge cases. At the time of
writing, we hold the top position on the GAIA public leaderboard. Our code can
be found in
\href{https://github.com/adobe-research/dynasaur}{https://github.com/adobe-research/dynasaur}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Wave Network: An Ultra-Small Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02674v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02674v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Zhang, Victor S. Sheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose an innovative token representation and update method in a new
ultra-small language model: the Wave network. Specifically, we use a
\textbf{complex vector} to represent each token, encoding both global and local
semantics of the input text. A \textbf{complex vector} consists of two
components: a magnitude vector representing the \textit{global semantics} of
the input text, and a phase vector capturing the \textit{relationships between
individual tokens and global semantics}. Experiments on the AG News text
classification task demonstrate that, when generating complex vectors from
randomly initialized token embeddings, our single-layer Wave Network achieves
90.91\% accuracy with wave interference and 91.66\% with wave modulation --
outperforming a single Transformer layer using BERT pre-trained embeddings by
19.23\% and 19.98\%, respectively, and approaching the accuracy of the
pre-trained and fine-tuned BERT base model (94.64\%). Additionally, compared to
BERT base, the Wave Network reduces video memory usage and training time by
77.34\% and 85.62\% during wave modulation. In summary, we used a
2.4-million-parameter small language model to achieve accuracy comparable to a
100-million-parameter BERT model in text classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zebra-Llama: A Context-Aware Large Language Model for Democratizing Rare
  Disease Knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02657v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02657v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karthik Soman, Andrew Langdon, Catalina Villouta, Chinmay Agrawal, Lashaw Salta, Braian Peetoom, Gianmarco Bellucci, Orion J Buske
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rare diseases present unique challenges in healthcare, often suffering from
delayed diagnosis and fragmented information landscapes. The scarcity of
reliable knowledge in these conditions poses a distinct challenge for Large
Language Models (LLMs) in supporting clinical management and delivering precise
patient information underscoring the need for focused training on these 'zebra'
cases. We present Zebra-Llama, a specialized context-aware language model with
high precision Retrieval Augmented Generation (RAG) capability, focusing on
Ehlers-Danlos Syndrome (EDS) as our case study. EDS, affecting 1 in 5,000
individuals, exemplifies the complexities of rare diseases with its diverse
symptoms, multiple subtypes, and evolving diagnostic criteria. By implementing
a novel context-aware fine-tuning methodology trained on questions derived from
medical literature, patient experiences, and clinical resources, along with
expertly curated responses, Zebra-Llama demonstrates unprecedented capabilities
in handling EDS-related queries. On a test set of real-world questions
collected from EDS patients and clinicians, medical experts evaluated the
responses generated by both models, revealing Zebra-Llama's substantial
improvements over base model (Llama 3.1-8B-Instruct) in thoroughness (77.5% vs.
70.1%), accuracy (83.0% vs. 78.8%), clarity (74.7% vs. 72.0%) and citation
reliability (70.6% vs. 52.3%). Released as an open-source resource, Zebra-Llama
not only provides more accessible and reliable EDS information but also
establishes a framework for developing specialized AI solutions for other rare
conditions. This work represents a crucial step towards democratizing
expert-level knowledge in rare disease management, potentially transforming how
healthcare providers and patients navigate the complex landscape of rare
diseases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 4 figures, 1 supplementary figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comparative Analysis of Counterfactual Explanation Methods for Text
  Classifiers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02643v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02643v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stephen McAleese, Mark Keane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Counterfactual explanations can be used to interpret and debug text
classifiers by producing minimally altered text inputs that change a
classifier's output. In this work, we evaluate five methods for generating
counterfactual explanations for a BERT text classifier on two datasets using
three evaluation metrics. The results of our experiments suggest that
established white-box substitution-based methods are effective at generating
valid counterfactuals that change the classifier's output. In contrast, newer
methods based on large language models (LLMs) excel at producing natural and
linguistically plausible text counterfactuals but often fail to generate valid
counterfactuals that alter the classifier's output. Based on these results, we
recommend developing new counterfactual explanation methods that combine the
strengths of established gradient-based approaches and newer LLM-based
techniques to generate high-quality, valid, and plausible text counterfactual
explanations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extracting Unlearned Information from LLMs with Activation Steering <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02631v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02631v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atakan Seyitoğlu, Aleksei Kuvshinov, Leo Schwinn, Stephan Günnemann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An unintended consequence of the vast pretraining of Large Language Models
(LLMs) is the verbatim memorization of fragments of their training data, which
may contain sensitive or copyrighted information. In recent years, unlearning
has emerged as a solution to effectively remove sensitive knowledge from models
after training. Yet, recent work has shown that supposedly deleted information
can still be extracted by malicious actors through various attacks. Still,
current attacks retrieve sets of possible candidate generations and are unable
to pinpoint the output that contains the actual target information. We propose
activation steering as a method for exact information retrieval from unlearned
LLMs. We introduce a novel approach to generating steering vectors, named
Anonymized Activation Steering. Additionally, we develop a simple word
frequency method to pinpoint the correct answer among a set of candidates when
retrieving unlearned information. Our evaluation across multiple unlearning
techniques and datasets demonstrates that activation steering successfully
recovers general knowledge (e.g., widely known fictional characters) while
revealing limitations in retrieving specific information (e.g., details about
non-public individuals). Overall, our results demonstrate that exact
information retrieval from unlearned models is possible, highlighting a severe
vulnerability of current unlearning techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024 Workshop Safe Generative AI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TeleOracle: Fine-Tuned Retrieval-Augmented Generation with Long-Context
  Support for Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02617v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02617v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nouf Alabbasi, Omar Erak, Omar Alhussein, Ismail Lotfi, Sami Muhaidat, Merouane Debbah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The telecommunications industry's rapid evolution demands intelligent systems
capable of managing complex networks and adapting to emerging technologies.
While large language models (LLMs) show promise in addressing these challenges,
their deployment in telecom environments faces significant constraints due to
edge device limitations and inconsistent documentation. To bridge this gap, we
present TeleOracle, a telecom-specialized retrieval-augmented generation (RAG)
system built on the Phi-2 small language model (SLM). To improve context
retrieval, TeleOracle employs a two-stage retriever that incorporates semantic
chunking and hybrid keyword and semantic search. Additionally, we expand the
context window during inference to enhance the model's performance on
open-ended queries. We also employ low-rank adaption for efficient fine-tuning.
A thorough analysis of the model's performance indicates that our RAG framework
is effective in aligning Phi-2 to the telecom domain in a downstream question
and answer (QnA) task, achieving a 30% improvement in accuracy over the base
Phi-2 model, reaching an overall accuracy of 81.20%. Notably, we show that our
model not only performs on par with the much larger LLMs but also achieves a
higher faithfulness score, indicating higher adherence to the retrieved
context.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Investigating Idiomaticity in Word Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02610v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02610v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei He, Tiago Kramer Vieira, Marcos Garcia, Carolina Scarton, Marco Idiart, Aline Villavicencio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Idiomatic expressions are an integral part of human languages, often used to
express complex ideas in compressed or conventional ways (e.g. eager beaver as
a keen and enthusiastic person). However, their interpretations may not be
straightforwardly linked to the meanings of their individual components in
isolation and this may have an impact for compositional approaches. In this
paper, we investigate to what extent word representation models are able to go
beyond compositional word combinations and capture multiword expression
idiomaticity and some of the expected properties related to idiomatic meanings.
We focus on noun compounds of varying levels of idiomaticity in two languages
(English and Portuguese), presenting a dataset of minimal pairs containing
human idiomaticity judgments for each noun compound at both type and token
levels, their paraphrases and their occurrences in naturalistic and
sense-neutral contexts, totalling 32,200 sentences. We propose this set of
minimal pairs for evaluating how well a model captures idiomatic meanings, and
define a set of fine-grained metrics of Affinity and Scaled Similarity, to
determine how sensitive the models are to perturbations that may lead to
changes in idiomaticity. The results obtained with a variety of representative
and widely used models indicate that, despite superficial indications to the
contrary in the form of high similarities, idiomaticity is not yet accurately
represented in current models. Moreover, the performance of models with
different levels of contextualisation suggests that their ability to capture
context is not yet able to go beyond more superficial lexical clues provided by
the words and to actually incorporate the relevant semantic clues needed for
idiomaticity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FactTest: Factuality Testing in Large Language Models with Statistical
  Guarantees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02603v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02603v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Nie, Xiaotian Hou, Shuhang Lin, James Zou, Huaxiu Yao, Linjun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The propensity of Large Language Models (LLMs) to generate hallucinations and
non-factual content undermines their reliability in high-stakes domains, where
rigorous control over Type I errors (the conditional probability of incorrectly
classifying hallucinations as truthful content) is essential. Despite its
importance, formal verification of LLM factuality with such guarantees remains
largely unexplored. In this paper, we introduce FactTest, a novel framework
that statistically assesses whether an LLM can confidently provide correct
answers to given questions with high-probability correctness guarantees. We
formulate factuality testing as hypothesis testing problem to enforce an upper
bound of Type I errors at user-specified significance levels. Notably, we prove
that our framework also ensures strong Type II error control under mild
conditions and can be extended to maintain its effectiveness when covariate
shifts exist. %These analyses are amenable to the principled NP framework. Our
approach is distribution-free and works for any number of human-annotated
samples. It is model-agnostic and applies to any black-box or white-box LM.
Extensive experiments on question-answering (QA) and multiple-choice benchmarks
demonstrate that \approach effectively detects hallucinations and improves the
model's ability to abstain from answering unknown questions, leading to an over
40% accuracy improvement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vocal Sandbox: Continual Learning and Adaptation for Situated
  Human-Robot Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02599v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02599v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jennifer Grannen, Siddharth Karamcheti, Suvir Mirchandani, Percy Liang, Dorsa Sadigh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Vocal Sandbox, a framework for enabling seamless human-robot
collaboration in situated environments. Systems in our framework are
characterized by their ability to adapt and continually learn at multiple
levels of abstraction from diverse teaching modalities such as spoken dialogue,
object keypoints, and kinesthetic demonstrations. To enable such adaptation, we
design lightweight and interpretable learning algorithms that allow users to
build an understanding and co-adapt to a robot's capabilities in real-time, as
they teach new behaviors. For example, after demonstrating a new low-level
skill for "tracking around" an object, users are provided with trajectory
visualizations of the robot's intended motion when asked to track a new object.
Similarly, users teach high-level planning behaviors through spoken dialogue,
using pretrained language models to synthesize behaviors such as "packing an
object away" as compositions of low-level skills $-$ concepts that can be
reused and built upon. We evaluate Vocal Sandbox in two settings: collaborative
gift bag assembly and LEGO stop-motion animation. In the first setting, we run
systematic ablations and user studies with 8 non-expert participants,
highlighting the impact of multi-level teaching. Across 23 hours of total robot
interaction time, users teach 17 new high-level behaviors with an average of 16
novel low-level skills, requiring 22.1% less active supervision compared to
baselines and yielding more complex autonomous performance (+19.7%) with fewer
failures (-67.1%). Qualitatively, users strongly prefer Vocal Sandbox systems
due to their ease of use (+20.6%) and overall performance (+13.9%). Finally, we
pair an experienced system-user with a robot to film a stop-motion animation;
over two hours of continuous collaboration, the user teaches progressively more
complex motion skills to shoot a 52 second (232 frame) movie.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at CoRL 2024. 24 pages, 8 figures. Project Page:
  https://vocal-sandbox.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ "It's a conversation, not a quiz": A Risk Taxonomy and Reflection Tool
  for LLM Adoption in Public Health 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02594v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02594v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Zhou, Amy Z. Chen, Darshi Shah, Laura Schwab Reese, Munmun De Choudhury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent breakthroughs in large language models (LLMs) have generated both
interest and concern about their potential adoption as accessible information
sources or communication tools across different domains. In public health --
where stakes are high and impacts extend across populations -- adopting LLMs
poses unique challenges that require thorough evaluation. However, structured
approaches for assessing potential risks in public health remain
under-explored. To address this gap, we conducted focus groups with health
professionals and health issue experiencers to unpack their concerns, situated
across three distinct and critical public health issues that demand
high-quality information: vaccines, opioid use disorder, and intimate partner
violence. We synthesize participants' perspectives into a risk taxonomy,
distinguishing and contextualizing the potential harms LLMs may introduce when
positioned alongside traditional health communication. This taxonomy highlights
four dimensions of risk in individual behaviors, human-centered care,
information ecosystem, and technology accountability. For each dimension, we
discuss specific risks and example reflection questions to help practitioners
adopt a risk-reflexive approach. This work offers a shared vocabulary and
reflection tool for experts in both computing and public health to
collaboratively anticipate, evaluate, and mitigate risks in deciding when to
employ LLM capabilities (or not) and how to mitigate harm when they are used.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Geometry of orofacial neuromuscular signals: speech articulation
  decoding using surface electromyography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02591v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02591v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harshavardhana T. Gowda, Zachary D. McNaughton, Lee M. Miller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Each year, millions of individuals lose the ability to speak intelligibly due
to causes such as neuromuscular disease, stroke, trauma, and head/neck cancer
surgery (e.g. laryngectomy) or treatment (e.g. radiotherapy toxicity to the
speech articulators). Effective communication is crucial for daily activities,
and losing the ability to speak leads to isolation, depression, anxiety, and a
host of detrimental sequelae. Noninvasive surface electromyography (sEMG) has
shown promise to restore speech output in these individuals. The goal is to
collect sEMG signals from multiple articulatory sites as people silently
produce speech and then decode the signals to enable fluent and natural
communication. Currently, many fundamental properties of orofacial
neuromuscular signals relating to speech articulation remain unanswered. They
include questions relating to 1) the data structure of the orofacial sEMG
signals, 2)the signal distribution shift of sEMG across individuals, 3) ability
of sEMG signals to span the entire English language phonetic space during
silent speech articulations, and 4) the generalization capability of
non-invasive sEMG based silent speech interfaces. We address these questions
through a series of experiments involving healthy human subjects. We show that
sEMG signals evince graph data structure and that the signal distribution shift
is given by a change of basis. Furthermore, we show that silently voiced
articulations spanning the entire English language phonetic space can be
decoded using small neural networks which can be trained with little data and
that such architectures work well across individuals. To ensure transparency
and reproducibility, we open-source all the data and codes used in this study.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Context-Informed Machine Translation of Manga using <span class="highlight-title">Multimodal</span> Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02589v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02589v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philip Lippmann, Konrad Skublicki, Joshua Tanner, Shonosuke Ishiwatari, Jie Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the significant time and effort required for handcrafting
translations, most manga never leave the domestic Japanese market. Automatic
manga translation is a promising potential solution. However, it is a budding
and underdeveloped field and presents complexities even greater than those
found in standard translation due to the need to effectively incorporate visual
elements into the translation process to resolve ambiguities. In this work, we
investigate to what extent multimodal large language models (LLMs) can provide
effective manga translation, thereby assisting manga authors and publishers in
reaching wider audiences. Specifically, we propose a methodology that leverages
the vision component of multimodal LLMs to improve translation quality and
evaluate the impact of translation unit size, context length, and propose a
token efficient approach for manga translation. Moreover, we introduce a new
evaluation dataset -- the first parallel Japanese-Polish manga translation
dataset -- as part of a benchmark to be used in future research. Finally, we
contribute an open-source software suite, enabling others to benchmark LLMs for
manga translation. Our findings demonstrate that our proposed methods achieve
state-of-the-art results for Japanese-English translation and set a new
standard for Japanese-Polish.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Social Support Detection from Social Media Texts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02580v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02580v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zahra Ahani, Moein Shahiki Tash, Fazlourrahman Balouchzahi, Luis Ramos, Grigori Sidorov, Alexander Gelbukh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social support, conveyed through a multitude of interactions and platforms
such as social media, plays a pivotal role in fostering a sense of belonging,
aiding resilience in the face of challenges, and enhancing overall well-being.
This paper introduces Social Support Detection (SSD) as a Natural language
processing (NLP) task aimed at identifying supportive interactions within
online communities. The study presents the task of Social Support Detection
(SSD) in three subtasks: two binary classification tasks and one multiclass
task, with labels detailed in the dataset section. We conducted experiments on
a dataset comprising 10,000 YouTube comments. Traditional machine learning
models were employed, utilizing various feature combinations that encompass
linguistic, psycholinguistic, emotional, and sentiment information.
Additionally, we experimented with neural network-based models using various
word embeddings to enhance the performance of our models across these
subtasks.The results reveal a prevalence of group-oriented support in online
dialogues, reflecting broader societal patterns. The findings demonstrate the
effectiveness of integrating psycholinguistic, emotional, and sentiment
features with n-grams in detecting social support and distinguishing whether it
is directed toward an individual or a group. The best results for different
subtasks across all experiments range from 0.72 to 0.82.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MM-Embed: Universal <span class="highlight-title">Multimodal</span> Retrieval with <span class="highlight-title">Multimodal</span> LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02571v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02571v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sheng-Chieh Lin, Chankyu Lee, Mohammad Shoeybi, Jimmy Lin, Bryan Catanzaro, Wei Ping
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art retrieval models typically address a straightforward search
scenario, where retrieval tasks are fixed (e.g., finding a passage to answer a
specific question) and only a single modality is supported for both queries and
retrieved results. This paper introduces techniques for advancing information
retrieval with multimodal large language models (MLLMs), enabling a broader
search scenario, termed universal multimodal retrieval, where multiple
modalities and diverse retrieval tasks are accommodated. To this end, we first
study fine-tuning an MLLM as a bi-encoder retriever on 10 datasets with 16
retrieval tasks. Our empirical results show that the fine-tuned MLLM retriever
is capable of understanding challenging queries, composed of both text and
image, but underperforms a smaller CLIP retriever in cross-modal retrieval
tasks due to modality bias from MLLMs. To address the issue, we propose
modality-aware hard negative mining to mitigate the modality bias exhibited by
MLLM retrievers. Second, we propose to continually fine-tune the universal
multimodal retriever to enhance its text retrieval capability while maintaining
multimodal retrieval capability. As a result, our model, MM-Embed, achieves
state-of-the-art performance on the multimodal retrieval benchmark M-BEIR,
which spans multiple domains and tasks, while also surpassing the
state-of-the-art text retrieval model, NV-Embed-v1, on MTEB retrieval
benchmark. Finally, we explore to prompt the off-the-shelf MLLMs as the
zero-shot rerankers to refine the ranking of the candidates from the multimodal
retriever. We find that through prompt-and-reranking, MLLMs can further improve
multimodal retrieval when the user queries (e.g., text-image composed queries)
are more complex and challenging to understand. These findings also pave the
way to advance universal multimodal retrieval in the future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We release the model weights at:
  https://huggingface.co/nvidia/MM-Embed</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Risk Assessment in Transformers with Loss-at-Risk Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02558v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02558v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinghan Zhang, Henry Xie, Xinhao Zhang, Kunpeng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the financial field, precise risk assessment tools are essential for
decision-making. Recent studies have challenged the notion that traditional
network loss functions like Mean Square Error (MSE) are adequate, especially
under extreme risk conditions that can lead to significant losses during market
upheavals. Transformers and Transformer-based models are now widely used in
financial forecasting according to their outstanding performance in
time-series-related predictions. However, these models typically lack
sensitivity to extreme risks and often underestimate great financial losses. To
address this problem, we introduce a novel loss function, the Loss-at-Risk,
which incorporates Value at Risk (VaR) and Conditional Value at Risk (CVaR)
into Transformer models. This integration allows Transformer models to
recognize potential extreme losses and further improves their capability to
handle high-stakes financial decisions. Moreover, we conduct a series of
experiments with highly volatile financial datasets to demonstrate that our
Loss-at-Risk function improves the Transformers' risk prediction and management
capabilities without compromising their decision-making accuracy or efficiency.
The results demonstrate that integrating risk-aware metrics during training
enhances the Transformers' risk assessment capabilities while preserving their
core strengths in decision-making and reasoning across diverse scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICKG 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Transformer-Based Models for Predicting Inflection Classes of
  Words in an Endangered Sami Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02556v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02556v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khalid Alnajjar, Mika Hämäläinen, Jack Rueter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a methodology for training a transformer-based model to
classify lexical and morphosyntactic features of Skolt Sami, an endangered
Uralic language characterized by complex morphology. The goal of our approach
is to create an effective system for understanding and analyzing Skolt Sami,
given the limited data availability and linguistic intricacies inherent to the
language. Our end-to-end pipeline includes data extraction, augmentation, and
training a transformer-based model capable of predicting inflection classes.
The motivation behind this work is to support language preservation and
revitalization efforts for minority languages like Skolt Sami. Accurate
classification not only helps improve the state of Finite-State Transducers
(FSTs) by providing greater lexical coverage but also contributes to systematic
linguistic documentation for researchers working with newly discovered words
from literature and native speakers. Our model achieves an average weighted F1
score of 1.00 for POS classification and 0.81 for inflection class
classification. The trained model and code will be released publicly to
facilitate future research in endangered NLP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IWCLUL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Triplet<span class="highlight-title">CLIP</span>: Improving Compositional Reasoning of <span class="highlight-title">CLIP</span> via Synthetic
  <span class="highlight-title">Vision-Language</span> Negatives <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02545v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02545v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maitreya Patel, Abhiram Kusumba, Sheng Cheng, Changhoon Kim, Tejas Gokhale, Chitta Baral, Yezhou Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive Language-Image Pretraining (CLIP) models maximize the mutual
information between text and visual modalities to learn representations. This
makes the nature of the training data a significant factor in the efficacy of
CLIP for downstream tasks. However, the lack of compositional diversity in
contemporary image-text datasets limits the compositional reasoning ability of
CLIP. We show that generating ``hard'' negative captions via in-context
learning and synthesizing corresponding negative images with text-to-image
generators offers a solution. We introduce a novel contrastive pre-training
strategy that leverages these hard negative captions and images in an
alternating fashion to train CLIP. We demonstrate that our method, named
TripletCLIP, when applied to existing datasets such as CC3M and CC12M, enhances
the compositional capabilities of CLIP, resulting in an absolute improvement of
over 9% on the SugarCrepe benchmark on an equal computational budget, as well
as improvements in zero-shot image classification and image retrieval. Our
code, models, and data are available at: https://tripletclip.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at: NeurIPS 2024 | Project Page:
  https://tripletclip.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MILU: A Multi-task Indic Language Understanding Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02538v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02538v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sshubam Verma, Mohammed Safi Ur Rahman Khan, Vishwajeet Kumar, Rudra Murthy, Jaydeep Sen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating Large Language Models (LLMs) in low-resource and linguistically
diverse languages remains a significant challenge in NLP, particularly for
languages using non-Latin scripts like those spoken in India. Existing
benchmarks predominantly focus on English, leaving substantial gaps in
assessing LLM capabilities in these languages. We introduce MILU, a Multi task
Indic Language Understanding Benchmark, a comprehensive evaluation benchmark
designed to address this gap. MILU spans 8 domains and 42 subjects across 11
Indic languages, reflecting both general and culturally specific knowledge.
With an India-centric design, incorporates material from regional and
state-level examinations, covering topics such as local history, arts,
festivals, and laws, alongside standard subjects like science and mathematics.
We evaluate over 42 LLMs, and find that current LLMs struggle with MILU, with
GPT-4o achieving the highest average accuracy at 72 percent. Open multilingual
models outperform language-specific fine-tuned models, which perform only
slightly better than random baselines. Models also perform better in high
resource languages as compared to low resource ones. Domain-wise analysis
indicates that models perform poorly in culturally relevant areas like Arts and
Humanities, Law and Governance compared to general fields like STEM. To the
best of our knowledge, MILU is the first of its kind benchmark focused on Indic
languages, serving as a crucial step towards comprehensive cultural evaluation.
All code, benchmarks, and artifacts will be made publicly available to foster
open research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ INQUIRE: A Natural World Text-to-Image Retrieval Benchmark <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02537v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02537v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edward Vendrow, Omiros Pantazis, Alexander Shepard, Gabriel Brostow, Kate E. Jones, Oisin Mac Aodha, Sara Beery, Grant Van Horn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce INQUIRE, a text-to-image retrieval benchmark designed to
challenge multimodal vision-language models on expert-level queries. INQUIRE
includes iNaturalist 2024 (iNat24), a new dataset of five million natural world
images, along with 250 expert-level retrieval queries. These queries are paired
with all relevant images comprehensively labeled within iNat24, comprising
33,000 total matches. Queries span categories such as species identification,
context, behavior, and appearance, emphasizing tasks that require nuanced image
understanding and domain expertise. Our benchmark evaluates two core retrieval
tasks: (1) INQUIRE-Fullrank, a full dataset ranking task, and (2)
INQUIRE-Rerank, a reranking task for refining top-100 retrievals. Detailed
evaluation of a range of recent multimodal models demonstrates that INQUIRE
poses a significant challenge, with the best models failing to achieve an
mAP@50 above 50%. In addition, we show that reranking with more powerful
multimodal models can enhance retrieval performance, yet there remains a
significant margin for improvement. By focusing on scientifically-motivated
ecological challenges, INQUIRE aims to bridge the gap between AI capabilities
and the needs of real-world scientific inquiry, encouraging the development of
retrieval systems that can assist with accelerating ecological and biodiversity
research. Our dataset and code are available at
https://inquire-benchmark.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in NeurIPS 2024, Datasets and Benchmarks Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Leveraging News Media to Support Impact Assessment of AI
  Technologies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02536v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02536v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mowafak Allaham, Kimon Kieslich, Nicholas Diakopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Expert-driven frameworks for impact assessments (IAs) may inadvertently
overlook the effects of AI technologies on the public's social behavior,
policy, and the cultural and geographical contexts shaping the perception of AI
and the impacts around its use. This research explores the potentials of
fine-tuning LLMs on negative impacts of AI reported in a diverse sample of
articles from 266 news domains spanning 30 countries around the world to
incorporate more diversity into IAs. Our findings highlight (1) the potential
of fine-tuned open-source LLMs in supporting IA of AI technologies by
generating high-quality negative impacts across four qualitative dimensions:
coherence, structure, relevance, and plausibility, and (2) the efficacy of
small open-source LLM (Mistral-7B) fine-tuned on impacts from news media in
capturing a wider range of categories of impacts that GPT-4 had gaps in
covering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2401.18028</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What Goes Into a LM Acceptability Judgment? Rethinking the Impact of
  Frequency and Length 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02528v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02528v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lindia Tjuatja, Graham Neubig, Tal Linzen, Sophie Hao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When comparing the linguistic capabilities of language models (LMs) with
humans using LM probabilities, factors such as the length of the sequence and
the unigram frequency of lexical items have a significant effect on LM
probabilities in ways that humans are largely robust to. Prior works in
comparing LM and human acceptability judgments treat these effects uniformly
across models, making a strong assumption that models require the same degree
of adjustment to control for length and unigram frequency effects. We propose
MORCELA, a new linking theory between LM scores and acceptability judgments
where the optimal level of adjustment for these effects is estimated from data
via learned parameters for length and unigram frequency. We first show that
MORCELA outperforms a commonly used linking theory for acceptability--SLOR
(Pauls and Klein, 2012; Lau et al. 2017)--across two families of transformer
LMs (Pythia and OPT). Furthermore, we demonstrate that the assumed degrees of
adjustment in SLOR for length and unigram frequency overcorrect for these
confounds, and that larger models require a lower relative degree of adjustment
for unigram frequency, though a significant amount of adjustment is still
necessary for all models. Finally, our subsequent analysis shows that larger
LMs' lower susceptibility to frequency effects can be explained by an ability
to better predict rarer words in context.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fantastic LLMs for Preference Data Annotation and How to (not) Find Them 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02481v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02481v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangxuan Xu, Kai Xu, Shivchander Sudalairaj, Hao Wang, Akash Srivastava
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Preference tuning of large language models (LLMs) relies on high-quality
human preference data, which is often expensive and time-consuming to gather.
While existing methods can use trained reward models or proprietary model as
judges for preference annotation, they have notable drawbacks: training reward
models remain dependent on initial human data, and using proprietary model
imposes license restrictions that inhibits commercial usage. In this paper, we
introduce customized density ratio (CDR) that leverages open-source LLMs for
data annotation, offering an accessible and effective solution. Our approach
uses the log-density ratio between a well-aligned LLM and a less aligned LLM as
a reward signal. We explores 221 different LLMs pairs and empirically
demonstrate that increasing the performance gap between paired LLMs correlates
with better reward generalization. Furthermore, we show that tailoring the
density ratio reward function with specific criteria and preference exemplars
enhances performance across domains and within target areas.
  In our experiment using density ratio from a pair of Mistral-7B models, CDR
achieves a RewardBench score of 82.6, outperforming the best in-class trained
reward functions and demonstrating competitive performance against SoTA models
in Safety (91.0) and Reasoning (88.0) domains. We use CDR to annotate an
on-policy preference dataset with which we preference tune Llama-3-8B-Instruct
with SimPO. The final model achieves a 37.4% (+15.1%) win rate on ArenaHard and
a 40.7% (+17.8%) win rate on Length-Controlled AlpacaEval 2.0, along with a
score of 8.0 on MT-Bench.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comparative Analysis of Instruction <span class="highlight-title">Fine-Tuning</span> LLMs for Financial
  Text Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02476v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02476v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sorouralsadat Fatemi, Yuheng Hu, Maryam Mousavi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated impressive capabilities across
diverse Natural Language Processing (NLP) tasks, including language
understanding, reasoning, and generation. However, general-domain LLMs often
struggle with financial tasks due to the technical and specialized nature of
financial texts. This study investigates the efficacy of instruction
fine-tuning smaller-scale LLMs, including Mistral-7B, Llama3-8B, and Phi3-mini,
to enhance their performance in financial text classification tasks. We
fine-tuned both instruction-tuned and base models across four financial
classification tasks, achieving significant improvements in task-specific
performance. Furthermore, we evaluated the zero-shot capabilities of these
fine-tuned models on three unseen complex financial tasks, including argument
classification, deal completeness classification, and causal classification.
Our results indicate while base model fine-tuning led to greater degradation,
instruction-tuned models maintained more robust performance. To address this
degradation, we employed model merging techniques, integrating single-task
domain-specific fine-tuned models with the base model. Using this merging
method resulted in significant enhancements in zero-shot performance, even
exceeding the original model's accuracy on certain datasets. Our findings
underscore the effectiveness of instruction fine-tuning and model merging for
adapting LLMs to specialized financial text classification tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AmbigNLG: Addressing Task Ambiguity in Instruction for NLG <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17717v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17717v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayana Niwa, Hayate Iso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce AmbigNLG, a novel task designed to tackle the challenge of task
ambiguity in instructions for Natural Language Generation (NLG). Ambiguous
instructions often impede the performance of Large Language Models (LLMs),
especially in complex NLG tasks. To tackle this issue, we propose an ambiguity
taxonomy that categorizes different types of instruction ambiguities and
refines initial instructions with clearer specifications. Accompanying this
task, we present AmbigSNI-NLG, a dataset comprising 2,500 instances annotated
to facilitate research in AmbigNLG. Through comprehensive experiments with
state-of-the-art LLMs, we demonstrate that our method significantly enhances
the alignment of generated text with user expectations, achieving up to a
15.02-point increase in ROUGE scores. Our findings highlight the critical
importance of addressing task ambiguity to fully harness the capabilities of
LLMs in NLG tasks. Furthermore, we confirm the effectiveness of our method in
practical settings involving interactive ambiguity mitigation with users,
underscoring the benefits of leveraging LLMs for interactive clarification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 (main)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EMMA: End-to-End <span class="highlight-title">Multimodal</span> Model for Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23262v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23262v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jyh-Jing Hwang, Runsheng Xu, Hubert Lin, Wei-Chih Hung, Jingwei Ji, Kristy Choi, Di Huang, Tong He, Paul Covington, Benjamin Sapp, Yin Zhou, James Guo, Dragomir Anguelov, Mingxing Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce EMMA, an End-to-end Multimodal Model for Autonomous driving.
Built on a multi-modal large language model foundation, EMMA directly maps raw
camera sensor data into various driving-specific outputs, including planner
trajectories, perception objects, and road graph elements. EMMA maximizes the
utility of world knowledge from the pre-trained large language models, by
representing all non-sensor inputs (e.g. navigation instructions and ego
vehicle status) and outputs (e.g. trajectories and 3D locations) as natural
language text. This approach allows EMMA to jointly process various driving
tasks in a unified language space, and generate the outputs for each task using
task-specific prompts. Empirically, we demonstrate EMMA's effectiveness by
achieving state-of-the-art performance in motion planning on nuScenes as well
as competitive results on the Waymo Open Motion Dataset (WOMD). EMMA also
yields competitive results for camera-primary 3D object detection on the Waymo
Open Dataset (WOD). We show that co-training EMMA with planner trajectories,
object detection, and road graph tasks yields improvements across all three
domains, highlighting EMMA's potential as a generalist model for autonomous
driving applications. However, EMMA also exhibits certain limitations: it can
process only a small amount of image frames, does not incorporate accurate 3D
sensing modalities like LiDAR or radar and is computationally expensive. We
hope that our results will inspire further research to mitigate these issues
and to further evolve the state of the art in autonomous driving model
architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Blog post: https://waymo.com/blog/2024/10/introducing-emma/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM-Ref: Enhancing Reference Handling in Technical Writing with Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00294v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00294v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kazi Ahmed Asif Fuad, Lizhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) excel in data synthesis but can be inaccurate in
domain-specific tasks, which retrieval-augmented generation (RAG) systems
address by leveraging user-provided data. However, RAGs require optimization in
both retrieval and generation stages, which can affect output quality. In this
paper, we present LLM-Ref, a writing assistant tool that aids researchers in
writing articles from multiple source documents with enhanced reference
synthesis and handling capabilities. Unlike traditional RAG systems that use
chunking and indexing, our tool retrieves and generates content directly from
text paragraphs. This method facilitates direct reference extraction from the
generated outputs, a feature unique to our tool. Additionally, our tool employs
iterative response generation, effectively managing lengthy contexts within the
language model's constraints. Compared to baseline RAG-based systems, our
approach achieves a $3.25\times$ to $6.26\times$ increase in Ragas score, a
comprehensive metric that provides a holistic view of a RAG system's ability to
produce accurate, relevant, and contextually appropriate responses. This
improvement shows our method enhances the accuracy and contextual relevance of
writing assistance tools.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Compact Language Models via Pruning and Knowledge Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.14679v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.14679v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saurav Muralidharan, Sharath Turuvekere Sreenivas, Raviraj Joshi, Marcin Chochowski, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro, Jan Kautz, Pavlo Molchanov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) targeting different deployment scales and sizes
are currently produced by training each variant from scratch; this is extremely
compute-intensive. In this paper, we investigate if pruning an existing LLM and
then re-training it with a fraction (<3%) of the original training data can be
a suitable alternative to repeated, full retraining. To this end, we develop a
set of practical and effective compression best practices for LLMs that combine
depth, width, attention and MLP pruning with knowledge distillation-based
retraining; we arrive at these best practices through a detailed empirical
exploration of pruning strategies for each axis, methods to combine axes,
distillation strategies, and search techniques for arriving at optimal
compressed architectures. We use this guide to compress the Nemotron-4 family
of LLMs by a factor of 2-4x, and compare their performance to similarly-sized
models on a variety of language modeling tasks. Deriving 8B and 4B models from
an already pretrained 15B model using our approach requires up to 40x fewer
training tokens per model compared to training from scratch; this results in
compute cost savings of 1.8x for training the full model family (15B, 8B, and
4B). Minitron models exhibit up to a 16% improvement in MMLU scores compared to
training from scratch, perform comparably to other community models such as
Mistral 7B, Gemma 7B and Llama-3 8B, and outperform state-of-the-art
compression techniques from the literature. We have open-sourced Minitron model
weights on Huggingface, with corresponding supplementary material including
example code available on GitHub.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimizing Contextual Speech Recognition Using Vector Quantization for
  Efficient Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00664v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00664v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolaos Flemotomos, Roger Hsiao, Pawel Swietojanski, Takaaki Hori, Dogan Can, Xiaodan Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural contextual biasing allows speech recognition models to leverage
contextually relevant information, leading to improved transcription accuracy.
However, the biasing mechanism is typically based on a cross-attention module
between the audio and a catalogue of biasing entries, which means computational
complexity can pose severe practical limitations on the size of the biasing
catalogue and consequently on accuracy improvements. This work proposes an
approximation to cross-attention scoring based on vector quantization and
enables compute- and memory-efficient use of large biasing catalogues. We
propose to use this technique jointly with a retrieval based contextual biasing
approach. First, we use an efficient quantized retrieval module to shortlist
biasing entries by grounding them on audio. Then we use retrieved entries for
biasing. Since the proposed approach is agnostic to the biasing method, we
investigate using full cross-attention, LLM prompting, and a combination of the
two. We show that retrieval based shortlisting allows the system to efficiently
leverage biasing catalogues of several thousands of entries, resulting in up to
71% relative error rate reduction in personal entity recognition. At the same
time, the proposed approximation algorithm reduces compute time by 20% and
memory usage by 85-95%, for lists of up to one million entries, when compared
to standard dot-product cross-attention.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 7 figures, submitted to IEEE/ACM Transactions on Audio,
  Speech, and Language Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Training Zero-Shot Generalizable End-to-End Task-Oriented Dialog System
  Without Turn-level Dialog Annotations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15055v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15055v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adib Mosharrof, A. B. Siddique
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Task-oriented dialogue (TOD) systems enable users to achieve their goals
through natural language interactions. Traditionally, these systems have relied
on turn-level manually annotated metadata, such as dialogue states and policy
annotations, which are expensive, time-consuming, and often inconsistent or
error-prone. This dependence limits the potential to leverage vast amounts of
readily available conversational data for training TOD systems. Additionally, a
critical challenge in TOD system design is determining when and how to access
and integrate information from external sources. Current approaches typically
expect this information to be provided alongside the dialogue context, rather
than learning to identify and retrieve it autonomously. While pre-trained large
language models (LLMs) have been used to develop TOD systems, their potential
to train such systems without laborious annotations remains largely unexplored.
This work employs multi-task instruction fine-tuning to create more efficient
and scalable TOD systems that can effectively leverage natural language
conversational data without manual annotations, while autonomously managing
external information retrieval. Our extensive experimental evaluations, using
three diverse TOD datasets and three LLMs of varying sizes, demonstrate that
our approach can generalize to new, unseen domains. Notably, our approach
outperforms both state-of-the-art models trained on annotated data and
billion-scale parameter off-the-shelf ChatGPT models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Retrieving Implicit and Explicit Emotional Events Using Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.19128v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.19128v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guimin Hu, Hasti Seifi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have garnered significant attention in recent
years due to their impressive performance. While considerable research has
evaluated these models from various perspectives, the extent to which LLMs can
perform implicit and explicit emotion retrieval remains largely unexplored. To
address this gap, this study investigates LLMs' emotion retrieval capabilities
in commonsense. Through extensive experiments involving multiple models, we
systematically evaluate the ability of LLMs on emotion retrieval. Specifically,
we propose a supervised contrastive probing method to verify LLMs' performance
for implicit and explicit emotion retrieval, as well as the diversity of the
emotional events they retrieve. The results offer valuable insights into the
strengths and limitations of LLMs in handling emotion retrieval.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Illuminating Blind Spots of Language Models with Targeted
  Agent-in-the-Loop Synthetic Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17860v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17860v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philip Lippmann, Matthijs T. J. Spaan, Jie Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models (LMs) have achieved impressive accuracy across a variety of
tasks but remain vulnerable to high-confidence misclassifications, also
referred to as unknown unknowns (UUs). These UUs cluster into blind spots in
the feature space, leading to significant risks in high-stakes applications.
This is particularly relevant for smaller, lightweight LMs that are more
susceptible to such errors. While the identification of UUs has been
extensively studied, their mitigation remains an open challenge, including how
to use identified UUs to eliminate unseen blind spots. In this work, we propose
a novel approach to address blind spot mitigation through the use of
intelligent agents -- either humans or large LMs -- as teachers to characterize
UU-type errors. By leveraging the generalization capabilities of intelligent
agents, we identify patterns in high-confidence misclassifications and use them
to generate targeted synthetic samples to improve model robustness and reduce
blind spots. We conduct an extensive evaluation of our method on three
classification tasks and demonstrate its effectiveness in reducing the number
of UUs, all while maintaining a similar level of accuracy. We find that the
effectiveness of human computation has a high ceiling but is highly dependent
on familiarity with the underlying task. Moreover, the cost gap between humans
and LMs surpasses an order of magnitude, as LMs attain human-like
generalization and generation performance while being more scalable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GSCo: Towards Generalizable AI in Medicine via Generalist-Specialist
  Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.15127v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.15127v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunan He, Yuxiang Nie, Hongmei Wang, Shu Yang, Yihui Wang, Zhiyuan Cai, Zhixuan Chen, Yingxue Xu, Luyang Luo, Huiling Xiang, Xi Lin, Mingxiang Wu, Yifan Peng, George Shih, Ziyang Xu, Xian Wu, Qiong Wang, Ronald Cheong Kin Chan, Varut Vardhanabhuti, Winnie Chiu Wing Chu, Yefeng Zheng, Pranav Rajpurkar, Kang Zhang, Hao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generalist foundation models (GFMs) are renowned for their exceptional
capability and flexibility in effectively generalizing across diverse tasks and
modalities. In the field of medicine, while GFMs exhibit superior
generalizability based on their extensive intrinsic knowledge as well as
proficiency in instruction following and in-context learning, specialist models
excel in precision due to their domain knowledge. In this work, for the first
time, we explore the synergy between the GFM and specialist models, to enable
precise medical image analysis on a broader scope. Specifically, we propose a
cooperative framework, Generalist-Specialist Collaboration (GSCo), which
consists of two stages, namely the construction of GFM and specialists, and
collaborative inference on downstream tasks. In the construction stage, we
develop MedDr, the largest open-source GFM tailored for medicine, showcasing
exceptional instruction-following and in-context learning capabilities.
Meanwhile, a series of lightweight specialists are crafted for downstream tasks
with low computational cost. In the collaborative inference stage, we introduce
two cooperative mechanisms, Mixture-of-Expert Diagnosis and Retrieval-Augmented
Diagnosis, to harvest the generalist's in-context learning abilities alongside
the specialists' domain expertise. For a comprehensive evaluation, we curate a
large-scale benchmark featuring 28 datasets and about 250,000 images. Extensive
results demonstrate that MedDr consistently outperforms state-of-the-art GFMs
on downstream datasets. Furthermore, GSCo exceeds both GFMs and specialists
across all out-of-domain disease diagnosis datasets. These findings indicate a
significant paradigm shift in the application of GFMs, transitioning from
separate models for specific tasks to a collaborative approach between GFMs and
specialists, thereby advancing the frontiers of generalizable AI in medicine.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PediatricsGPT: Large Language Models as Chinese Medical Assistants for
  Pediatric Applications <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19266v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19266v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dingkang Yang, Jinjie Wei, Dongling Xiao, Shunli Wang, Tong Wu, Gang Li, Mingcheng Li, Shuaibing Wang, Jiawei Chen, Yue Jiang, Qingyao Xu, Ke Li, Peng Zhai, Lihua Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing intelligent pediatric consultation systems offers promising
prospects for improving diagnostic efficiency, especially in China, where
healthcare resources are scarce. Despite recent advances in Large Language
Models (LLMs) for Chinese medicine, their performance is sub-optimal in
pediatric applications due to inadequate instruction data and vulnerable
training procedures. To address the above issues, this paper builds PedCorpus,
a high-quality dataset of over 300,000 multi-task instructions from pediatric
textbooks, guidelines, and knowledge graph resources to fulfil diverse
diagnostic demands. Upon well-designed PedCorpus, we propose PediatricsGPT, the
first Chinese pediatric LLM assistant built on a systematic and robust training
pipeline. In the continuous pre-training phase, we introduce a hybrid
instruction pre-training mechanism to mitigate the internal-injected knowledge
inconsistency of LLMs for medical domain adaptation. Immediately, the
full-parameter Supervised Fine-Tuning (SFT) is utilized to incorporate the
general medical knowledge schema into the models. After that, we devise a
direct following preference optimization to enhance the generation of
pediatrician-like humanistic responses. In the parameter-efficient secondary
SFT phase, a mixture of universal-specific experts strategy is presented to
resolve the competency conflict between medical generalist and pediatric
expertise mastery. Extensive results based on the metrics, GPT-4, and doctor
evaluations on distinct doctor downstream tasks show that PediatricsGPT
consistently outperforms previous Chinese medical LLMs. Our model and dataset
will be open-source for community development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024. A Technical Report on a Chinese Medical
  Large Language Model</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ACC-Debate: An Actor-Critic Approach to Multi-Agent Debate 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00053v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00053v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Estornell, Jean-Francois Ton, Yuanshun Yao, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated a remarkable ability to serve
as general-purpose tools for various language-based tasks. Recent works have
demonstrated that the efficacy of such models can be improved through iterative
dialog between multiple models, frequently referred to as multi-agent debate
(MAD). While debate shows promise as a means of improving model efficacy, most
works in this area treat debate as an emergent behavior, rather than a learned
behavior. In doing so, current debate frameworks rely on collaborative
behaviors to have been sufficiently trained into off-the-shelf models. To
address this limitation, we propose ACC-Debate, an Actor-Critic based learning
framework to produce a two-agent team specialized in debate. We demonstrate
that ACC-Debate outperforms SotA debate techniques on a wide array of
benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tool Learning with Large Language Models: A Survey 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17935v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17935v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, tool learning with large language models (LLMs) has emerged as a
promising paradigm for augmenting the capabilities of LLMs to tackle highly
complex problems. Despite growing attention and rapid advancements in this
field, the existing literature remains fragmented and lacks systematic
organization, posing barriers to entry for newcomers. This gap motivates us to
conduct a comprehensive survey of existing works on tool learning with LLMs. In
this survey, we focus on reviewing existing literature from the two primary
aspects (1) why tool learning is beneficial and (2) how tool learning is
implemented, enabling a comprehensive understanding of tool learning with LLMs.
We first explore the "why" by reviewing both the benefits of tool integration
and the inherent benefits of the tool learning paradigm from six specific
aspects. In terms of "how", we systematically review the literature according
to a taxonomy of four key stages in the tool learning workflow: task planning,
tool selection, tool calling, and response generation. Additionally, we provide
a detailed summary of existing benchmarks and evaluation methods, categorizing
them according to their relevance to different stages. Finally, we discuss
current challenges and outline potential future directions, aiming to inspire
both researchers and industrial developers to further explore this emerging and
promising area. We also maintain a GitHub repository to continually keep track
of the relevant papers and resources in this rising area at
https://github.com/quchangle1/LLM-Tool-Survey.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The article has been accepted by Frontiers of Computer Science (FCS),
  with the DOI: {10.1007/s11704-024-40678-2}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ fMRI predictors based on language models of increasing complexity
  recover brain left lateralization <span class="chip">NeurIPS
  2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17992v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17992v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laurent Bonnasse-Gahot, Christophe Pallier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the past decade, studies of naturalistic language processing where
participants are scanned while listening to continuous text have flourished.
Using word embeddings at first, then large language models, researchers have
created encoding models to analyze the brain signals. Presenting these models
with the same text as the participants allows to identify brain areas where
there is a significant correlation between the functional magnetic resonance
imaging (fMRI) time series and the ones predicted by the models' artificial
neurons. One intriguing finding from these studies is that they have revealed
highly symmetric bilateral activation patterns, somewhat at odds with the
well-known left lateralization of language processing. Here, we report analyses
of an fMRI dataset where we manipulate the complexity of large language models,
testing 28 pretrained models from 8 different families, ranging from 124M to
14.2B parameters. First, we observe that the performance of models in
predicting brain responses follows a scaling law, where the fit with brain
activity increases linearly with the logarithm of the number of parameters of
the model (and its performance on natural language processing tasks). Second,
although this effect is present in both hemispheres, it is stronger in the left
than in the right hemisphere. Specifically, the left-right difference in brain
correlation follows a scaling law with the number of parameters. This finding
reconciles computational analyses of brain activity using large language models
with the classic observation from aphasic patients showing left hemisphere
dominance for language.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38th Conference on Neural Information Processing Systems (NeurIPS
  2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TorchTitan: One-stop PyTorch native solution for production ready LLM
  pre-training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06511v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06511v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanchao Liang, Tianyu Liu, Less Wright, Will Constable, Andrew Gu, Chien-Chin Huang, Iris Zhang, Wei Feng, Howard Huang, Junjie Wang, Sanket Purandare, Gokul Nadathur, Stratos Idreos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of large language models (LLMs) has been instrumental in
advancing state-of-the-art natural language processing applications. Training
LLMs with billions of parameters and trillions of tokens require sophisticated
distributed systems that enable composing and comparing several
state-of-the-art techniques in order to efficiently scale across thousands of
accelerators. However, existing solutions are complex, scattered across
multiple libraries/repositories, lack interoperability, and are cumbersome to
maintain. Thus, curating and empirically comparing training recipes require
non-trivial engineering effort.
  This paper introduces TorchTitan, an open-source, PyTorch-native distributed
training system that unifies state-of-the-art techniques, streamlining
integration and reducing overhead. TorchTitan enables 3D parallelism in a
modular manner with elastic scaling, providing comprehensive logging,
checkpointing, and debugging tools for production-ready training. It also
incorporates hardware-software co-designed solutions, leveraging features like
Float8 training and SymmetricMemory. As a flexible test bed, TorchTitan
facilitates custom recipe curation and comparison, allowing us to develop
optimized training recipes for Llama 3.1 and provide guidance on selecting
techniques for maximum efficiency based on our experiences.
  We thoroughly assess TorchTitan on the Llama 3.1 family of LLMs, spanning 8
billion to 405 billion parameters, and showcase its exceptional performance,
modular composability, and elastic scalability. By stacking training
optimizations, we demonstrate accelerations of 65.08% with 1D parallelism at
the 128-GPU scale (Llama 3.1 8B), an additional 12.59% with 2D parallelism at
the 256-GPU scale (Llama 3.1 70B), and an additional 30% with 3D parallelism at
the 512-GPU scale (Llama 3.1 405B) on NVIDIA H100 GPUs over optimized
baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Novel Metric for Measuring the Robustness of Large Language Models in
  Non-adversarial Scenarios <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01963v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01963v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Ackerman, Ella Rabinovich, Eitan Farchi, Ateret Anaby-Tavor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We evaluate the robustness of several large language models on multiple
datasets. Robustness here refers to the relative insensitivity of the model's
answers to meaning-preserving variants of their input. Benchmark datasets are
constructed by introducing naturally-occurring, non-malicious perturbations, or
by generating semantically equivalent paraphrases of input questions or
statements. We further propose a novel metric for assessing a model robustness,
and demonstrate its benefits in the non-adversarial scenario by empirical
evaluation of several models on the created datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in the 2024 Conference on Empirical Methods in Natural
  Language Processing (EMNLP) findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CMMMU: A Chinese Massive Multi-discipline <span class="highlight-title">Multimodal</span> Understanding
  Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.11944v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.11944v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ge Zhang, Xinrun Du, Bei Chen, Yiming Liang, Tongxu Luo, Tianyu Zheng, Kang Zhu, Yuyang Cheng, Chunpu Xu, Shuyue Guo, Haoran Zhang, Xingwei Qu, Junjie Wang, Ruibin Yuan, Yizhi Li, Zekun Wang, Yudong Liu, Yu-Hsuan Tsai, Fengji Zhang, Chenghua Lin, Wenhao Huang, Jie Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the capabilities of large multimodal models (LMMs) continue to advance,
evaluating the performance of LMMs emerges as an increasing need. Additionally,
there is an even larger gap in evaluating the advanced knowledge and reasoning
abilities of LMMs in non-English contexts such as Chinese. We introduce CMMMU,
a new Chinese Massive Multi-discipline Multimodal Understanding benchmark
designed to evaluate LMMs on tasks demanding college-level subject knowledge
and deliberate reasoning in a Chinese context. CMMMU is inspired by and
strictly follows the annotation and analysis pattern of MMMU. CMMMU includes
12k manually collected multimodal questions from college exams, quizzes, and
textbooks, covering six core disciplines: Art & Design, Business, Science,
Health & Medicine, Humanities & Social Science, and Tech & Engineering, like
its companion, MMMU. These questions span 30 subjects and comprise 39 highly
heterogeneous image types, such as charts, diagrams, maps, tables, music
sheets, and chemical structures. CMMMU focuses on complex perception and
reasoning with domain-specific knowledge in the Chinese context. We evaluate 11
open-source LLMs and one proprietary GPT-4V(ision). Even GPT-4V only achieves
accuracies of 42%, indicating a large space for improvement. CMMMU will boost
the community to build the next-generation LMMs towards expert artificial
intelligence and promote the democratization of LMMs by providing diverse
language contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BiVLC: Extending <span class="highlight-title">Vision-Language</span> Compositionality Evaluation with
  Text-to-Image Retrieval <span class="chip">NeurIPS 24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09952v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09952v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Imanol Miranda, Ander Salaberria, Eneko Agirre, Gorka Azkune
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing Vision-Language Compositionality (VLC) benchmarks like SugarCrepe
are formulated as image-to-text retrieval problems, where, given an image, the
models need to select between the correct textual description and a synthetic
hard negative text. In this work, we present the Bidirectional Vision-Language
Compositionality (BiVLC) dataset. The novelty of BiVLC is to add a synthetic
hard negative image generated from the synthetic text, resulting in two
image-to-text retrieval examples (one for each image) and, more importantly,
two text-to-image retrieval examples (one for each text). Human annotators
filter out ill-formed examples ensuring the validity of the benchmark. The
experiments on BiVLC uncover a weakness of current multimodal models, as they
perform poorly in the text-to-image direction. In fact, when considering both
retrieval directions, the conclusions obtained in previous works change
significantly. In addition to the benchmark, we show that a contrastive model
trained using synthetic images and texts significantly improves over the base
model in SugarCrepe and in BiVLC for both retrieval directions. The gap to
human performance in BiVLC confirms that Vision-Language Compositionality is
still a challenging problem. BiVLC and code are available at
https://imirandam.github.io/BiVLC_project_page.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 24 Datasets and Benchmarks Track; Project page
  at: https://imirandam.github.io/BiVLC_project_page/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can GPT-4 learn to analyse moves in research article abstracts? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15612v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15612v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danni Yu, Marina Bondi, Ken Hyland
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the most powerful and enduring ideas in written discourse analysis is
that genres can be described in terms of the moves which structure a writer's
purpose. Considerable research has sought to identify these distinct
communicative acts, but analyses have been beset by problems of subjectivity,
reliability and the time-consuming need for multiple coders to confirm
analyses. In this paper we employ the affordances of GPT-4 to automate the
annotation process by using natural language prompts. Focusing on abstracts
from articles in four applied linguistics journals, we devise prompts which
enable the model to identify moves effectively. The annotated outputs of these
prompts were evaluated by two assessors with a third addressing disagreements.
The results show that an 8-shot prompt was more effective than one using two,
confirming that the inclusion of examples illustrating areas of variability can
enhance GPT-4's ability to recognize multiple moves in a single sentence and
reduce bias related to textual position. We suggest that GPT-4 offers
considerable potential in automating this annotation process, when human actors
with domain specific linguistic expertise inform the prompting process.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ReverseNER: A Self-Generated Example-Driven Framework for Zero-Shot
  Named Entity Recognition with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00533v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00533v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anbang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents ReverseNER, a framework aimed at overcoming the
limitations of large language models (LLMs) in zero-shot Named Entity
Recognition (NER) tasks, particularly in cases where certain entity types have
ambiguous boundaries. ReverseNER tackles this challenge by constructing a
reliable example library with the reversed process of NER. Rather than
beginning with sentences, this method uses an LLM to generate entities based on
their definitions and then expands them into full sentences. During sentence
generation, the LLM is guided to replicate the structure of a specific 'feature
sentence', extracted from the task sentences by clustering. This results in
well-annotated sentences with clearly labeled entities, while preserving
semantic and structural similarity to the task sentences. Once the example
library is constructed, the method selects the most semantically similar
example labels for each task sentence to support the LLM's inference. We also
propose an entity-level self-consistency scoring mechanism to improve NER
performance with LLMs. Experiments show that ReverseNER significantly
outperforms traditional zero-shot NER with LLMs and surpasses several few-shot
methods, marking a notable improvement in NER for domains with limited labeled
data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online Adaptation of Language Models with a Memory of Amortized Contexts <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04317v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04317v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jihoon Tack, Jaehyung Kim, Eric Mitchell, Jinwoo Shin, Yee Whye Teh, Jonathan Richard Schwarz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the rapid generation and dissemination of information, large language
models (LLMs) quickly run out of date despite enormous development costs. To
address the crucial need to keep models updated, online learning has emerged as
a critical tool when utilizing LLMs for real-world applications. However, given
the ever-expanding corpus of unseen documents and the large parameter space of
modern LLMs, efficient adaptation is essential. To address these challenges, we
propose Memory of Amortized Contexts (MAC), an efficient and effective online
adaptation framework for LLMs with strong knowledge retention. We propose a
feature extraction and memory-augmentation approach to compress and extract
information from new documents into compact modulations stored in a memory
bank. When answering questions, our model attends to and extracts relevant
knowledge from this memory bank. To learn informative modulations in an
efficient manner, we utilize amortization-based meta-learning, which
substitutes an otherwise required optimization process with a single forward
pass of the encoder. Subsequently, we learn to choose from and aggregate
selected documents into a single modulation by conditioning on the question,
allowing us to adapt a frozen language model during test time without requiring
further gradient updates. Our experiment demonstrates the superiority of MAC in
multiple aspects, including online adaptation performance, time, and memory
efficiency. In addition, we show how MAC can be combined with and improve the
performance of popular alternatives such as retrieval augmented generations
(RAGs). Code is available at: https://github.com/jihoontack/MAC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference proceeding for NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OneBit: Towards Extremely Low-bit Large Language Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11295v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11295v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuzhuang Xu, Xu Han, Zonghan Yang, Shuo Wang, Qingfu Zhu, Zhiyuan Liu, Weidong Liu, Wanxiang Che
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model quantification uses low bit-width values to represent the weight
matrices of existing models to be quantized, which is a promising approach to
reduce both storage and computational overheads of deploying highly anticipated
LLMs. However, current quantization methods suffer severe performance
degradation when the bit-width is extremely reduced, and thus focus on
utilizing 4-bit or 8-bit values to quantize models. This paper boldly quantizes
the weight matrices of LLMs to 1-bit, paving the way for the extremely low
bit-width deployment of LLMs. For this target, we introduce a 1-bit model
compressing framework named OneBit, including a novel 1-bit parameter
representation method to better quantize LLMs as well as an effective parameter
initialization method based on matrix decomposition to improve the convergence
speed of the quantization framework. Sufficient experimental results indicate
that OneBit achieves good performance (at least 81% of the non-quantized
performance on LLaMA models) with robust training processes when only using
1-bit weight matrices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Crafting Tomorrow's Headlines: Neural News Generation and Detection in
  English, Turkish, Hungarian, and Persian <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.10724v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.10724v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cem Üyük, Danica Rovó, Shaghayegh Kolli, Rabia Varol, Georg Groh, Daryna Dementieva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the era dominated by information overload and its facilitation with Large
Language Models (LLMs), the prevalence of misinformation poses a significant
threat to public discourse and societal well-being. A critical concern at
present involves the identification of machine-generated news. In this work, we
take a significant step by introducing a benchmark dataset designed for neural
news detection in four languages: English, Turkish, Hungarian, and Persian. The
dataset incorporates outputs from multiple multilingual generators (in both,
zero-shot and fine-tuned setups) such as BloomZ, LLaMa-2, Mistral, Mixtral, and
GPT-4. Next, we experiment with a variety of classifiers, ranging from those
based on linguistic features to advanced Transformer-based models and LLMs
prompting. We present the detection results aiming to delve into the
interpretablity and robustness of machine-generated texts detectors across all
target languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 NLP4PI Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attr-Int: A Simple and Effective Entity Alignment Framework for
  Heterogeneous Knowledge Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13409v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13409v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linyan Yang, Jingwei Cheng, Chuanhao Xu, Xihao Wang, Jiayi Li, Fu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Entity alignment (EA) refers to the task of linking entities in different
knowledge graphs (KGs). Existing EA methods rely heavily on structural
isomorphism. However, in real-world KGs, aligned entities usually have
non-isomorphic neighborhood structures, which paralyses the application of
these structure-dependent methods. In this paper, we investigate and tackle the
problem of entity alignment between heterogeneous KGs. First, we propose two
new benchmarks to closely simulate real-world EA scenarios of heterogeneity.
Then we conduct extensive experiments to evaluate the performance of
representative EA methods on the new benchmarks. Finally, we propose a simple
and effective entity alignment framework called Attr-Int, in which innovative
attribute information interaction methods can be seamlessly integrated with any
embedding encoder for entity alignment, improving the performance of existing
entity alignment techniques. Experiments demonstrate that our framework
outperforms the state-of-the-art approaches on two new benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MHPP: Exploring the Capabilities and Limitations of Language Models
  Beyond Basic Code Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.11430v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.11430v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianbo Dai, Jianqiao Lu, Yunlong Feng, Dong Huang, Guangtao Zeng, Rongju Ruan, Ming Cheng, Haochen Tan, Zhijiang Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) have greatly improved
code generation, specifically at the function level. For instance, GPT-4o has
achieved a 91.0\% pass rate on HumanEval. However, this draws into question the
adequacy of existing benchmarks in thoroughly assessing function-level code
generation capabilities. Our study analyzed two common benchmarks, HumanEval
and MBPP, and found that these might not thoroughly evaluate LLMs' code
generation capacities due to limitations in quality, difficulty, and
granularity. To resolve this, we introduce the Mostly Hard Python Problems
(MHPP) dataset, consisting of 210 unique human-curated problems. By focusing on
the combination of natural language and code reasoning, MHPP gauges LLMs'
abilities to comprehend specifications and restrictions, engage in multi-step
reasoning, and apply coding knowledge effectively. Initial evaluations of 26
LLMs using MHPP showed many high-performing models on HumanEval failed to
achieve similar success on MHPP. Moreover, MHPP highlighted various previously
undiscovered limitations within various LLMs, leading us to believe that it
could pave the way for a better understanding of LLMs' capabilities and
limitations. MHPP, evaluation pipeline, and leaderboard can be found in
https://github.com/SparksofAGI/MHPP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>43 pages, dataset and code are available at
  https://github.com/SparksofAGI/MHPP, leaderboard can be found at
  https://sparksofagi.github.io/MHPP/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Plug, Play, and Fuse: Zero-Shot Joint Decoding via Word-Level Re-ranking
  Across Diverse Vocabularies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.11327v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.11327v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sai Koneru, Matthias Huck, Miriam Exel, Jan Niehues
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in NLP have resulted in models with specialized
strengths, such as processing multimodal inputs or excelling in specific
domains. However, real-world tasks, like multimodal translation, often require
a combination of these strengths, such as handling both translation and image
processing. While individual translation and vision models are powerful, they
typically lack the ability to perform both tasks in a single system. Combining
these models poses challenges, particularly due to differences in their
vocabularies, which limit the effectiveness of traditional ensemble methods to
post-generation techniques like N-best list re-ranking. In this work, we
propose a novel zero-shot ensembling strategy that allows for the integration
of different models during the decoding phase without the need for additional
training. Our approach re-ranks beams during decoding by combining scores at
the word level, using heuristics to predict when a word is completed. We
demonstrate the effectiveness of this method in machine translation scenarios,
showing that it enables the generation of translations that are both speech-
and image-aware while also improving overall translation quality (We will
release the code upon paper acceptance.).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WMT 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Retrieval-augmented Text-to-SQL with AST-based Ranking and
  Schema Pruning <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.03227v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.03227v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhili Shen, Pavlos Vougiouklis, Chenxin Diao, Kaustubh Vyas, Yuanyi Ji, Jeff Z. Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We focus on Text-to-SQL semantic parsing from the perspective of
retrieval-augmented generation. Motivated by challenges related to the size of
commercial database schemata and the deployability of business intelligence
solutions, we propose $\text{ASTReS}$ that dynamically retrieves input database
information and uses abstract syntax trees to select few-shot examples for
in-context learning.
  Furthermore, we investigate the extent to which an in-parallel semantic
parser can be leveraged for generating approximated versions of the expected
SQL queries, to support our retrieval. We take this approach to the extreme--we
adapt a model consisting of less than $500$M parameters, to act as an extremely
efficient approximator, enhancing it with the ability to process schemata in a
parallelised manner. We apply $\text{ASTReS}$ to monolingual and cross-lingual
benchmarks for semantic parsing, showing improvements over state-of-the-art
baselines. Comprehensive experiments highlight the contribution of modules
involved in this retrieval-augmented generation setting, revealing interesting
directions for future work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scalable and Domain-General Abstractive Proposition Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19803v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19803v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Javad Hosseini, Yang Gao, Tim Baumgärtner, Alex Fabrikant, Reinald Kim Amplayo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Segmenting text into fine-grained units of meaning is important to a wide
range of NLP applications. The default approach of segmenting text into
sentences is often insufficient, especially since sentences are usually complex
enough to include multiple units of meaning that merit separate treatment in
the downstream task. We focus on the task of abstractive proposition
segmentation (APS): transforming text into simple, self-contained, well-formed
sentences. Several recent works have demonstrated the utility of proposition
segmentation with few-shot prompted LLMs for downstream tasks such as
retrieval-augmented grounding and fact verification. However, this approach
does not scale to large amounts of text and may not always extract all the
facts from the input text. In this paper, we first introduce evaluation metrics
for the task to measure several dimensions of quality. We then propose a
scalable, yet accurate, proposition segmentation model. We model proposition
segmentation as a supervised task by training LLMs on existing annotated
datasets and show that training yields significantly improved results. We
further show that by using the fine-tuned LLMs (Gemini Pro and Gemini Ultra) as
teachers for annotating large amounts of multi-domain synthetic distillation
data, we can train smaller student models (Gemma 1 2B and 7B) with results
similar to the teacher LLMs. We then demonstrate that our technique leads to
effective domain generalization, by annotating data in two domains outside the
original training data and evaluating on them. Finally, as a key contribution
of the paper, we share an easy-to-use API for NLP practitioners to use.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nash CoT: Multi-Path Inference with Preference Equilibrium 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.07099v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.07099v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqi Zhang, Cunxiang Wang, Xiong Xiao, Yue Zhang, Donglin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chain of thought (CoT) is a reasoning framework that can enhance the
performance of Large Language Models (LLMs) on complex inference tasks. In
particular, among various studies related to CoT, multi-path inference stands
out as a simple yet effective improvement. However, there is no optimal setting
for the number of inference paths. Therefore, we have to increase the number of
inference paths to obtain better results, which in turn increases the inference
cost. To address this limitation, we can utilize question-related role
templates to guide LLMs into relevant roles, thereby increasing the possibility
of correct inferences for each path and further reducing dependence on the
number of inference paths while improving reasoning accuracy. However, placing
LLMs into specific roles may reduce their reasoning diversity and performance
on a few tasks where role dependence is low. To alleviate the excessive
immersion of the LLM into a specific role, we propose Nash CoT by constructing
a competitive system on each path that balances the generation from
role-specific LLMs' and the general LLMs' generation, thereby ensuring both
effective role adoption and diversity in LLM generation further maintaining the
performance of multi-path inference while reducing the requirement of the
number of inference paths. We evaluate Nash CoT across various inference tasks,
including Arabic Reasoning, Commonsense Question Answering, and Symbolic
Inference, achieving results that are comparable to or better than those of
multi-path CoT with the equal number of inference paths.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Synthetic Dataset for Personal Attribute Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.07217v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.07217v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanna Yukhymenko, Robin Staab, Mark Vero, Martin Vechev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, powerful Large Language Models (LLMs) have become easily accessible
to hundreds of millions of users world-wide. However, their strong capabilities
and vast world knowledge do not come without associated privacy risks. In this
work, we focus on the emerging privacy threat LLMs pose -- the ability to
accurately infer personal information from online texts. Despite the growing
importance of LLM-based author profiling, research in this area has been
hampered by a lack of suitable public datasets, largely due to ethical and
privacy concerns associated with real personal data. We take two steps to
address this problem: (i) we construct a simulation framework for the popular
social media platform Reddit using LLM agents seeded with synthetic personal
profiles; (ii) using this framework, we generate SynthPAI, a diverse synthetic
dataset of over 7800 comments manually labeled for personal attributes. We
validate our dataset with a human study showing that humans barely outperform
random guessing on the task of distinguishing our synthetic comments from real
ones. Further, we verify that our dataset enables meaningful personal attribute
inference research by showing across 18 state-of-the-art LLMs that our
synthetic comments allow us to draw the same conclusions as real-world data.
Combined, our experimental results, dataset and pipeline form a strong basis
for future privacy-preserving research geared towards understanding and
mitigating inference-based privacy threats that LLMs pose.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Have You Merged My Model? On The Robustness of Large Language Model IP
  Protection Methods Against Model Merging <span class="chip">CCS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.05188v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.05188v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianshuo Cong, Delong Ran, Zesen Liu, Xinlei He, Jinyuan Liu, Yichen Gong, Qi Li, Anyu Wang, Xiaoyun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model merging is a promising lightweight model empowerment technique that
does not rely on expensive computing devices (e.g., GPUs) or require the
collection of specific training data. Instead, it involves editing different
upstream model parameters to absorb their downstream task capabilities.
However, uncertified model merging can infringe upon the Intellectual Property
(IP) rights of the original upstream models. In this paper, we conduct the
first study on the robustness of IP protection methods under model merging
scenarios. Specifically, we investigate two state-of-the-art IP protection
techniques: Quantization Watermarking and Instructional Fingerprint, along with
various advanced model merging technologies, such as Task Arithmetic,
TIES-MERGING, and so on. Experimental results indicate that current Large
Language Model (LLM) watermarking techniques cannot survive in the merged
models, whereas model fingerprinting techniques can. Our research aims to
highlight that model merging should be an indispensable consideration in the
robustness assessment of model IP protection techniques, thereby promoting the
healthy development of the open-source LLM community. Our code is available at
https://github.com/ThuCCSLab/MergeGuard.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM CCS-LAMPS 2024 (Best Paper Award)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Localizing and Mitigating Errors in Long-form Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11930v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11930v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rachneet Sachdeva, Yixiao Song, Mohit Iyyer, Iryna Gurevych
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-form question answering (LFQA) aims to provide thorough and in-depth
answers to complex questions, enhancing comprehension. However, such detailed
responses are prone to hallucinations and factual inconsistencies, challenging
their faithful evaluation. This work introduces HaluQuestQA, the first
hallucination dataset with localized error annotations for human-written and
model-generated LFQA answers. HaluQuestQA comprises 698 QA pairs with 1.8k
span-level error annotations for five different error types by expert
annotators, along with preference judgments. Using our collected data, we
thoroughly analyze the shortcomings of long-form answers and find that they
lack comprehensiveness and provide unhelpful references. We train an automatic
feedback model on this dataset that predicts error spans with incomplete
information and provides associated explanations. Finally, we propose a
prompt-based approach, Error-informed refinement, that uses signals from the
learned feedback model to refine generated answers, which we show reduces
errors and improves answer quality across multiple models. Furthermore, humans
find answers generated by our approach comprehensive and highly prefer them
(84%) over the baseline answers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and data are available:
  https://github.com/UKPLab/arxiv2024-lfqa-hallucination</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A LLM-Based Ranking Method for the Evaluation of Automatic
  Counter-Narrative Generation <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15227v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15227v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Irune Zubiaga, Aitor Soroa, Rodrigo Agerri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a novel approach to evaluate Counter Narrative (CN)
generation using a Large Language Model (LLM) as an evaluator. We show that
traditional automatic metrics correlate poorly with human judgements and fail
to capture the nuanced relationship between generated CNs and human perception.
To alleviate this, we introduce a model ranking pipeline based on pairwise
comparisons of generated CNs from different models, organized in a
tournament-style format. The proposed evaluation method achieves a high
correlation with human preference, with a $\rho$ score of 0.88. As an
additional contribution, we leverage LLMs as zero-shot CN generators and
provide a comparative analysis of chat, instruct, and base models, exploring
their respective strengths and limitations. Through meticulous evaluation,
including fine-tuning experiments, we elucidate the differences in performance
and responsiveness to domain-specific data. We conclude that chat-aligned
models in zero-shot are the best option for carrying out the task, provided
they do not refuse to generate an answer due to security concerns.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for Findings of the Association for Computational
  Linguistics: EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Word Embedding Dimension Reduction via Weakly-Supervised Feature
  Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12342v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12342v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jintang Xue, Yun-Cheng Wang, Chengwei Wei, C. -C. Jay Kuo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a fundamental task in natural language processing, word embedding converts
each word into a representation in a vector space. A challenge with word
embedding is that as the vocabulary grows, the vector space's dimension
increases, which can lead to a vast model size. Storing and processing word
vectors are resource-demanding, especially for mobile edge-devices
applications. This paper explores word embedding dimension reduction. To
balance computational costs and performance, we propose an efficient and
effective weakly-supervised feature selection method named WordFS. It has two
variants, each utilizing novel criteria for feature selection. Experiments on
various tasks (e.g., word and sentence similarity and binary and multi-class
classification) indicate that the proposed WordFS model outperforms other
dimension reduction methods at lower computational costs. We have released the
code for reproducibility along with the paper.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LoQT: Low-Rank Adapters for Quantized Pretraining 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.16528v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.16528v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian Loeschcke, Mads Toftrup, Michael J. Kastoryano, Serge Belongie, Vésteinn Snæbjarnarson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite advances using low-rank adapters and quantization, pretraining of
large models on consumer hardware has not been possible without model sharding,
offloading during training, or per-layer gradient updates. To address these
limitations, we propose Low-Rank Adapters for Quantized Training (LoQT), a
method for efficiently training quantized models. LoQT uses gradient-based
tensor factorization to initialize low-rank trainable weight matrices that are
periodically merged into quantized full-rank weight matrices. Our approach is
suitable for both pretraining and fine-tuning models. We demonstrate this for
language modeling and downstream task adaptation, finding that LoQT enables
efficient training of models up to 7B parameters on a 24GB GPU. We also
demonstrate the feasibility of training a 13B model using per-layer gradient
updates on the same hardware.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GINopic: Topic Modeling with Graph Isomorphism Network <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.02115v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.02115v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suman Adhya, Debarshi Kumar Sanyal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topic modeling is a widely used approach for analyzing and exploring large
document collections. Recent research efforts have incorporated pre-trained
contextualized language models, such as BERT embeddings, into topic modeling.
However, they often neglect the intrinsic informational value conveyed by
mutual dependencies between words. In this study, we introduce GINopic, a topic
modeling framework based on graph isomorphism networks to capture the
correlation between words. By conducting intrinsic (quantitative as well as
qualitative) and extrinsic evaluations on diverse benchmark datasets, we
demonstrate the effectiveness of GINopic compared to existing topic models and
highlight its potential for advancing topic modeling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a long paper for NAACL 2024 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ StreamingDialogue: Prolonged Dialogue Learning via Long Context
  Compression with Minimal Losses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08312v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08312v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jia-Nan Li, Quan Tu, Cunli Mao, Zhengtao Yu, Ji-Rong Wen, Rui Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Standard Large Language Models (LLMs) struggle with handling dialogues with
long contexts due to efficiency and consistency issues. According to our
observation, dialogue contexts are highly structured, and the special token of
\textit{End-of-Utterance} (EoU) in dialogues has the potential to aggregate
information. We refer to the EoU tokens as ``conversational attention sinks''
(conv-attn sinks). Accordingly, we introduce StreamingDialogue, which
compresses long dialogue history into conv-attn sinks with minimal losses, and
thus reduces computational complexity quadratically with the number of sinks
(i.e., the number of utterances). Current LLMs already demonstrate the ability
to handle long context window, e.g., a window size of 200K or more. To this
end, by compressing utterances into EoUs, our method has the potential to
handle more than 200K of utterances, resulting in a prolonged dialogue
learning. In order to minimize information losses from reconstruction after
compression, we design two learning strategies of short-memory reconstruction
(SMR) and long-memory reactivation (LMR). Our method outperforms strong
baselines in dialogue tasks and achieves a 4 $\times$ speedup while reducing
memory usage by 18 $\times$ compared to dense attention recomputation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 3-in-1: 2D Rotary Adaptation for Efficient Fine<span class="highlight-title">tuning</span>, Efficient
  Batching and Composability <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.00119v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.00119v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baohao Liao, Christof Monz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parameter-efficient finetuning (PEFT) methods effectively adapt large
language models (LLMs) to diverse downstream tasks, reducing storage and GPU
memory demands. Despite these advantages, several applications pose new
challenges to PEFT beyond mere parameter efficiency. One notable challenge
involves the efficient deployment of LLMs equipped with multiple task- or
user-specific adapters, particularly when different adapters are needed for
distinct requests within the same batch. Another challenge is the
interpretability of LLMs, which is crucial for understanding how LLMs function.
Previous studies introduced various approaches to address different challenges.
In this paper, we introduce a novel method, RoAd, which employs a
straightforward 2D rotation to adapt LLMs and addresses all the above
challenges: (1) RoAd is remarkably parameter-efficient, delivering optimal
performance on GLUE, eight commonsense reasoning tasks and four arithmetic
reasoning tasks with $<0.1\%$ trainable parameters; (2) RoAd facilitates the
efficient serving of requests requiring different adapters within a batch, with
an overhead comparable to element-wise multiplication instead of batch matrix
multiplication; (3) RoAd enhances LLM's interpretability through integration
within a framework of distributed interchange intervention, demonstrated via
composition experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024. Code: https://github.com/BaohaoLiao/road</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Internalizing ASR with Implicit Chain of Thought for Efficient
  Speech-to-Speech Conversational LLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17353v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17353v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robin Shing-Hei Yuen, Timothy Tin-Long Tse, Jian Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current speech-based LLMs are predominantly trained on extensive ASR and TTS
datasets, excelling in tasks related to these domains. However, their ability
to handle direct speech-to-speech conversations remains notably constrained.
These models often rely on an ASR-to-TTS chain-of-thought pipeline, converting
speech into text for processing before generating audio responses, which
introduces latency and loses audio features. We propose a method that
implicitly internalizes ASR chain of thought into a speech LLM, enhancing its
native speech understanding capabilities. Our approach reduces latency and
improves the model's native understanding of speech, paving the way for more
efficient and natural real-time audio interactions. We also release a
large-scale synthetic conversational dataset to facilitate further research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated for reviewer comments</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CVQA: Culturally-diverse Multilingual Visual Question Answering
  Benchmark <span class="chip">NeurIPS
  2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05967v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05967v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Romero, Chenyang Lyu, Haryo Akbarianto Wibowo, Teresa Lynn, Injy Hamed, Aditya Nanda Kishore, Aishik Mandal, Alina Dragonetti, Artem Abzaliev, Atnafu Lambebo Tonja, Bontu Fufa Balcha, Chenxi Whitehouse, Christian Salamea, Dan John Velasco, David Ifeoluwa Adelani, David Le Meur, Emilio Villa-Cueva, Fajri Koto, Fauzan Farooqui, Frederico Belcavello, Ganzorig Batnasan, Gisela Vallejo, Grainne Caulfield, Guido Ivetta, Haiyue Song, Henok Biadglign Ademtew, Hernán Maina, Holy Lovenia, Israel Abebe Azime, Jan Christian Blaise Cruz, Jay Gala, Jiahui Geng, Jesus-German Ortiz-Barajas, Jinheon Baek, Jocelyn Dunstan, Laura Alonso Alemany, Kumaranage Ravindu Yasas Nagasinghe, Luciana Benotti, Luis Fernando D'Haro, Marcelo Viridiano, Marcos Estecha-Garitagoitia, Maria Camila Buitrago Cabrera, Mario Rodríguez-Cantelar, Mélanie Jouitteau, Mihail Mihaylov, Mohamed Fazli Mohamed Imam, Muhammad Farid Adilazuarda, Munkhjargal Gochoo, Munkh-Erdene Otgonbold, Naome Etori, Olivier Niyomugisha, Paula Mónica Silva, Pranjal Chitale, Raj Dabre, Rendi Chevi, Ruochen Zhang, Ryandito Diandaru, Samuel Cahyawijaya, Santiago Góngora, Soyeong Jeong, Sukannya Purkayastha, Tatsuki Kuribayashi, Teresa Clifford, Thanmay Jayakumar, Tiago Timponi Torrent, Toqeer Ehsan, Vladimir Araujo, Yova Kementchedjhieva, Zara Burzo, Zheng Wei Lim, Zheng Xin Yong, Oana Ignat, Joan Nwatu, Rada Mihalcea, Thamar Solorio, Alham Fikri Aji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Question Answering (VQA) is an important task in multimodal AI, and it
is often used to test the ability of vision-language models to understand and
reason on knowledge present in both visual and textual data. However, most of
the current VQA models use datasets that are primarily focused on English and a
few major world languages, with images that are typically Western-centric.
While recent efforts have tried to increase the number of languages covered on
VQA datasets, they still lack diversity in low-resource languages. More
importantly, although these datasets often extend their linguistic range via
translation or some other approaches, they usually keep images the same,
resulting in narrow cultural representation. To address these limitations, we
construct CVQA, a new Culturally-diverse multilingual Visual Question Answering
benchmark, designed to cover a rich set of languages and cultures, where we
engage native speakers and cultural experts in the data collection process. As
a result, CVQA includes culturally-driven images and questions from across 30
countries on four continents, covering 31 languages with 13 scripts, providing
a total of 10k questions. We then benchmark several Multimodal Large Language
Models (MLLMs) on CVQA, and show that the dataset is challenging for the
current state-of-the-art models. This benchmark can serve as a probing
evaluation suite for assessing the cultural capability and bias of multimodal
models and hopefully encourage more research efforts toward increasing cultural
awareness and linguistic diversity in this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38th Conference on Neural Information Processing Systems (NeurIPS
  2024) Track on Datasets and Benchmarks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Model Compression with Neural Architecture Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06479v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06479v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rhea Sanjay Sukthanker, Benedikt Staffler, Frank Hutter, Aaron Klein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) exhibit remarkable reasoning abilities, allowing
them to generalize across a wide range of downstream tasks, such as commonsense
reasoning or instruction following. However, as LLMs scale, inference costs
become increasingly prohibitive, accumulating significantly over their life
cycle. This poses the question: Can we compress pre-trained LLMs to meet
diverse size and latency requirements? We leverage Neural Architecture Search
(NAS) to compress LLMs by pruning structural components, such as attention
heads, neurons, and layers, aiming to achieve a Pareto-optimal balance between
performance and efficiency. While NAS already achieved promising results on
small language models in previous work, in this paper we propose various
extensions that allow us to scale to LLMs. Compared to structural pruning
baselines, we show that NAS improves performance up to 3.4% on MMLU with an
on-device latency speedup.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WER We Stand: Benchmarking Urdu ASR Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11252v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11252v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samee Arif, Sualeha Farid, Aamina Jamal Khan, Mustafa Abbas, Agha Ali Raza, Awais Athar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a comprehensive evaluation of Urdu Automatic Speech
Recognition (ASR) models. We analyze the performance of three ASR model
families: Whisper, MMS, and Seamless-M4T using Word Error Rate (WER), along
with a detailed examination of the most frequent wrong words and error types
including insertions, deletions, and substitutions. Our analysis is conducted
using two types of datasets, read speech and conversational speech. Notably, we
present the first conversational speech dataset designed for benchmarking Urdu
ASR models. We find that seamless-large outperforms other ASR models on the
read speech dataset, while whisper-large performs best on the conversational
speech dataset. Furthermore, this evaluation highlights the complexities of
assessing ASR models for low-resource languages like Urdu using quantitative
metrics alone and emphasizes the need for a robust Urdu text normalization
system. Our findings contribute valuable insights for developing robust ASR
systems for low-resource languages like Urdu.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DREsS: Dataset for Rubric-based Essay Scoring on EFL Writing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16733v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16733v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haneul Yoo, Jieun Han, So-Yeon Ahn, Alice Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated essay scoring (AES) is a useful tool in English as a Foreign
Language (EFL) writing education, offering real-time essay scores for students
and instructors. However, previous AES models were trained on essays and scores
irrelevant to the practical scenarios of EFL writing education and usually
provided a single holistic score due to the lack of appropriate datasets. In
this paper, we release DREsS, a large-scale, standard dataset for rubric-based
automated essay scoring. DREsS comprises three sub-datasets: DREsS_New,
DREsS_Std., and DREsS_CASE. We collect DREsS_New, a real-classroom dataset with
2.3K essays authored by EFL undergraduate students and scored by English
education experts. We also standardize existing rubric-based essay scoring
datasets as DREsS_Std. We suggest CASE, a corruption-based augmentation
strategy for essays, which generates 40.1K synthetic samples of DREsS_CASE and
improves the baseline results by 45.44%. DREsS will enable further research to
provide a more accurate and practical AES system for EFL writing education.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Experimental Narratives: A Comparison of Human Crowdsourced Storytelling
  and AI Storytelling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.12902v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.12902v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nina Begus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper proposes a framework that combines behavioral and computational
experiments employing fictional prompts as a novel tool for investigating
cultural artifacts and social biases in storytelling both by humans and
generative AI. The study analyzes 250 stories authored by crowdworkers in June
2019 and 80 stories generated by GPT-3.5 and GPT-4 in March 2023 by merging
methods from narratology and inferential statistics. Both crowdworkers and
large language models responded to identical prompts about creating and falling
in love with an artificial human. The proposed experimental paradigm allows a
direct and controlled comparison between human and LLM-generated storytelling.
Responses to the Pygmalionesque prompts confirm the pervasive presence of the
Pygmalion myth in the collective imaginary of both humans and large language
models. All solicited narratives present a scientific or technological pursuit.
The analysis reveals that narratives from GPT-3.5 and particularly GPT-4 are
more progressive in terms of gender roles and sexuality than those written by
humans. While AI narratives with default settings and no additional prompting
can occasionally provide innovative plot twists, they offer less imaginative
scenarios and rhetoric than human-authored texts. The proposed framework argues
that fiction can be used as a window into human and AI-based collective
imaginary and social dimensions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ChartGemma: Visual Instruction-<span class="highlight-title">tuning</span> for Chart Reasoning in the Wild 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.04172v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.04172v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Masry, Megh Thakkar, Aayush Bajaj, Aaryaman Kartha, Enamul Hoque, Shafiq Joty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the ubiquity of charts as a data analysis, visualization, and
decision-making tool across industries and sciences, there has been a growing
interest in developing pre-trained foundation models as well as general purpose
instruction-tuned models for chart understanding and reasoning. However,
existing methods suffer crucial drawbacks across two critical axes affecting
the performance of chart representation models: they are trained on data
generated from underlying data tables of the charts, ignoring the visual trends
and patterns in chart images, and use weakly aligned vision-language backbone
models for domain-specific training, limiting their generalizability when
encountering charts in the wild. We address these important drawbacks and
introduce ChartGemma, a novel chart understanding and reasoning model developed
over PaliGemma. Rather than relying on underlying data tables, ChartGemma is
trained on instruction-tuning data generated directly from chart images, thus
capturing both high-level trends and low-level visual information from a
diverse set of charts. Our simple approach achieves state-of-the-art results
across $5$ benchmarks spanning chart summarization, question answering, and
fact-checking, and our elaborate qualitative studies on real-world charts show
that ChartGemma generates more realistic and factually correct summaries
compared to its contemporaries. We release the code, model checkpoints,
dataset, and demos at https://github.com/vis-nlp/ChartGemma.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Topic-Conversation Relevance (TCR) Dataset and Benchmarks <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00038v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00038v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaran Fan, Jamie Pool, Senja Filipi, Ross Cutler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Workplace meetings are vital to organizational collaboration, yet a large
percentage of meetings are rated as ineffective. To help improve meeting
effectiveness by understanding if the conversation is on topic, we create a
comprehensive Topic-Conversation Relevance (TCR) dataset that covers a variety
of domains and meeting styles. The TCR dataset includes 1,500 unique meetings,
22 million words in transcripts, and over 15,000 meeting topics, sourced from
both newly collected Speech Interruption Meeting (SIM) data and existing public
datasets. Along with the text data, we also open source scripts to generate
synthetic meetings or create augmented meetings from the TCR dataset to enhance
data diversity. For each data source, benchmarks are created using GPT-4 to
evaluate the model accuracy in understanding transcription-topic relevance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in 38th Conference on Neural Information Processing
  Systems (NeurIPS 2024) Track on Datasets and Benchmarks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pipeline Parallelism with Controllable Memory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15362v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15362v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Penghui Qi, Xinyi Wan, Nyamdavaa Amar, Min Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pipeline parallelism has been widely explored, but most existing schedules
lack a systematic methodology. In this paper, we propose a framework to
decompose pipeline schedules as repeating a building block, and show that the
lifespan of the building block decides the peak activation memory of the
pipeline schedule. Guided by the observations, we find that almost all existing
pipeline schedules, to the best of our knowledge, are memory inefficient. To
address this, we introduce a family of memory efficient building blocks with
controllable activation memory, which can reduce the peak activation memory to
1/2 of 1F1B without sacrificing efficiency, and even to 1/3 with comparable
throughput. We can also achieve almost zero pipeline bubbles while maintaining
the same activation memory as 1F1B. Our evaluations demonstrate that in pure
pipeline parallelism settings, our methods outperform 1F1B by from 7% to 55% in
terms of throughput. When employing a grid search over hybrid parallelism
hyperparameters in practical scenarios, our methods demonstrate a 16%
throughput improvement over the 1F1B baseline for large language models. The
implementation is open-sourced at
https://github.com/sail-sg/zero-bubble-pipeline-parallelism.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Retrieval: End-to-End Information Retrieval with One Large Language
  Model <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00801v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00801v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiaoyu Tang, Jiawei Chen, Zhuoqun Li, Bowen Yu, Yaojie Lu, Cheng Fu, Haiyang Yu, Hongyu Lin, Fei Huang, Ben He, Xianpei Han, Le Sun, Yongbin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of large language models (LLMs) has significantly transformed both
the construction and application of information retrieval (IR) systems.
However, current interactions between IR systems and LLMs remain limited, with
LLMs merely serving as part of components within IR systems, and IR systems
being constructed independently of LLMs. This separated architecture restricts
knowledge sharing and deep collaboration between them. In this paper, we
introduce Self-Retrieval, a novel end-to-end LLM-driven information retrieval
architecture. Self-Retrieval unifies all essential IR functions within a single
LLM, leveraging the inherent capabilities of LLMs throughout the IR process.
Specifically, Self-Retrieval internalizes the retrieval corpus through
self-supervised learning, transforms the retrieval process into sequential
passage generation, and performs relevance assessment for reranking.
Experimental results demonstrate that Self-Retrieval not only outperforms
existing retrieval approaches by a significant margin, but also substantially
enhances the performance of LLM-driven downstream applications like
retrieval-augmented generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 Camera-ready Version. Code:
  https://github.com/icip-cas/SelfRetrieval</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GestureGPT: Toward Zero-Shot Free-Form Hand Gesture Understanding with
  Large Language Model Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.12821v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.12821v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Zeng, Xiaoyu Wang, Tengxiang Zhang, Chun Yu, Shengdong Zhao, Yiqiang Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing gesture interfaces only work with a fixed set of gestures defined
either by interface designers or by users themselves, which introduces learning
or demonstration efforts that diminish their naturalness. Humans, on the other
hand, understand free-form gestures by synthesizing the gesture, context,
experience, and common sense. In this way, the user does not need to learn,
demonstrate, or associate gestures. We introduce GestureGPT, a free-form hand
gesture understanding framework that mimics human gesture understanding
procedures to enable a natural free-form gestural interface. Our framework
leverages multiple Large Language Model agents to manage and synthesize gesture
and context information, then infers the interaction intent by associating the
gesture with an interface function. More specifically, our triple-agent
framework includes a Gesture Description Agent that automatically segments and
formulates natural language descriptions of hand poses and movements based on
hand landmark coordinates. The description is deciphered by a Gesture Inference
Agent through self-reasoning and querying about the interaction context (e.g.,
interaction history, gaze data), which is managed by a Context Management
Agent. Following iterative exchanges, the Gesture Inference Agent discerns the
user's intent by grounding it to an interactive function. We validated our
framework offline under two real-world scenarios: smart home control and online
video streaming. The average zero-shot Top-1/Top-5 grounding accuracies are
44.79%/83.59% for smart home tasks and 37.50%/73.44% for video streaming tasks.
We also provide an extensive discussion that includes rationale for model
selection, generalizability, and future research directions for a practical
system etc.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted to the ISS 2024 track of the Proceedings
  of the ACM on Human-Computer Interaction</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ToolPlanner: A Tool Augmented LLM for Multi Granularity Instructions
  with Path Planning and Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14826v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14826v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinzhuo Wu, Wei Liu, Jian Luan, Bin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, tool-augmented LLMs have gained increasing attention. Given an
instruction, tool-augmented LLMs can interact with various external tools in
multiple rounds and provide a final answer. However, previous LLMs were trained
on overly detailed instructions, which included API names or parameters, while
real users would not explicitly mention these API details. This leads to a gap
between trained LLMs and real-world scenarios. In addition, most works ignore
whether the interaction process follows the instruction. To address these
issues, we constructed a training dataset called MGToolBench, which contains
statement and category-level instructions to better reflect real-world
scenarios. In addition, we propose ToolPlanner, a two-stage reinforcement
learning framework that utilizes path planning and two feedback mechanisms to
enhance the LLM's task completion and instruction-following capabilities.
Experimental results show that ToolPlanner significantly improves the Match
Rate, Pass Rate and Win Rate by 26.8%, 20.2%, and 5.6% compared to the SOTA
model. Human evaluation verifies that the multi-granularity instructions can
better align with users' usage habits. Our data and code will be released upon
acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DISP-LLM: Dimension-Independent Structural Pruning for Large Language
  Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11988v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11988v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shangqian Gao, Chi-Heng Lin, Ting Hua, Tang Zheng, Yilin Shen, Hongxia Jin, Yen-Chang Hsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have achieved remarkable success in various
natural language processing tasks, including language modeling, understanding,
and generation. However, the increased memory and computational costs
associated with these models pose significant challenges for deployment on
resource-limited devices. Structural pruning has emerged as a promising
solution to reduce the costs of LLMs without requiring post-processing steps.
Prior structural pruning methods either follow the dependence of structures at
the cost of limiting flexibility, or introduce non-trivial additional
parameters by incorporating different projection matrices. In this work, we
propose a novel approach that relaxes the constraint imposed by regular
structural pruning methods and eliminates the structural dependence along the
embedding dimension. Our dimension-independent structural pruning method offers
several benefits. Firstly, our method enables different blocks to utilize
different subsets of the feature maps. Secondly, by removing structural
dependence, we facilitate each block to possess varying widths along its input
and output dimensions, thereby significantly enhancing the flexibility of
structural pruning. We evaluate our method on various LLMs, including OPT,
LLaMA, LLaMA-2, Phi-1.5, and Phi-2. Experimental results demonstrate that our
approach outperforms other state-of-the-art methods, showing for the first time
that structural pruning can achieve an accuracy similar to semi-structural
pruning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MARAGS: A Multi-Adapter System for Multi-Task Retrieval Augmented
  Generation Question Answering <span class="chip">KDD</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.03171v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.03171v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mitchell DeHaven
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we present a multi-adapter retrieval augmented generation
system (MARAGS) for Meta's Comprehensive RAG (CRAG) competition for KDD CUP
2024. CRAG is a question answering dataset contains 3 different subtasks aimed
at realistic question and answering RAG related tasks, with a diverse set of
question topics, question types, time dynamic answers, and questions featuring
entities of varying popularity.
  Our system follows a standard setup for web based RAG, which uses processed
web pages to provide context for an LLM to produce generations, while also
querying API endpoints for additional information. MARAGS also utilizes
multiple different adapters to solve the various requirements for these tasks
with a standard cross-encoder model for ranking candidate passages relevant for
answering the question. Our system achieved 2nd place for Task 1 as well as 3rd
place on Task 2.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CRAG KDD Cup 24 Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Foundations of Tokenization: Statistical and Computational Concerns 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11606v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11606v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan Luis Gastaldi, John Terilla, Luca Malagutti, Brian DuSell, Tim Vieira, Ryan Cotterell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tokenization - the practice of converting strings of characters from an
alphabet into sequences of tokens over a vocabulary - is a critical step in the
NLP pipeline. The use of token representations is widely credited with
increased model performance but is also the source of many undesirable
behaviors, such as spurious ambiguity or inconsistency. Despite its recognized
importance as a standard representation method in NLP, the theoretical
underpinnings of tokenization are not yet fully understood. In particular, the
impact of tokenization on statistical estimation has been investigated mostly
through empirical means. The present paper contributes to addressing this
theoretical gap by proposing a unified formal framework for representing and
analyzing tokenizer models. Based on the category of stochastic maps, this
framework enables us to establish general conditions for a principled use of
tokenizers, and most importantly, the necessary and sufficient conditions for a
tokenizer model to preserve the consistency of statistical estimators.
Additionally, we discuss statistical and computational concerns crucial for
designing and implementing tokenizer models, such as inconsistency, ambiguity,
tractability, and boundedness. The framework and results advanced in this paper
contribute to building robust theoretical foundations for representations in
neural language modeling that can inform future empirical research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Grammar-Aligned Decoding <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.21047v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.21047v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kanghee Park, Jiayu Wang, Taylor Berg-Kirkpatrick, Nadia Polikarpova, Loris D'Antoni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) struggle with reliably generating highly
structured outputs, such as program code, mathematical formulas, or well-formed
markup. Constrained decoding approaches mitigate this problem by greedily
restricting what tokens an LLM can output at each step to guarantee that the
output matches a given constraint. Specifically, in grammar-constrained
decoding (GCD), the LLM's output must follow a given grammar. In this paper, we
demonstrate that GCD techniques (and in general constrained decoding
techniques) can distort the LLM's distribution, leading to outputs that are
grammatical but appear with likelihoods that are not proportional to the ones
given by the LLM, and so ultimately are low-quality. We call the problem of
aligning sampling with a grammar constraint, grammar-aligned decoding (GAD),
and propose adaptive sampling with approximate expected futures (ASAp), a
decoding algorithm that guarantees the output to be grammatical while provably
producing outputs that match the conditional probability of the LLM's
distribution conditioned on the given grammar constraint. Our algorithm uses
prior sample outputs to soundly overapproximate the future grammaticality of
different output prefixes. Our evaluation on code generation and structured NLP
tasks shows how ASAp often produces outputs with higher likelihood (according
to the LLM's distribution) than existing GCD techniques, while still enforcing
the desired grammatical constraints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Weakly Supervised Veracity Classification with LLM-Predicted Credibility
  Signals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.07601v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.07601v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        João A. Leite, Olesya Razuvayevskaya, Kalina Bontcheva, Carolina Scarton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Credibility signals represent a wide range of heuristics typically used by
journalists and fact-checkers to assess the veracity of online content.
Automating the extraction of credibility signals presents significant
challenges due to the necessity of training high-accuracy, signal-specific
extractors, coupled with the lack of sufficiently large annotated datasets.
This paper introduces Pastel (Prompted weAk Supervision wiTh crEdibility
signaLs), a weakly supervised approach that leverages large language models
(LLMs) to extract credibility signals from web content, and subsequently
combines them to predict the veracity of content without relying on human
supervision. We validate our approach using four article-level misinformation
detection datasets, demonstrating that Pastel outperforms zero-shot veracity
detection by 38.3% and achieves 86.7% of the performance of the
state-of-the-art system trained with human supervision. Moreover, in
cross-domain settings where training and testing datasets originate from
different domains, Pastel significantly outperforms the state-of-the-art
supervised model by 63%. We further study the association between credibility
signals and veracity, and perform an ablation study showing the impact of each
signal on model performance. Our findings reveal that 12 out of the 19 proposed
signals exhibit strong associations with veracity across all datasets, while
some signals show domain-specific strengths.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Autonomous Agents: Adaptive-planning, Reasoning, and Acting in
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.06458v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.06458v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhishek Dutta, Yen-Che Hsiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel in-context learning algorithm for building autonomous
decision-making language agents. The language agent continuously attempts to
solve the same task by self-correcting each time the task fails. Our selected
language agent demonstrates the ability to solve tasks in a text-based game
environment. Our results show that the gemma-2-9b-it language model, using our
proposed method, can successfully complete two of six tasks that failed in the
first attempt. This highlights the effectiveness of our approach in enhancing
the problem-solving capabilities of a single language model through
self-correction, paving the way for more advanced autonomous agents. The code
is publicly available at
https://github.com/YenCheHsiao/AutonomousLLMAgentwithAdaptingPlanning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Device-Directed Speech Detection for Follow-up Conversations Using Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00023v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00023v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         Ognjen,  Rudovic, Pranay Dighe, Yi Su, Vineet Garg, Sameer Dharur, Xiaochuan Niu, Ahmed H. Abdelaziz, Saurabh Adya, Ahmed Tewfik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Follow-up conversations with virtual assistants (VAs) enable a user to
seamlessly interact with a VA without the need to repeatedly invoke it using a
keyword (after the first query). Therefore, accurate Device-directed Speech
Detection (DDSD) from the follow-up queries is critical for enabling
naturalistic user experience. To this end, we explore the notion of Large
Language Models (LLMs) and model the first query when making inference about
the follow-ups (based on the ASR-decoded text), via prompting of a pretrained
LLM, or by adapting a binary classifier on top of the LLM. In doing so, we also
exploit the ASR uncertainty when designing the LLM prompts. We show on the
real-world dataset of follow-up conversations that this approach yields large
gains (20-40% reduction in false alarms at 10% fixed false rejects) due to the
joint modeling of the previous speech context and ASR uncertainty, compared to
when follow-ups are modeled alone.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Large Language Models in Theory of Mind Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.02083v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.02083v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michal Kosinski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Eleven Large Language Models (LLMs) were assessed using a custom-made battery
of false-belief tasks, considered a gold standard in testing Theory of Mind
(ToM) in humans. The battery included 640 prompts spread across 40 diverse
tasks, each one including a false-belief scenario, three closely matched
true-belief control scenarios, and the reversed versions of all four. To solve
a single task, a model needed to correctly answer 16 prompts across all eight
scenarios. Smaller and older models solved no tasks; GPT-3-davinci-003 (from
November 2022) and ChatGPT-3.5-turbo (from March 2023) solved 20% of the tasks;
ChatGPT-4 (from June 2023) solved 75% of the tasks, matching the performance of
six-year-old children observed in past studies. We explore the potential
interpretation of these findings, including the intriguing possibility that
ToM, previously considered exclusive to humans, may have spontaneously emerged
as a byproduct of LLMs' improving language skills.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>TRY RUNNING ToM EXPERIMENTS ON YOUR OWN: The code and tasks used in
  this study are available at Colab
  (https://colab.research.google.com/drive/1ZRtmw87CdA4xp24DNS_Ik_uA2ypaRnoU).
  Don't worry if you are not an expert coder, you should be able to run this
  code with no-to-minimum Python skills. Or copy-paste the tasks to ChatGPT's
  web interface. Proceedings of the National Academy of Sciences (PNAS) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gazelle: An Instruction Dataset for Arabic Writing Assistance <span class="chip">EMNLP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18163v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18163v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samar M. Magdy, Fakhraddin Alwajih, Sang Yun Kwon, Reem Abdel-Salam, Muhammad Abdul-Mageed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Writing has long been considered a hallmark of human intelligence and remains
a pinnacle task for artificial intelligence (AI) due to the intricate cognitive
processes involved. Recently, rapid advancements in generative AI, particularly
through the development of Large Language Models (LLMs), have significantly
transformed the landscape of writing assistance. However, underrepresented
languages like Arabic encounter significant challenges in the development of
advanced AI writing tools, largely due to the limited availability of data.
This scarcity constrains the training of effective models, impeding the
creation of sophisticated writing assistance technologies. To address these
issues, we present Gazelle, a comprehensive dataset for Arabic writing
assistance. In addition, we offer an evaluation framework designed to enhance
Arabic writing assistance tools. Our human evaluation of leading LLMs,
including GPT-4, GPT-4o, Cohere Command R+, and Gemini 1.5 Pro, highlights
their respective strengths and limitations in addressing the challenges of
Arabic writing. Our findings underscore the need for continuous model training
and dataset enrichment to manage the complexities of Arabic language
processing, paving the way for more effective AI-powered Arabic writing tools.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP2024 Finding Camara-ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ReDel: A Toolkit for LLM-Powered Recursive Multi-Agent Systems <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02248v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02248v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Zhu, Liam Dugan, Chris Callison-Burch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, there has been increasing interest in using Large Language Models
(LLMs) to construct complex multi-agent systems to perform tasks such as
compiling literature reviews, drafting consumer reports, and planning
vacations. Many tools and libraries exist for helping create such systems,
however none support recursive multi-agent systems -- where the models
themselves flexibly decide when to delegate tasks and how to organize their
delegation structure. In this work, we introduce ReDel: a toolkit for recursive
multi-agent systems that supports custom tool-use, delegation schemes,
event-based logging, and interactive replay in an easy-to-use web interface. We
show that, using ReDel, we are able to easily identify potential areas of
improvements through the visualization and debugging tools. Our code,
documentation, and PyPI package are open-source and free to use under the MIT
license at https://github.com/zhudotexe/redel.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 (Demo Track)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decoding Diffusion: A Scalable Framework for Unsupervised Analysis of
  Latent Space Biases and Representations Using Natural Language <span class="highlight-title">Prompt</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21314v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21314v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        E. Zhixuan Zeng, Yuhao Chen, Alexander Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in image generation have made diffusion models powerful tools
for creating high-quality images. However, their iterative denoising process
makes understanding and interpreting their semantic latent spaces more
challenging than other generative models, such as GANs. Recent methods have
attempted to address this issue by identifying semantically meaningful
directions within the latent space. However, they often need manual
interpretation or are limited in the number of vectors that can be trained,
restricting their scope and utility. This paper proposes a novel framework for
unsupervised exploration of diffusion latent spaces. We directly leverage
natural language prompts and image captions to map latent directions. This
method allows for the automatic understanding of hidden features and supports a
broader range of analysis without the need to train specific vectors. Our
method provides a more scalable and interpretable understanding of the semantic
knowledge encoded within diffusion models, facilitating comprehensive analysis
of latent biases and the nuanced representations these models learn.
Experimental results show that our framework can uncover hidden patterns and
associations in various domains, offering new insights into the
interpretability of diffusion model latent spaces.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Low-Resource Authorship Style Transfer: Can Non-Famous Authors Be
  Imitated? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08986v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08986v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ajay Patel, Nicholas Andrews, Chris Callison-Burch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Authorship style transfer involves altering text to match the style of a
target author whilst preserving the original meaning. Existing unsupervised
approaches like STRAP have largely focused on style transfer to target authors
with many examples of their writing style in books, speeches, or other
published works. This high-resource training data requirement (often greater
than 100,000 words) makes these approaches primarily useful for style transfer
to published authors, politicians, or other well-known figures and authorship
styles, while style transfer to non-famous authors has not been well-studied.
We introduce the low-resource authorship style transfer task, a more
challenging class of authorship style transfer where only a limited amount of
text in the target author's style may exist. In our experiments, we
specifically choose source and target authors from Reddit and style transfer
their Reddit posts, limiting ourselves to just 16 posts (on average ~500 words)
of the target author's style. Style transfer accuracy is typically measured by
how often a classifier or human judge will classify an output as written by the
target author. Recent authorship representations models excel at authorship
identification even with just a few writing samples, making automatic
evaluation of this task possible for the first time through evaluation metrics
we propose. Our results establish an in-context learning technique we develop
as the strongest baseline, though we find current approaches do not yet achieve
mastery of this challenging task. We release our data and implementations to
encourage further investigation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AV2Wav: Diffusion-Based Re-synthesis from Continuous Self-supervised
  Features for Audio-Visual Speech Enhancement <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.08030v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.08030v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ju-Chieh Chou, Chung-Ming Chien, Karen Livescu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech enhancement systems are typically trained using pairs of clean and
noisy speech. In audio-visual speech enhancement (AVSE), there is not as much
ground-truth clean data available; most audio-visual datasets are collected in
real-world environments with background noise and reverberation, hampering the
development of AVSE. In this work, we introduce AV2Wav, a resynthesis-based
audio-visual speech enhancement approach that can generate clean speech despite
the challenges of real-world training data. We obtain a subset of nearly clean
speech from an audio-visual corpus using a neural quality estimator, and then
train a diffusion model on this subset to generate waveforms conditioned on
continuous speech representations from AV-HuBERT with noise-robust training. We
use continuous rather than discrete representations to retain prosody and
speaker information. With this vocoding task alone, the model can perform
speech enhancement better than a masking-based baseline. We further fine-tune
the diffusion model on clean/noisy utterance pairs to improve the performance.
Our approach outperforms a masking-based baseline in terms of both automatic
metrics and a human listening test and is close in quality to the target speech
in the listening test. Audio samples can be found at
https://home.ttic.edu/~jcchou/demo/avse/avse_demo.html.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>extended version for the accepted paper at ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unlocking Temporal Question Answering for Large Language Models with
  Tailor-Made Reasoning Logic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.15014v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.15014v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingxuan Li, Liying Cheng, Qingyu Tan, Hwee Tou Ng, Shafiq Joty, Lidong Bing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The temporal aspect is a significant dimension of our reality. We notice the
challenge that large language models (LLMs) face when engaging in temporal
reasoning. Our preliminary experiments show that methods involving the
generation of intermediate reasoning steps, such as chain-of-thought and
program-aided language models, do not consistently boost the performance of
complex temporal question-answering tasks. This limitation can be attributed to
the LLMs' inadequate understanding of temporal information. To address this
problem, we propose TempLogic, a novel framework designed specifically for
temporal question-answering tasks across three levels of reasoning. TempLogic
incorporates retrieval-guided context distillation, temporal data extraction,
and tailor-made logic reasoning. Extensive experiments and analysis demonstrate
the effectiveness of our framework in solving intricate time-bound reasoning
tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">135</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Caching for Faster Video Generation with Diffusion Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02397v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02397v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kumara Kahatapitiya, Haozhe Liu, Sen He, Ding Liu, Menglin Jia, Michael S. Ryoo, Tian Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating temporally-consistent high-fidelity videos can be computationally
expensive, especially over longer temporal spans. More-recent Diffusion
Transformers (DiTs) -- despite making significant headway in this context --
have only heightened such challenges as they rely on larger models and heavier
attention mechanisms, resulting in slower inference speeds. In this paper, we
introduce a training-free method to accelerate video DiTs, termed Adaptive
Caching (AdaCache), which is motivated by the fact that "not all videos are
created equal": meaning, some videos require fewer denoising steps to attain a
reasonable quality than others. Building on this, we not only cache
computations through the diffusion process, but also devise a caching schedule
tailored to each video generation, maximizing the quality-latency trade-off. We
further introduce a Motion Regularization (MoReg) scheme to utilize video
information within AdaCache, essentially controlling the compute allocation
based on motion content. Altogether, our plug-and-play contributions grant
significant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video
generation) without sacrificing the generation quality, across multiple video
DiT baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project-page is available at https://adacache-dit.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AutoVFX: Physically Realistic Video Editing from Natural Language
  Instructions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02394v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02394v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao-Yu Hsu, Zhi-Hao Lin, Albert Zhai, Hongchi Xia, Shenlong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern visual effects (VFX) software has made it possible for skilled artists
to create imagery of virtually anything. However, the creation process remains
laborious, complex, and largely inaccessible to everyday users. In this work,
we present AutoVFX, a framework that automatically creates realistic and
dynamic VFX videos from a single video and natural language instructions. By
carefully integrating neural scene modeling, LLM-based code generation, and
physical simulation, AutoVFX is able to provide physically-grounded,
photorealistic editing effects that can be controlled directly using natural
language instructions. We conduct extensive experiments to validate AutoVFX's
efficacy across a diverse spectrum of videos and instructions. Quantitative and
qualitative results suggest that AutoVFX outperforms all competing methods by a
large margin in generative quality, instruction alignment, editing versatility,
and physical plausibility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://haoyuhsu.github.io/autovfx-website/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Training-free Regional <span class="highlight-title">Prompt</span>ing for Diffusion Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02395v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02395v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anthony Chen, Jianjin Xu, Wenzhao Zheng, Gaole Dai, Yida Wang, Renrui Zhang, Haofan Wang, Shanghang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have demonstrated excellent capabilities in text-to-image
generation. Their semantic understanding (i.e., prompt following) ability has
also been greatly improved with large language models (e.g., T5, Llama).
However, existing models cannot perfectly handle long and complex text prompts,
especially when the text prompts contain various objects with numerous
attributes and interrelated spatial relationships. While many regional
prompting methods have been proposed for UNet-based models (SD1.5, SDXL), but
there are still no implementations based on the recent Diffusion Transformer
(DiT) architecture, such as SD3 and FLUX.1.In this report, we propose and
implement regional prompting for FLUX.1 based on attention manipulation, which
enables DiT with fined-grained compositional text-to-image generation
capability in a training-free manner. Code is available at
https://github.com/antonioo-c/Regional-Prompting-FLUX.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at
  https://github.com/antonioo-c/Regional-Prompting-FLUX</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Length Image Tokenization via Recurrent Allocation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02393v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02393v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shivam Duggal, Phillip Isola, Antonio Torralba, William T. Freeman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current vision systems typically assign fixed-length representations to
images, regardless of the information content. This contrasts with human
intelligence - and even large language models - which allocate varying
representational capacities based on entropy, context and familiarity. Inspired
by this, we propose an approach to learn variable-length token representations
for 2D images. Our encoder-decoder architecture recursively processes 2D image
tokens, distilling them into 1D latent tokens over multiple iterations of
recurrent rollouts. Each iteration refines the 2D tokens, updates the existing
1D latent tokens, and adaptively increases representational capacity by adding
new tokens. This enables compression of images into a variable number of
tokens, ranging from 32 to 256. We validate our tokenizer using reconstruction
loss and FID metrics, demonstrating that token count aligns with image entropy,
familiarity and downstream task requirements. Recurrent token processing with
increasing representational capacity in each iteration shows signs of token
specialization, revealing potential for object / part discovery.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code at: https://github.com/ShivamDuggal4/adaptive-length-tokenizer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Far is Video Generation from World Model: A Physical Law Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02385v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02385v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bingyi Kang, Yang Yue, Rui Lu, Zhijie Lin, Yang Zhao, Kaixin Wang, Gao Huang, Jiashi Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  OpenAI's Sora highlights the potential of video generation for developing
world models that adhere to fundamental physical laws. However, the ability of
video generation models to discover such laws purely from visual data without
human priors can be questioned. A world model learning the true law should give
predictions robust to nuances and correctly extrapolate on unseen scenarios. In
this work, we evaluate across three key scenarios: in-distribution,
out-of-distribution, and combinatorial generalization. We developed a 2D
simulation testbed for object movement and collisions to generate videos
deterministically governed by one or more classical mechanics laws. This
provides an unlimited supply of data for large-scale experimentation and
enables quantitative evaluation of whether the generated videos adhere to
physical laws. We trained diffusion-based video generation models to predict
object movements based on initial frames. Our scaling experiments show perfect
generalization within the distribution, measurable scaling behavior for
combinatorial generalization, but failure in out-of-distribution scenarios.
Further experiments reveal two key insights about the generalization mechanisms
of these models: (1) the models fail to abstract general physical rules and
instead exhibit "case-based" generalization behavior, i.e., mimicking the
closest training example; (2) when generalizing to new cases, models are
observed to prioritize different factors when referencing training data: color
> size > velocity > shape. Our study suggests that scaling alone is
insufficient for video generation models to uncover fundamental physical laws,
despite its role in Sora's broader success. See our project page at
https://phyworld.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning General-Purpose Biomedical Volume Representations using
  Randomized Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02372v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02372v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neel Dey, Benjamin Billot, Hallee E. Wong, Clinton J. Wang, Mengwei Ren, P. Ellen Grant, Adrian V. Dalca, Polina Golland
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current volumetric biomedical foundation models struggle to generalize as
public 3D datasets are small and do not cover the broad diversity of medical
procedures, conditions, anatomical regions, and imaging protocols. We address
this by creating a representation learning method that instead anticipates
strong domain shifts at training time itself. We first propose a data engine
that synthesizes highly variable training samples that enable generalization to
new biomedical contexts. To then train a single 3D network for any voxel-level
task, we develop a contrastive learning method that pretrains the network to be
stable against nuisance imaging variation simulated by the data engine, a key
inductive bias for generalization. This network's features can be used as
robust representations of input images for downstream tasks and its weights
provide a strong, dataset-agnostic initialization for finetuning on new
datasets. As a result, we set new standards across both multimodality
registration and few-shot segmentation, a first for any 3D biomedical vision
model, all without (pre-)training on any existing dataset of real images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and model weights available at
  https://github.com/neel-dey/anatomix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Machine learning identification of maternal inflammatory response and
  histologic choroamnionitis from placental membrane whole slide images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02354v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02354v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhishek Sharma, Ramin Nateghi, Marina Ayad, Lee A. D. Cooper, Jeffery A. Goldstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The placenta forms a critical barrier to infection through pregnancy, labor
and, delivery. Inflammatory processes in the placenta have short-term, and
long-term consequences for offspring health. Digital pathology and machine
learning can play an important role in understanding placental inflammation,
and there have been very few investigations into methods for predicting and
understanding Maternal Inflammatory Response (MIR). This work intends to
investigate the potential of using machine learning to understand MIR based on
whole slide images (WSI), and establish early benchmarks. To that end, we use
Multiple Instance Learning framework with 3 feature extractors: ImageNet-based
EfficientNet-v2s, and 2 histopathology foundation models, UNI and Phikon to
investigate predictability of MIR stage from histopathology WSIs. We also
interpret predictions from these models using the learned attention maps from
these models. We also use the MIL framework for predicting white blood cells
count (WBC) and maximum fever temperature ($T_{max}$). Attention-based MIL
models are able to classify MIR with a balanced accuracy of up to 88.5% with a
Cohen's Kappa ($\kappa$) of up to 0.772. Furthermore, we found that the
pathology foundation models (UNI and Phikon) are both able to achieve higher
performance with balanced accuracy and $\kappa$, compared to ImageNet-based
feature extractor (EfficientNet-v2s). For WBC and $T_{max}$ prediction, we
found mild correlation between actual values and those predicted from
histopathology WSIs. We used MIL framework for predicting MIR stage from WSIs,
and compared effectiveness of foundation models as feature extractors, with
that of an ImageNet-based model. We further investigated model failure cases
and found them to be either edge cases prone to interobserver variability,
examples of pathologist's overreach, or mislabeled due to processing errors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physically Based Neural Bidirectional Reflectance Distribution Function 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02347v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02347v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenliang Zhou, Alejandro Sztrajman, Gilles Rainer, Fangcheng Zhong, Fazilet Gokbudak, Zhilin Guo, Weihao Xia, Rafal Mantiuk, Cengiz Oztireli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the physically based neural bidirectional reflectance
distribution function (PBNBRDF), a novel, continuous representation for
material appearance based on neural fields. Our model accurately reconstructs
real-world materials while uniquely enforcing physical properties for realistic
BRDFs, specifically Helmholtz reciprocity via reparametrization and energy
passivity via efficient analytical integration. We conduct a systematic
analysis demonstrating the benefits of adhering to these physical laws on the
visual quality of reconstructed materials. Additionally, we enhance the color
accuracy of neural BRDFs by introducing chromaticity enforcement supervising
the norms of RGB channels. Through both qualitative and quantitative
experiments on multiple databases of measured real-world BRDFs, we show that
adhering to these physical constraints enables neural fields to more faithfully
and stably represent the original data and achieve higher rendering quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MVPaint: Synchronized Multi-View Diffusion for Painting Anything 3D 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02336v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02336v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Cheng, Juncheng Mu, Xianfang Zeng, Xin Chen, Anqi Pang, Chi Zhang, Zhibin Wang, Bin Fu, Gang Yu, Ziwei Liu, Liang Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Texturing is a crucial step in the 3D asset production workflow, which
enhances the visual appeal and diversity of 3D assets. Despite recent
advancements in Text-to-Texture (T2T) generation, existing methods often yield
subpar results, primarily due to local discontinuities, inconsistencies across
multiple views, and their heavy dependence on UV unwrapping outcomes. To tackle
these challenges, we propose a novel generation-refinement 3D texturing
framework called MVPaint, which can generate high-resolution, seamless textures
while emphasizing multi-view consistency. MVPaint mainly consists of three key
modules. 1) Synchronized Multi-view Generation (SMG). Given a 3D mesh model,
MVPaint first simultaneously generates multi-view images by employing an SMG
model, which leads to coarse texturing results with unpainted parts due to
missing observations. 2) Spatial-aware 3D Inpainting (S3I). To ensure complete
3D texturing, we introduce the S3I method, specifically designed to effectively
texture previously unobserved areas. 3) UV Refinement (UVR). Furthermore,
MVPaint employs a UVR module to improve the texture quality in the UV space,
which first performs a UV-space Super-Resolution, followed by a Spatial-aware
Seam-Smoothing algorithm for revising spatial texturing discontinuities caused
by UV unwrapping. Moreover, we establish two T2T evaluation benchmarks: the
Objaverse T2T benchmark and the GSO T2T benchmark, based on selected
high-quality 3D meshes from the Objaverse dataset and the entire GSO dataset,
respectively. Extensive experimental results demonstrate that MVPaint surpasses
existing state-of-the-art methods. Notably, MVPaint could generate
high-fidelity textures with minimal Janus issues and highly enhanced cross-view
consistency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://mvpaint.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusion-based Generative Multicasting with Intent-aware Semantic
  Decomposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02334v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02334v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinkai Liu, Mahdi Boloursaz Mashhadi, Li Qiao, Yi Ma, Rahim Tafazolli, Mehdi Bennis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative diffusion models (GDMs) have recently shown great success in
synthesizing multimedia signals with high perceptual quality enabling highly
efficient semantic communications in future wireless networks. In this paper,
we develop an intent-aware generative semantic multicasting framework utilizing
pre-trained diffusion models. In the proposed framework, the transmitter
decomposes the source signal to multiple semantic classes based on the
multi-user intent, i.e. each user is assumed to be interested in details of
only a subset of the semantic classes. The transmitter then sends to each user
only its intended classes, and multicasts a highly compressed semantic map to
all users over shared wireless resources that allows them to locally synthesize
the other classes, i.e. non-intended classes, utilizing pre-trained diffusion
models. The signal retrieved at each user is thereby partially reconstructed
and partially synthesized utilizing the received semantic map. This improves
utilization of the wireless resources, with better preserving privacy of the
non-intended classes. We design a communication/computation-aware scheme for
per-class adaptation of the communication parameters, such as the transmission
power and compression rate to minimize the total latency of retrieving signals
at multiple receivers, tailored to the prevailing channel conditions as well as
the users reconstruction/synthesis distortion/perception requirements. The
simulation results demonstrate significantly reduced per-user latency compared
with non-generative and intent-unaware multicasting benchmarks while
maintaining high perceptual quality of the signals retrieved at the users.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PPLLaVA: Varied Video Sequence Understanding With <span class="highlight-title">Prompt</span> Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02327v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02327v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruyang Liu, Haoran Tang, Haibo Liu, Yixiao Ge, Ying Shan, Chen Li, Jiankun Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The past year has witnessed the significant advancement of video-based large
language models. However, the challenge of developing a unified model for both
short and long video understanding remains unresolved. Most existing video LLMs
cannot handle hour-long videos, while methods custom for long videos tend to be
ineffective for shorter videos and images. In this paper, we identify the key
issue as the redundant content in videos. To address this, we propose a novel
pooling strategy that simultaneously achieves token compression and
instruction-aware visual feature aggregation. Our model is termed Prompt-guided
Pooling LLaVA, or PPLLaVA for short. Specifically, PPLLaVA consists of three
core components: the CLIP-based visual-prompt alignment that extracts visual
information relevant to the user's instructions, the prompt-guided pooling that
compresses the visual sequence to arbitrary scales using convolution-style
pooling, and the clip context extension designed for lengthy prompt common in
visual dialogue. Moreover, our codebase also integrates the most advanced video
Direct Preference Optimization (DPO) and visual interleave training. Extensive
experiments have validated the performance of our model. With superior
throughput and only 1024 visual context, PPLLaVA achieves better results on
image benchmarks as a video LLM, while achieving state-of-the-art performance
across various video benchmarks, excelling in tasks ranging from caption
generation to multiple-choice questions, and handling video lengths from
seconds to hours. Codes have been available at
https://github.com/farewellthree/PPLLaVA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GenXD: Generating Any 3D and 4D Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02319v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02319v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuyang Zhao, Chung-Ching Lin, Kevin Lin, Zhiwen Yan, Linjie Li, Zhengyuan Yang, Jianfeng Wang, Gim Hee Lee, Lijuan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent developments in 2D visual generation have been remarkably successful.
However, 3D and 4D generation remain challenging in real-world applications due
to the lack of large-scale 4D data and effective model design. In this paper,
we propose to jointly investigate general 3D and 4D generation by leveraging
camera and object movements commonly observed in daily life. Due to the lack of
real-world 4D data in the community, we first propose a data curation pipeline
to obtain camera poses and object motion strength from videos. Based on this
pipeline, we introduce a large-scale real-world 4D scene dataset: CamVid-30K.
By leveraging all the 3D and 4D data, we develop our framework, GenXD, which
allows us to produce any 3D or 4D scene. We propose multiview-temporal modules,
which disentangle camera and object movements, to seamlessly learn from both 3D
and 4D data. Additionally, GenXD employs masked latent conditions to support a
variety of conditioning views. GenXD can generate videos that follow the camera
trajectory as well as consistent 3D views that can be lifted into 3D
representations. We perform extensive evaluations across various real-world and
synthetic datasets, demonstrating GenXD's effectiveness and versatility
compared to previous methods in 3D and 4D generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Grouped Discrete Representation for Object-Centric Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02299v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02299v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rongzhen Zhao, Vivienne Wang, Juho Kannala, Joni Pajarinen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object-Centric Learning (OCL) can discover objects in images or videos by
simply reconstructing the input. For better object discovery, representative
OCL methods reconstruct the input as its Variational Autoencoder (VAE)
intermediate representation, which suppresses pixel noises and promotes object
separability by discretizing continuous super-pixels with template features.
However, treating features as units overlooks their composing attributes, thus
impeding model generalization; indexing features with scalar numbers loses
attribute-level similarities and differences, thus hindering model convergence.
We propose \textit{Grouped Discrete Representation} (GDR) for OCL. We decompose
features into combinatorial attributes via organized channel grouping, and
compose these attributes into discrete representation via tuple indexes.
Experiments show that our GDR improves both Transformer- and Diffusion-based
OCL methods consistently on various datasets. Visualizations show that our GDR
captures better object separability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hunyuan3D-1.0: A Unified Framework for Text-to-3D and Image-to-3D
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02293v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02293v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianghui Yang, Huiwen Shi, Bowen Zhang, Fan Yang, Jiacheng Wang, Hongxu Zhao, Xinhai Liu, Xinzhou Wang, Qingxiang Lin, Jiaao Yu, Lifu Wang, Zhuo Chen, Sicong Liu, Yuhong Liu, Yong Yang, Di Wang, Jie Jiang, Chunchao Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While 3D generative models have greatly improved artists' workflows, the
existing diffusion models for 3D generation suffer from slow generation and
poor generalization. To address this issue, we propose a two-stage approach
named Hunyuan3D-1.0 including a lite version and a standard version, that both
support text- and image-conditioned generation. In the first stage, we employ a
multi-view diffusion model that efficiently generates multi-view RGB in
approximately 4 seconds. These multi-view images capture rich details of the 3D
asset from different viewpoints, relaxing the tasks from single-view to
multi-view reconstruction. In the second stage, we introduce a feed-forward
reconstruction model that rapidly and faithfully reconstructs the 3D asset
given the generated multi-view images in approximately 7 seconds. The
reconstruction network learns to handle noises and in-consistency introduced by
the multi-view diffusion and leverages the available information from the
condition image to efficiently recover the 3D structure. % Extensive
experimental results demonstrate the effectiveness of Hunyuan3D-1.0 in
generating high-quality 3D assets. Our framework involves the text-to-image
model ~\ie, Hunyuan-DiT, making it a unified framework to support both text-
and image-conditioned 3D generation. Our standard version has $10\times$ more
parameters than our lite and other existing model. Our Hunyuan3D-1.0 achieves
an impressive balance between speed and quality, significantly reducing
generation time while maintaining the quality and diversity of the produced
assets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conformal-in-the-Loop for Learning with Imbalanced Noisy Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02281v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02281v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Brandon Graham-Knight, Jamil Fayyad, Nourhan Bayasi, Patricia Lasserre, Homayoun Najjaran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class imbalance and label noise are pervasive in large-scale datasets, yet
much of machine learning research assumes well-labeled, balanced data, which
rarely reflects real world conditions. Existing approaches typically address
either label noise or class imbalance in isolation, leading to suboptimal
results when both issues coexist. In this work, we propose
Conformal-in-the-Loop (CitL), a novel training framework that addresses both
challenges with a conformal prediction-based approach. CitL evaluates sample
uncertainty to adjust weights and prune unreliable examples, enhancing model
resilience and accuracy with minimal computational cost. Our extensive
experiments include a detailed analysis showing how CitL effectively emphasizes
impactful data in noisy, imbalanced datasets. Our results show that CitL
consistently boosts model performance, achieving up to a 6.1% increase in
classification accuracy and a 5.0 mIoU improvement in segmentation. Our code is
publicly available: CitL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unified Speech Recognition: A Single Model for Auditory, Visual, and
  Audiovisual Inputs <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02256v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02256v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandros Haliassos, Rodrigo Mira, Honglie Chen, Zoe Landgraf, Stavros Petridis, Maja Pantic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research in auditory, visual, and audiovisual speech recognition (ASR, VSR,
and AVSR, respectively) has traditionally been conducted independently. Even
recent self-supervised studies addressing two or all three tasks simultaneously
tend to yield separate models, leading to disjoint inference pipelines with
increased memory requirements and redundancies. This paper proposes unified
training strategies for these systems. We demonstrate that training a single
model for all three tasks enhances VSR and AVSR performance, overcoming typical
optimisation challenges when training from scratch. Moreover, we introduce a
greedy pseudo-labelling approach to more effectively leverage unlabelled
samples, addressing shortcomings in related self-supervised methods. Finally,
we develop a self-supervised pre-training method within our framework, proving
its effectiveness alongside our semi-supervised approach. Despite using a
single model for all tasks, our unified approach achieves state-of-the-art
performance compared to recent methods on LRS3 and LRS2 for ASR, VSR, and AVSR,
as well as on the newly released WildVSR dataset. Code and models are available
at https://github.com/ahaliassos/usr.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024. Code: https://github.com/ahaliassos/usr</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D Audio-Visual Segmentation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02236v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02236v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Artem Sokolov, Swapnil Bhosale, Xiatian Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recognizing the sounding objects in scenes is a longstanding objective in
embodied AI, with diverse applications in robotics and AR/VR/MR. To that end,
Audio-Visual Segmentation (AVS), taking as condition an audio signal to
identify the masks of the target sounding objects in an input image with
synchronous camera and microphone sensors, has been recently advanced. However,
this paradigm is still insufficient for real-world operation, as the mapping
from 2D images to 3D scenes is missing. To address this fundamental limitation,
we introduce a novel research problem, 3D Audio-Visual Segmentation, extending
the existing AVS to the 3D output space. This problem poses more challenges due
to variations in camera extrinsics, audio scattering, occlusions, and diverse
acoustics across sounding object categories. To facilitate this research, we
create the very first simulation based benchmark, 3DAVS-S34-O7, providing
photorealistic 3D scene environments with grounded spatial audio under
single-instance and multi-instance settings, across 34 scenes and 7 object
categories. This is made possible by re-purposing the Habitat simulator to
generate comprehensive annotations of sounding object locations and
corresponding 3D masks. Subsequently, we propose a new approach, EchoSegnet,
characterized by integrating the ready-to-use knowledge from pretrained 2D
audio-visual foundation models synergistically with 3D visual scene
representation through spatial audio-aware mask alignment and refinement.
Extensive experiments demonstrate that EchoSegnet can effectively segment
sounding objects in 3D space on our new benchmark, representing a significant
advancement in the field of embodied AI. Project page:
https://surrey-uplab.github.io/research/3d-audio-visual-segmentation/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the NeurIPS 2024 Workshop on Audio Imagination</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FewViewGS: Gaussian Splatting with Few View Matching and Multi-stage
  Training <span class="chip">NeurIPS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02229v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02229v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruihong Yin, Vladimir Yugay, Yue Li, Sezer Karaoglu, Theo Gevers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of novel view synthesis from images has seen rapid advancements
with the introduction of Neural Radiance Fields (NeRF) and more recently with
3D Gaussian Splatting. Gaussian Splatting became widely adopted due to its
efficiency and ability to render novel views accurately. While Gaussian
Splatting performs well when a sufficient amount of training images are
available, its unstructured explicit representation tends to overfit in
scenarios with sparse input images, resulting in poor rendering performance. To
address this, we present a 3D Gaussian-based novel view synthesis method using
sparse input images that can accurately render the scene from the viewpoints
not covered by the training images. We propose a multi-stage training scheme
with matching-based consistency constraints imposed on the novel views without
relying on pre-trained depth estimation or diffusion models. This is achieved
by using the matches of the available training images to supervise the
generation of the novel views sampled between the training frames with color,
geometry, and semantic losses. In addition, we introduce a locality preserving
regularization for 3D Gaussians which removes rendering artifacts by preserving
the local color structure of the scene. Evaluation on synthetic and real-world
datasets demonstrates competitive or superior performance of our method in
few-shot novel view synthesis compared to existing state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SIRA: Scalable Inter-frame Relation and Association for Radar Perception <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02220v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02220v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryoma Yataka, Pu Perry Wang, Petros Boufounos, Ryuhei Takahashi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional radar feature extraction faces limitations due to low spatial
resolution, noise, multipath reflection, the presence of ghost targets, and
motion blur. Such limitations can be exacerbated by nonlinear object motion,
particularly from an ego-centric viewpoint. It becomes evident that to address
these challenges, the key lies in exploiting temporal feature relation over an
extended horizon and enforcing spatial motion consistency for effective
association. To this end, this paper proposes SIRA (Scalable Inter-frame
Relation and Association) with two designs. First, inspired by Swin
Transformer, we introduce extended temporal relation, generalizing the existing
temporal relation layer from two consecutive frames to multiple inter-frames
with temporally regrouped window attention for scalability. Second, we propose
motion consistency track with the concept of a pseudo-tracklet generated from
observational data for better trajectory prediction and subsequent object
association. Our approach achieves 58.11 mAP@0.5 for oriented object detection
and 47.79 MOTA for multiple object tracking on the Radiate dataset, surpassing
previous state-of-the-art by a margin of +4.11 mAP@0.5 and +9.94 MOTA,
respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, Accepted to CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ One VLM to Keep it Learning: Generation and Balancing for Data-free
  Continual Visual Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02210v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02210v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deepayan Das, Davide Talon, Massimiliano Mancini, Yiming Wang, Elisa Ricci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language Models (VLMs) have shown significant promise in Visual
Question Answering (VQA) tasks by leveraging web-scale multimodal datasets.
However, these models often struggle with continual learning due to
catastrophic forgetting when adapting to new tasks. As an effective remedy to
mitigate catastrophic forgetting, rehearsal strategy uses the data of past
tasks upon learning new task. However, such strategy incurs the need of storing
past data, which might not be feasible due to hardware constraints or privacy
concerns. In this work, we propose the first data-free method that leverages
the language generation capability of a VLM, instead of relying on external
models, to produce pseudo-rehearsal data for addressing continual VQA. Our
proposal, named as GaB, generates pseudo-rehearsal data by posing previous task
questions on new task data. Yet, despite being effective, the distribution of
generated questions skews towards the most frequently posed questions due to
the limited and task-specific training data. To mitigate this issue, we
introduce a pseudo-rehearsal balancing module that aligns the generated data
towards the ground-truth data distribution using either the question
meta-statistics or an unsupervised clustering method. We evaluate our proposed
method on two recent benchmarks, \ie VQACL-VQAv2 and CLOVE-function benchmarks.
GaB outperforms all the data-free baselines with substantial improvement in
maintaining VQA performance across evolving tasks, while being on-par with
methods with access to the past data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Digi2Real: Bridging the Realism Gap in Synthetic Data Face Recognition
  via Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02188v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02188v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anjith George, Sebastien Marcel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The accuracy of face recognition systems has improved significantly in the
past few years, thanks to the large amount of data collected and the
advancement in neural network architectures. However, these large-scale
datasets are often collected without explicit consent, raising ethical and
privacy concerns. To address this, there have been proposals to use synthetic
datasets for training face recognition models. Yet, such models still rely on
real data to train the generative models and generally exhibit inferior
performance compared to those trained on real datasets. One of these datasets,
DigiFace, uses a graphics pipeline to generate different identities and
different intra-class variations without using real data in training the
models. However, the performance of this approach is poor on face recognition
benchmarks, possibly due to the lack of realism in the images generated from
the graphics pipeline. In this work, we introduce a novel framework for realism
transfer aimed at enhancing the realism of synthetically generated face images.
Our method leverages the large-scale face foundation model, and we adapt the
pipeline for realism enhancement. By integrating the controllable aspects of
the graphics pipeline with our realism enhancement technique, we generate a
large amount of realistic variations-combining the advantages of both
approaches. Our empirical evaluations demonstrate that models trained using our
enhanced dataset significantly improve the performance of face recognition
systems over the baseline. The source code and datasets will be made available
publicly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Double Descent Meets Out-of-Distribution Detection: Theoretical Insights
  and Empirical Analysis on the role of model complexity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02184v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02184v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mouïn Ben Ammar, David Brellmann, Arturo Mendoza, Antoine Manzanera, Gianni Franchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While overparameterization is known to benefit generalization, its impact on
Out-Of-Distribution (OOD) detection is less understood. This paper investigates
the influence of model complexity in OOD detection. We propose an expected OOD
risk metric to evaluate classifiers confidence on both training and OOD
samples. Leveraging Random Matrix Theory, we derive bounds for the expected OOD
risk of binary least-squares classifiers applied to Gaussian data. We show that
the OOD risk depicts an infinite peak, when the number of parameters is equal
to the number of samples, which we associate with the double descent
phenomenon. Our experimental study on different OOD detection methods across
multiple neural architectures extends our theoretical insights and highlights a
double descent curve. Our observations suggest that overparameterization does
not necessarily lead to better OOD detection. Using the Neural Collapse
framework, we provide insights to better understand this behavior. To
facilitate reproducibility, our code will be made publicly available upon
publication.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detect an Object At Once without <span class="highlight-title">Fine-tuning</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02181v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02181v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyu Hao, Jianheng Liu, Yongjia Zhao, Zuofan Chen, Qi Sun, Jinlong Chen, Jianguo Wei, Minghao Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When presented with one or a few photos of a previously unseen object, humans
can instantly recognize it in different scenes. Although the human brain
mechanism behind this phenomenon is still not fully understood, this work
introduces a novel technical realization of this task. It consists of two
phases: (1) generating a Similarity Density Map (SDM) by convolving the scene
image with the given object image patch(es) so that the highlight areas in the
SDM indicate the possible locations; (2) obtaining the object occupied areas in
the scene through a Region Alignment Network (RAN). The RAN is constructed on a
backbone of Deep Siamese Network (DSN), and different from the traditional
DSNs, it aims to obtain the object accurate regions by regressing the location
and area differences between the ground truths and the predicted ones indicated
by the highlight areas in SDM. By pre-learning from labels annotated in
traditional datasets, the SDM-RAN can detect previously unknown objects without
fine-tuning. Experiments were conducted on the MS COCO, PASCAL VOC datasets.
The results indicate that the proposed method outperforms state-of-the-art
methods on the same task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CleAR: Robust Context-Guided Generative Lighting Estimation for Mobile
  Augmented Reality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02179v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02179v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiqin Zhao, Mallesham Dasari, Tian Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-quality environment lighting is the foundation of creating immersive
user experiences in mobile augmented reality (AR) applications. However,
achieving visually coherent environment lighting estimation for Mobile AR is
challenging due to several key limitations associated with AR device sensing
capabilities, including limitations in device camera FoV and pixel dynamic
ranges. Recent advancements in generative AI, which can generate high-quality
images from different types of prompts, including texts and images, present a
potential solution for high-quality lighting estimation. Still, to effectively
use generative image diffusion models, we must address their key limitations of
generation hallucination and slow inference process. To do so, in this work, we
design and implement a generative lighting estimation system called CleAR that
can produce high-quality and diverse environment maps in the format of
360$^\circ$ images. Specifically, we design a two-step generation pipeline
guided by AR environment context data to ensure the results follow physical
environment visual context and color appearances. To improve the estimation
robustness under different lighting conditions, we design a real-time
refinement component to adjust lighting estimation results on AR devices. To
train and test our generative models, we curate a large-scale environment
lighting estimation dataset with diverse lighting conditions. Through
quantitative evaluation and user study, we show that CleAR outperforms
state-of-the-art lighting estimation methods on both estimation accuracy and
robustness. Moreover, CleAR supports real-time refinement of lighting
estimation results, ensuring robust and timely environment lighting updates for
AR applications. Our end-to-end generative estimation takes as fast as 3.2
seconds, outperforming state-of-the-art methods by 110x.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SAFE: Slow and Fast Parameter-Efficient <span class="highlight-title">Tuning</span> for Continual Learning
  with Pre-Trained Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02175v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02175v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linglan Zhao, Xuerui Zhang, Ke Yan, Shouhong Ding, Weiran Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual learning aims to incrementally acquire new concepts in data streams
while resisting forgetting previous knowledge. With the rise of powerful
pre-trained models (PTMs), there is a growing interest in training incremental
learning systems using these foundation models, rather than learning from
scratch. Existing works often view PTMs as a strong initial point and directly
apply parameter-efficient tuning (PET) in the first session for adapting to
downstream tasks. In the following sessions, most methods freeze model
parameters for tackling forgetting issues. However, applying PET directly to
downstream data cannot fully explore the inherent knowledge in PTMs.
Additionally, freezing the parameters in incremental sessions hinders models'
plasticity to novel concepts not covered in the first session. To solve the
above issues, we propose a Slow And Fast parameter-Efficient tuning (SAFE)
framework. In particular, to inherit general knowledge from foundation models,
we include a transfer loss function by measuring the correlation between the
PTM and the PET-applied model. After calibrating in the first session, the slow
efficient tuning parameters can capture more informative features, improving
generalization to incoming classes. Moreover, to further incorporate novel
concepts, we strike a balance between stability and plasticity by fixing slow
efficient tuning parameters and continuously updating the fast ones.
Specifically, a cross-classification loss with feature alignment is proposed to
circumvent catastrophic forgetting. During inference, we introduce an
entropy-based aggregation strategy to dynamically utilize the complementarity
in the slow and fast learners. Extensive experiments on seven benchmark
datasets verify the effectiveness of our method by significantly surpassing the
state-of-the-art.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Domain Generalization in Self-supervised Monocular Depth
  Estimation via Stabilized Adversarial Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02149v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02149v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanqi Yao, Gang Wu, Kui Jiang, Siao Liu, Jian Kuai, Xianming Liu, Junjun Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning a self-supervised Monocular Depth Estimation (MDE) model with great
generalization remains significantly challenging. Despite the success of
adversarial augmentation in the supervised learning generalization, naively
incorporating it into self-supervised MDE models potentially causes
over-regularization, suffering from severe performance degradation. In this
paper, we conduct qualitative analysis and illuminate the main causes: (i)
inherent sensitivity in the UNet-alike depth network and (ii) dual optimization
conflict caused by over-regularization. To tackle these issues, we propose a
general adversarial training framework, named Stabilized Conflict-optimization
Adversarial Training (SCAT), integrating adversarial data augmentation into
self-supervised MDE methods to achieve a balance between stability and
generalization. Specifically, we devise an effective scaling depth network that
tunes the coefficients of long skip connection and effectively stabilizes the
training process. Then, we propose a conflict gradient surgery strategy, which
progressively integrates the adversarial gradient and optimizes the model
toward a conflict-free direction. Extensive experiments on five benchmarks
demonstrate that SCAT can achieve state-of-the-art performance and
significantly improve the generalization capability of existing self-supervised
MDE methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advanced computer vision for extracting georeferenced vehicle
  trajectories from drone imagery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02136v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02136v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert Fonod, Haechan Cho, Hwasoo Yeo, Nikolas Geroliminis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a framework for extracting georeferenced vehicle
trajectories from high-altitude drone footage, addressing key challenges in
urban traffic monitoring and limitations of traditional ground-based systems.
We employ state-of-the-art computer vision and deep learning to create an
end-to-end pipeline that enhances vehicle detection, tracking, and trajectory
stabilization. Conducted in the Songdo International Business District, South
Korea, the study used a multi-drone experiment over 20 intersections, capturing
approximately 12TB of 4K video data over four days. We developed a novel track
stabilization method that uses detected vehicle bounding boxes as exclusion
masks during image registration, which, combined with advanced georeferencing
techniques, accurately transforms vehicle coordinates into real-world
geographical data. Additionally, our framework includes robust vehicle
dimension estimation and detailed road segmentation for in-depth traffic
analysis. The framework produced two high-quality datasets: the Songdo Traffic
dataset, comprising nearly 1 million unique vehicle trajectories, and the
Songdo Vision dataset, containing over 5,000 human-annotated frames with about
300,000 vehicle instances in four classes. Comparisons between drone-derived
data and high-precision sensor data from an instrumented probe vehicle
highlight the accuracy and consistency of our framework's extraction in dense
urban settings. By publicly releasing these datasets and the pipeline source
code, this work sets new benchmarks for data quality, reproducibility, and
scalability in traffic research. Results demonstrate the potential of
integrating drone technology with advanced computer vision for precise,
cost-effective urban traffic monitoring, providing valuable resources for the
research community to develop intelligent transportation systems and improve
traffic management strategies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancements and limitations of LLMs in replicating human color-word
  associations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02116v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02116v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Makoto Fukushima, Shusuke Eshita, Hiroshige Fukuhara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Color-word associations play a fundamental role in human cognition and design
applications. Large Language Models (LLMs) have become widely available and
demonstrated intelligent behaviors in various benchmarks with natural
conversation skills. However, their ability to replicate human color-word
associations remains understudied. We compared multiple generations of LLMs
(from GPT-3 to GPT- 4o) against human color-word associations using data
collected from over 10,000 Japanese participants, involving 17 colors and words
from eight categories in Japanese. Our findings reveal a clear progression in
LLM performance across generations, with GPT-4o achieving the highest accuracy
in predicting the best voted word for each color and category, particularly
when using visual inputs rather than text-based color codes. However, the
highest median performance was approximately 50% even for GPT4-o with visual
inputs (chance level is 10%), and the performance levels varied significantly
across word categories and colors, indicating a failure to fully replicate
human color-word associations. On the other hand, color discrimination ability
estimated from our color-word association data showed that LLMs demonstrated
high correlation with human color discrimination patterns, similarly to
previous studies. Our study highlights both the advancements in LLM
capabilities and their persistent limitations, suggesting differences in
semantic memory structures between humans and LLMs in representing color-word
associations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 7 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Multi-modal</span> biometric authentication: Leveraging shared layer
  architectures for enhanced security 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02112v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02112v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vatchala S, Yogesh C, Yeshwanth Govindarajan, Krithik Raja M, Vishal Pramav Amirtha Ganesan, Aashish Vinod A, Dharun Ramesh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we introduce a novel multi-modal biometric authentication
system that integrates facial, vocal, and signature data to enhance security
measures. Utilizing a combination of Convolutional Neural Networks (CNNs) and
Recurrent Neural Networks (RNNs), our model architecture uniquely incorporates
dual shared layers alongside modality-specific enhancements for comprehensive
feature extraction. The system undergoes rigorous training with a joint loss
function, optimizing for accuracy across diverse biometric inputs.
Feature-level fusion via Principal Component Analysis (PCA) and classification
through Gradient Boosting Machines (GBM) further refine the authentication
process. Our approach demonstrates significant improvements in authentication
accuracy and robustness, paving the way for advanced secure identity
verification solutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning on 3D Semantic Segmentation: A Detailed Review 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02104v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02104v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thodoris Betsas, Andreas Georgopoulos, Anastasios Doulamis, Pierre Grussenmeyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper an exhaustive review and comprehensive analysis of recent and
former deep learning methods in 3D Semantic Segmentation (3DSS) is presented.
In the related literature, the taxonomy scheme used for the classification of
the 3DSS deep learning methods is ambiguous. Based on the taxonomy schemes of 9
existing review papers, a new taxonomy scheme of the 3DSS deep learning methods
is proposed, aiming to standardize it and improve the comparability and clarity
across related studies. Furthermore, an extensive overview of the available
3DSS indoor and outdoor datasets is provided along with their links. The core
part of the review is the detailed presentation of recent and former 3DSS deep
learning methods and their classification using the proposed taxonomy scheme
along with their GitHub repositories. Additionally, a brief but informative
analysis of the evaluation metrics and loss functions used in 3DSS is included.
Finally, a fruitful discussion of the examined 3DSS methods and datasets, is
presented to foster new research directions and applications in the field of
3DSS. Supplementary, to this review a GitHub repository is provided
(https://github.com/thobet/Deep-Learning-on-3D-Semantic-Segmentation-a-
Detailed-Review) including a quick classification of over 400 3DSS methods,
using the proposed taxonomy scheme.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differentially Private Integrated Decision Gradients (IDG-DP) for
  Radar-based Human Activity Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02099v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02099v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Idris Zakariyya, Linda Tran, Kaushik Bhargav Sivangi, Paul Henderson, Fani Deligianni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human motion analysis offers significant potential for healthcare monitoring
and early detection of diseases. The advent of radar-based sensing systems has
captured the spotlight for they are able to operate without physical contact
and they can integrate with pre-existing Wi-Fi networks. They are also seen as
less privacy-invasive compared to camera-based systems. However, recent
research has shown high accuracy in recognizing subjects or gender from radar
gait patterns, raising privacy concerns. This study addresses these issues by
investigating privacy vulnerabilities in radar-based Human Activity Recognition
(HAR) systems and proposing a novel method for privacy preservation using
Differential Privacy (DP) driven by attributions derived with Integrated
Decision Gradient (IDG) algorithm. We investigate Black-box Membership
Inference Attack (MIA) Models in HAR settings across various levels of
attacker-accessible information. We extensively evaluated the effectiveness of
the proposed IDG-DP method by designing a CNN-based HAR model and rigorously
assessing its resilience against MIAs. Experimental results demonstrate the
potential of IDG-DP in mitigating privacy attacks while maintaining utility
across all settings, particularly excelling against label-only and shadow model
black-box MIA attacks. This work represents a crucial step towards balancing
the need for effective radar-based HAR with robust privacy protection in
healthcare environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The evolution of volumetric video: A survey of smart transcoding and
  compression approaches 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02095v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02095v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Preetish Kakkar, Hariharan Ragothaman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Volumetric video, the capture and display of three-dimensional (3D) imagery,
has emerged as a revolutionary technology poised to transform the media
landscape, enabling immersive experiences that transcend the limitations of
traditional 2D video. One of the key challenges in this domain is the efficient
delivery of these high-bandwidth, data-intensive volumetric video streams,
which requires innovative transcoding and compression techniques. This research
paper explores the state-of-the-art in volumetric video compression and
delivery, with a focus on the potential of AI-driven solutions to address the
unique challenges posed by this emerging medium.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GraphVL: Graph-Enhanced Semantic Modeling via <span class="highlight-title">Vision-Language</span> Models for
  Generalized Class Discovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02074v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02074v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhupendra Solanki, Ashwin Nair, Mainak Singha, Souradeep Mukhopadhyay, Ankit Jha, Biplab Banerjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generalized Category Discovery (GCD) aims to cluster unlabeled images into
known and novel categories using labeled images from known classes. To address
the challenge of transferring features from known to unknown classes while
mitigating model bias, we introduce GraphVL, a novel approach for
vision-language modeling in GCD, leveraging CLIP. Our method integrates a graph
convolutional network (GCN) with CLIP's text encoder to preserve class
neighborhood structure. We also employ a lightweight visual projector for image
data, ensuring discriminative features through margin-based contrastive losses
for image-text mapping. This neighborhood preservation criterion effectively
regulates the semantic space, making it less sensitive to known classes.
Additionally, we learn textual prompts from known classes and align them to
create a more contextually meaningful semantic feature space for the GCN layer
using a contextual similarity loss. Finally, we represent unlabeled samples
based on their semantic distance to class prompts from the GCN, enabling
semi-supervised clustering for class discovery and minimizing errors. Our
experiments on seven benchmark datasets consistently demonstrate the
superiority of GraphVL when integrated with the CLIP backbone.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ACM ICVGIP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model Integrity when Unlearning with T2I Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02068v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02068v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Schioppa, Emiel Hoogeboom, Jonathan Heek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of text-to-image Diffusion Models has led to their
widespread public accessibility. However these models, trained on large
internet datasets, can sometimes generate undesirable outputs. To mitigate
this, approximate Machine Unlearning algorithms have been proposed to modify
model weights to reduce the generation of specific types of images,
characterized by samples from a ``forget distribution'', while preserving the
model's ability to generate other images, characterized by samples from a
``retain distribution''. While these methods aim to minimize the influence of
training data in the forget distribution without extensive additional
computation, we point out that they can compromise the model's integrity by
inadvertently affecting generation for images in the retain distribution.
Recognizing the limitations of FID and CLIPScore in capturing these effects, we
introduce a novel retention metric that directly assesses the perceptual
difference between outputs generated by the original and the unlearned models.
We then propose unlearning algorithms that demonstrate superior effectiveness
in preserving model integrity compared to existing baselines. Given their
straightforward implementation, these algorithms serve as valuable benchmarks
for future advancements in approximate Machine Unlearning for Diffusion Models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AM Flow: Adapters for Temporal Processing in Action Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02065v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02065v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tanay Agrawal, Abid Ali, Antitza Dantcheva, Francois Bremond
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning models, in particular \textit{image} models, have recently
gained generalisability and robustness. %are becoming more general and robust
by the day. In this work, we propose to exploit such advances in the realm of
\textit{video} classification. Video foundation models suffer from the
requirement of extensive pretraining and a large training time. Towards
mitigating such limitations, we propose "\textit{Attention Map (AM) Flow}" for
image models, a method for identifying pixels relevant to motion in each input
video frame. In this context, we propose two methods to compute AM flow,
depending on camera motion. AM flow allows the separation of spatial and
temporal processing, while providing improved results over combined
spatio-temporal processing (as in video models). Adapters, one of the popular
techniques in parameter efficient transfer learning, facilitate the
incorporation of AM flow into pretrained image models, mitigating the need for
full-finetuning. We extend adapters to "\textit{temporal processing adapters}"
by incorporating a temporal processing unit into the adapters. Our work
achieves faster convergence, therefore reducing the number of epochs needed for
training. Moreover, we endow an image model with the ability to achieve
state-of-the-art results on popular action recognition datasets. This reduces
training time and simplifies pretraining. We present experiments on
Kinetics-400, Something-Something v2, and Toyota Smarthome datasets, showcasing
state-of-the-art or comparable results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploiting Unlabeled Data with Multiple Expert Teachers for Open
  Vocabulary Aerial Object Detection and Its Orientation Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02057v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02057v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Li, Weiwei Guo, Xue Yang, Ning Liao, Shaofeng Zhang, Yi Yu, Wenxian Yu, Junchi Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, aerial object detection has been increasingly pivotal in
various earth observation applications. However, current algorithms are limited
to detecting a set of pre-defined object categories, demanding sufficient
annotated training samples, and fail to detect novel object categories. In this
paper, we put forth a novel formulation of the aerial object detection problem,
namely open-vocabulary aerial object detection (OVAD), which can detect objects
beyond training categories without costly collecting new labeled data. We
propose CastDet, a CLIP-activated student-teacher detection framework that
serves as the first OVAD detector specifically designed for the challenging
aerial scenario, where objects often exhibit weak appearance features and
arbitrary orientations. Our framework integrates a robust localization teacher
along with several box selection strategies to generate high-quality proposals
for novel objects. Additionally, the RemoteCLIP model is adopted as an
omniscient teacher, which provides rich knowledge to enhance classification
capabilities for novel categories. A dynamic label queue is devised to maintain
high-quality pseudo-labels during training. By doing so, the proposed CastDet
boosts not only novel object proposals but also classification. Furthermore, we
extend our approach from horizontal OVAD to oriented OVAD with tailored
algorithm designs to effectively manage bounding box representation and
pseudo-label generation. Extensive experiments for both tasks on multiple
existing aerial object detection datasets demonstrate the effectiveness of our
approach. The code is available at https://github.com/lizzy8587/CastDet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Addressing Representation Collapse in Vector Quantized Models with One
  Linear Layer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02038v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02038v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongxin Zhu, Bocheng Li, Yifei Xin, Linli Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vector Quantization (VQ) is a widely used method for converting continuous
representations into discrete codes, which has become fundamental in
unsupervised representation learning and latent generative models. However, VQ
models are often hindered by the problem of representation collapse in the
latent space, which leads to low codebook utilization and limits the
scalability of the codebook for large-scale training. Existing methods designed
to mitigate representation collapse typically reduce the dimensionality of
latent space at the expense of model capacity, which do not fully resolve the
core issue. In this study, we conduct a theoretical analysis of representation
collapse in VQ models and identify its primary cause as the disjoint
optimization of the codebook, where only a small subset of code vectors are
updated through gradient descent. To address this issue, we propose
\textbf{SimVQ}, a novel method which reparameterizes the code vectors through a
linear transformation layer based on a learnable latent basis. This
transformation optimizes the \textit{entire linear space} spanned by the
codebook, rather than merely updating \textit{the code vector} selected by the
nearest-neighbor search in vanilla VQ models. Although it is commonly
understood that the multiplication of two linear matrices is equivalent to
applying a single linear layer, our approach works surprisingly well in
resolving the collapse issue in VQ models with just one linear layer. We
validate the efficacy of SimVQ through extensive experiments across various
modalities, including image and audio data with different model architectures.
Our code is available at \url{https://github.com/youngsheen/SimVQ}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tree level change detection over Ahmedabad city using very high
  resolution satellite images and Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02009v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02009v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jai G Singla, Gautam Jaiswal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, 0.5m high resolution satellite datasets over Indian urban
region was used to demonstrate the applicability of deep learning models over
Ahmedabad, India. Here, YOLOv7 instance segmentation model was trained on well
curated trees canopy dataset (6500 images) in order to carry out the change
detection. During training, evaluation metrics such as bounding box regression
and mask regression loss, mean average precision (mAP) and stochastic gradient
descent algorithm were used for evaluating and optimizing the performance of
model. After the 500 epochs, the mAP of 0.715 and 0.699 for individual tree
detection and tree canopy mask segmentation were obtained. However, by further
tuning hyper parameters of the model, maximum accuracy of 80 % of trees
detection with false segmentation rate of 2% on data was obtained.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ QCS:Feature Refining from Quadruplet Cross Similarity for Facial
  Expression Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01988v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01988v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengpeng Wang, Li Chen, Lili Wang, Zhaofan Li, Xuebin Lv
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  On facial expression datasets with complex and numerous feature types, where
the significance and dominance of labeled features are difficult to predict,
facial expression recognition(FER) encounters the challenges of inter-class
similarity and intra-class variances, making it difficult to mine effective
features. We aim to solely leverage the feature similarity among facial samples
to address this. We introduce the Cross Similarity Attention (CSA), an
input-output position-sensitive attention mechanism that harnesses feature
similarity across different images to compute the corresponding global spatial
attention. Based on this, we propose a four-branch circular framework, called
Quadruplet Cross Similarity (QCS), to extract discriminative features from the
same class and eliminate redundant ones from different classes synchronously to
refine cleaner features. The symmetry of the network ensures balanced and
stable training and reduces the amount of CSA interaction matrix. Contrastive
residual distillation is utilized to transfer the information learned in the
cross module back to the base network. The cross-attention module exists during
training, and only one base branch is retained during inference. our proposed
QCS model outperforms state-of-the-art methods on several popular FER datasets,
without requiring additional landmark information or other extra training data.
The code is available at https://github.com/birdwcp/QCS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Typicalness-Aware Learning for Failure Detection <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01981v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01981v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijun Liu, Jiequan Cui, Zhuotao Tian, Senqiao Yang, Qingdong He, Xiaoling Wang, Jingyong Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) often suffer from the overconfidence issue, where
incorrect predictions are made with high confidence scores, hindering the
applications in critical systems. In this paper, we propose a novel approach
called Typicalness-Aware Learning (TAL) to address this issue and improve
failure detection performance. We observe that, with the cross-entropy loss,
model predictions are optimized to align with the corresponding labels via
increasing logit magnitude or refining logit direction. However, regarding
atypical samples, the image content and their labels may exhibit disparities.
This discrepancy can lead to overfitting on atypical samples, ultimately
resulting in the overconfidence issue that we aim to address. To tackle the
problem, we have devised a metric that quantifies the typicalness of each
sample, enabling the dynamic adjustment of the logit magnitude during the
training process. By allowing atypical samples to be adequately fitted while
preserving reliable logit direction, the problem of overconfidence can be
mitigated. TAL has been extensively evaluated on benchmark datasets, and the
results demonstrate its superiority over existing failure detection methods.
Specifically, TAL achieves a more than 5% improvement on CIFAR100 in terms of
the Area Under the Risk-Coverage Curve (AURC) compared to the state-of-the-art.
Code is available at https://github.com/liuyijungoon/TAL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SPECTRUM: Semantic Processing and Emotion-informed video-Captioning
  Through Retrieval and Understanding Modalities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01975v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01975v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ehsan Faghihi, Mohammedreza Zarenejad, Ali-Asghar Beheshti Shirazi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Capturing a video's meaning and critical concepts by analyzing the subtle
details is a fundamental yet challenging task in video captioning. Identifying
the dominant emotional tone in a video significantly enhances the perception of
its context. Despite a strong emphasis on video captioning, existing models
often need to adequately address emotional themes, resulting in suboptimal
captioning results. To address these limitations, this paper proposes a novel
Semantic Processing and Emotion-informed video-Captioning Through Retrieval and
Understanding Modalities (SPECTRUM) framework to empower the generation of
emotionally and semantically credible captions. Leveraging our pioneering
structure, SPECTRUM discerns multimodal semantics and emotional themes using
Visual Text Attribute Investigation (VTAI) and determines the orientation of
descriptive captions through a Holistic Concept-Oriented Theme (HCOT),
expressing emotionally-informed and field-acquainted references. They exploit
video-to-text retrieval capabilities and the multifaceted nature of video
content to estimate the emotional probabilities of candidate captions. Then,
the dominant theme of the video is determined by appropriately weighting
embedded attribute vectors and applying coarse- and fine-grained emotional
concepts, which define the video's contextual alignment. Furthermore, using two
loss functions, SPECTRUM is optimized to integrate emotional information and
minimize prediction errors. Extensive experiments on the EmVidCap, MSVD, and
MSRVTT video captioning datasets demonstrate that our model significantly
surpasses state-of-the-art methods. Quantitative and qualitative evaluations
highlight the model's ability to accurately capture and convey video emotions
and multimodal attributes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Active Gaze Behavior Boosts Self-Supervised Object Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01969v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01969v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengyang Yu, Arthur Aubret, Marcel C. Raabe, Jane Yang, Chen Yu, Jochen Triesch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to significant variations in the projection of the same object from
different viewpoints, machine learning algorithms struggle to recognize the
same object across various perspectives. In contrast, toddlers quickly learn to
recognize objects from different viewpoints with almost no supervision. Recent
works argue that toddlers develop this ability by mapping close-in-time visual
inputs to similar representations while interacting with objects. High acuity
vision is only available in the central visual field, which may explain why
toddlers (much like adults) constantly move their gaze around during such
interactions. It is unclear whether/how much toddlers curate their visual
experience through these eye movements to support learning object
representations. In this work, we explore whether a bio inspired visual
learning model can harness toddlers' gaze behavior during a play session to
develop view-invariant object recognition. Exploiting head-mounted eye tracking
during dyadic play, we simulate toddlers' central visual field experience by
cropping image regions centered on the gaze location. This visual stream feeds
a time-based self-supervised learning algorithm. Our experiments demonstrate
that toddlers' gaze strategy supports the learning of invariant object
representations. Our analysis also reveals that the limited size of the central
visual field where acuity is high is crucial for this. We further find that
toddlers' visual experience elicits more robust representations compared to
adults' mostly because toddlers look at objects they hold themselves for longer
bouts. Overall, our work reveals how toddlers' gaze behavior supports
self-supervised learning of view-invariant object recognition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UnSegMedGAT: Unsupervised Medical Image Segmentation using Graph
  Attention Networks Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01966v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01966v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        A. Mudit Adityaja, Saurabh J. Shigwan, Nitin Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The data-intensive nature of supervised classification drives the interest of
the researchers towards unsupervised approaches, especially for problems such
as medical image segmentation, where labeled data is scarce. Building on the
recent advancements of Vision transformers (ViT) in computer vision, we propose
an unsupervised segmentation framework using a pre-trained Dino-ViT. In the
proposed method, we leverage the inherent graph structure within the image to
realize a significant performance gain for segmentation in medical images. For
this, we introduce a modularity-based loss function coupled with a Graph
Attention Network (GAT) to effectively capture the inherent graph topology
within the image. Our method achieves state-of-the-art performance, even
significantly surpassing or matching that of existing (semi)supervised
technique such as MedSAM which is a Segment Anything Model in medical images.
We demonstrate this using two challenging medical image datasets ISIC-2018 and
CVC-ColonDB. This work underscores the potential of unsupervised approaches in
advancing medical image analysis in scenarios where labeled data is scarce. The
github repository of the code is available on
[https://github.com/mudit-adityaja/UnSegMedGAT].
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning for Leopard Individual Identification: An Adaptive Angular
  Margin Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01962v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01962v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Colomer Matachana
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate identification of individual leopards across camera trap images is
critical for population monitoring and ecological studies. This paper
introduces a deep learning framework to distinguish between individual leopards
based on their unique spot patterns. This approach employs a novel adaptive
angular margin method in the form of a modified CosFace architecture. In
addition, I propose a preprocessing pipeline that combines RGB channels with an
edge detection channel to underscore the critical features learned by the
model.
  This approach significantly outperforms the Triplet Network baseline,
achieving a Dynamic Top-5 Average Precision of 0.8814 and a Top-5 Rank Match
Detection of 0.9533, demonstrating its potential for open-set learning in
wildlife identification. While not surpassing the performance of the SIFT-based
Hotspotter algorithm, this method represents a substantial advancement in
applying deep learning to patterned wildlife identification.
  This research contributes to the field of computer vision and provides a
valuable tool for biologists aiming to study and protect leopard populations.
It also serves as a stepping stone for applying the power of deep learning in
Capture-Recapture studies for other patterned species.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust plug-and-play methods for highly accelerated non-Cartesian MRI
  reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01955v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01955v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pierre-Antoine Comby, Benjamin Lapostolle, Matthieu Terris, Philippe Ciuciu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Achieving high-quality Magnetic Resonance Imaging (MRI) reconstruction at
accelerated acquisition rates remains challenging due to the inherent ill-posed
nature of the inverse problem. Traditional Compressed Sensing (CS) methods,
while robust across varying acquisition settings, struggle to maintain good
reconstruction quality at high acceleration factors ($\ge$ 8). Recent advances
in deep learning have improved reconstruction quality, but purely data-driven
methods are prone to overfitting and hallucination effects, notably when the
acquisition setting is varying. Plug-and-Play (PnP) approaches have been
proposed to mitigate the pitfalls of both frameworks. In a nutshell, PnP
algorithms amount to replacing suboptimal handcrafted CS priors with powerful
denoising deep neural network (DNNs). However, in MRI reconstruction, existing
PnP methods often yield suboptimal results due to instabilities in the proximal
gradient descent (PGD) schemes and the lack of curated, noiseless datasets for
training robust denoisers. In this work, we propose a fully unsupervised
preprocessing pipeline to generate clean, noiseless complex MRI signals from
multicoil data, enabling training of a high-performance denoising DNN.
Furthermore, we introduce an annealed Half-Quadratic Splitting (HQS) algorithm
to address the instability issues, leading to significant improvements over
existing PnP algorithms. When combined with preconditioning techniques, our
approach achieves state-of-the-art results, providing a robust and efficient
solution for high-quality MRI reconstruction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Where to Edit Vision Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01948v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01948v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunqiao Yang, Long-Kai Huang, Shengzhuang Chen, Kede Ma, Ying Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model editing aims to data-efficiently correct predictive errors of large
pre-trained models while ensuring generalization to neighboring failures and
locality to minimize unintended effects on unrelated examples. While
significant progress has been made in editing Transformer-based large language
models, effective strategies for editing vision Transformers (ViTs) in computer
vision remain largely untapped. In this paper, we take initial steps towards
correcting predictive errors of ViTs, particularly those arising from
subpopulation shifts. Taking a locate-then-edit approach, we first address the
where-to-edit challenge by meta-learning a hypernetwork on CutMix-augmented
data generated for editing reliability. This trained hypernetwork produces
generalizable binary masks that identify a sparse subset of structured model
parameters, responsive to real-world failure samples. Afterward, we solve the
how-to-edit problem by simply fine-tuning the identified parameters using a
variant of gradient descent to achieve successful edits. To validate our
method, we construct an editing benchmark that introduces subpopulation shifts
towards natural underrepresented images and AI-generated images, thereby
revealing the limitations of pre-trained ViTs for object recognition. Our
approach not only achieves superior performance on the proposed benchmark but
also allows for adjustable trade-offs between generalization and locality. Our
code is available at https://github.com/hustyyq/Where-to-Edit.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploiting Contextual Uncertainty of Visual Data for Efficient Training
  of Deep Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01925v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01925v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sharat Agarwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Objects, in the real world, rarely occur in isolation and exhibit typical
arrangements governed by their independent utility, and their expected
interaction with humans and other objects in the context. For example, a chair
is expected near a table, and a computer is expected on top. Humans use this
spatial context and relative placement as an important cue for visual
recognition in case of ambiguities. Similar to human's, DNN's exploit
contextual information from data to learn representations. Our research focuses
on harnessing the contextual aspects of visual data to optimize data annotation
and enhance the training of deep networks. Our contributions can be summarized
as follows: (1) We introduce the notion of contextual diversity for active
learning CDAL and show its applicability in three different visual tasks
semantic segmentation, object detection and image classification, (2) We
propose a data repair algorithm to curate contextually fair data to reduce
model bias, enabling the model to detect objects out of their obvious context,
(3) We propose Class-based annotation, where contextually relevant classes are
selected that are complementary for model training under domain shift.
Understanding the importance of well-curated data, we also emphasize the
necessity of involving humans in the loop to achieve accurate annotations and
to develop novel interaction strategies that allow humans to serve as
fact-checkers. In line with this we are working on developing image retrieval
system for wildlife camera trap images and reliable warning system for poor
quality rural roads. For large-scale annotation, we are employing a strategic
combination of human expertise and zero-shot models, while also integrating
human input at various stages for continuous feedback.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICVGIP, Young Researchers Symposium</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-Time Polygonal Semantic Mapping for Humanoid Robot Stair Climbing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01919v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01919v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Teng Bin, Jianming Yao, Tin Lun Lam, Tianwei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel algorithm for real-time planar semantic mapping tailored
for humanoid robots navigating complex terrains such as staircases. Our method
is adaptable to any odometry input and leverages GPU-accelerated processes for
planar extraction, enabling the rapid generation of globally consistent
semantic maps. We utilize an anisotropic diffusion filter on depth images to
effectively minimize noise from gradient jumps while preserving essential edge
details, enhancing normal vector images' accuracy and smoothness. Both the
anisotropic diffusion and the RANSAC-based plane extraction processes are
optimized for parallel processing on GPUs, significantly enhancing
computational efficiency. Our approach achieves real-time performance,
processing single frames at rates exceeding $30~Hz$, which facilitates detailed
plane extraction and map management swiftly and efficiently. Extensive testing
underscores the algorithm's capabilities in real-time scenarios and
demonstrates its practical application in humanoid robot gait planning,
significantly improving its ability to navigate dynamic environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by The 2024 IEEE-RAS International Conference on Humanoid
  Robots. The code: https://github.com/BTFrontier/polygon_mapping</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Masked Autoencoders are Parameter-Efficient Federated Continual Learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01916v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01916v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchen He, Xiangfeng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning is a specific distributed learning paradigm in which a
central server aggregates updates from multiple clients' local models, thereby
enabling the server to learn without requiring clients to upload their private
data, maintaining data privacy. While existing federated learning methods are
primarily designed for static data, real-world applications often require
clients to learn new categories over time. This challenge necessitates the
integration of continual learning techniques, resulting in federated continual
learning (FCL). Although advanced prompt-based continual learning methods
leverage pre-trained transformers to mitigate catastrophic forgetting, they do
not adequately address the non-IID challenges in federated learning. To address
both catastrophic forgetting and non-IID issues, we propose to use masked
autoencoders (MAEs) as parameter-efficient federated continual learners, called
pMAE. pMAE learns reconstructive prompt on the client side through image
reconstruction using MAEs. On the server side, it reconstructs the uploaded
restore information to capture the data distribution across previous tasks and
different clients, using these reconstructed images to finetune discriminative
prompt and classifier parameters designed for classification, thereby
alleviating catastrophic forgetting and non-IID challenges on a global scale.
Experimental results demonstrate that pMAE achieves performance comparable to
existing prompt-based methods and can enhance their effectiveness, particularly
when using self-supervised pre-trained transformers as the backbone. Code is
available at: https://github.com/ycheoo/pMAE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FPPL: An Efficient and Non-IID Robust Federated Continual Learning
  Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01904v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01904v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchen He, Chuyun Shen, Xiangfeng Wang, Bo Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated continual learning (FCL) aims to learn from sequential data stream
in the decentralized federated learning setting, while simultaneously
mitigating the catastrophic forgetting issue in classical continual learning.
Existing FCL methods usually employ typical rehearsal mechanisms, which could
result in privacy violations or additional onerous storage and computational
burdens. In this work, an efficient and non-IID robust federated continual
learning framework, called Federated Prototype-Augmented Prompt Learning
(FPPL), is proposed. The FPPL can collaboratively learn lightweight prompts
augmented by prototypes without rehearsal. On the client side, a fusion
function is employed to fully leverage the knowledge contained in task-specific
prompts for alleviating catastrophic forgetting. Additionally, global
prototypes aggregated from the server are used to obtain unified representation
through contrastive learning, mitigating the impact of non-IID-derived data
heterogeneity. On the server side, locally uploaded prototypes are utilized to
perform debiasing on the classifier, further alleviating the performance
degradation caused by both non-IID and catastrophic forgetting. Empirical
evaluations demonstrate the effectiveness of FPPL, achieving notable
performance with an efficient design while remaining robust to diverse non-IID
degrees. Code is available at: https://github.com/ycheoo/FPPL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MBDRes-U-Net: Multi-Scale Lightweight Brain Tumor Segmentation Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01896v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01896v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Longfeng Shen, Yanqi Hou, Jiacong Chen, Liangjin Diao, Yaxi Duan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate segmentation of brain tumors plays a key role in the diagnosis and
treatment of brain tumor diseases. It serves as a critical technology for
quantifying tumors and extracting their features. With the increasing
application of deep learning methods, the computational burden has become
progressively heavier. To achieve a lightweight model with good segmentation
performance, this study proposes the MBDRes-U-Net model using the
three-dimensional (3D) U-Net codec framework, which integrates multibranch
residual blocks and fused attention into the model. The computational burden of
the model is reduced by the branch strategy, which effectively uses the rich
local features in multimodal images and enhances the segmentation performance
of subtumor regions. Additionally, during encoding, an adaptive weighted
expansion convolution layer is introduced into the multi-branch residual block,
which enriches the feature expression and improves the segmentation accuracy of
the model. Experiments on the Brain Tumor Segmentation (BraTS) Challenge 2018
and 2019 datasets show that the architecture could maintain a high precision of
brain tumor segmentation while considerably reducing the calculation
overhead.Our code is released at
https://github.com/Huaibei-normal-university-cv-laboratory/mbdresunet
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Brain tumor segmentation, lightweight model, Brain Tumor Segmentation
  (BraTS) Challenge, group convolution</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Global Depth-Range-Free Multi-View Stereo Transformer Network with
  Pose Embedding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01893v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01893v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yitong Dong, Yijin Li, Zhaoyang Huang, Weikang Bian, Jingbo Liu, Hujun Bao, Zhaopeng Cui, Hongsheng Li, Guofeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel multi-view stereo (MVS) framework that gets
rid of the depth range prior. Unlike recent prior-free MVS methods that work in
a pair-wise manner, our method simultaneously considers all the source images.
Specifically, we introduce a Multi-view Disparity Attention (MDA) module to
aggregate long-range context information within and across multi-view images.
Considering the asymmetry of the epipolar disparity flow, the key to our method
lies in accurately modeling multi-view geometric constraints. We integrate pose
embedding to encapsulate information such as multi-view camera poses, providing
implicit geometric constraints for multi-view disparity feature fusion
dominated by attention. Additionally, we construct corresponding hidden states
for each source image due to significant differences in the observation quality
of the same pixel in the reference frame across multiple source frames. We
explicitly estimate the quality of the current pixel corresponding to sampled
points on the epipolar line of the source image and dynamically update hidden
states through the uncertainty estimation module. Extensive results on the DTU
dataset and Tanks&Temple benchmark demonstrate the effectiveness of our method.
The code is available at our project page:
https://zju3dv.github.io/GD-PoseMVS/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LiDAttack: Robust Black-box Attack on LiDAR-based Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01889v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01889v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinyin Chen, Danxin Liao, Sheng Xiang, Haibin Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since DNN is vulnerable to carefully crafted adversarial examples,
adversarial attack on LiDAR sensors have been extensively studied. We introduce
a robust black-box attack dubbed LiDAttack. It utilizes a genetic algorithm
with a simulated annealing strategy to strictly limit the location and number
of perturbation points, achieving a stealthy and effective attack. And it
simulates scanning deviations, allowing it to adapt to dynamic changes in real
world scenario variations. Extensive experiments are conducted on 3 datasets
(i.e., KITTI, nuScenes, and self-constructed data) with 3 dominant object
detection models (i.e., PointRCNN, PointPillar, and PV-RCNN++). The results
reveal the efficiency of the LiDAttack when targeting a wide range of object
detection models, with an attack success rate (ASR) up to 90%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mining and Transferring Feature-Geometry Coherence for Unsupervised
  Point Cloud Registration <span class="chip">NeurIPS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01870v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01870v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kezheng Xiong, Haoen Xiang, Qingshan Xu, Chenglu Wen, Siqi Shen, Jonathan Li, Cheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Point cloud registration, a fundamental task in 3D vision, has achieved
remarkable success with learning-based methods in outdoor environments.
Unsupervised outdoor point cloud registration methods have recently emerged to
circumvent the need for costly pose annotations. However, they fail to
establish reliable optimization objectives for unsupervised training, either
relying on overly strong geometric assumptions, or suffering from poor-quality
pseudo-labels due to inadequate integration of low-level geometric and
high-level contextual information. We have observed that in the feature space,
latent new inlier correspondences tend to cluster around respective positive
anchors that summarize features of existing inliers. Motivated by this
observation, we propose a novel unsupervised registration method termed INTEGER
to incorporate high-level contextual information for reliable pseudo-label
mining. Specifically, we propose the Feature-Geometry Coherence Mining module
to dynamically adapt the teacher for each mini-batch of data during training
and discover reliable pseudo-labels by considering both high-level feature
representations and low-level geometric cues. Furthermore, we propose
Anchor-Based Contrastive Learning to facilitate contrastive learning with
anchors for a robust feature space. Lastly, we introduce a Mixed-Density
Student to learn density-invariant features, addressing challenges related to
density variation and low overlap in the outdoor scenario. Extensive
experiments on KITTI and nuScenes datasets demonstrate that our INTEGER
achieves competitive performance in terms of accuracy and generalizability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Novel Deep Learning Tractography Fiber Clustering Framework for
  Functionally Consistent White Matter Parcellation Using <span class="highlight-title">Multimodal</span> Diffusion
  MRI and Functional MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01859v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01859v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin Wang, Bocheng Guo, Yijie Li, Junyi Wang, Yuqian Chen, Jarrett Rushmore, Nikos Makris, Yogesh Rathi, Lauren J O'Donnell, Fan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tractography fiber clustering using diffusion MRI (dMRI) is a crucial
strategy for white matter (WM) parcellation. Current methods primarily use the
geometric information of fibers (i.e., the spatial trajectories) to group
similar fibers into clusters, overlooking the important functional signals
present along the fiber tracts. There is increasing evidence that neural
activity in the WM can be measured using functional MRI (fMRI), offering
potentially valuable multimodal information for fiber clustering. In this
paper, we develop a novel deep learning fiber clustering framework, namely Deep
Multi-view Fiber Clustering (DMVFC), that uses joint dMRI and fMRI data to
enable functionally consistent WM parcellation. DMVFC can effectively integrate
the geometric characteristics of the WM fibers with the fMRI BOLD signals along
the fiber tracts. It includes two major components: 1) a multi-view pretraining
module to compute embedding features from fiber geometric information and
functional signals separately, and 2) a collaborative fine-tuning module to
simultaneously refine the two kinds of embeddings. In the experiments, we
compare DMVFC with two state-of-the-art fiber clustering methods and
demonstrate superior performance in achieving functionally meaningful and
consistent WM parcellation results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GVKF: Gaussian Voxel Kernel Functions for Highly Efficient Surface
  Reconstruction in Open Scenes <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01853v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01853v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaochao Song, Chong Cheng, Hao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we present a novel method for efficient and effective 3D
surface reconstruction in open scenes. Existing Neural Radiance Fields (NeRF)
based works typically require extensive training and rendering time due to the
adopted implicit representations. In contrast, 3D Gaussian splatting (3DGS)
uses an explicit and discrete representation, hence the reconstructed surface
is built by the huge number of Gaussian primitives, which leads to excessive
memory consumption and rough surface details in sparse Gaussian areas. To
address these issues, we propose Gaussian Voxel Kernel Functions (GVKF), which
establish a continuous scene representation based on discrete 3DGS through
kernel regression. The GVKF integrates fast 3DGS rasterization and highly
effective scene implicit representations, achieving high-fidelity open scene
surface reconstruction. Experiments on challenging scene datasets demonstrate
the efficiency and effectiveness of our proposed GVKF, featuring with high
reconstruction quality, real-time rendering speed, significant savings in
storage and training memory consumption.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Silver medal Solution for Image Matching Challenge 2024 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01851v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01851v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yian Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image Matching Challenge 2024 is a competition focused on building 3D maps
from diverse image sets, requiring participants to solve fundamental computer
vision challenges in image matching across varying angles, lighting, and
seasonal changes. This project develops a Pipeline method that combines
multiple advanced techniques: using pre-trained EfficientNet-B7 for initial
feature extraction and cosine distance-based image pair filtering, employing
both KeyNetAffNetHardNet and SuperPoint for keypoint feature extraction,
utilizing AdaLAM and SuperGlue for keypoint matching, and finally applying
Pycolmap for 3D spatial analysis. The methodology achieved an excellent score
of 0.167 on the private leaderboard, with experimental results demonstrating
that the combination of KeyNetAffNetHardNet and SuperPoint provides significant
advantages in keypoint detection and matching, particularly when dealing with
challenging variations in surface texture and environmental conditions that
typically degrade traditional algorithm performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KptLLM: Unveiling the Power of Large Language Model for Keypoint
  Comprehension <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01846v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01846v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Yang, Wang Zeng, Sheng Jin, Lumin Xu, Wentao Liu, Chen Qian, Ruimao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Multimodal Large Language Models (MLLMs) have greatly
improved their abilities in image understanding. However, these models often
struggle with grasping pixel-level semantic details, e.g., the keypoints of an
object. To bridge this gap, we introduce the novel challenge of Semantic
Keypoint Comprehension, which aims to comprehend keypoints across different
task scenarios, including keypoint semantic understanding, visual prompt-based
keypoint detection, and textual prompt-based keypoint detection. Moreover, we
introduce KptLLM, a unified multimodal model that utilizes an
identify-then-detect strategy to effectively address these challenges. KptLLM
underscores the initial discernment of semantics in keypoints, followed by the
precise determination of their positions through a chain-of-thought process.
With several carefully designed modules, KptLLM adeptly handles various
modality inputs, facilitating the interpretation of both semantic contents and
keypoint locations. Our extensive experiments demonstrate KptLLM's superiority
in various keypoint detection benchmarks and its unique semantic capabilities
in interpreting keypoints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OwMatch: Conditional Self-Labeling with Consistency for Open-World
  Semi-Supervised Learning <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01833v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01833v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengjie Niu, Lifan Lin, Jian Huang, Chao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised learning (SSL) offers a robust framework for harnessing the
potential of unannotated data. Traditionally, SSL mandates that all classes
possess labeled instances. However, the emergence of open-world SSL (OwSSL)
introduces a more practical challenge, wherein unlabeled data may encompass
samples from unseen classes. This scenario leads to misclassification of unseen
classes as known ones, consequently undermining classification accuracy. To
overcome this challenge, this study revisits two methodologies from
self-supervised and semi-supervised learning, self-labeling and consistency,
tailoring them to address the OwSSL problem. Specifically, we propose an
effective framework called OwMatch, combining conditional self-labeling and
open-world hierarchical thresholding. Theoretically, we analyze the estimation
of class distribution on unlabeled data through rigorous statistical analysis,
thus demonstrating that OwMatch can ensure the unbiasedness of the self-label
assignment estimator with reliability. Comprehensive empirical analyses
demonstrate that our method yields substantial performance enhancements across
both known and unknown classes in comparison to previous studies. Code is
available at https://github.com/niusj03/OwMatch.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 camera-ready (10 pages, 4 figures) with the appendices
  (10 pages, 7 figures)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distribution alignment based transfer fusion frameworks on quantum
  devices for seeking quantum advantages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01822v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01822v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xi He, Feiyu Du, Xiaohan Yu, Yang Zhao, Tao Lei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The scarcity of labelled data is specifically an urgent challenge in the
field of quantum machine learning (QML). Two transfer fusion frameworks are
proposed in this paper to predict the labels of a target domain data by
aligning its distribution to a different but related labelled source domain on
quantum devices. The frameworks fuses the quantum data from two different, but
related domains through a quantum information infusion channel. The predicting
tasks in the target domain can be achieved with quantum advantages by
post-processing quantum measurement results. One framework, the quantum basic
linear algebra subroutines (QBLAS) based implementation, can theoretically
achieve the procedure of transfer fusion with quadratic speedup on a universal
quantum computer. In addition, the other framework, a hardware-scalable
architecture, is implemented on the noisy intermediate-scale quantum (NISQ)
devices through a variational hybrid quantum-classical procedure. Numerical
experiments on the synthetic and handwritten digits datasets demonstrate that
the variatioinal transfer fusion (TF) framework can reach state-of-the-art
(SOTA) quantum DA method performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffuMask-Editor: A Novel Paradigm of Integration Between the
  Segmentation Diffusion Model and Image Editing to Improve Segmentation
  Ability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01819v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01819v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Gao, Fangxu Xing, Daniel Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic segmentation models, like mask2former, often demand a substantial
amount of manually annotated data, which is time-consuming and inefficient to
acquire. Leveraging state-of-the-art text-to-image models like Midjourney and
Stable Diffusion has emerged as an effective strategy for automatically
generating synthetic data instead of human annotations. However, prior
approaches have been constrained to synthesizing single-instance images due to
the instability inherent in generating multiple instances with Stable
Diffusion. To expand the domains and diversity of synthetic datasets, this
paper introduces a novel paradigm named DiffuMask-Editor, which combines the
Diffusion Model for Segmentation with Image Editing. By integrating multiple
objects into images using Text2Image models, our method facilitates the
creation of more realistic datasets that closely resemble open-world settings
while simultaneously generating accurate masks. Our approach significantly
reduces the laborious effort associated with manual annotation while ensuring
precise mask generation. Experimental results demonstrate that synthetic data
generated by DiffuMask-Editor enable segmentation methods to achieve superior
performance compared to real data. Particularly in zero-shot backgrounds,
DiffuMask-Editor achieves new state-of-the-art results on Unseen classes of VOC
2012. The code and models will be publicly available soon.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages,4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bootstrapping Top-down Information for Self-modulating Slot Attention <span class="chip">NeurIPS2</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01801v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01801v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongwon Kim, Seoyeon Kim, Suha Kwak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object-centric learning (OCL) aims to learn representations of individual
objects within visual scenes without manual supervision, facilitating efficient
and effective visual reasoning. Traditional OCL methods primarily employ
bottom-up approaches that aggregate homogeneous visual features to represent
objects. However, in complex visual environments, these methods often fall
short due to the heterogeneous nature of visual features within an object. To
address this, we propose a novel OCL framework incorporating a top-down
pathway. This pathway first bootstraps the semantics of individual objects and
then modulates the model to prioritize features relevant to these semantics. By
dynamically modulating the model based on its own output, our top-down pathway
enhances the representational quality of objects. Our framework achieves
state-of-the-art performance across multiple synthetic and real-world
object-discovery benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS2 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Expanding Sparse <span class="highlight-title">Tuning</span> for Low Memory Usage <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01800v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01800v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shufan Shen, Junshu Sun, Xiangyang Ji, Qingming Huang, Shuhui Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parameter-efficient fine-tuning (PEFT) is an effective method for adapting
pre-trained vision models to downstream tasks by tuning a small subset of
parameters. Among PEFT methods, sparse tuning achieves superior performance by
only adjusting the weights most relevant to downstream tasks, rather than
densely tuning the whole weight matrix. However, this performance improvement
has been accompanied by increases in memory usage, which stems from two
factors, i.e., the storage of the whole weight matrix as learnable parameters
in the optimizer and the additional storage of tunable weight indexes. In this
paper, we propose a method named SNELL (Sparse tuning with kerNELized LoRA) for
sparse tuning with low memory usage. To achieve low memory usage, SNELL
decomposes the tunable matrix for sparsification into two learnable low-rank
matrices, saving from the costly storage of the whole original matrix. A
competition-based sparsification mechanism is further proposed to avoid the
storage of tunable weight indexes. To maintain the effectiveness of sparse
tuning with low-rank matrices, we extend the low-rank decomposition by applying
nonlinear kernel functions to the whole-matrix merging. Consequently, we gain
an increase in the rank of the merged matrix, enhancing the ability of SNELL in
adapting the pre-trained models to downstream tasks. Extensive experiments on
multiple downstream tasks show that SNELL achieves state-of-the-art performance
with low memory usage, endowing PEFT with sparse tuning to large-scale models.
Codes are available at https://github.com/ssfgunner/SNELL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AIWR: Aerial Image Water Resource Dataset for Segmentation Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01797v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01797v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sangdaow Noppitaka, Emmanuel Okafor, Olarik Surinta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective water resource management is crucial in agricultural regions like
northeastern Thailand, where limited water retention in sandy soils poses
significant challenges. In response to this issue, the Aerial Image Water
Resource (AIWR) dataset was developed, comprising 800 aerial images focused on
natural and artificial water bodies in this region. The dataset was created
using Bing Maps and follows the standards of the Fundamental Geographic Data
Set (FGDS). It includes ground truth annotations validated by experts in remote
sensing, making it an invaluable resource for researchers in geoinformatics,
computer vision, and artificial intelligence. The AIWR dataset presents
considerable challenges, such as segmentation due to variations in the size,
color, shape, and similarity of water bodies, which often resemble other land
use categories.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Non rigid geometric distortions correction -- Application to atmospheric
  turbulence stabilization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01788v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01788v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Mao, Jerome Gilles
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A novel approach is presented to recover an image degraded by atmospheric
turbulence. Given a sequence of frames affected by turbulence, we construct a
variational model to characterize the static image. The optimization problem is
solved by Bregman Iteration and the operator splitting method. Our algorithm is
simple, efficient, and can be easily generalized for different scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MSTA3D: Multi-scale Twin-attention for 3D Instance Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01781v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01781v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Duc Dang Trung Tran, Byeongkeun Kang, Yeejin Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, transformer-based techniques incorporating superpoints have become
prevalent in 3D instance segmentation. However, they often encounter an
over-segmentation problem, especially noticeable with large objects.
Additionally, unreliable mask predictions stemming from superpoint mask
prediction further compound this issue. To address these challenges, we propose
a novel framework called MSTA3D. It leverages multi-scale feature
representation and introduces a twin-attention mechanism to effectively capture
them. Furthermore, MSTA3D integrates a box query with a box regularizer,
offering a complementary spatial constraint alongside semantic queries.
Experimental evaluations on ScanNetV2, ScanNet200 and S3DIS datasets
demonstrate that our approach surpasses state-of-the-art 3D instance
segmentation methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 9 figures, 7 tables, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning predictable and robust neural representations by straightening
  image sequences <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01777v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01777v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xueyan Niu, Cristina Savin, Eero P. Simoncelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prediction is a fundamental capability of all living organisms, and has been
proposed as an objective for learning sensory representations. Recent work
demonstrates that in primate visual systems, prediction is facilitated by
neural representations that follow straighter temporal trajectories than their
initial photoreceptor encoding, which allows for prediction by linear
extrapolation. Inspired by these experimental findings, we develop a
self-supervised learning (SSL) objective that explicitly quantifies and
promotes straightening. We demonstrate the power of this objective in training
deep feedforward neural networks on smoothly-rendered synthetic image sequences
that mimic commonly-occurring properties of natural videos. The learned model
contains neural embeddings that are predictive, but also factorize the
geometric, photometric, and semantic attributes of objects. The representations
also prove more robust to noise and adversarial attacks compared to previous
SSL methods that optimize for invariance to random augmentations. Moreover,
these beneficial properties can be transferred to other training procedures by
using the straightening objective as a regularizer, suggesting a broader
utility for straightening as a principle for robust unsupervised learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ARN-LSTM: A Multi-Stream Attention-Based Model for Action Recognition
  with Temporal Dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01769v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01769v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuanchuan Wang, Ahmad Sufril Azlan Mohmamed, Xiao Yang, Xiang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents ARN-LSTM, a novel multi-stream action recognition model
designed to address the challenge of simultaneously capturing spatial motion
and temporal dynamics in action sequences. Traditional methods often focus
solely on spatial or temporal features, limiting their ability to comprehend
complex human activities fully. Our proposed model integrates joint, motion,
and temporal information through a multi-stream fusion architecture.
Specifically, it comprises a joint stream for extracting skeleton features, a
temporal stream for capturing dynamic temporal features, and an ARN-LSTM block
that utilizes Time-Distributed Long Short-Term Memory (TD-LSTM) layers followed
by an Attention Relation Network (ARN) to model temporal relations. The outputs
from these streams are fused in a fully connected layer to provide the final
action prediction. Evaluations on the NTU RGB+D 60 and NTU RGB+D 120 datasets
demonstrate the effectiveness of our model, achieving effective performance,
particularly in group activity recognition.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic Structured Pruning for Efficient Architecture in Federated
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01759v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01759v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thai Vu Nguyen, Long Bao Le, Anderson Avila
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Federated Learning (FL), training is conducted on client devices,
typically with limited computational resources and storage capacity. To address
these constraints, we propose an automatic pruning scheme tailored for FL
systems. Our solution improves computation efficiency on client devices, while
minimizing communication costs. One of the challenges of tuning pruning
hyper-parameters in FL systems is the restricted access to local data. Thus, we
introduce an automatic pruning paradigm that dynamically determines pruning
boundaries. Additionally, we utilized a structured pruning algorithm optimized
for mobile devices that lack hardware support for sparse computations.
Experimental results demonstrate the effectiveness of our approach, achieving
accuracy comparable to existing methods. Our method notably reduces the number
of parameters by 89% and FLOPS by 90%, with minimal impact on the accuracy of
the FEMNIST and CelebFaces datasets. Furthermore, our pruning method decreases
communication overhead by up to 5x and halves inference time when deployed on
Android devices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Disentangled PET Lesion Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01758v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01758v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tanya Gatsak, Kumar Abhishek, Hanene Ben Yedder, Saeid Asgari Taghanaki, Ghassan Hamarneh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  PET imaging is an invaluable tool in clinical settings as it captures the
functional activity of both healthy anatomy and cancerous lesions. Developing
automatic lesion segmentation methods for PET images is crucial since manual
lesion segmentation is laborious and prone to inter- and intra-observer
variability. We propose PET-Disentangler, a 3D disentanglement method that uses
a 3D UNet-like encoder-decoder architecture to disentangle disease and normal
healthy anatomical features with losses for segmentation, reconstruction, and
healthy component plausibility. A critic network is used to encourage the
healthy latent features to match the distribution of healthy samples and thus
encourages these features to not contain any lesion-related features. Our
quantitative results show that PET-Disentangler is less prone to incorrectly
declaring healthy and high tracer uptake regions as cancerous lesions, since
such uptake pattern would be assigned to the disentangled healthy component.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 2 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ChatTracker: Enhancing Visual Tracking Performance via Chatting with
  <span class="highlight-title">Multimodal</span> Large Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01756v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01756v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Sun, Fan Yu, Shaoxiang Chen, Yu Zhang, Junwei Huang, Chenhui Li, Yang Li, Changbo Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual object tracking aims to locate a targeted object in a video sequence
based on an initial bounding box. Recently, Vision-Language~(VL) trackers have
proposed to utilize additional natural language descriptions to enhance
versatility in various applications. However, VL trackers are still inferior to
State-of-The-Art (SoTA) visual trackers in terms of tracking performance. We
found that this inferiority primarily results from their heavy reliance on
manual textual annotations, which include the frequent provision of ambiguous
language descriptions. In this paper, we propose ChatTracker to leverage the
wealth of world knowledge in the Multimodal Large Language Model (MLLM) to
generate high-quality language descriptions and enhance tracking performance.
To this end, we propose a novel reflection-based prompt optimization module to
iteratively refine the ambiguous and inaccurate descriptions of the target with
tracking feedback. To further utilize semantic information produced by MLLM, a
simple yet effective VL tracking framework is proposed and can be easily
integrated as a plug-and-play module to boost the performance of both VL and
visual trackers. Experimental results show that our proposed ChatTracker
achieves a performance comparable to existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-task Geometric Estimation of Depth and Surface Normal from
  Monocular 360° Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01749v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01749v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Huang, Fang-Lue Zhang, Fangfang Zhang, Yu-Kun Lai, Paul Rosin, Neil A. Dodgson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Geometric estimation is required for scene understanding and analysis in
panoramic 360{\deg} images. Current methods usually predict a single feature,
such as depth or surface normal. These methods can lack robustness, especially
when dealing with intricate textures or complex object surfaces. We introduce a
novel multi-task learning (MTL) network that simultaneously estimates depth and
surface normals from 360{\deg} images. Our first innovation is our MTL
architecture, which enhances predictions for both tasks by integrating
geometric information from depth and surface normal estimation, enabling a
deeper understanding of 3D scene structure. Another innovation is our fusion
module, which bridges the two tasks, allowing the network to learn shared
representations that improve accuracy and robustness. Experimental results
demonstrate that our MTL architecture significantly outperforms
state-of-the-art methods in both depth and surface normal estimation, showing
superior performance in complex and diverse scenes. Our model's effectiveness
and generalizability, particularly in handling intricate surface textures,
establish it as a new benchmark in 360{\deg} image geometric estimation. The
code and model are available at
\url{https://github.com/huangkun101230/360MTLGeometricEstimation}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, this paper is accepted by Computational Visual Media
  Journal (CVMJ) but not pushlished yet</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rotation Perturbation Robustness in Point Cloud Analysis: A Perspective
  of Manifold Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01748v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01748v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Xu, Huazhen Liu, Feiming Wei, Huilin Xiong, Wenxian Yu, Tao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Point cloud is often regarded as a discrete sampling of Riemannian manifold
and plays a pivotal role in the 3D image interpretation. Particularly, rotation
perturbation, an unexpected small change in rotation caused by various factors
(like equipment offset, system instability, measurement errors and so on), can
easily lead to the inferior results in point cloud learning tasks. However,
classical point cloud learning methods are sensitive to rotation perturbation,
and the existing networks with rotation robustness also have much room for
improvements in terms of performance and noise tolerance. Given these, this
paper remodels the point cloud from the perspective of manifold as well as
designs a manifold distillation method to achieve the robustness of rotation
perturbation without any coordinate transformation. In brief, during the
training phase, we introduce a teacher network to learn the rotation robustness
information and transfer this information to the student network through online
distillation. In the inference phase, the student network directly utilizes the
original 3D coordinate information to achieve the robustness of rotation
perturbation. Experiments carried out on four different datasets verify the
effectiveness of our method. Averagely, on the Modelnet40 and ScanobjectNN
classification datasets with random rotation perturbations, our classification
accuracy has respectively improved by 4.92% and 4.41%, compared to popular
rotation-robust networks; on the ShapeNet and S3DIS segmentation datasets,
compared to the rotation-robust networks, the improvements of mIoU are 7.36%
and 4.82%, respectively. Besides, from the experimental results, the proposed
algorithm also shows excellent performance in resisting noise and outliers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 8 figures, submitted to TCSVT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning from Convolution-based Unlearnable Datastes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01742v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01742v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dohyun Kim, Pedro Sandoval-Segura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The construction of large datasets for deep learning has raised concerns
regarding unauthorized use of online data, leading to increased interest in
protecting data from third-parties who want to use it for training. The
Convolution-based Unlearnable DAtaset (CUDA) method aims to make data
unlearnable by applying class-wise blurs to every image in the dataset so that
neural networks learn relations between blur kernels and labels, as opposed to
informative features for classifying clean data. In this work, we evaluate
whether CUDA data remains unlearnable after image sharpening and frequency
filtering, finding that this combination of simple transforms improves the
utility of CUDA data for training. In particular, we observe a substantial
increase in test accuracy over adversarial training for models trained with
CUDA unlearnable data from CIFAR-10, CIFAR-100, and ImageNet-100. In training
models to high accuracy using unlearnable data, we underscore the need for
ongoing refinement in data poisoning techniques to ensure data privacy. Our
method opens new avenues for enhancing the robustness of unlearnable datasets
by highlighting that simple methods such as sharpening and frequency filtering
are capable of breaking convolution-based unlearnable datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Not Just Object, But State: Compositional Incremental Learning without
  Forgetting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01739v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01739v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanyi Zhang, Binglin Qiu, Qi Jia, Yu Liu, Ran He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most incremental learners excessively prioritize coarse classes of objects
while neglecting various kinds of states (e.g. color and material) attached to
the objects. As a result, they are limited in the ability to reason
fine-grained compositionality of state-object pairs. To remedy this limitation,
we propose a novel task called Compositional Incremental Learning
(composition-IL), enabling the model to recognize state-object compositions as
a whole in an incremental learning fashion. Since the lack of suitable
benchmarks, we re-organize two existing datasets and make them tailored for
composition-IL. Then, we propose a prompt-based Composition Incremental Learner
(CompILer), to overcome the ambiguous composition boundary problem which
challenges composition-IL largely. Specifically, we exploit multi-pool prompt
learning, which is regularized by inter-pool prompt discrepancy and intra-pool
prompt diversity. Besides, we devise object-injected state prompting by using
object prompts to guide the selection of state prompts. Furthermore, we fuse
the selected prompts by a generalized-mean strategy, to eliminate irrelevant
information learned in the prompts. Extensive experiments on two datasets
exhibit state-of-the-art performance achieved by CompILer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Next Best View For Point-Cloud Model Acquisition: Bayesian Approximation
  and Uncertainty Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01734v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01734v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Madalena Caldeira, Plinio Moreno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Next Best View problem is a computer vision problem widely studied in
robotics. To solve it, several methodologies have been proposed over the years.
Some, more recently, propose the use of deep learning models. Predictions
obtained with the help of deep learning models naturally have some uncertainty
associated with them. Despite this, the standard models do not allow for their
quantification. However, Bayesian estimation theory contributed to the
demonstration that dropout layers allow to estimate prediction uncertainty in
neural networks.
  This work adapts the point-net-based neural network for Next-Best-View
(PC-NBV). It incorporates dropout layers into the model's architecture, thus
allowing the computation of the uncertainty estimate associated with its
predictions. The aim of the work is to improve the network's accuracy in
correctly predicting the next best viewpoint, proposing a way to make the 3D
reconstruction process more efficient.
  Two uncertainty measurements capable of reflecting the prediction's error and
accuracy, respectively, were obtained. These enabled the reduction of the
model's error and the increase in its accuracy from 30\% to 80\% by identifying
and disregarding predictions with high values of uncertainty. Another method
that directly uses these uncertainty metrics to improve the final prediction
was also proposed. However, it showed very residual improvements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Probabilistic Formulation of LiDAR Mapping with Neural Radiance Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01725v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01725v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew McDermott, Jason Rife
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we reexamine the process through which a Neural Radiance Field
(NeRF) can be trained to produce novel LiDAR views of a scene. Unlike image
applications where camera pixels integrate light over time, LiDAR pulses arrive
at specific times. As such, multiple LiDAR returns are possible for any given
detector and the classification of these returns is inherently probabilistic.
Applying a traditional NeRF training routine can result in the network learning
phantom surfaces in free space between conflicting range measurements, similar
to how floater aberrations may be produced by an image model. We show that by
formulating loss as an integral of probability (rather than as an integral of
optical density) the network can learn multiple peaks for a given ray, allowing
the sampling of first, nth, or strongest returns from a single output channel.
Code is available at https://github.com/mcdermatt/PLINK
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EMMA: End-to-End <span class="highlight-title">Multimodal</span> Model for Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23262v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23262v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jyh-Jing Hwang, Runsheng Xu, Hubert Lin, Wei-Chih Hung, Jingwei Ji, Kristy Choi, Di Huang, Tong He, Paul Covington, Benjamin Sapp, Yin Zhou, James Guo, Dragomir Anguelov, Mingxing Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce EMMA, an End-to-end Multimodal Model for Autonomous driving.
Built on a multi-modal large language model foundation, EMMA directly maps raw
camera sensor data into various driving-specific outputs, including planner
trajectories, perception objects, and road graph elements. EMMA maximizes the
utility of world knowledge from the pre-trained large language models, by
representing all non-sensor inputs (e.g. navigation instructions and ego
vehicle status) and outputs (e.g. trajectories and 3D locations) as natural
language text. This approach allows EMMA to jointly process various driving
tasks in a unified language space, and generate the outputs for each task using
task-specific prompts. Empirically, we demonstrate EMMA's effectiveness by
achieving state-of-the-art performance in motion planning on nuScenes as well
as competitive results on the Waymo Open Motion Dataset (WOMD). EMMA also
yields competitive results for camera-primary 3D object detection on the Waymo
Open Dataset (WOD). We show that co-training EMMA with planner trajectories,
object detection, and road graph tasks yields improvements across all three
domains, highlighting EMMA's potential as a generalist model for autonomous
driving applications. However, EMMA also exhibits certain limitations: it can
process only a small amount of image frames, does not incorporate accurate 3D
sensing modalities like LiDAR or radar and is computationally expensive. We
hope that our results will inspire further research to mitigate these issues
and to further evolve the state of the art in autonomous driving model
architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Blog post: https://waymo.com/blog/2024/10/introducing-emma/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Taxonomy-Aware Continual Semantic Segmentation in Hyperbolic Spaces for
  Open-World Perception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.18145v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.18145v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julia Hindel, Daniele Cattaneo, Abhinav Valada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic segmentation models are typically trained on a fixed set of classes,
limiting their applicability in open-world scenarios. Class-incremental
semantic segmentation aims to update models with emerging new classes while
preventing catastrophic forgetting of previously learned ones. However,
existing methods impose strict rigidity on old classes, reducing their
effectiveness in learning new incremental classes. In this work, we propose
Taxonomy-Oriented Poincar\'e-regularized Incremental-Class Segmentation
(TOPICS) that learns feature embeddings in hyperbolic space following explicit
taxonomy-tree structures. This supervision provides plasticity for old classes,
updating ancestors based on new classes while integrating new classes at
fitting positions. Additionally, we maintain implicit class relational
constraints on the geometric basis of the Poincar\'e ball. This ensures that
the latent space can continuously adapt to new constraints while maintaining a
robust structure to combat catastrophic forgetting. We also establish eight
realistic incremental learning protocols for autonomous driving scenarios,
where novel classes can originate from known classes or the background.
Extensive evaluations of TOPICS on the Cityscapes and Mapillary Vistas 2.0
benchmarks demonstrate that it achieves state-of-the-art performance. We make
the code and trained models publicly available at
http://topics.cs.uni-freiburg.de.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interpreting <span class="highlight-title">CLIP</span> with Sparse Linear Concept Embeddings (SpLiCE) <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.10376v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.10376v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Usha Bhalla, Alex Oesterling, Suraj Srinivas, Flavio P. Calmon, Himabindu Lakkaraju
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  CLIP embeddings have demonstrated remarkable performance across a wide range
of multimodal applications. However, these high-dimensional, dense vector
representations are not easily interpretable, limiting our understanding of the
rich structure of CLIP and its use in downstream applications that require
transparency. In this work, we show that the semantic structure of CLIP's
latent space can be leveraged to provide interpretability, allowing for the
decomposition of representations into semantic concepts. We formulate this
problem as one of sparse recovery and propose a novel method, Sparse Linear
Concept Embeddings, for transforming CLIP representations into sparse linear
combinations of human-interpretable concepts. Distinct from previous work,
SpLiCE is task-agnostic and can be used, without training, to explain and even
replace traditional dense CLIP representations, maintaining high downstream
performance while significantly improving their interpretability. We also
demonstrate significant use cases of SpLiCE representations including detecting
spurious correlations and model editing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 15 figures, NeurIPS 2024. Code is provided at
  https://github.com/AI4LIFE-GROUP/SpLiCE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ xMIL: Insightful Explanations for Multiple Instance Learning in
  Histopathology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04280v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04280v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julius Hense, Mina Jamshidi Idaji, Oliver Eberle, Thomas Schnake, Jonas Dippel, Laure Ciernik, Oliver Buchstab, Andreas Mock, Frederick Klauschen, Klaus-Robert Müller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multiple instance learning (MIL) is an effective and widely used approach for
weakly supervised machine learning. In histopathology, MIL models have achieved
remarkable success in tasks like tumor detection, biomarker prediction, and
outcome prognostication. However, MIL explanation methods are still lagging
behind, as they are limited to small bag sizes or disregard instance
interactions. We revisit MIL through the lens of explainable AI (XAI) and
introduce xMIL, a refined framework with more general assumptions. We
demonstrate how to obtain improved MIL explanations using layer-wise relevance
propagation (LRP) and conduct extensive evaluation experiments on three toy
settings and four real-world histopathology datasets. Our approach consistently
outperforms previous explanation attempts with particularly improved
faithfulness scores on challenging biomarker prediction tasks. Finally, we
showcase how xMIL explanations enable pathologists to extract insights from MIL
models, representing a significant advance for knowledge discovery and model
debugging in digital histopathology. Codes are available at:
https://github.com/tubml-pathology/xMIL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fashion-VDM: Video Diffusion Model for Virtual Try-On <span class="chip">SIGGRAPH</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00225v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00225v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johanna Karras, Yingwei Li, Nan Liu, Luyang Zhu, Innfarn Yoo, Andreas Lugmayr, Chris Lee, Ira Kemelmacher-Shlizerman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Fashion-VDM, a video diffusion model (VDM) for generating virtual
try-on videos. Given an input garment image and person video, our method aims
to generate a high-quality try-on video of the person wearing the given
garment, while preserving the person's identity and motion. Image-based virtual
try-on has shown impressive results; however, existing video virtual try-on
(VVT) methods are still lacking garment details and temporal consistency. To
address these issues, we propose a diffusion-based architecture for video
virtual try-on, split classifier-free guidance for increased control over the
conditioning inputs, and a progressive temporal training strategy for
single-pass 64-frame, 512px video generation. We also demonstrate the
effectiveness of joint image-video training for video try-on, especially when
video data is limited. Our qualitative and quantitative experiments show that
our approach sets the new state-of-the-art for video virtual try-on. For
additional results, visit our project page:
https://johannakarras.github.io/Fashion-VDM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to SIGGRAPH Asia 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SPEAK: Speech-Driven Pose and Emotion-Adjustable Talking Head Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.07257v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.07257v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changpeng Cai, Guinan Guo, Jiao Li, Junhao Su, Fei Shen, Chenghao He, Jing Xiao, Yuanxu Chen, Lei Dai, Feiyu Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most earlier researches on talking face generation have focused on the
synchronization of lip motion and speech content. However, head pose and facial
emotions are equally important characteristics of natural faces. While
audio-driven talking face generation has seen notable advancements, existing
methods either overlook facial emotions or are limited to specific individuals
and cannot be applied to arbitrary subjects. In this paper, we propose a novel
one-shot Talking Head Generation framework (SPEAK) that distinguishes itself
from the general Talking Face Generation by enabling emotional and postural
control. Specifically, we introduce Inter-Reconstructed Feature Disentanglement
(IRFD) module to decouple facial features into three latent spaces. Then we
design a face editing module that modifies speech content and facial latent
codes into a single latent space. Subsequently, we present a novel generator
that employs modified latent codes derived from the editing module to regulate
emotional expression, head poses, and speech content in synthesizing facial
animations. Extensive trials demonstrate that our method ensures lip
synchronization with the audio while enabling decoupled control of facial
features, it can generate realistic talking head with coordinated lip motions,
authentic facial emotions, and smooth head movements. The demo video is
available: https://anonymous.4open.science/r/SPEAK-8A22
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GSCo: Towards Generalizable AI in Medicine via Generalist-Specialist
  Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.15127v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.15127v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunan He, Yuxiang Nie, Hongmei Wang, Shu Yang, Yihui Wang, Zhiyuan Cai, Zhixuan Chen, Yingxue Xu, Luyang Luo, Huiling Xiang, Xi Lin, Mingxiang Wu, Yifan Peng, George Shih, Ziyang Xu, Xian Wu, Qiong Wang, Ronald Cheong Kin Chan, Varut Vardhanabhuti, Winnie Chiu Wing Chu, Yefeng Zheng, Pranav Rajpurkar, Kang Zhang, Hao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generalist foundation models (GFMs) are renowned for their exceptional
capability and flexibility in effectively generalizing across diverse tasks and
modalities. In the field of medicine, while GFMs exhibit superior
generalizability based on their extensive intrinsic knowledge as well as
proficiency in instruction following and in-context learning, specialist models
excel in precision due to their domain knowledge. In this work, for the first
time, we explore the synergy between the GFM and specialist models, to enable
precise medical image analysis on a broader scope. Specifically, we propose a
cooperative framework, Generalist-Specialist Collaboration (GSCo), which
consists of two stages, namely the construction of GFM and specialists, and
collaborative inference on downstream tasks. In the construction stage, we
develop MedDr, the largest open-source GFM tailored for medicine, showcasing
exceptional instruction-following and in-context learning capabilities.
Meanwhile, a series of lightweight specialists are crafted for downstream tasks
with low computational cost. In the collaborative inference stage, we introduce
two cooperative mechanisms, Mixture-of-Expert Diagnosis and Retrieval-Augmented
Diagnosis, to harvest the generalist's in-context learning abilities alongside
the specialists' domain expertise. For a comprehensive evaluation, we curate a
large-scale benchmark featuring 28 datasets and about 250,000 images. Extensive
results demonstrate that MedDr consistently outperforms state-of-the-art GFMs
on downstream datasets. Furthermore, GSCo exceeds both GFMs and specialists
across all out-of-domain disease diagnosis datasets. These findings indicate a
significant paradigm shift in the application of GFMs, transitioning from
separate models for specific tasks to a collaborative approach between GFMs and
specialists, thereby advancing the frontiers of generalizable AI in medicine.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast yet Safe: Early-Exiting with Risk Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.20915v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.20915v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Metod Jazbec, Alexander Timans, Tin Hadži Veljković, Kaspar Sakmann, Dan Zhang, Christian A. Naesseth, Eric Nalisnick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling machine learning models significantly improves their performance.
However, such gains come at the cost of inference being slow and
resource-intensive. Early-exit neural networks (EENNs) offer a promising
solution: they accelerate inference by allowing intermediate layers to exit and
produce a prediction early. Yet a fundamental issue with EENNs is how to
determine when to exit without severely degrading performance. In other words,
when is it 'safe' for an EENN to go 'fast'? To address this issue, we
investigate how to adapt frameworks of risk control to EENNs. Risk control
offers a distribution-free, post-hoc solution that tunes the EENN's exiting
mechanism so that exits only occur when the output is of sufficient quality. We
empirically validate our insights on a range of vision and language tasks,
demonstrating that risk control can produce substantial computational savings,
all the while preserving user-specified performance goals.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 13 figures, 4 tables (incl. appendix)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Manipulation Facing Threats: Evaluating Physical Vulnerabilities in
  End-to-End <span class="highlight-title">Vision Language</span> Action Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13174v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13174v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Cheng, Erjia Xiao, Chengyuan Yu, Zhao Yao, Jiahang Cao, Qiang Zhang, Jiaxu Wang, Mengshu Sun, Kaidi Xu, Jindong Gu, Renjing Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, driven by advancements in Multimodal Large Language Models (MLLMs),
Vision Language Action Models (VLAMs) are being proposed to achieve better
performance in open-vocabulary scenarios for robotic manipulation tasks. Since
manipulation tasks involve direct interaction with the physical world, ensuring
robustness and safety during the execution of this task is always a very
critical issue. In this paper, by synthesizing current safety research on MLLMs
and the specific application scenarios of the manipulation task in the physical
world, we comprehensively evaluate VLAMs in the face of potential physical
threats. Specifically, we propose the Physical Vulnerability Evaluating
Pipeline (PVEP) that can incorporate as many visual modal physical threats as
possible for evaluating the physical robustness of VLAMs. The physical threats
in PVEP specifically include Out-of-Distribution, Typography-based Visual
Prompts, and Adversarial Patch Attacks. By comparing the performance
fluctuations of VLAMs before and after being attacked, we provide generalizable
Analyses of how VLAMs respond to different physical security threats. Our
project page is in this link:
https://chaducheng.github.io/Manipulat-Facing-Threats/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UniRGB-IR: A Unified Framework for RGB-Infrared Semantic Tasks via
  Adapter <span class="highlight-title">Tuning</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.17360v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.17360v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maoxun Yuan, Bo Cui, Tianyi Zhao, Jiayi Wang, Shan Fu, Xingxing Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic analysis on visible (RGB) and infrared (IR) images has gained
attention for its ability to be more accurate and robust under low-illumination
and complex weather conditions. Due to the lack of pre-trained foundation
models on the large-scale infrared image datasets, existing methods prefer to
design task-specific frameworks and directly fine-tune them with pre-trained
foundation models on their RGB-IR semantic relevance datasets, which results in
poor scalability and limited generalization. In this work, we propose a general
and efficient framework called UniRGB-IR to unify RGB-IR semantic tasks, in
which a novel adapter is developed to efficiently introduce richer RGB-IR
features into the pre-trained RGB-based foundation model. Specifically, our
framework consists of a RGB-based foundation model, a Multi-modal Feature Pool
(MFP) module and a Supplementary Feature Injector (SFI) module. The MFP and SFI
modules cooperate with each other as an adapter to effectively complement the
RGB-based features with the rich RGB-IR features. During training process, we
freeze the entire foundation model to inherit prior knowledge and only optimize
the proposed adapter. Furthermore, to verify the effectiveness of our
framework, we utilize the vanilla vision transformer (ViT-Base) as the
pre-trained foundation model to perform extensive experiments. Experimental
results on various RGB-IR downstream tasks demonstrate that our method can
achieve state-of-the-art performance. The source code and results are available
at https://github.com/PoTsui99/UniRGB-IR.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Modular Quantization-Aware Training for 6D Object Pose Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06753v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06753v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saqib Javed, Chengkun Li, Andrew Price, Yinlin Hu, Mathieu Salzmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Edge applications, such as collaborative robotics and spacecraft rendezvous,
demand efficient 6D object pose estimation on resource-constrained embedded
platforms. Existing 6D pose estimation networks are often too large for such
deployments, necessitating compression while maintaining reliable performance.
To address this challenge, we introduce Modular Quantization-Aware Training
(MQAT), an adaptive and mixed-precision quantization-aware training strategy
that exploits the modular structure of modern 6D pose estimation architectures.
MQAT guides a systematic gradated modular quantization sequence and determines
module-specific bit precisions, leading to quantized models that outperform
those produced by state-of-the-art uniform and mixed-precision quantization
techniques. Our experiments showcase the generality of MQAT across datasets,
architectures, and quantization algorithms. Remarkably, MQAT-trained quantized
models achieve a significant accuracy boost (>7%) over the baseline
full-precision network while reducing model size by a factor of 4x or more. Our
project website is at: https://saqibjaved1.github.io/MQAT_/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Transactions on Machine Learning Research (TMLR), 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PointNCBW: Towards Dataset Ownership Verification for Point Clouds via
  Negative Clean-label Backdoor Watermark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.05500v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.05500v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Wei, Yang Wang, Kuofeng Gao, Shuo Shao, Yiming Li, Zhibo Wang, Zhan Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, point clouds have been widely used in computer vision, whereas
their collection is time-consuming and expensive. As such, point cloud datasets
are the valuable intellectual property of their owners and deserve protection.
To detect and prevent unauthorized use of these datasets, especially for
commercial or open-sourced ones that cannot be sold again or used commercially
without permission, we intend to identify whether a suspicious third-party
model is trained on our protected dataset under the black-box setting. We
achieve this goal by designing a scalable clean-label backdoor-based dataset
watermark for point clouds that ensures both effectiveness and stealthiness.
Unlike existing clean-label watermark schemes, which are susceptible to the
number of categories, our method could watermark samples from all classes
instead of only from the target one. Accordingly, it can still preserve high
effectiveness even on large-scale datasets with many classes. Specifically, we
perturb selected point clouds with non-target categories in both shape-wise and
point-wise manners before inserting trigger patterns without changing their
labels. The features of perturbed samples are similar to those of benign
samples from the target class. As such, models trained on the watermarked
dataset will have a distinctive yet stealthy backdoor behavior, i.e.,
misclassifying samples from the target class whenever triggers appear, since
the trained DNNs will treat the inserted trigger pattern as a signal to deny
predicting the target label. We also design a hypothesis-test-guided dataset
ownership verification based on the proposed watermark. Extensive experiments
on benchmark datasets are conducted, verifying the effectiveness of our method
and its resistance to potential removal methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper was accepted by IEEE Transactions on Information Forensics
  and Security (TIFS), 2024. 16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FilterViT and DropoutViT: Lightweight Vision Transformer Models for
  Efficient Attention Mechanisms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22709v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22709v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bohang Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we introduce FilterViT, an enhanced version of MobileViT,
which leverages an attention-based mechanism for early-stage downsampling.
Traditional QKV operations on high-resolution feature maps are computationally
intensive due to the abundance of tokens. To address this, we propose a filter
attention mechanism using a convolutional neural network (CNN) to generate an
importance mask, focusing attention on key image regions. The method
significantly reduces computational complexity while maintaining
interpretability, as it highlights essential image areas. Experimental results
show that FilterViT achieves substantial gains in both efficiency and accuracy
compared to other models. We also introduce DropoutViT, a variant that uses a
stochastic approach for pixel selection, further enhancing robustness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Advanced Vision Transformers and Open-Set Learning for Robust Mosquito
  Classification: A Novel Approach to Entomological Studies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.06457v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.06457v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Akib Jawad Karim, Muhammad Zawad Mahmud, Riasat Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mosquito-related diseases pose a significant threat to global public health,
necessitating efficient and accurate mosquito classification for effective
surveillance and control. This work presents an innovative approach to mosquito
classification by leveraging state-of-the-art vision transformers and open-set
learning techniques. A novel framework has been introduced that integrates
Transformer-based deep learning models with comprehensive data augmentation and
preprocessing methods, enabling robust and precise identification of ten
mosquito species. The Swin Transformer model achieves the best performance for
traditional closed-set learning with 99.80% accuracy and 0.998 F1 score. The
lightweight MobileViT technique attains an almost similar accuracy of 98.90%
with significantly reduced parameters and model complexities. Next, the applied
deep learning models' adaptability and generalizability in a static environment
have been enhanced by using new classes of data samples during the inference
stage that have not been included in the training set. The proposed framework's
ability to handle unseen classes like insects similar to mosquitoes, even
humans, through open-set learning further enhances its practical applicability
by employing the OpenMax technique and Weibull distribution. The traditional
CNN model, Xception, outperforms the latest transformer with higher accuracy
and F1 score for open-set learning. The study's findings highlight the
transformative potential of advanced deep-learning architectures in entomology,
providing a strong groundwork for future research and development in mosquito
surveillance and vector control. The implications of this work extend beyond
mosquito classification, offering valuable insights for broader ecological and
environmental monitoring applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Model Pairing Using Embedding Translation for Backdoor Attack Detection
  on Open-Set Classification Tasks <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.18718v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.18718v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Unnervik, Hatef Otroshi Shahreza, Anjith George, Sébastien Marcel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Backdoor attacks allow an attacker to embed a specific vulnerability in a
machine learning algorithm, activated when an attacker-chosen pattern is
presented, causing a specific misprediction. The need to identify backdoors in
biometric scenarios has led us to propose a novel technique with different
trade-offs. In this paper we propose to use model pairs on open-set
classification tasks for detecting backdoors. Using a simple linear operation
to project embeddings from a probe model's embedding space to a reference
model's embedding space, we can compare both embeddings and compute a
similarity score. We show that this score, can be an indicator for the presence
of a backdoor despite models being of different architectures, having been
trained independently and on different datasets. This technique allows for the
detection of backdoors on models designed for open-set classification tasks,
which is little studied in the literature. Additionally, we show that backdoors
can be detected even when both models are backdoored. The source code is made
available for reproducibility purposes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in NeurIPS 2024 Safe Generative AI Workshop (oral
  presentation)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Framer: Interactive Frame Interpolation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18978v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18978v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wen Wang, Qiuyu Wang, Kecheng Zheng, Hao Ouyang, Zhekai Chen, Biao Gong, Hao Chen, Yujun Shen, Chunhua Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Framer for interactive frame interpolation, which targets
producing smoothly transitioning frames between two images as per user
creativity. Concretely, besides taking the start and end frames as inputs, our
approach supports customizing the transition process by tailoring the
trajectory of some selected keypoints. Such a design enjoys two clear benefits.
First, incorporating human interaction mitigates the issue arising from
numerous possibilities of transforming one image to another, and in turn
enables finer control of local motions. Second, as the most basic form of
interaction, keypoints help establish the correspondence across frames,
enhancing the model to handle challenging cases (e.g., objects on the start and
end frames are of different shapes and styles). It is noteworthy that our
system also offers an "autopilot" mode, where we introduce a module to estimate
the keypoints and refine the trajectory automatically, to simplify the usage in
practice. Extensive experimental results demonstrate the appealing performance
of Framer on various applications, such as image morphing, time-lapse video
generation, cartoon interpolation, etc. The code, the model, and the interface
will be released to facilitate further research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://aim-uofa.github.io/Framer/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SegEarth-OV: Towards Training-Free Open-Vocabulary Segmentation for
  Remote Sensing Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01768v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01768v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaiyu Li, Ruixun Liu, Xiangyong Cao, Xueru Bai, Feng Zhou, Deyu Meng, Zhi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Remote sensing image plays an irreplaceable role in fields such as
agriculture, water resources, military, and disaster relief. Pixel-level
interpretation is a critical aspect of remote sensing image applications;
however, a prevalent limitation remains the need for extensive manual
annotation. For this, we try to introduce open-vocabulary semantic segmentation
(OVSS) into the remote sensing context. However, due to the sensitivity of
remote sensing images to low-resolution features, distorted target shapes and
ill-fitting boundaries are exhibited in the prediction mask. To tackle this
issue, we propose a simple and general upsampler, SimFeatUp, to restore lost
spatial information in deep features in a training-free style. Further, based
on the observation of the abnormal response of local patch tokens to [CLS]
token in CLIP, we propose to execute a straightforward subtraction operation to
alleviate the global bias in patch tokens. Extensive experiments are conducted
on 17 remote sensing datasets spanning semantic segmentation, building
extraction, road detection, and flood detection tasks. Our method achieves an
average of 5.8%, 8.2%, 4.0%, and 15.3% improvement over state-of-the-art
methods on 4 tasks. All codes are released.
\url{https://earth-insights.github.io/SegEarth-OV}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CMMMU: A Chinese Massive Multi-discipline <span class="highlight-title">Multimodal</span> Understanding
  Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.11944v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.11944v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ge Zhang, Xinrun Du, Bei Chen, Yiming Liang, Tongxu Luo, Tianyu Zheng, Kang Zhu, Yuyang Cheng, Chunpu Xu, Shuyue Guo, Haoran Zhang, Xingwei Qu, Junjie Wang, Ruibin Yuan, Yizhi Li, Zekun Wang, Yudong Liu, Yu-Hsuan Tsai, Fengji Zhang, Chenghua Lin, Wenhao Huang, Jie Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the capabilities of large multimodal models (LMMs) continue to advance,
evaluating the performance of LMMs emerges as an increasing need. Additionally,
there is an even larger gap in evaluating the advanced knowledge and reasoning
abilities of LMMs in non-English contexts such as Chinese. We introduce CMMMU,
a new Chinese Massive Multi-discipline Multimodal Understanding benchmark
designed to evaluate LMMs on tasks demanding college-level subject knowledge
and deliberate reasoning in a Chinese context. CMMMU is inspired by and
strictly follows the annotation and analysis pattern of MMMU. CMMMU includes
12k manually collected multimodal questions from college exams, quizzes, and
textbooks, covering six core disciplines: Art & Design, Business, Science,
Health & Medicine, Humanities & Social Science, and Tech & Engineering, like
its companion, MMMU. These questions span 30 subjects and comprise 39 highly
heterogeneous image types, such as charts, diagrams, maps, tables, music
sheets, and chemical structures. CMMMU focuses on complex perception and
reasoning with domain-specific knowledge in the Chinese context. We evaluate 11
open-source LLMs and one proprietary GPT-4V(ision). Even GPT-4V only achieves
accuracies of 42%, indicating a large space for improvement. CMMMU will boost
the community to build the next-generation LMMs towards expert artificial
intelligence and promote the democratization of LMMs by providing diverse
language contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BiVLC: Extending <span class="highlight-title">Vision-Language</span> Compositionality Evaluation with
  Text-to-Image Retrieval <span class="chip">NeurIPS 24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09952v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09952v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Imanol Miranda, Ander Salaberria, Eneko Agirre, Gorka Azkune
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing Vision-Language Compositionality (VLC) benchmarks like SugarCrepe
are formulated as image-to-text retrieval problems, where, given an image, the
models need to select between the correct textual description and a synthetic
hard negative text. In this work, we present the Bidirectional Vision-Language
Compositionality (BiVLC) dataset. The novelty of BiVLC is to add a synthetic
hard negative image generated from the synthetic text, resulting in two
image-to-text retrieval examples (one for each image) and, more importantly,
two text-to-image retrieval examples (one for each text). Human annotators
filter out ill-formed examples ensuring the validity of the benchmark. The
experiments on BiVLC uncover a weakness of current multimodal models, as they
perform poorly in the text-to-image direction. In fact, when considering both
retrieval directions, the conclusions obtained in previous works change
significantly. In addition to the benchmark, we show that a contrastive model
trained using synthetic images and texts significantly improves over the base
model in SugarCrepe and in BiVLC for both retrieval directions. The gap to
human performance in BiVLC confirms that Vision-Language Compositionality is
still a challenging problem. BiVLC and code are available at
https://imirandam.github.io/BiVLC_project_page.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 24 Datasets and Benchmarks Track; Project page
  at: https://imirandam.github.io/BiVLC_project_page/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hyper-SD: Trajectory Segmented Consistency Model for Efficient Image
  Synthesis <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.13686v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.13686v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxi Ren, Xin Xia, Yanzuo Lu, Jiacheng Zhang, Jie Wu, Pan Xie, Xing Wang, Xuefeng Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, a series of diffusion-aware distillation algorithms have emerged to
alleviate the computational overhead associated with the multi-step inference
process of Diffusion Models (DMs). Current distillation techniques often
dichotomize into two distinct aspects: i) ODE Trajectory Preservation; and ii)
ODE Trajectory Reformulation. However, these approaches suffer from severe
performance degradation or domain shifts. To address these limitations, we
propose Hyper-SD, a novel framework that synergistically amalgamates the
advantages of ODE Trajectory Preservation and Reformulation, while maintaining
near-lossless performance during step compression. Firstly, we introduce
Trajectory Segmented Consistency Distillation to progressively perform
consistent distillation within pre-defined time-step segments, which
facilitates the preservation of the original ODE trajectory from a higher-order
perspective. Secondly, we incorporate human feedback learning to boost the
performance of the model in a low-step regime and mitigate the performance loss
incurred by the distillation process. Thirdly, we integrate score distillation
to further improve the low-step generation capability of the model and offer
the first attempt to leverage a unified LoRA to support the inference process
at all steps. Extensive experiments and user studies demonstrate that Hyper-SD
achieves SOTA performance from 1 to 8 inference steps for both SDXL and SD1.5.
For example, Hyper-SDXL surpasses SDXL-Lightning by +0.68 in CLIP Score and
+0.51 in Aes Score in the 1-step inference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024 (Camera-Ready Version). Project Page:
  https://hyper-sd.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RaLF: Flow-based Global and Metric Radar Localization in LiDAR Maps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.09875v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.09875v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhijeet Nayak, Daniele Cattaneo, Abhinav Valada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Localization is paramount for autonomous robots. While camera and LiDAR-based
approaches have been extensively investigated, they are affected by adverse
illumination and weather conditions. Therefore, radar sensors have recently
gained attention due to their intrinsic robustness to such conditions. In this
paper, we propose RaLF, a novel deep neural network-based approach for
localizing radar scans in a LiDAR map of the environment, by jointly learning
to address both place recognition and metric localization. RaLF is composed of
radar and LiDAR feature encoders, a place recognition head that generates
global descriptors, and a metric localization head that predicts the 3-DoF
transformation between the radar scan and the map. We tackle the place
recognition task by learning a shared embedding space between the two
modalities via cross-modal metric learning. Additionally, we perform metric
localization by predicting pixel-level flow vectors that align the query radar
scan with the LiDAR map. We extensively evaluate our approach on multiple
real-world driving datasets and show that RaLF achieves state-of-the-art
performance for both place recognition and metric localization. Moreover, we
demonstrate that our approach can effectively generalize to different cities
and sensor setups than the ones used during training. We make the code and
trained models publicly available at http://ralf.cs.uni-freiburg.de.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Local Concept Embeddings for Analysis of Concept Distributions in DNN
  Feature Spaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.14435v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.14435v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgii Mikriukov, Gesina Schwalbe, Korinna Bade
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Insights into the learned latent representations are imperative for verifying
deep neural networks (DNNs) in critical computer vision (CV) tasks. Therefore,
state-of-the-art supervised Concept-based eXplainable Artificial Intelligence
(C-XAI) methods associate user-defined concepts like ``car'' each with a single
vector in the DNN latent space (concept embedding vector). In the case of
concept segmentation, these linearly separate between activation map pixels
belonging to a concept and those belonging to background. Existing methods for
concept segmentation, however, fall short of capturing sub-concepts (e.g.,
``proximate car'' and ``distant car''), and concept overlap (e.g., between
``bus'' and ``truck''). In other words, they do not capture the full
distribution of concept representatives in latent space. For the first time,
this work shows that these simplifications are frequently broken and that
distribution information can be particularly useful for understanding
DNN-learned notions of sub-concepts, concept confusion, and concept outliers.
To allow exploration of learned concept distributions, we propose a novel local
concept analysis framework. Instead of optimizing a single global concept
vector on the complete dataset, it generates a local concept embedding (LoCE)
vector for each individual sample. We use the distribution formed by LoCEs to
explore the latent concept distribution by fitting Gaussian mixture models
(GMMs), hierarchical clustering, and concept-level information retrieval and
outlier detection. Despite its context sensitivity, our method's concept
segmentation performance is competitive to global baselines. Analysis results
are obtained on two datasets and five diverse vision DNN architectures,
including vision transformers (ViTs).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BodySLAM: A Generalized Monocular Visual SLAM Framework for Surgical
  Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03078v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03078v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        G. Manni, C. Lauretti, F. Prata, R. Papalia, L. Zollo, P. Soda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Endoscopic surgery relies on two-dimensional views, posing challenges for
surgeons in depth perception and instrument manipulation. While Monocular
Visual Simultaneous Localization and Mapping (MVSLAM) has emerged as a
promising solution, its implementation in endoscopic procedures faces
significant challenges due to hardware limitations, such as the use of a
monocular camera and the absence of odometry sensors. This study presents
BodySLAM, a robust deep learning-based MVSLAM approach that addresses these
challenges through three key components: CycleVO, a novel unsupervised
monocular pose estimation module; the integration of the state-of-the-art Zoe
architecture for monocular depth estimation; and a 3D reconstruction module
creating a coherent surgical map. The approach is rigorously evaluated using
three publicly available datasets (Hamlyn, EndoSLAM, and SCARED) spanning
laparoscopy, gastroscopy, and colonoscopy scenarios, and benchmarked against
four state-of-the-art methods. Results demonstrate that CycleVO exhibited
competitive performance with the lowest inference time among pose estimation
methods, while maintaining robust generalization capabilities, whereas Zoe
significantly outperformed existing algorithms for depth estimation in
endoscopy. BodySLAM's strong performance across diverse endoscopic scenarios
demonstrates its potential as a viable MVSLAM solution for endoscopic
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PitRSDNet: Predicting Intra-operative Remaining Surgery Duration in
  Endoscopic Pituitary Surgery <span class="chip">MICCAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16998v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16998v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anjana Wijekoon, Adrito Das, Roxana R. Herrera, Danyal Z. Khan, John Hanrahan, Eleanor Carter, Valpuri Luoma, Danail Stoyanov, Hani J. Marcus, Sophia Bano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate intra-operative Remaining Surgery Duration (RSD) predictions allow
for anaesthetists to more accurately decide when to administer anaesthetic
agents and drugs, as well as to notify hospital staff to send in the next
patient. Therefore RSD plays an important role in improving patient care and
minimising surgical theatre costs via efficient scheduling. In endoscopic
pituitary surgery, it is uniquely challenging due to variable workflow
sequences with a selection of optional steps contributing to high variability
in surgery duration. This paper presents PitRSDNet for predicting RSD during
pituitary surgery, a spatio-temporal neural network model that learns from
historical data focusing on workflow sequences. PitRSDNet integrates workflow
knowledge into RSD prediction in two forms: 1) multi-task learning for
concurrently predicting step and RSD; and 2) incorporating prior steps as
context in temporal learning and inference. PitRSDNet is trained and evaluated
on a new endoscopic pituitary surgery dataset with 88 videos to show
competitive performance improvements over previous statistical and machine
learning methods. The findings also highlight how PitRSDNet improve RSD
precision on outlier cases utilising the knowledge of prior steps.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the Augmented Environments for Computer-Assisted
  Interventions (AE-CAI) Workshop at the Medical Image Computing and
  Computer-Assisted Interventions (MICCAI) Conference 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DualDn: Dual-domain Denoising via Differentiable ISP <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18783v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18783v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruikang Li, Yujin Wang, Shiqi Chen, Fan Zhang, Jinwei Gu, Tianfan Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image denoising is a critical component in a camera's Image Signal Processing
(ISP) pipeline. There are two typical ways to inject a denoiser into the ISP
pipeline: applying a denoiser directly to captured raw frames (raw domain) or
to the ISP's output sRGB images (sRGB domain). However, both approaches have
their limitations. Residual noise from raw-domain denoising can be amplified by
the subsequent ISP processing, and the sRGB domain struggles to handle
spatially varying noise since it only sees noise distorted by the ISP.
Consequently, most raw or sRGB domain denoising works only for specific noise
distributions and ISP configurations. To address these challenges, we propose
DualDn, a novel learning-based dual-domain denoising. Unlike previous
single-domain denoising, DualDn consists of two denoising networks: one in the
raw domain and one in the sRGB domain. The raw domain denoising adapts to
sensor-specific noise as well as spatially varying noise levels, while the sRGB
domain denoising adapts to ISP variations and removes residual noise amplified
by the ISP. Both denoising networks are connected with a differentiable ISP,
which is trained end-to-end and discarded during the inference stage. With this
design, DualDn achieves greater generalizability compared to most
learning-based denoising methods, as it can adapt to different unseen noises,
ISP parameters, and even novel ISP pipelines. Experiments show that DualDn
achieves state-of-the-art performance and can adapt to different denoising
architectures. Moreover, DualDn can be used as a plug-and-play denoising module
with real cameras without retraining, and still demonstrate better performance
than commercial on-camera denoising. The project website is available at:
https://openimaginglab.github.io/DualDn/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ECCV 2024, Project page:
  https://openimaginglab.github.io/DualDn/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A citizen science toolkit to collect human perceptions of urban
  environments using open street view images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00174v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00174v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Danish, SM Labib, Britta Ricker, Marco Helbich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Street View Imagery (SVI) is a valuable data source for studies (e.g.,
environmental assessments, green space identification or land cover
classification). While commercial SVI is available, such providers commonly
restrict copying or reuse in ways necessary for research. Open SVI datasets are
readily available from less restrictive sources, such as Mapillary, but due to
the heterogeneity of the images, these require substantial preprocessing,
filtering, and careful quality checks. We present an efficient method for
automated downloading, processing, cropping, and filtering open SVI, to be used
in a survey of human perceptions of the streets portrayed in these images. We
demonstrate our open-source reusable SVI preparation and smartphone-friendly
perception-survey software with Amsterdam (Netherlands) as the case study.
Using a citizen science approach, we collected from 331 people 22,637 ratings
about their perceptions for various criteria. We have published our software in
a public repository for future re-use and reproducibility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MV2Cyl: Reconstructing 3D Extrusion Cylinders from Multi-View Images <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10853v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10853v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eunji Hong, Minh Hieu Nguyen, Mikaela Angelina Uy, Minhyuk Sung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present MV2Cyl, a novel method for reconstructing 3D from 2D multi-view
images, not merely as a field or raw geometry but as a sketch-extrude CAD
model. Extracting extrusion cylinders from raw 3D geometry has been extensively
researched in computer vision, while the processing of 3D data through neural
networks has remained a bottleneck. Since 3D scans are generally accompanied by
multi-view images, leveraging 2D convolutional neural networks allows these
images to be exploited as a rich source for extracting extrusion cylinder
information. However, we observe that extracting only the surface information
of the extrudes and utilizing it results in suboptimal outcomes due to the
challenges in the occlusion and surface segmentation. By synergizing with the
extracted base curve information, we achieve the optimal reconstruction result
with the best accuracy in 2D sketch and extrude parameter estimation. Our
experiments, comparing our method with previous work that takes a raw 3D point
cloud as input, demonstrate the effectiveness of our approach by taking
advantage of multi-view images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Private Attribute Inference from Images with <span class="highlight-title">Vision-Language</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.10618v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.10618v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Batuhan Tömekçe, Mark Vero, Robin Staab, Martin Vechev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) become ubiquitous in our daily tasks and
digital interactions, associated privacy risks are increasingly in focus. While
LLM privacy research has primarily focused on the leakage of model training
data, it has recently been shown that LLMs can make accurate privacy-infringing
inferences from previously unseen texts. With the rise of vision-language
models (VLMs), capable of understanding both images and text, a key question is
whether this concern transfers to the previously unexplored domain of benign
images posted online. To answer this question, we compile an image dataset with
human-annotated labels of the image owner's personal attributes. In order to
understand the privacy risks posed by VLMs beyond traditional human attribute
recognition, our dataset consists of images where the inferable private
attributes do not stem from direct depictions of humans. On this dataset, we
evaluate 7 state-of-the-art VLMs, finding that they can infer various personal
attributes at up to 77.6% accuracy. Concerningly, we observe that accuracy
scales with the general capabilities of the models, implying that future models
can be misused as stronger inferential adversaries, establishing an imperative
for the development of adequate defenses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visual Self-supervised Learning Scheme for Dense Prediction Tasks on
  X-ray Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08421v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08421v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shervin Halat, Mohammad Rahmati, Ehsan Nazerfard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, significant advancements in artificial intelligence have been
attributed to the integration of self-supervised learning (SSL) scheme. While
SSL has shown impressive achievements in natural language processing (NLP), its
progress in computer vision has comparatively lagged behind. However, the
incorporation of contrastive learning into existing visual SSL models has led
to considerable progress, often surpassing supervised counterparts.
Nonetheless, these improvements have been mostly limited to classification
tasks. Moreover, few studies have evaluated visual SSL models in real-world
scenarios, as most have focused on datasets with class-wise portrait images,
notably ImageNet. Here, we focus on dense prediction tasks using security
inspection x-ray images to evaluate our proposed model, Segment Localization
(SegLoc). Based upon the Instance Localization (InsLoc) model, SegLoc addresses
one of the key challenges of contrastive learning, i.e., false negative pairs
of query embeddings. Our pre-training dataset is synthesized by cutting,
transforming, and pasting labeled segments from an existing labeled dataset
(PIDray) as foregrounds onto instances from an unlabeled dataset (SIXray) as
backgrounds. Furthermore, we fully leverage the labeled data by incorporating
the concept, one queue per class, into the MoCo-v2 memory bank, thereby
avoiding false negative pairs. In our experiments, SegLoc outperformed random
initialization by 3% to 6% while underperformed supervised initialization, in
terms of AR and AP metrics across different IoU values over 20 to 30
pre-training epochs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PT43D: A Probabilistic Transformer for Generating 3D Shapes from Single
  Highly-Ambiguous RGB Images <span class="chip">BMVC 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.11914v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.11914v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiheng Xiong, Angela Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating 3D shapes from single RGB images is essential in various
applications such as robotics. Current approaches typically target images
containing clear and complete visual descriptions of the object, without
considering common realistic cases where observations of objects that are
largely occluded or truncated. We thus propose a transformer-based
autoregressive model to generate the probabilistic distribution of 3D shapes
conditioned on an RGB image containing potentially highly ambiguous
observations of the object. To handle realistic scenarios such as occlusion or
field-of-view truncation, we create simulated image-to-shape training pairs
that enable improved fine-tuning for real-world scenarios. We then adopt
cross-attention to effectively identify the most relevant region of interest
from the input image for shape generation. This enables inference of sampled
shapes with reasonable diversity and strong alignment with the input image. We
train and test our model on our synthetic data then fine-tune and test it on
real-world data. Experiments demonstrate that our model outperforms state of
the art in both scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures. Accepted to BMVC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 3D Equivariant Pose Regression via Direct Wigner-D Harmonics Prediction <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00543v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00543v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jongmin Lee, Minsu Cho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Determining the 3D orientations of an object in an image, known as
single-image pose estimation, is a crucial task in 3D vision applications.
Existing methods typically learn 3D rotations parametrized in the spatial
domain using Euler angles or quaternions, but these representations often
introduce discontinuities and singularities. SO(3)-equivariant networks enable
the structured capture of pose patterns with data-efficient learning, but the
parametrizations in spatial domain are incompatible with their architecture,
particularly spherical CNNs, which operate in the frequency domain to enhance
computational efficiency. To overcome these issues, we propose a
frequency-domain approach that directly predicts Wigner-D coefficients for 3D
rotation regression, aligning with the operations of spherical CNNs. Our
SO(3)-equivariant pose harmonics predictor overcomes the limitations of spatial
parameterizations, ensuring consistent pose estimation under arbitrary
rotations. Trained with a frequency-domain regression loss, our method achieves
state-of-the-art results on benchmarks such as ModelNet10-SO(3) and PASCAL3D+,
with significant improvements in accuracy, robustness, and data efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024, Project webpage at
  http://cvlab.postech.ac.kr/research/3D_EquiPose</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ QIS : Interactive Segmentation via Quasi-Conformal Mappings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14695v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14695v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Zhang, Daoping Zhang, Lok Ming Lui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image segmentation plays a crucial role in extracting important objects of
interest from images, enabling various applications. While existing methods
have shown success in segmenting clean images, they often struggle to produce
accurate segmentation results when dealing with degraded images, such as those
containing noise or occlusions. To address this challenge, interactive
segmentation has emerged as a promising approach, allowing users to provide
meaningful input to guide the segmentation process. However, an important
problem in interactive segmentation lies in determining how to incorporate
minimal yet meaningful user guidance into the segmentation model. In this
paper, we propose the quasi-conformal interactive segmentation (QIS) model,
which incorporates user input in the form of positive and negative clicks.
Users mark a few pixels belonging to the object region as positive clicks,
indicating that the segmentation model should include a region around these
clicks. Conversely, negative clicks are provided on pixels belonging to the
background, instructing the model to exclude the region near these clicks from
the segmentation mask. Additionally, the segmentation mask is obtained by
deforming a template mask with the same topology as the object of interest
using an orientation-preserving quasiconformal mapping. This approach helps to
avoid topological errors in the segmentation results. We provide a thorough
analysis of the proposed model, including theoretical support for the ability
of QIS to include or exclude regions of interest or disinterest based on the
user's indication. To evaluate the performance of QIS, we conduct experiments
on synthesized images, medical images, natural images and noisy natural images.
The results demonstrate the efficacy of our proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Target Detection of Safety Protective Gear Using the Improved YOLOv5 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.05964v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.05964v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Liu, Xue Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In high-risk railway construction, personal protective equipment monitoring
is critical but challenging due to small and frequently obstructed targets. We
propose YOLO-EA, an innovative model that enhances safety measure detection by
integrating ECA into its backbone's convolutional layers, improving discernment
of minuscule objects like hardhats. YOLO-EA further refines target recognition
under occlusion by replacing GIoU with EIoU loss. YOLO-EA's effectiveness was
empirically substantiated using a dataset derived from real-world railway
construction site surveillance footage. It outperforms YOLOv5, achieving 98.9%
precision and 94.7% recall, up 2.5% and 0.5% respectively, while maintaining
real-time performance at 70.774 fps. This highly efficient and precise YOLO-EA
holds great promise for practical application in intricate construction
scenarios, enforcing stringent safety compliance during complex railway
construction projects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ S3PT: Scene Semantics and Structure Guided Clustering to Boost
  Self-Supervised Pre-Training for Autonomous Driving <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23085v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23085v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maciej K. Wozniak, Hariprasath Govindarajan, Marvin Klingner, Camille Maurice, B Ravi Kiran, Senthil Yogamani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent self-supervised clustering-based pre-training techniques like DINO and
Cribo have shown impressive results for downstream detection and segmentation
tasks. However, real-world applications such as autonomous driving face
challenges with imbalanced object class and size distributions and complex
scene geometries. In this paper, we propose S3PT a novel scene semantics and
structure guided clustering to provide more scene-consistent objectives for
self-supervised training. Specifically, our contributions are threefold: First,
we incorporate semantic distribution consistent clustering to encourage better
representation of rare classes such as motorcycles or animals. Second, we
introduce object diversity consistent spatial clustering, to handle imbalanced
and diverse object sizes, ranging from large background areas to small objects
such as pedestrians and traffic signs. Third, we propose a depth-guided spatial
clustering to regularize learning based on geometric information of the scene,
thus further refining region separation on the feature level. Our learned
representations significantly improve performance in downstream semantic
segmentation and 3D object detection tasks on the nuScenes, nuImages, and
Cityscapes datasets and show promising domain translation properties.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for WACV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EfficientNet with Hybrid Attention Mechanisms for Enhanced Breast
  Histopathology Classification: A Comprehensive Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22392v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22392v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naren Sengodan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Breast cancer histopathology image classification is crucial for early cancer
detection, offering the potential to reduce mortality rates through timely
diagnosis. This paper introduces a novel approach integrating Hybrid
EfficientNet models with advanced attention mechanisms, including Convolutional
Block Attention Module (CBAM), Self-Attention, and Deformable Attention, to
enhance feature extraction and focus on critical image regions. We evaluate the
performance of our models across multiple magnification scales using publicly
available histopathological datasets. Our method achieves significant
improvements, with accuracy reaching 98.42% at 400X magnification, surpassing
several state-of-the-art models, including VGG and ResNet architectures. The
results are validated using metrics such as accuracy, F1-score, precision, and
recall, demonstrating the clinical potential of our model in improving
diagnostic accuracy. Furthermore, the proposed method shows increased
computational efficiency, making it suitable for integration into real-time
diagnostic workflows.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VGA: Vision GUI Assistant -- Minimizing Hallucinations through
  Image-Centric <span class="highlight-title">Fine-Tuning</span> <span class="chip">EMNLP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14056v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14056v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyang Meng, Yu Dai, Zezheng Gong, Shaoxiong Guo, Minglong Tang, Tongquan Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Large Vision-Language Models (LVLMs) have significantly
improve performance in image comprehension tasks, such as formatted charts and
rich-content images. Yet, Graphical User Interface (GUI) pose a greater
challenge due to their structured format and detailed textual information.
Existing LVLMs often overly depend on internal knowledge and neglect image
content, resulting in hallucinations and incorrect responses in GUI
comprehension. To address these issues, we introduce VGA, a fine-tuned model
designed for comprehensive GUI understanding. Our model aims to enhance the
interpretation of visual data of GUI and reduce hallucinations. We first
construct a Vision Question Answering (VQA) dataset of 63.8k high-quality
examples with our propose Referent Method, which ensures the model's responses
are highly depend on visual content within the image. We then design a
two-stage fine-tuning method called Foundation and Advanced Comprehension (FAC)
to enhance both the model's ability to extract information from image content
and alignment with human intent. Experiments show that our approach enhances
the model's ability to extract information from images and achieves
state-of-the-art results in GUI understanding tasks. Our dataset and
fine-tuning script will be released soon.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OneDiff: A Generalist Model for Image Difference Captioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.05645v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.05645v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erdong Hu, Longteng Guo, Tongtian Yue, Zijia Zhao, Shuning Xue, Jing Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In computer vision, Image Difference Captioning (IDC) is crucial for
accurately describing variations between closely related images. Traditional
IDC methods often rely on specialist models, which restrict their applicability
across varied contexts. This paper introduces the OneDiff model, a novel
generalist approach that utilizes a robust vision-language model architecture,
integrating a siamese image encoder with a Visual Delta Module. This innovative
configuration allows for the precise detection and articulation of fine-grained
differences between image pairs. OneDiff is trained through a dual-phase
strategy, encompassing Coupled Sample Training and multi-task learning across a
diverse array of data types, supported by our newly developed DiffCap Dataset.
This dataset merges real-world and synthetic data, enhancing the training
process and bolstering the model's robustness. Extensive testing on diverse IDC
benchmarks, such as Spot-the-Diff, Image-Editing-Request, and Birds-to-Words,
shows that OneDiff consistently outperforms existing state-of-the-art models in
accuracy and adaptability, achieving improvements of up to 97% CIDEr points in
average. By setting a new benchmark in IDC, OneDiff paves the way for more
versatile and effective applications in detecting and describing visual
differences. The code, models, and data will be made publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GACL: Exemplar-Free Generalized Analytic Continual Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15706v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15706v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huiping Zhuang, Yizhu Chen, Di Fang, Run He, Kai Tong, Hongxin Wei, Ziqian Zeng, Cen Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class incremental learning (CIL) trains a network on sequential tasks with
separated categories in each task but suffers from catastrophic forgetting,
where models quickly lose previously learned knowledge when acquiring new
tasks. The generalized CIL (GCIL) aims to address the CIL problem in a more
real-world scenario, where incoming data have mixed data categories and unknown
sample size distribution. Existing attempts for the GCIL either have poor
performance or invade data privacy by saving exemplars. In this paper, we
propose a new exemplar-free GCIL technique named generalized analytic continual
learning (GACL). The GACL adopts analytic learning (a gradient-free training
technique) and delivers an analytical (i.e., closed-form) solution to the GCIL
scenario. This solution is derived via decomposing the incoming data into
exposed and unexposed classes, thereby attaining a weight-invariant property, a
rare yet valuable property supporting an equivalence between incremental
learning and its joint training. Such an equivalence is crucial in GCIL
settings as data distributions among different tasks no longer pose challenges
to adopting our GACL. Theoretically, this equivalence property is validated
through matrix analysis tools. Empirically, we conduct extensive experiments
where, compared with existing GCIL methods, our GACL exhibits a consistently
leading performance across various datasets and GCIL settings. Source code is
available at https://github.com/CHEN-YIZHU/GACL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visual Anchors Are Strong Information Aggregators For <span class="highlight-title">Multimodal</span> Large
  Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17815v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17815v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haogeng Liu, Quanzeng You, Xiaotian Han, Yongfei Liu, Huaibo Huang, Ran He, Hongxia Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of Multimodal Large Language Models (MLLMs), vision-language
connector plays a crucial role to link the pre-trained vision encoders with
Large Language Models (LLMs). Despite its importance, the vision-language
connector has been relatively less explored. In this study, we aim to propose a
strong vision-language connector that enables MLLMs to achieve high accuracy
while maintain low computation cost. We first reveal the existence of the
visual anchors in Vision Transformer and propose a cost-effective search
algorithm to extract them. Building on these findings, we introduce the Anchor
Former (AcFormer), a novel vision-language connector designed to leverage the
rich prior knowledge obtained from these visual anchors during pretraining,
guiding the aggregation of information. Through extensive experimentation, we
demonstrate that the proposed method significantly reduces computational costs
by nearly two-thirds compared with baseline, while simultaneously outperforming
baseline methods. This highlights the effectiveness and efficiency of AcFormer.
Codes are available at https://github.com/liuhaogeng/Anchor-Former.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TRACE: Temporal Grounding Video LLM via Causal Event Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05643v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05643v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongxin Guo, Jingyu Liu, Mingda Li, Xiaoying Tang, Qingbin Liu, Xi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video Temporal Grounding (VTG) is a crucial capability for video
understanding models and plays a vital role in downstream tasks such as video
browsing and editing. To effectively handle various tasks simultaneously and
enable zero-shot prediction, there is a growing trend in employing video LLMs
for VTG tasks. However, current video LLM-based methods rely exclusively on
natural language generation, lacking the ability to model the clear structure
inherent in videos, which restricts their effectiveness in tackling VTG tasks.
To address this issue, this paper first formally introduces causal event
modeling framework, which represents videos as sequences of events, and predict
the current event using previous events, video inputs, and textural
instructions. Each event consists of three components: timestamps, salient
scores, and textual captions. We then propose a novel task-interleaved video
LLM called TRACE to effectively implement the causal event modeling framework
in practice. The TRACE processes visual frames, timestamps, salient scores, and
text as distinct tasks, employing various encoders and decoding heads for each.
Task tokens are arranged in an interleaved sequence according to the causal
event modeling framework's formulation. Extensive experiments on various VTG
tasks and datasets demonstrate the superior performance of TRACE compared to
state-of-the-art video LLMs. Our model and code are available at
\url{https://github.com/gyxxyg/TRACE}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeSparsify: Adversarial Attack Against Token Sparsification Mechanisms
  in Vision Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.02554v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.02554v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oryan Yehezkel, Alon Zolfi, Amit Baras, Yuval Elovici, Asaf Shabtai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision transformers have contributed greatly to advancements in the computer
vision domain, demonstrating state-of-the-art performance in diverse tasks
(e.g., image classification, object detection). However, their high
computational requirements grow quadratically with the number of tokens used.
Token sparsification mechanisms have been proposed to address this issue. These
mechanisms employ an input-dependent strategy, in which uninformative tokens
are discarded from the computation pipeline, improving the model's efficiency.
However, their dynamism and average-case assumption makes them vulnerable to a
new threat vector - carefully crafted adversarial examples capable of fooling
the sparsification mechanism, resulting in worst-case performance. In this
paper, we present DeSparsify, an attack targeting the availability of vision
transformers that use token sparsification mechanisms. The attack aims to
exhaust the operating system's resources, while maintaining its stealthiness.
Our evaluation demonstrates the attack's effectiveness on three token
sparsification mechanisms and examines the attack's transferability between
them and its effect on the GPU resources. To mitigate the impact of the attack,
we propose various countermeasures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ F-OAL: Forward-only Online Analytic Learning with Fast Training and Low
  Memory Footprint in Class Incremental Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15751v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15751v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huiping Zhuang, Yuchen Liu, Run He, Kai Tong, Ziqian Zeng, Cen Chen, Yi Wang, Lap-Pui Chau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online Class Incremental Learning (OCIL) aims to train models incrementally,
where data arrive in mini-batches, and previous data are not accessible. A
major challenge in OCIL is Catastrophic Forgetting, i.e., the loss of
previously learned knowledge. Among existing baselines, replay-based methods
show competitive results but requires extra memory for storing exemplars, while
exemplar-free (i.e., data need not be stored for replay in production) methods
are resource-friendly but often lack accuracy. In this paper, we propose an
exemplar-free approach--Forward-only Online Analytic Learning (F-OAL). Unlike
traditional methods, F-OAL does not rely on back-propagation and is
forward-only, significantly reducing memory usage and computational time.
Cooperating with a pre-trained frozen encoder with Feature Fusion, F-OAL only
needs to update a linear classifier by recursive least square. This approach
simultaneously achieves high accuracy and low resource consumption. Extensive
experiments on benchmark datasets demonstrate F-OAL's robust performance in
OCIL scenarios. Code is available at https://github.com/liuyuchen-cz/F-OAL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MemControl: Mitigating Memorization in Diffusion Models via Automated
  Parameter Selection <span class="chip">WACV'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19458v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19458v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raman Dutt, Ondrej Bohdal, Pedro Sanchez, Sotirios A. Tsaftaris, Timothy Hospedales
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models excel in generating images that closely resemble their
training data but are also susceptible to data memorization, raising privacy,
ethical, and legal concerns, particularly in sensitive domains such as medical
imaging. We hypothesize that this memorization stems from the
overparameterization of deep models and propose that regularizing model
capacity during fine-tuning can mitigate this issue. Firstly, we empirically
show that regulating the model capacity via Parameter-efficient fine-tuning
(PEFT) mitigates memorization to some extent, however, it further requires the
identification of the exact parameter subsets to be fine-tuned for high-quality
generation. To identify these subsets, we introduce a bi-level optimization
framework, MemControl, that automates parameter selection using memorization
and generation quality metrics as rewards during fine-tuning. The parameter
subsets discovered through MemControl achieve a superior tradeoff between
generation quality and memorization. For the task of medical image generation,
our approach outperforms existing state-of-the-art memorization mitigation
strategies by fine-tuning as few as 0.019% of model parameters. Moreover, we
demonstrate that the discovered parameter subsets are transferable to
non-medical domains. Our framework is scalable to large datasets, agnostic to
reward functions, and can be integrated with existing approaches for further
memorization mitigation. To the best of our knowledge, this is the first study
to empirically evaluate memorization in medical images and propose a targeted
yet universal mitigation strategy. The code is available at
https://github.com/Raman1121/Diffusion_Memorization_HPO
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at WACV'25 (Applications Track)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CVQA: Culturally-diverse Multilingual Visual Question Answering
  Benchmark <span class="chip">NeurIPS
  2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05967v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05967v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Romero, Chenyang Lyu, Haryo Akbarianto Wibowo, Teresa Lynn, Injy Hamed, Aditya Nanda Kishore, Aishik Mandal, Alina Dragonetti, Artem Abzaliev, Atnafu Lambebo Tonja, Bontu Fufa Balcha, Chenxi Whitehouse, Christian Salamea, Dan John Velasco, David Ifeoluwa Adelani, David Le Meur, Emilio Villa-Cueva, Fajri Koto, Fauzan Farooqui, Frederico Belcavello, Ganzorig Batnasan, Gisela Vallejo, Grainne Caulfield, Guido Ivetta, Haiyue Song, Henok Biadglign Ademtew, Hernán Maina, Holy Lovenia, Israel Abebe Azime, Jan Christian Blaise Cruz, Jay Gala, Jiahui Geng, Jesus-German Ortiz-Barajas, Jinheon Baek, Jocelyn Dunstan, Laura Alonso Alemany, Kumaranage Ravindu Yasas Nagasinghe, Luciana Benotti, Luis Fernando D'Haro, Marcelo Viridiano, Marcos Estecha-Garitagoitia, Maria Camila Buitrago Cabrera, Mario Rodríguez-Cantelar, Mélanie Jouitteau, Mihail Mihaylov, Mohamed Fazli Mohamed Imam, Muhammad Farid Adilazuarda, Munkhjargal Gochoo, Munkh-Erdene Otgonbold, Naome Etori, Olivier Niyomugisha, Paula Mónica Silva, Pranjal Chitale, Raj Dabre, Rendi Chevi, Ruochen Zhang, Ryandito Diandaru, Samuel Cahyawijaya, Santiago Góngora, Soyeong Jeong, Sukannya Purkayastha, Tatsuki Kuribayashi, Teresa Clifford, Thanmay Jayakumar, Tiago Timponi Torrent, Toqeer Ehsan, Vladimir Araujo, Yova Kementchedjhieva, Zara Burzo, Zheng Wei Lim, Zheng Xin Yong, Oana Ignat, Joan Nwatu, Rada Mihalcea, Thamar Solorio, Alham Fikri Aji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Question Answering (VQA) is an important task in multimodal AI, and it
is often used to test the ability of vision-language models to understand and
reason on knowledge present in both visual and textual data. However, most of
the current VQA models use datasets that are primarily focused on English and a
few major world languages, with images that are typically Western-centric.
While recent efforts have tried to increase the number of languages covered on
VQA datasets, they still lack diversity in low-resource languages. More
importantly, although these datasets often extend their linguistic range via
translation or some other approaches, they usually keep images the same,
resulting in narrow cultural representation. To address these limitations, we
construct CVQA, a new Culturally-diverse multilingual Visual Question Answering
benchmark, designed to cover a rich set of languages and cultures, where we
engage native speakers and cultural experts in the data collection process. As
a result, CVQA includes culturally-driven images and questions from across 30
countries on four continents, covering 31 languages with 13 scripts, providing
a total of 10k questions. We then benchmark several Multimodal Large Language
Models (MLLMs) on CVQA, and show that the dataset is challenging for the
current state-of-the-art models. This benchmark can serve as a probing
evaluation suite for assessing the cultural capability and bias of multimodal
models and hopefully encourage more research efforts toward increasing cultural
awareness and linguistic diversity in this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38th Conference on Neural Information Processing Systems (NeurIPS
  2024) Track on Datasets and Benchmarks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decay Pruning Method: Smooth Pruning With a Self-Rectifying Procedure 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.03879v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.03879v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minghao Yang, Linlin Gao, Pengyuan Li, Wenbo Li, Yihong Dong, Zhiying Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current structured pruning methods often result in considerable accuracy
drops due to abrupt network changes and loss of information from pruned
structures. To address these issues, we introduce the Decay Pruning Method
(DPM), a novel smooth pruning approach with a self-rectifying mechanism. DPM
consists of two key components: (i) Smooth Pruning: It converts conventional
single-step pruning into multi-step smooth pruning, gradually reducing
redundant structures to zero over N steps with ongoing optimization. (ii)
Self-Rectifying: This procedure further enhances the aforementioned process by
rectifying sub-optimal pruning based on gradient information. Our approach
demonstrates strong generalizability and can be easily integrated with various
existing pruning methods. We validate the effectiveness of DPM by integrating
it with three popular pruning methods: OTOv2, Depgraph, and Gate Decorator.
Experimental results show consistent improvements in performance compared to
the original pruning methods, along with further reductions of FLOPs in most
scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ eMoE-Tracker: Environmental MoE-based Transformer for Robust
  Event-guided Object Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20024v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20024v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yucheng Chen, Lin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The unique complementarity of frame-based and event cameras for high frame
rate object tracking has recently inspired some research attempts to develop
multi-modal fusion approaches. However, these methods directly fuse both
modalities and thus ignore the environmental attributes, e.g., motion blur,
illumination variance, occlusion, scale variation, etc. Meanwhile, insufficient
interaction between search and template features makes distinguishing target
objects and backgrounds difficult. As a result, performance degradation is
induced especially in challenging conditions. This paper proposes a novel and
effective Transformer-based event-guided tracking framework, called
eMoE-Tracker, which achieves new SOTA performance under various conditions. Our
key idea is to disentangle the environment into several learnable attributes to
dynamically learn the attribute-specific features and strengthen the target
information by improving the interaction between the target template and search
regions. To achieve the goal, we first propose an environmental Mix-of-Experts
(eMoE) module that is built upon the environmental Attributes Disentanglement
to learn attribute-specific features and environmental Attributes Assembling to
assemble the attribute-specific features by the learnable attribute scores
dynamically. The eMoE module is a subtle router that prompt-tunes the
transformer backbone more efficiently. We then introduce a contrastive relation
modeling (CRM) module to emphasize target information by leveraging a
contrastive learning strategy between the target template and search regions.
Extensive experiments on diverse event-based benchmark datasets showcase the
superior performance of our eMoE-Tracker compared to the prior arts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>RGB-event single object tracking</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Vectorized Backpropagation Algorithms for Training Feedforward
  Networks Composed of Quadratic Neurons 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02901v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02901v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mathew Mithra Noel, Venkataraman Muthiah-Nakarajan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Higher order artificial neurons whose outputs are computed by applying an
activation function to a higher order multinomial function of the inputs have
been considered in the past, but did not gain acceptance due to the extra
parameters and computational cost. However, higher order neurons have
significantly greater learning capabilities since the decision boundaries of
higher order neurons can be complex surfaces instead of just hyperplanes. The
boundary of a single quadratic neuron can be a general hyper-quadric surface
allowing it to learn many nonlinearly separable datasets. Since quadratic forms
can be represented by symmetric matrices, only $\frac{n(n+1)}{2}$ additional
parameters are needed instead of $n^2$. A quadratic Logistic regression model
is first presented. Solutions to the XOR problem with a single quadratic neuron
are considered. The complete vectorized equations for both forward and backward
propagation in feedforward networks composed of quadratic neurons are derived.
A reduced parameter quadratic neural network model with just $ n $ additional
parameters per neuron that provides a compromise between learning ability and
computational cost is presented. Comparison on benchmark classification
datasets are used to demonstrate that a final layer of quadratic neurons
enables networks to achieve higher accuracy with significantly fewer hidden
layer neurons. In particular this paper shows that any dataset composed of
$\mathcal{C}$ bounded clusters can be separated with only a single layer of
$\mathcal{C}$ quadratic neurons.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visual CoT: Advancing <span class="highlight-title">Multi-Modal</span> Language Models with a Comprehensive
  Dataset and Benchmark for Chain-of-Thought Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16999v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16999v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, Hongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-Modal Large Language Models (MLLMs) have demonstrated impressive
performance in various VQA tasks. However, they often lack interpretability and
struggle with complex visual inputs, especially when the resolution of the
input image is high or when the interested region that could provide key
information for answering the question is small. To address these challenges,
we collect and introduce the large-scale Visual CoT dataset comprising 438k
question-answer pairs, annotated with intermediate bounding boxes highlighting
key regions essential for answering the questions. Additionally, about 98k
pairs of them are annotated with detailed reasoning steps. Importantly, we
propose a multi-turn processing pipeline that dynamically focuses on visual
inputs and provides interpretable thoughts. We also introduce the related
benchmark to evaluate the MLLMs in scenarios requiring specific local region
identification. Extensive experiments demonstrate the effectiveness of our
framework and shed light on better inference strategies. The Visual CoT
dataset, benchmark, and pre-trained models are available on
https://hao-shao.com/projects/viscot.html to support further research in this
area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://hao-shao.com/projects/viscot.html</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding Generalizability of Diffusion Models Requires Rethinking
  the Hidden Gaussian Structure 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24060v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24060v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Li, Yixiang Dai, Qing Qu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we study the generalizability of diffusion models by looking
into the hidden properties of the learned score functions, which are
essentially a series of deep denoisers trained on various noise levels. We
observe that as diffusion models transition from memorization to
generalization, their corresponding nonlinear diffusion denoisers exhibit
increasing linearity. This discovery leads us to investigate the linear
counterparts of the nonlinear diffusion models, which are a series of linear
models trained to match the function mappings of the nonlinear diffusion
denoisers. Surprisingly, these linear denoisers are approximately the optimal
denoisers for a multivariate Gaussian distribution characterized by the
empirical mean and covariance of the training dataset. This finding implies
that diffusion models have the inductive bias towards capturing and utilizing
the Gaussian structure (covariance information) of the training dataset for
data generation. We empirically demonstrate that this inductive bias is a
unique property of diffusion models in the generalization regime, which becomes
increasingly evident when the model's capacity is relatively small compared to
the training dataset size. In the case that the model is highly
overparameterized, this inductive bias emerges during the initial training
phases before the model fully memorizes its training data. Our study provides
crucial insights into understanding the notable strong generalization
phenomenon recently observed in real-world diffusion models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vision-Aware Text Features in Referring Image Segmentation: From Object
  Understanding to Context Understanding <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.08590v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.08590v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hai Nguyen-Truong, E-Ro Nguyen, Tuan-Anh Vu, Minh-Triet Tran, Binh-Son Hua, Sai-Kit Yeung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Referring image segmentation is a challenging task that involves generating
pixel-wise segmentation masks based on natural language descriptions. The
complexity of this task increases with the intricacy of the sentences provided.
Existing methods have relied mostly on visual features to generate the
segmentation masks while treating text features as supporting components.
However, this under-utilization of text understanding limits the model's
capability to fully comprehend the given expressions. In this work, we propose
a novel framework that specifically emphasizes object and context comprehension
inspired by human cognitive processes through Vision-Aware Text Features.
Firstly, we introduce a CLIP Prior module to localize the main object of
interest and embed the object heatmap into the query initialization process.
Secondly, we propose a combination of two components: Contextual Multimodal
Decoder and Meaning Consistency Constraint, to further enhance the coherent and
consistent interpretation of language cues with the contextual understanding
obtained from the image. Our method achieves significant performance
improvements on three benchmark datasets RefCOCO, RefCOCO+ and G-Ref. Project
page: \url{https://vatex.hkustvgd.com/}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted in WACV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ChartGemma: Visual Instruction-<span class="highlight-title">tuning</span> for Chart Reasoning in the Wild 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.04172v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.04172v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Masry, Megh Thakkar, Aayush Bajaj, Aaryaman Kartha, Enamul Hoque, Shafiq Joty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the ubiquity of charts as a data analysis, visualization, and
decision-making tool across industries and sciences, there has been a growing
interest in developing pre-trained foundation models as well as general purpose
instruction-tuned models for chart understanding and reasoning. However,
existing methods suffer crucial drawbacks across two critical axes affecting
the performance of chart representation models: they are trained on data
generated from underlying data tables of the charts, ignoring the visual trends
and patterns in chart images, and use weakly aligned vision-language backbone
models for domain-specific training, limiting their generalizability when
encountering charts in the wild. We address these important drawbacks and
introduce ChartGemma, a novel chart understanding and reasoning model developed
over PaliGemma. Rather than relying on underlying data tables, ChartGemma is
trained on instruction-tuning data generated directly from chart images, thus
capturing both high-level trends and low-level visual information from a
diverse set of charts. Our simple approach achieves state-of-the-art results
across $5$ benchmarks spanning chart summarization, question answering, and
fact-checking, and our elaborate qualitative studies on real-world charts show
that ChartGemma generates more realistic and factually correct summaries
compared to its contemporaries. We release the code, model checkpoints,
dataset, and demos at https://github.com/vis-nlp/ChartGemma.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Survey on Adversarial Attack and Defense for Medical Image Analysis:
  Methods and Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.14133v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.14133v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junhao Dong, Junxi Chen, Xiaohua Xie, Jianhuang Lai, Hao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning techniques have achieved superior performance in computer-aided
medical image analysis, yet they are still vulnerable to imperceptible
adversarial attacks, resulting in potential misdiagnosis in clinical practice.
Oppositely, recent years have also witnessed remarkable progress in defense
against these tailored adversarial examples in deep medical diagnosis systems.
In this exposition, we present a comprehensive survey on recent advances in
adversarial attacks and defenses for medical image analysis with a systematic
taxonomy in terms of the application scenario. We also provide a unified
framework for different types of adversarial attack and defense methods in the
context of medical image analysis. For a fair comparison, we establish a new
benchmark for adversarially robust medical diagnosis models obtained by
adversarial training under various scenarios. To the best of our knowledge,
this is the first survey paper that provides a thorough evaluation of
adversarially robust medical diagnosis models. By analyzing qualitative and
quantitative results, we conclude this survey with a detailed discussion of
current challenges for adversarial attack and defense in medical image analysis
systems to shed light on future research directions. Code is available on
\href{https://github.com/tomvii/Adv_MIA}{\color{red}{GitHub}}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM Computing Surveys (CSUR) (DOI:
  https://doi.org/10.1145/3702638)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Camera-Based HRV Prediction for Remote Learning Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.04161v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.04161v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kegang Wang, Yantao Wei, Jiankai Tang, Yuntao Wang, Mingwen Tong, Jie Gao, Yujian Ma, Zhongjin Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, due to the widespread use of internet videos, remote
photoplethysmography (rPPG) has gained more and more attention in the fields of
affective computing. Restoring blood volume pulse (BVP) signals from facial
videos is a challenging task that involves a series of preprocessing, image
algorithms, and postprocessing to restore waveforms. Not only is the heart rate
metric utilized for affective computing, but the heart rate variability (HRV)
metric is even more significant. The challenge in obtaining HRV indices through
rPPG lies in the necessity for algorithms to precisely predict the BVP peak
positions. In this paper, we collected the Remote Learning Affect and
Physiology (RLAP) dataset, which includes over 32 hours of highly synchronized
video and labels from 58 subjects. This is a public dataset whose BVP labels
have been meticulously designed to better suit the training of HRV models.
Using the RLAP dataset, we trained a new model called Seq-rPPG, it is a model
based on one-dimensional convolution, and experimental results reveal that this
structure is more suitable for handling HRV tasks, which outperformed all other
baselines in HRV performance and also demonstrated significant advantages in
computational efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Pose Representation Learning for Generating and Transferring
  Non-Rigid Object Poses <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09728v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09728v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seungwoo Yoo, Juil Koo, Kyeongmin Yeo, Minhyuk Sung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel method for learning representations of poses for 3D
deformable objects, which specializes in 1) disentangling pose information from
the object's identity, 2) facilitating the learning of pose variations, and 3)
transferring pose information to other object identities. Based on these
properties, our method enables the generation of 3D deformable objects with
diversity in both identities and poses, using variations of a single object. It
does not require explicit shape parameterization such as skeletons or joints,
point-level or shape-level correspondence supervision, or variations of the
target object for pose transfer. To achieve pose disentanglement, compactness
for generative models, and transferability, we first design the pose extractor
to represent the pose as a keypoint-based hybrid representation and the pose
applier to learn an implicit deformation field. To better distill pose
information from the object's geometry, we propose the implicit pose applier to
output an intrinsic mesh property, the face Jacobian. Once the extracted pose
information is transferred to the target object, the pose applier is fine-tuned
in a self-supervised manner to better describe the target object's shapes with
pose variations. The extracted poses are also used to train a cascaded
diffusion model to enable the generation of novel poses. Our experiments with
the DeformThings4D and Human datasets demonstrate state-of-the-art performance
in pose transfer and the ability to generate diverse deformed shapes with
various objects and poses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ManiWAV: Learning Robot Manipulation from In-the-Wild Audio-Visual Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19464v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19464v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyi Liu, Cheng Chi, Eric Cousineau, Naveen Kuppuswamy, Benjamin Burchfiel, Shuran Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio signals provide rich information for the robot interaction and object
properties through contact. This information can surprisingly ease the learning
of contact-rich robot manipulation skills, especially when the visual
information alone is ambiguous or incomplete. However, the usage of audio data
in robot manipulation has been constrained to teleoperated demonstrations
collected by either attaching a microphone to the robot or object, which
significantly limits its usage in robot learning pipelines. In this work, we
introduce ManiWAV: an 'ear-in-hand' data collection device to collect
in-the-wild human demonstrations with synchronous audio and visual feedback,
and a corresponding policy interface to learn robot manipulation policy
directly from the demonstrations. We demonstrate the capabilities of our system
through four contact-rich manipulation tasks that require either passively
sensing the contact events and modes, or actively sensing the object surface
materials and states. In addition, we show that our system can generalize to
unseen in-the-wild environments by learning from diverse in-the-wild human
demonstrations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Conference on Robot Learning (CoRL) 2024; Project website:
  https://maniwav.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SyncTweedies: A General Generative Framework Based on Synchronized
  Diffusions <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14370v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14370v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaihoon Kim, Juil Koo, Kyeongmin Yeo, Minhyuk Sung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a general framework for generating diverse visual content,
including ambiguous images, panorama images, mesh textures, and Gaussian splat
textures, by synchronizing multiple diffusion processes. We present exhaustive
investigation into all possible scenarios for synchronizing multiple diffusion
processes through a canonical space and analyze their characteristics across
applications. In doing so, we reveal a previously unexplored case: averaging
the outputs of Tweedie's formula while conducting denoising in multiple
instance spaces. This case also provides the best quality with the widest
applicability to downstream tasks. We name this case SyncTweedies. In our
experiments generating visual content aforementioned, we demonstrate the
superior quality of generation by SyncTweedies compared to other
synchronization methods, optimization-based and iterative-update-based methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://synctweedies.github.io/ (NeurIPS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Convolutional Kolmogorov-Arnold Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13155v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13155v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Dylan Bodner, Antonio Santiago Tepsich, Jack Natan Spolski, Santiago Pourteau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce Convolutional Kolmogorov-Arnold Networks
(Convolutional KANs), an innovative alternative to the standard Convolutional
Neural Networks (CNNs) that have revolutionized the field of computer vision.
By integrating the learneable non-linear activation functions presented in
Kolmogorov-Arnold Networks (KANs) into convolutions, we propose a new layer.
Throughout the paper, we empirically validate the performance of Convolutional
KANs against traditional architectures across Fashion-MNIST dataset, finding
that, in some cases, this new approach maintains a similar level of accuracy
while using half the number of parameters. This experiments show that KAN
Convolutions seem to learn more per kernel, which opens up a new horizon of
possibilities in deep learning for computer vision.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ReactFace: Online Multiple Appropriate Facial Reaction Generation in
  Dyadic Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.15748v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.15748v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Luo, Siyang Song, Weicheng Xie, Micol Spitale, Zongyuan Ge, Linlin Shen, Hatice Gunes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In dyadic interaction, predicting the listener's facial reactions is
challenging as different reactions could be appropriate in response to the same
speaker's behaviour. Previous approaches predominantly treated this task as an
interpolation or fitting problem, emphasizing deterministic outcomes but
ignoring the diversity and uncertainty of human facial reactions. Furthermore,
these methods often failed to model short-range and long-range dependencies
within the interaction context, leading to issues in the synchrony and
appropriateness of the generated facial reactions. To address these
limitations, this paper reformulates the task as an extrapolation or prediction
problem, and proposes an novel framework (called ReactFace) to generate
multiple different but appropriate facial reactions from a speaker behaviour
rather than merely replicating the corresponding listener facial behaviours.
Our ReactFace generates multiple different but appropriate photo-realistic
human facial reactions by: (i) learning an appropriate facial reaction
distribution representing multiple different but appropriate facial reactions;
and (ii) synchronizing the generated facial reactions with the speaker verbal
and non-verbal behaviours at each time stamp, resulting in realistic 2D facial
reaction sequences. Experimental results demonstrate the effectiveness of our
approach in generating multiple diverse, synchronized, and appropriate facial
reactions from each speaker's behaviour. The quality of the generated facial
reactions is intimately tied to the speaker's speech and facial expressions,
achieved through our novel speaker-listener interaction modules. Our code is
made publicly available at \url{https://github.com/lingjivoo/ReactFace}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE Transactions on Visualization and Computer Graphics
  (TVCG), 18 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-11-03T00:00:00Z">2024-11-03</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">56</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Weight Decay for Robust <span class="highlight-title">Fine-Tuning</span> of Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01713v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01713v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjiao Tian, Chengyue Huang, Zsolt Kira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern optimizers such as AdamW, equipped with momentum and adaptive learning
rate, are designed to escape local minima and explore the vast parameter space.
This exploration is beneficial for finding good loss basins when training from
scratch. It is not necessarily ideal when resuming from a powerful foundation
model because it can lead to large deviations from the pre-trained
initialization and, consequently, worse robustness and generalization. At the
same time, strong regularization on all parameters can lead to under-fitting.
We hypothesize that selectively regularizing the parameter space is the key to
fitting and retraining the pre-trained knowledge. This paper proposes a new
weight decay technique, Selective Projection Decay (SPD), that selectively
imposes a strong penalty on certain layers while allowing others to change
freely. Intuitively, SPD expands and contracts the parameter search space for
layers with consistent and inconsistent loss reduction, respectively.
Experimentally, when equipped with SPD, Adam consistently provides better
in-distribution generalization and out-of-distribution robustness performance
on multiple popular vision and language benchmarks. Code available
at~\url{https://github.com/GT-RIPL/Selective-Projection-Decay.git}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Neurips 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SPES: Spectrogram Perturbation for Explainable Speech-to-Text Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01710v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01710v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dennis Fucci, Marco Gaido, Beatrice Savoldi, Matteo Negri, Mauro Cettolo, Luisa Bentivogli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spurred by the demand for interpretable models, research on eXplainable AI
for language technologies has experienced significant growth, with feature
attribution methods emerging as a cornerstone of this progress. While prior
work in NLP explored such methods for classification tasks and textual
applications, explainability intersecting generation and speech is lagging,
with existing techniques failing to account for the autoregressive nature of
state-of-the-art models and to provide fine-grained, phonetically meaningful
explanations. We address this gap by introducing Spectrogram Perturbation for
Explainable Speech-to-text Generation (SPES), a feature attribution technique
applicable to sequence generation tasks with autoregressive models. SPES
provides explanations for each predicted token based on both the input
spectrogram and the previously generated tokens. Extensive evaluation on speech
recognition and translation demonstrates that SPES generates explanations that
are faithful and plausible to humans.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Investigating Large Language Models for Complex Word Identification in
  Multilingual and Multidomain Setups <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01706v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01706v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Răzvan-Alexandru Smădu, David-Gabriel Ion, Dumitru-Clementin Cercel, Florin Pop, Mihaela-Claudia Cercel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Complex Word Identification (CWI) is an essential step in the lexical
simplification task and has recently become a task on its own. Some variations
of this binary classification task have emerged, such as lexical complexity
prediction (LCP) and complexity evaluation of multi-word expressions (MWE).
Large language models (LLMs) recently became popular in the Natural Language
Processing community because of their versatility and capability to solve
unseen tasks in zero/few-shot settings. Our work investigates LLM usage,
specifically open-source models such as Llama 2, Llama 3, and Vicuna v1.5, and
closed-source, such as ChatGPT-3.5-turbo and GPT-4o, in the CWI, LCP, and MWE
settings. We evaluate zero-shot, few-shot, and fine-tuning settings and show
that LLMs struggle in certain conditions or achieve comparable results against
existing methods. In addition, we provide some views on meta-learning combined
with prompt learning. In the end, we conclude that the current state of LLMs
cannot or barely outperform existing methods, which are usually much smaller.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages, 16 figures, Accepted by EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data Extraction Attacks in Retrieval-Augmented Generation via Backdoors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01705v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01705v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuefeng Peng, Junda Wang, Hong Yu, Amir Houmansadr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant advancements, large language models (LLMs) still struggle
with providing accurate answers when lacking domain-specific or up-to-date
knowledge. Retrieval-Augmented Generation (RAG) addresses this limitation by
incorporating external knowledge bases, but it also introduces new attack
surfaces. In this paper, we investigate data extraction attacks targeting the
knowledge databases of RAG systems. We demonstrate that previous attacks on RAG
largely depend on the instruction-following capabilities of LLMs, and that
simple fine-tuning can reduce the success rate of such attacks to nearly zero.
This makes these attacks impractical since fine-tuning is a common practice
when deploying LLMs in specific domains. To further reveal the vulnerability,
we propose to backdoor RAG, where a small portion of poisoned data is injected
during the fine-tuning phase to create a backdoor within the LLM. When this
compromised LLM is integrated into a RAG system, attackers can exploit specific
triggers in prompts to manipulate the LLM to leak documents from the retrieval
database. By carefully designing the poisoned data, we achieve both verbatim
and paraphrased document extraction. We show that with only 3\% poisoned data,
our method achieves an average success rate of 79.7\% in verbatim extraction on
Llama2-7B, with a ROUGE-L score of 64.21, and a 68.6\% average success rate in
paraphrased extraction, with an average ROUGE score of 52.6 across four
datasets. These results underscore the privacy risks associated with the supply
chain when deploying RAG systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UniGuard: Towards Universal Safety Guardrails for Jailbreak Attacks on
  <span class="highlight-title">Multimodal</span> Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01703v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01703v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sejoon Oh, Yiqiao Jin, Megha Sharma, Donghyun Kim, Eric Ma, Gaurav Verma, Srijan Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) have revolutionized vision-language
understanding but are vulnerable to multimodal jailbreak attacks, where
adversaries meticulously craft inputs to elicit harmful or inappropriate
responses. We propose UniGuard, a novel multimodal safety guardrail that
jointly considers the unimodal and cross-modal harmful signals. UniGuard is
trained such that the likelihood of generating harmful responses in a toxic
corpus is minimized, and can be seamlessly applied to any input prompt during
inference with minimal computational costs. Extensive experiments demonstrate
the generalizability of UniGuard across multiple modalities and attack
strategies. It demonstrates impressive generalizability across multiple
state-of-the-art MLLMs, including LLaVA, Gemini Pro, GPT-4, MiniGPT-4, and
InstructBLIP, thereby broadening the scope of our solution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unlocking the Theory Behind Scaling 1-Bit Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01663v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01663v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Majid Daliri, Zhao Song, Chiwun Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, 1-bit Large Language Models (LLMs) have emerged, showcasing an
impressive combination of efficiency and performance that rivals traditional
LLMs. Research by Wang et al. (2023); Ma et al. (2024) indicates that the
performance of these 1-bit LLMs progressively improves as the number of
parameters increases, hinting at the potential existence of a Scaling Law for
1-bit Neural Networks. In this paper, we present the first theoretical result
that rigorously establishes this scaling law for 1-bit models. We prove that,
despite the constraint of weights restricted to $\{-1, +1\}$, the dynamics of
model training inevitably align with kernel behavior as the network width
grows. This theoretical breakthrough guarantees convergence of the 1-bit model
to an arbitrarily small loss as width increases. Furthermore, we introduce the
concept of the generalization difference, defined as the gap between the
outputs of 1-bit networks and their full-precision counterparts, and
demonstrate that this difference maintains a negligible level as network width
scales. Building on the work of Kaplan et al. (2020), we conclude by examining
how the training loss scales as a power-law function of the model size, dataset
size, and computational resources utilized for training. Our findings
underscore the promising potential of scaling 1-bit neural networks, suggesting
that int1 could become the standard in future neural network precision.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diagnosing Medical Datasets with Training Dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01653v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01653v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laura Wenderoth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study explores the potential of using training dynamics as an automated
alternative to human annotation for evaluating the quality of training data.
The framework used is Data Maps, which classifies data points into categories
such as easy-to-learn, hard-to-learn, and ambiguous (Swayamdipta et al., 2020).
Swayamdipta et al. (2020) highlight that difficult-to-learn examples often
contain errors, and ambiguous cases significantly impact model training. To
confirm the reliability of these findings, we replicated the experiments using
a challenging dataset, with a focus on medical question answering. In addition
to text comprehension, this field requires the acquisition of detailed medical
knowledge, which further complicates the task. A comprehensive evaluation was
conducted to assess the feasibility and transferability of the Data Maps
framework to the medical domain. The evaluation indicates that the framework is
unsuitable for addressing datasets' unique challenges in answering medical
questions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/LauraWenderoth/training-dynamics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EcoAct: Economic Agent Determines When to Register What Action 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01643v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01643v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaokun Zhang, Jieyu Zhang, Dujian Ding, Mirian Hipolito Garcia, Ankur Mallick, Daniel Madrigal, Menglin Xia, Victor Rühle, Qingyun Wu, Chi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements have enabled Large Language Models (LLMs) to function as
agents that can perform actions using external tools. This requires
registering, i.e., integrating tool information into the LLM context prior to
taking actions. Current methods indiscriminately incorporate all candidate
tools into the agent's context and retain them across multiple reasoning steps.
This process remains opaque to LLM agents and is not integrated into their
reasoning procedures, leading to inefficiencies due to increased context length
from irrelevant tools. To address this, we introduce EcoAct, a tool using
algorithm that allows LLMs to selectively register tools as needed, optimizing
context use. By integrating the tool registration process into the reasoning
procedure, EcoAct reduces computational costs by over 50% in multiple steps
reasoning tasks while maintaining performance, as demonstrated through
extensive experiments. Moreover, it can be plugged into any reasoning pipeline
with only minor modifications to the prompt, making it applicable to LLM agents
now and future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Microservices Architecture for Dynamic Pricing in the Travel
  Industry: Algorithms, Scalability, and Impact on Revenue and Customer
  Satisfaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01636v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01636v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Biman Barua, M. Shamim Kaiser
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research investigates the implementation of a real-time,
microservices-oriented dynamic pricing system for the travel sector. The system
is designed to address factors such as demand, competitor pricing, and other
external circumstances in real-time. Both controlled simulation and real-life
application showed a respectable gain of 22% in revenue generation and a 17%
improvement in pricing response time which concern the issues of scaling and
flexibility of classical pricing mechanisms. Demand forecasting, competitor
pricing strategies, and event-based pricing were implemented as separate
microservices to enhance their scalability and reduce resource consumption by
30% during peak loads. Customers were also more content as depicted by a 15%
increase in satisfaction score post-implementation given the appreciation of
more appropriate pricing. This research enhances the existing literature with
practical illustrations of the possible application of microservices technology
in developing dynamic pricing solutions in a complex and data-driven context.
There exist however areas for improvement for instance inter-service latency
and the need for extensive real-time data pipelines. The present research goes
on to suggest combining these with direct data capture from customer behavior
at the same time as machine learning capacity developments in pricing
algorithms to assist in more accurate real time pricing. It is determined that
the use of microservices is a reasonable and efficient model for dynamic
pricing, allowing the tourism sector to employ evidence-based and customer
centric pricing techniques, which ensures that their profits are not
jeopardized because of the need for customers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 18 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ontology Population using LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01612v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01612v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanaz Saki Norouzi, Adrita Barua, Antrea Christou, Nikita Gautam, Andrew Eells, Pascal Hitzler, Cogan Shimizu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge graphs (KGs) are increasingly utilized for data integration,
representation, and visualization. While KG population is critical, it is often
costly, especially when data must be extracted from unstructured text in
natural language, which presents challenges, such as ambiguity and complex
interpretations. Large Language Models (LLMs) offer promising capabilities for
such tasks, excelling in natural language understanding and content generation.
However, their tendency to ``hallucinate'' can produce inaccurate outputs.
Despite these limitations, LLMs offer rapid and scalable processing of natural
language data, and with prompt engineering and fine-tuning, they can
approximate human-level performance in extracting and structuring data for KGs.
This study investigates LLM effectiveness for the KG population, focusing on
the Enslaved.org Hub Ontology. In this paper, we report that compared to the
ground truth, LLM's can extract ~90% of triples, when provided a modular
ontology as guidance in the prompts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explaining and Improving Contrastive Decoding by Extrapolating the
  Probabilities of a Huge and Hypothetical LM <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01610v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01610v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haw-Shiuan Chang, Nanyun Peng, Mohit Bansal, Anil Ramakrishna, Tagyoung Chung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive decoding (CD) (Li et al., 2023) improves the next-token
distribution of a large expert language model (LM) using a small amateur LM.
Although CD is applied to various LMs and domains to enhance open-ended text
generation, it is still unclear why CD often works well, when it could fail,
and how we can make it better. To deepen our understanding of CD, we first
theoretically prove that CD could be viewed as linearly extrapolating the
next-token logits from a huge and hypothetical LM. We also highlight that the
linear extrapolation could make CD unable to output the most obvious answers
that have already been assigned high probabilities by the amateur LM.
  To overcome CD's limitation, we propose a new unsupervised decoding method
called $\mathbf{A}$symptotic $\mathbf{P}$robability $\mathbf{D}$ecoding (APD).
APD explicitly extrapolates the probability curves from the LMs of different
sizes to infer the asymptotic probabilities from an infinitely large LM without
inducing more inference costs than CD. In FactualityPrompts, an open-ended text
generation benchmark, sampling using APD significantly boosts factuality in
comparison to the CD sampling and its variants, and achieves state-of-the-art
results for Pythia 6.9B and OPT 6.7B. Furthermore, in five commonsense QA
datasets, APD is often significantly better than CD and achieves a similar
effect of using a larger LLM. For example, the perplexity of APD on top of
Pythia 6.9B is even lower than the perplexity of Pythia 12B in CommonsenseQA
and LAMBADA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Oral</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are LLMs good pragmatic speakers? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01562v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01562v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyue Jian, Siddharth Narayanaswamy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are trained on data assumed to include natural
language pragmatics, but do they actually behave like pragmatic speakers? We
attempt to answer this question using the Rational Speech Act (RSA) framework,
which models pragmatic reasoning in human communication. Using the paradigm of
a reference game constructed from the TUNA corpus, we score candidate
referential utterances in both a state-of-the-art LLM (Llama3-8B-Instruct) and
in the RSA model, comparing and contrasting these scores. Given that RSA
requires defining alternative utterances and a truth-conditional meaning
function, we explore such comparison for different choices of each of these
requirements. We find that while scores from the LLM have some positive
correlation with those from RSA, there isn't sufficient evidence to claim that
it behaves like a pragmatic speaker. This initial study paves way for further
targeted efforts exploring different models and settings, including
human-subject evaluation, to see if LLMs truly can, or be made to, behave like
pragmatic speakers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLMs and the Madness of Crowds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01539v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01539v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William F. Bradley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the patterns of incorrect answers produced by large language
models (LLMs) during evaluation. These errors exhibit highly non-intuitive
behaviors unique to each model. By analyzing these patterns, we measure the
similarities between LLMs and construct a taxonomy that categorizes them based
on their error correlations. Our findings reveal that the incorrect responses
are not randomly distributed but systematically correlated across models,
providing new insights into the underlying structures and relationships among
LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing LLM Evaluations: The Garbling Trick 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01533v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01533v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William F. Bradley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) become increasingly powerful, traditional
evaluation metrics tend to saturate, making it challenging to distinguish
between models based on their performance. We propose a general method to
transform existing LLM evaluations into a series of progressively more
difficult tasks. These enhanced evaluations emphasize reasoning capabilities
and can reveal relative performance differences that are not apparent in the
original assessments.
  To demonstrate the effectiveness of our approach, we create a new
multiple-choice test corpus, extend it into a family of evaluations, and assess
a collection of LLMs. Our results offer insights into the comparative reasoning
abilities of these models, particularly highlighting distinctions between
OpenAI's o1-preview and Google's gemini-pro-1.5-002.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DAG: Dictionary-Augmented Generation for Disambiguation of Sentences in
  Endangered Uralic Languages using ChatGPT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01531v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01531v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mika Hämäläinen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We showcase that ChatGPT can be used to disambiguate lemmas in two endangered
languages ChatGPT is not proficient in, namely Erzya and Skolt Sami. We augment
our prompt by providing dictionary translations of the candidate lemmas to a
majority language - Finnish in our case. This dictionary augmented generation
approach results in 50\% accuracy for Skolt Sami and 41\% accuracy for Erzya.
On a closer inspection, many of the error types were of the kind even an
untrained human annotator would make.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IWCLUL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SinaTools: Open Source Toolkit for Arabic Natural Language Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01523v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01523v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tymaa Hammouda, Mustafa Jarrar, Mohammed Khalilia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce SinaTools, an open-source Python package for Arabic natural
language processing and understanding. SinaTools is a unified package allowing
people to integrate it into their system workflow, offering solutions for
various tasks such as flat and nested Named Entity Recognition (NER),
fully-flagged Word Sense Disambiguation (WSD), Semantic Relatedness, Synonymy
Extractions and Evaluation, Lemmatization, Part-of-speech Tagging, Root
Tagging, and additional helper utilities such as corpus processing, text
stripping methods, and diacritic-aware word matching. This paper presents
SinaTools and its benchmarking results, demonstrating that SinaTools
outperforms all similar tools on the aforementioned tasks, such as Flat NER
(87.33%), Nested NER (89.42%), WSD (82.63%), Semantic Relatedness (0.49
Spearman rank), Lemmatization (90.5%), POS tagging (97.5%), among others.
SinaTools can be downloaded from (https://sina.birzeit.edu/sinatools).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Integration of Large <span class="highlight-title">Vision Language</span> Models for Efficient Post-disaster
  Damage Assessment and Reporting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01511v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01511v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaohui Chen, Elyas Asadi Shamsabadi, Sheng Jiang, Luming Shen, Daniel Dias-da-Costa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional natural disaster response involves significant coordinated
teamwork where speed and efficiency are key. Nonetheless, human limitations can
delay critical actions and inadvertently increase human and economic losses.
Agentic Large Vision Language Models (LVLMs) offer a new avenue to address this
challenge, with the potential for substantial socio-economic impact,
particularly by improving resilience and resource access in underdeveloped
regions. We introduce DisasTeller, the first multi-LVLM-powered framework
designed to automate tasks in post-disaster management, including on-site
assessment, emergency alerts, resource allocation, and recovery planning. By
coordinating four specialised LVLM agents with GPT-4 as the core model,
DisasTeller autonomously implements disaster response activities, reducing
human execution time and optimising resource distribution. Our evaluations
through both LVLMs and humans demonstrate DisasTeller's effectiveness in
streamlining disaster response. This framework not only supports expert teams
but also simplifies access to disaster management processes for non-experts,
bridging the gap between traditional response methods and LVLM-driven
efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sample-Efficient Alignment for LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01493v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01493v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zichen Liu, Changyu Chen, Chao Du, Wee Sun Lee, Min Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study methods for efficiently aligning large language models (LLMs) with
human preferences given budgeted online feedback. We first formulate the LLM
alignment problem in the frame of contextual dueling bandits. This formulation,
subsuming recent paradigms such as online RLHF and online DPO, inherently
quests for sample-efficient algorithms that incorporate online active
exploration. Leveraging insights from bandit theory, we introduce a unified
algorithm based on Thompson sampling and highlight its applications in two
distinct LLM alignment scenarios. The practical agent that efficiently
implements this algorithm, named SEA (Sample-Efficient Alignment), is
empirically validated through extensive experiments across three model scales
(1B, 2.8B, 6.9B) and three preference learning algorithms (DPO, IPO, SLiC). The
results demonstrate that SEA achieves highly sample-efficient alignment with
oracle's preferences, outperforming recent active exploration methods for LLMs.
Additionally, we release the implementation of SEA together with an efficient
codebase designed for online alignment of LLMs, aiming to accelerate future
research in this field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Domain-specific Guided Summarization for Mental Health Posts <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01485v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01485v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lu Qian, Yuqi Wang, Zimu Wang, Haiyang Zhang, Wei Wang, Ting Yu, Anh Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In domain-specific contexts, particularly mental health, abstractive
summarization requires advanced techniques adept at handling specialized
content to generate domain-relevant and faithful summaries. In response to
this, we introduce a guided summarizer equipped with a dual-encoder and an
adapted decoder that utilizes novel domain-specific guidance signals, i.e.,
mental health terminologies and contextually rich sentences from the source
document, to enhance its capacity to align closely with the content and context
of guidance, thereby generating a domain-relevant summary. Additionally, we
present a post-editing correction model to rectify errors in the generated
summary, thus enhancing its consistency with the original content in detail.
Evaluation on the MentSum dataset reveals that our model outperforms existing
baseline models in terms of both ROUGE and FactCC scores. Although the
experiments are specifically designed for mental health posts, the methodology
we've developed offers broad applicability, highlighting its versatility and
effectiveness in producing high-quality domain-specific summaries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at PACLIC 2024. Camera-ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Teaching Models to Improve on Tape 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01483v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01483v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liat Bezalel, Eyal Orgad, Amir Globerson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) often struggle when prompted to generate content
under specific constraints. However, in such cases it is often easy to check
whether these constraints are satisfied or violated. Recent works have shown
that LLMs can benefit from such ``corrective feedback''. Here we claim that
this skill of LLMs can be significantly enhanced via training. We introduce an
RL framework for teaching models to use such rewards, by simulating interaction
sessions, and rewarding the model according to its ability to satisfy the
constraints. We refer to our method as CORGI (Controlled Generation with RL for
Guided Interaction), and evaluate it on a variety of controlled generation
tasks using unlabeled training data. We find that CORGI consistently
outperforms the baseline reinforcement learning method that does not
incorporate conversational feedback. Furthermore, CORGI's interactive framework
enables meta-learning, allowing the LLM to generalize better to guided
interaction in new tasks. Our results clearly show that conversational
optimization, when combined with reinforcement learning, significantly improves
the effectiveness of LLMs in controlled generation contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DPCL-Diff: The Temporal Knowledge Graph Reasoning based on Graph Node
  Diffusion Model with Dual-Domain Periodic Contrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01477v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01477v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yukun Cao, Lisheng Wang, Luobing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal knowledge graph (TKG) reasoning that infers future missing facts is
an essential and challenging task. Predicting future events typically relies on
closely related historical facts, yielding more accurate results for repetitive
or periodic events. However, for future events with sparse historical
interactions, the effectiveness of this method, which focuses on leveraging
high-frequency historical information, diminishes. Recently, the capabilities
of diffusion models in image generation have opened new opportunities for TKG
reasoning. Therefore, we propose a graph node diffusion model with dual-domain
periodic contrastive learning (DPCL-Diff). Graph node diffusion model (GNDiff)
introduces noise into sparsely related events to simulate new events,
generating high-quality data that better conforms to the actual distribution.
This generative mechanism significantly enhances the model's ability to reason
about new events. Additionally, the dual-domain periodic contrastive learning
(DPCL) maps periodic and non-periodic event entities to Poincar\'e and
Euclidean spaces, leveraging their characteristics to distinguish similar
periodic events effectively. Experimental results on four public datasets
demonstrate that DPCL-Diff significantly outperforms state-of-the-art TKG
models in event prediction, demonstrating our approach's effectiveness. This
study also investigates the combined effectiveness of GNDiff and DPCL in TKG
tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MoCE: Adaptive Mixture of Contextualization Experts for Byte-based
  Neural Machine Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01474v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01474v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Langlin Huang, Mengyu Bu, Yang Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Byte-based machine translation systems have shown significant potential in
massively multilingual settings. Unicode encoding, which maps each character to
specific byte(s), eliminates the emergence of unknown words, even in new
languages, enabling broad language scalability. However, byte-level
tokenization results in sequences that are hard to interpret due to limited
semantic information per byte. Local contextualization has proven effective in
assigning initial semantics to tokens, improving sentence comprehension.
Nevertheless, variations in encoding rules across languages necessitate an
adaptive approach for effective contextualization. To this end, we propose
Adaptive MultiScale-Headed Attention (Ada-MSHA), adaptively selecting and
mixing attention heads, which are treated as contextualization experts. This
enhances the flexibility of contextualization scales and improves the potential
to discover a better strategy than previous methods. Experiment results show
that our method outperforms existing methods without extensive manual
adjustment of hyper-parameters and surpasses subword-based models with fewer
parameters in Ted-59 dataset. Our code is available at
https://github.com/ictnlp/MoCE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Classifier-guided Gradient Modulation for Enhanced <span class="highlight-title">Multimodal</span> Learning <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01409v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01409v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zirun Guo, Tao Jin, Jingyuan Chen, Zhou Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal learning has developed very fast in recent years. However, during
the multimodal training process, the model tends to rely on only one modality
based on which it could learn faster, thus leading to inadequate use of other
modalities. Existing methods to balance the training process always have some
limitations on the loss functions, optimizers and the number of modalities and
only consider modulating the magnitude of the gradients while ignoring the
directions of the gradients. To solve these problems, in this paper, we present
a novel method to balance multimodal learning with Classifier-Guided Gradient
Modulation (CGGM), considering both the magnitude and directions of the
gradients. We conduct extensive experiments on four multimodal datasets:
UPMC-Food 101, CMU-MOSI, IEMOCAP and BraTS 2021, covering classification,
regression and segmentation tasks. The results show that CGGM outperforms all
the baselines and other state-of-the-art methods consistently, demonstrating
its effectiveness and versatility. Our code is available at
https://github.com/zrguo/CGGM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Reason via Program Generation, Emulation, and Search <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.16337v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.16337v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathaniel Weir, Muhammad Khalifa, Linlu Qiu, Orion Weller, Peter Clark
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Program synthesis with language models (LMs) has unlocked a large set of
reasoning abilities; code-tuned LMs have proven adept at generating programs
that solve a wide variety of algorithmic symbolic manipulation tasks (e.g. word
concatenation). However, not all reasoning tasks are easily expressible as
code, e.g. tasks involving commonsense reasoning, moral decision-making, and
sarcasm understanding. Our goal is to extend an LM's program synthesis skills
to such tasks and evaluate the results via pseudo-programs, namely Python
programs where some leaf function calls are left undefined. To that end, we
propose, Code Generation and Emulated EXecution (CoGEX). CoGEX works by (1)
training LMs to generate pseudo-programs, (2) teaching them to emulate their
generated program's execution, including those leaf functions, allowing the
LM's knowledge to fill in the execution gaps; and (3) using them to search over
many programs to find an optimal one. To adapt the CoGEX model to a new task,
we introduce a method for performing program search to find a single program
whose pseudo-execution yields optimal performance when applied to all the
instances of a given dataset. We show that our approach yields large
improvements compared to standard in-context learning approaches on a battery
of tasks, both algorithmic and soft reasoning. This result thus demonstrates
that code synthesis can be applied to a much broader class of problems than
previously considered. Our released dataset, fine-tuned models, and
implementation can be found at \url{https://github.com/nweir127/CoGEX}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 camera ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TRAWL: Tensor Reduced and Approximated Weights for Large Language Models <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17261v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17261v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiran Luo, Het Patel, Yu Fu, Dawon Ahn, Jia Chen, Yue Dong, Evangelos E. Papalexakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research has shown that pruning large-scale language models for
inference is an effective approach to improving model efficiency, significantly
reducing model weights with minimal impact on performance. Interestingly,
pruning can sometimes even enhance accuracy by removing noise that accumulates
during training, particularly through matrix decompositions. However, recent
work has primarily focused on single matrix decompositions or lower precision
techniques, which may fail to fully capture structural patterns. To address
these limitations, we introduce TRAWL (Tensor Reduced and Approximated Weights
for Large Language Models), a technique that applies tensor decomposition
across multiple weight matrices to effectively denoise LLMs by capturing global
structural patterns. Our experiments show that TRAWL improves model performance
by up to 16% over baseline models on benchmark datasets, without requiring
additional data, training, or fine-tuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages. Submitted to NAACL 2025 and under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Language Models Replace Programmers? REPOCOD Says 'Not Yet' 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21647v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21647v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shanchao Liang, Yiran Hu, Nan Jiang, Lin Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have achieved high accuracy, i.e., more than 90%
pass@1, in solving Python coding problems in HumanEval and MBPP. Thus, a
natural question is, whether LLMs achieve comparable code completion
performance compared to human developers? Unfortunately, one cannot answer this
question using existing manual crafted or simple (e.g., single-line) code
generation benchmarks, since such tasks fail to represent real-world software
development tasks. In addition, existing benchmarks often use poor code
correctness metrics, providing misleading conclusions.
  To address these challenges, we create REPOCOD, a code generation benchmark
with 980 problems collected from 11 popular real-world projects, with more than
58% of them requiring file-level or repository-level context information. In
addition, REPOCOD has the longest average canonical solution length (331.6
tokens) and the highest average cyclomatic complexity (9.00) compared to
existing benchmarks. Each task in REPOCOD includes 313.5 developer-written test
cases on average for better correctness evaluation. In our evaluations of ten
LLMs, none of the models achieve more than 30% pass@1 on REPOCOD, indicating
the necessity of building stronger LLMs that can help developers in real-world
software development. REPOCOD is available at
https://github.com/lt-asset/REPOCOD
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CiteME: Can Language Models Accurately Cite Scientific Claims? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12861v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12861v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ori Press, Andreas Hochlehnert, Ameya Prabhu, Vishaal Udandarao, Ofir Press, Matthias Bethge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Thousands of new scientific papers are published each month. Such information
overload complicates researcher efforts to stay current with the
state-of-the-art as well as to verify and correctly attribute claims. We pose
the following research question: Given a text excerpt referencing a paper,
could an LM act as a research assistant to correctly identify the referenced
paper? We advance efforts to answer this question by building a benchmark that
evaluates the abilities of LMs in citation attribution. Our benchmark, CiteME,
consists of text excerpts from recent machine learning papers, each referencing
a single other paper. CiteME use reveals a large gap between frontier LMs and
human performance, with LMs achieving only 4.2-18.5% accuracy and humans 69.7%.
We close this gap by introducing CiteAgent, an autonomous system built on the
GPT-4o LM that can also search and read papers, which achieves an accuracy of
35.3\% on CiteME. Overall, CiteME serves as a challenging testbed for
open-ended claim attribution, driving the research community towards a future
where any claim made by an LM can be automatically verified and discarded if
found to be incorrect.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GenAI-Bench: Evaluating and Improving Compositional Text-to-Visual
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13743v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13743v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baiqi Li, Zhiqiu Lin, Deepak Pathak, Jiayao Li, Yixin Fei, Kewen Wu, Tiffany Ling, Xide Xia, Pengchuan Zhang, Graham Neubig, Deva Ramanan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While text-to-visual models now produce photo-realistic images and videos,
they struggle with compositional text prompts involving attributes,
relationships, and higher-order reasoning such as logic and comparison. In this
work, we conduct an extensive human study on GenAI-Bench to evaluate the
performance of leading image and video generation models in various aspects of
compositional text-to-visual generation. We also compare automated evaluation
metrics against our collected human ratings and find that VQAScore -- a metric
measuring the likelihood that a VQA model views an image as accurately
depicting the prompt -- significantly outperforms previous metrics such as
CLIPScore. In addition, VQAScore can improve generation in a black-box manner
(without finetuning) via simply ranking a few (3 to 9) candidate images.
Ranking by VQAScore is 2x to 3x more effective than other scoring methods like
PickScore, HPSv2, and ImageReward at improving human alignment ratings for
DALL-E 3 and Stable Diffusion, especially on compositional prompts that require
advanced visio-linguistic reasoning. We release a new GenAI-Rank benchmark with
over 40,000 human ratings to evaluate scoring metrics on ranking images
generated from the same prompt. Lastly, we discuss promising areas for
improvement in VQAScore, such as addressing fine-grained visual details. We
will release all human ratings (over 80,000) to facilitate scientific
benchmarking of both generative models and automated metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We open-source our dataset, model, and code at:
  https://linzhiqiu.github.io/papers/genai_bench ; Project page:
  https://linzhiqiu.github.io/papers/genai_bench ; GenAI-Bench was first
  introduced in arxiv:2404.01291. This article extends it with an additional
  GenAI-Rank benchmark</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Implicit Discourse Relation Classification For Nigerian Pidgin 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18776v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18776v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammed Saeed, Peter Bourgonje, Vera Demberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite attempts to make Large Language Models multi-lingual, many of the
world's languages are still severely under-resourced. This widens the
performance gap between NLP and AI applications aimed at well-financed, and
those aimed at less-resourced languages. In this paper, we focus on Nigerian
Pidgin (NP), which is spoken by nearly 100 million people, but has
comparatively very few NLP resources and corpora. We address the task of
Implicit Discourse Relation Classification (IDRC) and systematically compare an
approach translating NP data to English and then using a well-resourced IDRC
tool and back-projecting the labels versus creating a synthetic discourse
corpus for NP, in which we translate PDTB and project PDTB labels, and then
train an NP IDR classifier. The latter approach of learning a "native" NP
classifier outperforms our baseline by 13.27\% and 33.98\% in f$_{1}$ score for
4-way and 11-way classification, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ERBench: An Entity-Relationship based Automatically Verifiable
  Hallucination Benchmark for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05266v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05266v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jio Oh, Soyeon Kim, Junseok Seo, Jindong Wang, Ruochen Xu, Xing Xie, Steven Euijong Whang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have achieved unprecedented performances in
various applications, yet evaluating them is still challenging. Existing
benchmarks are either manually constructed or are automatic, but lack the
ability to evaluate the thought process of LLMs with arbitrary complexity. We
contend that utilizing existing relational databases based on the
entity-relationship (ER) model is a promising approach for constructing
benchmarks as they contain structured knowledge that can be used to question
LLMs. Unlike knowledge graphs, which are also used to evaluate LLMs, relational
databases have integrity constraints that can be used to better construct
complex in-depth questions and verify answers: (1) functional dependencies can
be used to pinpoint critical keywords that an LLM must know to properly answer
a given question containing certain attribute values; and (2) foreign key
constraints can be used to join relations and construct multi-hop questions,
which can be arbitrarily long and used to debug intermediate answers. We thus
propose ERBench, which uses these integrity constraints to convert any database
into an LLM benchmark. ERBench supports continuous evaluation as databases
change, multimodal questions, and various prompt engineering techniques. In our
experiments, we construct LLM benchmarks using databases of multiple domains
and make an extensive comparison of contemporary LLMs. We show how ERBench can
properly evaluate any LLM by not only checking for answer correctness, but also
effectively verifying the rationales by looking for the right keywords.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Why are Visually-Grounded Language Models Bad at Image Classification? <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.18415v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.18415v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhui Zhang, Alyssa Unell, Xiaohan Wang, Dhruba Ghosh, Yuchang Su, Ludwig Schmidt, Serena Yeung-Levy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image classification is one of the most fundamental capabilities of machine
vision intelligence. In this work, we revisit the image classification task
using visually-grounded language models (VLMs) such as GPT-4V and LLaVA. We
find that existing proprietary and public VLMs, despite often using CLIP as a
vision encoder and having many more parameters, significantly underperform CLIP
on standard image classification benchmarks like ImageNet. To understand the
reason, we explore several hypotheses concerning the inference algorithms,
training objectives, and data processing in VLMs. Our analysis reveals that the
primary cause is data-related: critical information for image classification is
encoded in the VLM's latent space but can only be effectively decoded with
enough training data. Specifically, there is a strong correlation between the
frequency of class exposure during VLM training and instruction-tuning and the
VLM's performance in those classes; when trained with sufficient data, VLMs can
match the accuracy of state-of-the-art classification models. Based on these
findings, we enhance a VLM by integrating classification-focused datasets into
its training, and demonstrate that the enhanced classification performance of
the VLM transfers to its general capabilities, resulting in an improvement of
11.8% on the newly collected ImageWikiQA dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Talking Heads: Understanding Inter-layer Communication in Transformer
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09519v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09519v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jack Merullo, Carsten Eickhoff, Ellie Pavlick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although it is known that transformer language models (LMs) pass features
from early layers to later layers, it is not well understood how this
information is represented and routed by the model. We analyze a mechanism used
in two LMs to selectively inhibit items in a context in one task, and find that
it underlies a commonly used abstraction across many context-retrieval
behaviors. Specifically, we find that models write into low-rank subspaces of
the residual stream to represent features which are then read out by later
layers, forming low-rank communication channels (Elhage et al., 2021) between
layers. A particular 3D subspace in model activations in GPT-2 can be traversed
to positionally index items in lists, and we show that this mechanism can
explain an otherwise arbitrary-seeming sensitivity of the model to the order of
items in the prompt. That is, the model has trouble copying the correct
information from context when many items ``crowd" this limited space. By
decomposing attention heads with the Singular Value Decomposition (SVD), we
find that previously described interactions between heads separated by one or
more layers can be predicted via analysis of their weight matrices alone. We
show that it is possible to manipulate the internal model representations as
well as edit model weights based on the mechanism we discover in order to
significantly improve performance on our synthetic Laundry List task, which
requires recall from a list, often improving task accuracy by over 20%. Our
analysis reveals a surprisingly intricate interpretable structure learned from
language model pretraining, and helps us understand why sophisticated LMs
sometimes fail in simple domains, facilitating future analysis of more complex
behaviors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Neurips 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PutnamBench: Evaluating Neural Theorem-Provers on the Putnam
  Mathematical Competition <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11214v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11214v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        George Tsoukalas, Jasper Lee, John Jennings, Jimmy Xin, Michelle Ding, Michael Jennings, Amitayush Thakur, Swarat Chaudhuri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present PutnamBench, a new multi-language benchmark for evaluating the
ability of neural theorem-provers to solve competition mathematics problems.
PutnamBench consists of 1692 hand-constructed formalizations of 640 theorems
sourced from the William Lowell Putnam Mathematical Competition, the premier
undergraduate-level mathematics competition in North America. All the problems
have formalizations in Lean 4 and Isabelle; a substantial subset also has Coq
formalizations. PutnamBench requires significant problem-solving ability and
proficiency in a broad range of topics taught in undergraduate mathematics
courses. We use PutnamBench to evaluate several established neural and symbolic
theorem-provers. These approaches can only solve a handful of the PutnamBench
problems, establishing the benchmark as a difficult open challenge for research
on neural theorem-proving. PutnamBench is available at
https://github.com/trishullab/PutnamBench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024 Datasets & Benchmarks Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CARES: A Comprehensive Benchmark of Trustworthiness in Medical Vision
  Language Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.06007v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.06007v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Xia, Ze Chen, Juanxi Tian, Yangrui Gong, Ruibo Hou, Yue Xu, Zhenbang Wu, Zhiyuan Fan, Yiyang Zhou, Kangyu Zhu, Wenhao Zheng, Zhaoyang Wang, Xiao Wang, Xuchao Zhang, Chetan Bansal, Marc Niethammer, Junzhou Huang, Hongtu Zhu, Yun Li, Jimeng Sun, Zongyuan Ge, Gang Li, James Zou, Huaxiu Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence has significantly impacted medical applications,
particularly with the advent of Medical Large Vision Language Models
(Med-LVLMs), sparking optimism for the future of automated and personalized
healthcare. However, the trustworthiness of Med-LVLMs remains unverified,
posing significant risks for future model deployment. In this paper, we
introduce CARES and aim to comprehensively evaluate the Trustworthiness of
Med-LVLMs across the medical domain. We assess the trustworthiness of Med-LVLMs
across five dimensions, including trustfulness, fairness, safety, privacy, and
robustness. CARES comprises about 41K question-answer pairs in both closed and
open-ended formats, covering 16 medical image modalities and 27 anatomical
regions. Our analysis reveals that the models consistently exhibit concerns
regarding trustworthiness, often displaying factual inaccuracies and failing to
maintain fairness across different demographic groups. Furthermore, they are
vulnerable to attacks and demonstrate a lack of privacy awareness. We publicly
release our benchmark and code in https://cares-ai.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 Datasets and Benchmarks Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ElectionSim: Massive Population Election Simulation Powered by Large
  Language Model Driven Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20746v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20746v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinnong Zhang, Jiayu Lin, Libo Sun, Weihong Qi, Yihang Yang, Yue Chen, Hanjia Lyu, Xinyi Mou, Siming Chen, Jiebo Luo, Xuanjing Huang, Shiping Tang, Zhongyu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The massive population election simulation aims to model the preferences of
specific groups in particular election scenarios. It has garnered significant
attention for its potential to forecast real-world social trends. Traditional
agent-based modeling (ABM) methods are constrained by their ability to
incorporate complex individual background information and provide interactive
prediction results. In this paper, we introduce ElectionSim, an innovative
election simulation framework based on large language models, designed to
support accurate voter simulations and customized distributions, together with
an interactive platform to dialogue with simulated voters. We present a
million-level voter pool sampled from social media platforms to support
accurate individual simulation. We also introduce PPE, a poll-based
presidential election benchmark to assess the performance of our framework
under the U.S. presidential election scenario. Through extensive experiments
and analyses, we demonstrate the effectiveness and robustness of our framework
in U.S. presidential election simulations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>42 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Are you still on track!? Catching LLM Task Drift with Activations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.00799v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.00799v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sahar Abdelnabi, Aideen Fay, Giovanni Cherubin, Ahmed Salem, Mario Fritz, Andrew Paverd
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models are commonly used in retrieval-augmented applications
to execute user instructions based on data from external sources. For example,
modern search engines use LLMs to answer queries based on relevant search
results; email plugins summarize emails by processing their content through an
LLM. However, the potentially untrusted provenance of these data sources can
lead to prompt injection attacks, where the LLM is manipulated by natural
language instructions embedded in the external data, causing it to deviate from
the user's original instruction(s). We define this deviation as task drift.
Task drift is a significant concern as it allows attackers to exfiltrate data
or influence the LLM's output for other users. We study LLM activations as a
solution to detect task drift, showing that activation deltas - the difference
in activations before and after processing external data - are strongly
correlated with this phenomenon. Through two probing methods, we demonstrate
that a simple linear classifier can detect drift with near-perfect ROC AUC on
an out-of-distribution test set. We evaluate these methods by making minimal
assumptions about how user's tasks, system prompts, and attacks can be phrased.
We observe that this approach generalizes surprisingly well to unseen task
domains, such as prompt injections, jailbreaks, and malicious instructions,
without being trained on any of these attacks. Interestingly, the fact that
this solution does not require any modifications to the LLM (e.g.,
fine-tuning), as well as its compatibility with existing meta-prompting
solutions, makes it cost-efficient and easy to deploy. To encourage further
research on activation-based task inspection, decoding, and interpretability,
we release our large-scale TaskTracker toolkit, featuring a dataset of over
500K instances, representations from six SoTA language models, and inspection
tools.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Augmenting Biomedical Named Entity Recognition with General-domain
  Resources 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10671v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10671v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Yin, Hyunjae Kim, Xiao Xiao, Chih Hsuan Wei, Jaewoo Kang, Zhiyong Lu, Hua Xu, Meng Fang, Qingyu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training a neural network-based biomedical named entity recognition (BioNER)
model usually requires extensive and costly human annotations. While several
studies have employed multi-task learning with multiple BioNER datasets to
reduce human effort, this approach does not consistently yield performance
improvements and may introduce label ambiguity in different biomedical corpora.
We aim to tackle those challenges through transfer learning from easily
accessible resources with fewer concept overlaps with biomedical datasets. In
this paper, we proposed GERBERA, a simple-yet-effective method that utilized a
general-domain NER dataset for training. Specifically, we performed multi-task
learning to train a pre-trained biomedical language model with both the target
BioNER dataset and the general-domain dataset. Subsequently, we fine-tuned the
models specifically for the BioNER dataset. We systematically evaluated GERBERA
on five datasets of eight entity types, collectively consisting of 81,410
instances. Despite using fewer biomedical resources, our models demonstrated
superior performance compared to baseline models trained with multiple
additional BioNER datasets. Specifically, our models consistently outperformed
the baselines in six out of eight entity types, achieving an average
improvement of 0.9% over the best baseline performance across eight biomedical
entity types sourced from five different corpora. Our method was especially
effective in amplifying performance on BioNER datasets characterized by limited
data, with a 4.7% improvement in F1 scores on the JNLPBA-RNA dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in JBI 2024. We make data, codes, and models publicly
  available via https://github.com/qingyu-qc/bioner_gerbera</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Real-world Instance-specific Image Goal Navigation: Bridging Domain Gaps
  via Contrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.09645v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.09645v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taichi Sakaguchi, Akira Taniguchi, Yoshinobu Hagiwara, Lotfi El Hafi, Shoichi Hasegawa, Tadahiro Taniguchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Improving instance-specific image goal navigation (InstanceImageNav), which
locates the identical object in a real-world environment from a query image, is
essential for robotic systems to assist users in finding desired objects. The
challenge lies in the domain gap between low-quality images observed by the
moving robot, characterized by motion blur and low-resolution, and high-quality
query images provided by the user. Such domain gaps could significantly reduce
the task success rate but have not been the focus of previous work. To address
this, we propose a novel method called Few-shot Cross-quality Instance-aware
Adaptation (CrossIA), which employs contrastive learning with an instance
classifier to align features between massive low- and few high-quality images.
This approach effectively reduces the domain gap by bringing the latent
representations of cross-quality images closer on an instance basis.
Additionally, the system integrates an object image collection with a
pre-trained deblurring model to enhance the observed image quality. Our method
fine-tunes the SimSiam model, pre-trained on ImageNet, using CrossIA. We
evaluated our method's effectiveness through an InstanceImageNav task with 20
different types of instances, where the robot identifies the same instance in a
real-world environment as a high-quality query image. Our experiments showed
that our method improves the task success rate by up to three times compared to
the baseline, a conventional approach based on SuperGlue. These findings
highlight the potential of leveraging contrastive learning and image
enhancement techniques to bridge the domain gap and improve object localization
in robotic applications. The project website is
https://emergentsystemlabstudent.github.io/DomainBridgingNav/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>See website at
  https://emergentsystemlabstudent.github.io/DomainBridgingNav/. Accepted to
  IEEE IRC2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Empirical Study of Validating Synthetic Data for Formula Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10657v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10657v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Usneek Singh, José Cambronero, Sumit Gulwani, Aditya Kanade, Anirudh Khatry, Vu Le, Mukul Singh, Gust Verbruggen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) can be leveraged to help with writing formulas
in spreadsheets, but resources on these formulas are scarce, impacting both the
base performance of pre-trained models and limiting the ability to fine-tune
them. Given a corpus of formulas, we can use a(nother) model to generate
synthetic natural language utterances for fine-tuning. However, it is important
to validate whether the NL generated by the LLM is indeed accurate to be
beneficial for fine-tuning. In this paper, we provide empirical results on the
impact of validating these synthetic training examples with surrogate
objectives that evaluate the accuracy of the synthetic annotations. We
demonstrate that validation improves performance over raw data across four
models (2 open and 2 closed weight). Interestingly, we show that although
validation tends to prune more challenging examples, it increases the
complexity of problems that models can solve after being fine-tuned on
validated data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spectral Editing of Activations for Large Language Model Alignment <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.09719v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.09719v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifu Qiu, Zheng Zhao, Yftah Ziser, Anna Korhonen, Edoardo M. Ponti, Shay B. Cohen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) often exhibit undesirable behaviours, such as
generating untruthful or biased content. Editing their internal representations
has been shown to be effective in mitigating such behaviours on top of the
existing alignment methods. We propose a novel inference-time editing method,
namely spectral editing of activations (SEA), to project the input
representations into directions with maximal covariance with the positive
demonstrations (e.g., truthful) while minimising covariance with the negative
demonstrations (e.g., hallucinated). We also extend our method to non-linear
editing using feature functions. We run extensive experiments on benchmarks
concerning truthfulness and bias with six open-source LLMs of different sizes
and model families. The results demonstrate the superiority of SEA in
effectiveness, generalisation to similar tasks, as well as computation and data
efficiency. We also show that SEA editing only has a limited negative impact on
other model capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CATS: Contextually-Aware Thresholding for Sparsity in Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.08763v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.08763v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Donghyun Lee, Je-Yong Lee, Genghan Zhang, Mo Tiwari, Azalia Mirhoseini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have dramatically advanced AI applications, yet
their deployment remains challenging due to their immense inference costs.
Recent studies ameliorate the computational costs of LLMs by increasing their
activation sparsity but suffer from significant performance degradation on
downstream tasks. In this work, we introduce a new framework for sparsifying
the activations of base LLMs and reducing inference costs, dubbed Contextually
Aware Thresholding for Sparsity (CATS). CATS is relatively simple, easy to
implement, and highly effective. At the heart of our framework is a new
non-linear activation function. We demonstrate that CATS can be applied to
various base models, including Mistral-7B and Llama2-7B, and outperforms
existing sparsification techniques in downstream task performance. More
precisely, CATS-based models often achieve downstream task performance within
1-2% of their base models without any fine-tuning and even at activation
sparsity levels of 50%. Furthermore, CATS-based models converge faster and
display better task performance than competing techniques when fine-tuning is
applied. Finally, we develop a custom GPU kernel for efficient implementation
of CATS that translates the activation of sparsity of CATS to real wall-clock
time speedups. Our custom kernel implementation of CATS results in a ~15%
improvement in wall-clock inference latency of token generation on both
Llama-7B and Mistral-7B.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache
  Compression <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11430v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11430v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessio Devoto, Yu Zhao, Simone Scardapane, Pasquale Minervini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The deployment of large language models (LLMs) is often hindered by the
extensive memory requirements of the Key-Value (KV) cache, especially as
context lengths increase. Existing approaches to reduce the KV cache size
involve either fine-tuning the model to learn a compression strategy or
leveraging attention scores to reduce the sequence length. We analyse the
attention distributions in decoder-only Transformers-based models and observe
that attention allocation patterns stay consistent across most layers.
Surprisingly, we find a clear correlation between the $L_2$ and the attention
scores over cached KV pairs, where a low $L_2$ of a key embedding usually leads
to a high attention score during decoding. This finding indicates that the
influence of a KV pair is potentially determined by the key embedding itself
before being queried. Based on this observation, we compress the KV cache based
on the $L_2$ of key embeddings. Our experimental results show that this simple
strategy can reduce the KV cache size by 50% on language modelling and
needle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing
accuracy. Moreover, without relying on the attention scores, this approach
remains compatible with FlashAttention, enabling broader applicability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is an extended version of a paper published in the proceedings
  of the 2024 Conference on Empirical Methods in Natural Language Processing
  (EMNLP 2024); this version was presented at the 4th NeurIPS Workshop on
  Efficient Natural Language and Speech Processing (ENLSP-IV)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unified Triplet-Level Hallucination Evaluation for Large <span class="highlight-title">Vision-Language</span>
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23114v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23114v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie Wu, Tsz Ting Chung, Kai Chen, Dit-Yan Yeung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the outstanding performance in vision-language reasoning, Large
Vision-Language Models (LVLMs) might generate hallucinated contents that do not
exist in the given image. Most existing LVLM hallucination benchmarks are
constrained to evaluate the object-related hallucinations. However, the
potential hallucination on the relations between two objects, i.e., relation
hallucination, still lacks investigation. To remedy that, in this paper we
design a unified framework to measure object and relation hallucination in
LVLMs simultaneously. The core idea of our framework is to conduct
hallucination evaluation on (object, relation, object) triplets extracted from
LVLMs' responses, and thus, could be easily generalized to different
vision-language tasks. Based on our framework, we further introduce Tri-HE, a
novel Triplet-level Hallucination Evaluation benchmark which can be used to
study both object and relation hallucination at the same time. We conduct
comprehensive evaluations on Tri-HE and observe that the relation hallucination
issue is even more serious than object hallucination among existing LVLMs,
highlighting a previously neglected problem towards reliable LVLMs. Moreover,
based on our findings, we design a simple yet effective training-free approach
to mitigate hallucinations for LVLMs, with which, we exceed all open-sourced
counterparts on Tri-HE, achieving comparable performance with the powerful
GPT-4V. Our dataset and code for the reproduction of our experiments are
available publicly at https://github.com/wujunjie1998/Tri-HE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://kaichen1998.github.io/projects/tri-he/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TOPA: Extending Large Language Models for Video Understanding via
  Text-Only Pre-Alignment <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.13911v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.13911v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Li, Hehe Fan, Yongkang Wong, Mohan Kankanhalli, Yi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in image understanding have benefited from the extensive
use of web image-text pairs. However, video understanding remains a challenge
despite the availability of substantial web video-text data. This difficulty
primarily arises from the inherent complexity of videos and the inefficient
language supervision in recent web-collected video-text datasets. In this
paper, we introduce Text-Only Pre-Alignment (TOPA), a novel approach to extend
large language models (LLMs) for video understanding, without the need for
pre-training on real video data. Specifically, we first employ an advanced LLM
to automatically generate Textual Videos comprising continuous textual frames,
along with corresponding annotations to simulate real video-text data. Then,
these annotated textual videos are used to pre-align a language-only LLM with
the video modality. To bridge the gap between textual and real videos, we
employ the CLIP model as the feature extractor to align image and text
modalities. During text-only pre-alignment, the continuous textual frames,
encoded as a sequence of CLIP text features, are analogous to continuous CLIP
image features, thus aligning the LLM with real video representation. Extensive
experiments, including zero-shot evaluation and finetuning on various video
understanding tasks, demonstrate that TOPA is an effective and efficient
framework for aligning video content with LLMs. In particular, without training
on any video data, the TOPA-Llama2-13B model achieves a Top-1 accuracy of 51.0%
on the challenging long-form video understanding benchmark, Egoschema. This
performance surpasses previous video-text pre-training approaches and proves
competitive with recent GPT-3.5-based video agents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 (Spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BERTtime Stories: Investigating the Role of Synthetic Story Data in
  Language pre-training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15365v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15365v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikitas Theodoropoulos, Giorgos Filandrianos, Vassilis Lyberatos, Maria Lymperaiou, Giorgos Stamou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We describe our contribution to the Strict and Strict-Small tracks of the 2nd
iteration of the BabyLM Challenge. The shared task is centered around efficient
pre-training given data constraints motivated by human development. In
response, we study the effect of synthetic story data in language pre-training
using TinyStories: a recently introduced dataset of short stories. Initially,
we train GPT-Neo models on subsets of TinyStories, while varying the amount of
available data. We find that, even with access to less than 100M words, the
models are able to generate high-quality, original completions to a given
story, and acquire substantial linguistic knowledge. To measure the effect of
synthetic story data, we train LTG-BERT encoder models on a combined dataset
of: a subset of TinyStories, story completions generated by GPT-Neo, and a
subset of the BabyLM dataset. Our experimentation reveals that synthetic data
can occasionally offer modest gains, but overall have a negative influence on
linguistic understanding. Our work offers an initial study on synthesizing
story data in low resource settings and underscores their potential for
augmentation in data-constrained language modeling. We publicly release our
models and implementation on our GitHub.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ READoc: A Unified Benchmark for Realistic Document Structured Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05137v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05137v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zichao Li, Aizier Abulaiti, Yaojie Lu, Xuanang Chen, Jia Zheng, Hongyu Lin, Xianpei Han, Le Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Document Structured Extraction (DSE) aims to extract structured content from
raw documents. Despite the emergence of numerous DSE systems, their unified
evaluation remains inadequate, significantly hindering the field's advancement.
This problem is largely attributed to existing benchmark paradigms, which
exhibit fragmented and localized characteristics. To address these limitations
and offer a thorough evaluation of DSE systems, we introduce a novel benchmark
named READoc, which defines DSE as a realistic task of converting unstructured
PDFs into semantically rich Markdown. The READoc dataset is derived from 2,233
diverse and real-world documents from arXiv and GitHub. In addition, we develop
a DSE Evaluation S$^3$uite comprising Standardization, Segmentation and Scoring
modules, to conduct a unified evaluation of state-of-the-art DSE approaches. By
evaluating a range of pipeline tools, expert visual models, and general VLMs,
we identify the gap between current work and the unified, realistic DSE
objective for the first time. We aspire that READoc will catalyze future
research in DSE, fostering more comprehensive and practical solutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Divide-and-Conquer Meets Consensus: Unleashing the Power of Functions in
  Code Generation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.20092v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.20092v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingchang Chen, Hongxuan Tang, Zheng Chu, Qianglong Chen, Zekun Wang, Ming Liu, Bing Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent progress made by large language models in code generation,
they still struggle with programs that meet complex requirements. Recent work
utilizes plan-and-solve decomposition to decrease the complexity and leverage
self-tests to refine the generated program. Yet, planning deep-inside
requirements in advance can be challenging, and the tests need to be accurate
to accomplish self-improvement. To this end, we propose FunCoder, a code
generation framework incorporating the divide-and-conquer strategy with
functional consensus. Specifically, FunCoder recursively branches off
sub-functions as smaller goals during code generation, represented by a tree
hierarchy. These sub-functions are then composited to attain more complex
objectives. Additionally, we designate functions via a consensus formed by
identifying similarities in program behavior, mitigating error propagation.
FunCoder outperforms state-of-the-art methods by +9.8% on average in HumanEval,
MBPP, xCodeEval and MATH with GPT-3.5 and GPT-4. Moreover, our method
demonstrates superiority on smaller models: With FunCoder, StableCode-3b
surpasses GPT-3.5 by +18.6% and achieves 97.7% of GPT-4's performance on
HumanEval. Further analysis reveals that our proposed dynamic function
decomposition is capable of handling complex requirements, and the functional
consensus prevails over self-testing in correctness evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 oral</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Little Giants: Synthesizing High-Quality Embedding Data at Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18634v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18634v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haonan Chen, Liang Wang, Nan Yang, Yutao Zhu, Ziliang Zhao, Furu Wei, Zhicheng Dou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthetic data generation has become an increasingly popular way of training
models without the need for large, manually labeled datasets. For tasks like
text embedding, synthetic data offers diverse and scalable training examples,
significantly reducing the cost of human annotation. However, most current
approaches rely heavily on proprietary models like GPT-4, which are expensive
and inefficient for generating large-scale embedding data. In this paper, we
introduce SPEED, a framework that aligns open-source small models (8B) to
efficiently generate large-scale synthetic embedding data. Through supervised
fine-tuning, preference optimization, and self-improvement, SPEED enables small
open-source models to produce high-quality data. Remarkably, SPEED uses only
less than 1/10 of the GPT API calls, outperforming the state-of-the-art
embedding model E5_mistral when both are trained solely on their synthetic
data. Using this efficient generator, we conduct a comprehensive study on how
various factors within the alignment pipeline impact data quality and reveal
the scaling law for synthetic embedding data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic and Textual Graph Generation Via Large-Scale LLM-based Agent
  Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09824v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09824v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiarui Ji, Runlin Lei, Jialing Bi, Zhewei Wei, Yankai Lin, Xuchen Pan, Yaliang Li, Bolin Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph generation is a fundamental task that has been extensively studied in
social, technological, and scientific analysis. For modeling the dynamic graph
evolution process, traditional rule-based methods struggle to capture community
structures within graphs, while deep learning methods only focus on fitting
training graphs. This limits existing graph generators to producing graphs that
adhere to predefined rules or closely resemble training datasets, achieving
poor performance in dynamic graph generation. Given that graphs are abstract
representations arising from pairwise interactions in human activities, a
realistic simulation of human-wise interaction could provide deeper insights
into the graph evolution mechanism. With the increasing recognition of large
language models (LLMs) in simulating human behavior, we introduce
GraphAgent-Generator (GAG), a novel simulation-based framework for dynamic
graph generation. Without training or fine-tuning process of LLM, our framework
effectively replicates seven macro-level structural characteristics in
established network science theories while surpassing existing baselines in
graph expansion tasks by 31\% on specific evaluation metrics. Through node
classification task, we validate GAG effectively preserves characteristics of
real-world network for node-wise textual features in generated text-rich graph.
Furthermore, by incorporating parallel acceleration, GAG supports generating
graphs with up to nearly 100,000 nodes or 10 million edges through large-scale
LLM-based agent simulation, with a minimum speed-up of 90.4\%. The source code
is available at https://anonymous.4open.science/r/GraphAgent-2206.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Tokens to Materials: Leveraging Language Models for Scientific
  Discovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16165v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16165v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuwei Wan, Tong Xie, Nan Wu, Wenjie Zhang, Chunyu Kit, Bram Hoex
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exploring the predictive capabilities of language models in material science
is an ongoing interest. This study investigates the application of language
model embeddings to enhance material property prediction in materials science.
By evaluating various contextual embedding methods and pre-trained models,
including Bidirectional Encoder Representations from Transformers (BERT) and
Generative Pre-trained Transformers (GPT), we demonstrate that domain-specific
models, particularly MatBERT significantly outperform general-purpose models in
extracting implicit knowledge from compound names and material properties. Our
findings reveal that information-dense embeddings from the third layer of
MatBERT, combined with a context-averaging approach, offer the most effective
method for capturing material-property relationships from the scientific
literature. We also identify a crucial "tokenizer effect," highlighting the
importance of specialized text processing techniques that preserve complete
compound names while maintaining consistent token counts. These insights
underscore the value of domain-specific training and tokenization in materials
science applications and offer a promising pathway for accelerating the
discovery and development of new materials through AI-driven approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Recursive Encoding for Cuneiform Signs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17283v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17283v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel M. Stelzer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the most significant problems in cuneiform pedagogy is the process of
looking up unknown signs, which often involves a tedious page-by-page search
through a sign list. This paper proposes a new "recursive encoding" for signs,
which represents the arrangement of strokes in a way a computer can process. A
series of new algorithms then offers students a new way to look up signs by any
distinctive component, as well as providing new ways to render signs and
tablets electronically.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 29 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ArEEG_Chars: Dataset for Envisioned Speech Recognition using EEG for
  Arabic Characters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15733v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15733v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hazem Darwish, Abdalrahman Al Malah, Khloud Al Jallad, Nada Ghneim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Brain-computer interfaces is an important and hot research topic that
revolutionize how people interact with the world, especially for individuals
with neurological disorders. While extensive research has been done in EEG
signals of English letters and words, a major limitation remains: the lack of
publicly available EEG datasets for many non-English languages, such as Arabic.
Although Arabic is one of the most spoken languages worldwide, to the best of
our knowledge, there is no publicly available dataset for EEG signals of Arabic
characters until now. To address this gap, we introduce ArEEG_Chars, a novel
EEG dataset for Arabic 31 characters collected from 30 participants (21 males
and 9 females), these records were collected using Epoc X 14 channels device
for 10 seconds long for each char record. The number of recorded signals were
930 EEG recordings. To make the EEG signals suitable for analyzing, each
recording has been split into multiple signals with a time duration of 250ms,
respectively. Therefore, a total of 39857 recordings of EEG signals have been
collected in this study. Moreover, ArEEG_Chars will be publicly available for
researchers. We do hope that this dataset will fill an important gap in the
research of Arabic EEG benefiting Arabic-speaking individuals with
disabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OpenMathInstruct-1: A 1.8 Million Math Instruction <span class="highlight-title">Tuning</span> Dataset <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.10176v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.10176v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shubham Toshniwal, Ivan Moshkov, Sean Narenthiran, Daria Gitman, Fei Jia, Igor Gitman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work has shown the immense potential of synthetically generated
datasets for training large language models (LLMs), especially for acquiring
targeted skills. Current large-scale math instruction tuning datasets such as
MetaMathQA (Yu et al., 2024) and MAmmoTH (Yue et al., 2024) are constructed
using outputs from closed-source LLMs with commercially restrictive licenses. A
key reason limiting the use of open-source LLMs in these data generation
pipelines has been the wide gap between the mathematical skills of the best
closed-source LLMs, such as GPT-4, and the best open-source LLMs. Building on
the recent progress in open-source LLMs, our proposed prompting novelty, and
some brute-force scaling, we construct OpenMathInstruct-1, a math instruction
tuning dataset with 1.8M problem-solution pairs. The dataset is constructed by
synthesizing code-interpreter solutions for GSM8K and MATH, two popular math
reasoning benchmarks, using the recently released and permissively licensed
Mixtral model. Our best model, OpenMath-CodeLlama-70B, trained on a subset of
OpenMathInstruct-1, achieves a score of 84.6% on GSM8K and 50.7% on MATH, which
is competitive with the best gpt-distilled models. We release our code, models,
and the OpenMathInstruct-1 dataset under a commercially permissive license.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-ready version for NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating LLMs' Mathematical and Coding Competency through
  Ontology-guided Interventions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.09395v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.09395v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengfei Hong, Navonil Majumder, Deepanway Ghosal, Somak Aditya, Rada Mihalcea, Soujanya Poria
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Language Models (LLMs) have showcased striking
results on existing logical reasoning benchmarks, with some models even
surpassing human performance. However, the true depth of their competencies and
robustness in reasoning tasks remains an open question. To this end, in this
paper, we focus on two popular reasoning tasks: arithmetic reasoning and code
generation. Particularly, we introduce (i) a general ontology of perturbations
for math and coding questions, (ii) a semi-automatic method to apply these
perturbations, and (iii) two datasets, GSMORE and HUMANEVAL-CORE, respectively,
of perturbed math and coding problems to probe LLM capabilities in numeric
reasoning and coding tasks. Through comprehensive evaluations of both
closed-source and open-source LLMs, we show a significant performance drop
across all the models against the perturbed questions, suggesting that the
current LLMs lack robust problem solving skills and structured reasoning
abilities in many areas, as defined by our ontology. We open-source the
datasets and source codes at: https://github.com/declare-lab/LLM-ReasoningTest.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>With o1 and GPT-4o results. Reformatted the data and presented more
  analysis</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Monotonic Paraphrasing Improves Generalization of Language Model
  <span class="highlight-title">Prompt</span>ing <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16038v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16038v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qin Liu, Fei Wang, Nan Xu, Tianyi Yan, Tao Meng, Muhao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Performance of large language models (LLMs) may vary with different prompts
or instructions of even the same task. One commonly recognized factor for this
phenomenon is the model's familiarity with the given prompt or instruction,
which is typically estimated by its perplexity. However, finding the prompt
with the lowest perplexity is challenging, given the enormous space of possible
prompting phrases. In this paper, we propose monotonic paraphrasing (MonoPara),
an end-to-end decoding strategy that paraphrases given prompts or instructions
into their lower perplexity counterparts based on an ensemble of a paraphrase
LM for prompt (or instruction) rewriting, and a target LM (i.e. the prompt or
instruction executor) that constrains the generation for lower perplexity. The
ensemble decoding process can efficiently paraphrase the original prompt
without altering its semantic meaning, while monotonically decreasing the
perplexity of each generation as calculated by the target LM. We explore in
detail both greedy and search-based decoding as two alternative decoding
schemes of MonoPara. Notably, MonoPara does not require any training and can
monotonically lower the perplexity of the paraphrased prompt or instruction,
leading to improved performance of zero-shot LM prompting as evaluated on a
wide selection of tasks. In addition, MonoPara is also shown to effectively
improve LMs' generalization on perturbed and unseen task instructions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Camera Ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Preference <span class="highlight-title">Tuning</span> with Human Feedback on Language, Speech, and Vision
  Tasks: A Survey 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11564v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11564v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Genta Indra Winata, Hanyang Zhao, Anirban Das, Wenpin Tang, David D. Yao, Shi-Xiong Zhang, Sambit Sahu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Preference tuning is a crucial process for aligning deep generative models
with human preferences. This survey offers a thorough overview of recent
advancements in preference tuning and the integration of human feedback. The
paper is organized into three main sections: 1) introduction and preliminaries:
an introduction to reinforcement learning frameworks, preference tuning tasks,
models, and datasets across various modalities: language, speech, and vision,
as well as different policy approaches, 2) in-depth exploration of each
preference tuning approach: a detailed analysis of the methods used in
preference tuning, and 3) applications, discussion, and future directions: an
exploration of the applications of preference tuning in downstream tasks,
including evaluation methods for different modalities, and an outlook on future
research directions. Our objective is to present the latest methodologies in
preference tuning and model alignment, enhancing the understanding of this
field for researchers and practitioners. We hope to encourage further
engagement and innovation in this area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Survey paper</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">15</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Weight Decay for Robust <span class="highlight-title">Fine-Tuning</span> of Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01713v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01713v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjiao Tian, Chengyue Huang, Zsolt Kira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern optimizers such as AdamW, equipped with momentum and adaptive learning
rate, are designed to escape local minima and explore the vast parameter space.
This exploration is beneficial for finding good loss basins when training from
scratch. It is not necessarily ideal when resuming from a powerful foundation
model because it can lead to large deviations from the pre-trained
initialization and, consequently, worse robustness and generalization. At the
same time, strong regularization on all parameters can lead to under-fitting.
We hypothesize that selectively regularizing the parameter space is the key to
fitting and retraining the pre-trained knowledge. This paper proposes a new
weight decay technique, Selective Projection Decay (SPD), that selectively
imposes a strong penalty on certain layers while allowing others to change
freely. Intuitively, SPD expands and contracts the parameter search space for
layers with consistent and inconsistent loss reduction, respectively.
Experimentally, when equipped with SPD, Adam consistently provides better
in-distribution generalization and out-of-distribution robustness performance
on multiple popular vision and language benchmarks. Code available
at~\url{https://github.com/GT-RIPL/Selective-Projection-Decay.git}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Neurips 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ROAD-Waymo: Action Awareness at Scale for Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01683v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01683v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Salman Khan, Izzeddin Teeti, Reza Javanmard Alitappeh, Mihaela C. Stoian, Eleonora Giunchiglia, Gurkirt Singh, Andrew Bradley, Fabio Cuzzolin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous Vehicle (AV) perception systems require more than simply seeing,
via e.g., object detection or scene segmentation. They need a holistic
understanding of what is happening within the scene for safe interaction with
other road users. Few datasets exist for the purpose of developing and training
algorithms to comprehend the actions of other road users. This paper presents
ROAD-Waymo, an extensive dataset for the development and benchmarking of
techniques for agent, action, location and event detection in road scenes,
provided as a layer upon the (US) Waymo Open dataset. Considerably larger and
more challenging than any existing dataset (and encompassing multiple cities),
it comes with 198k annotated video frames, 54k agent tubes, 3.9M bounding boxes
and a total of 12.4M labels. The integrity of the dataset has been confirmed
and enhanced via a novel annotation pipeline designed for automatically
identifying violations of requirements specifically designed for this dataset.
As ROAD-Waymo is compatible with the original (UK) ROAD dataset, it provides
the opportunity to tackle domain adaptation between real-world road scenarios
in different countries within a novel benchmark: ROAD++.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MamT$^4$: Multi-view Attention Networks for Mammography Cancer
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01669v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01669v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alisher Ibragimov, Sofya Senotrusova, Arsenii Litvinov, Egor Ushakov, Evgeny Karpulevich, Yury Markin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we introduce a novel method, called MamT$^4$, which is used
for simultaneous analysis of four mammography images. A decision is made based
on one image of a breast, with attention also devoted to three additional
images: another view of the same breast and two images of the other breast.
This approach enables the algorithm to closely replicate the practice of a
radiologist who reviews the entire set of mammograms for a patient.
Furthermore, this paper emphasizes the preprocessing of images, specifically
proposing a cropping model (U-Net based on ResNet-34) to help the method remove
image artifacts and focus on the breast region. To the best of our knowledge,
this study is the first to achieve a ROC-AUC of 84.0 $\pm$ 1.7 and an F1 score
of 56.0 $\pm$ 1.3 on an independent test dataset of Vietnam digital mammography
(VinDr-Mammo), which is preprocessed with the cropping model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The crop model is available here:
  https://github.com/ispras/mammo_crop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Degradation-Aware Residual-Conditioned Optimal Transport for Unified
  Image Restoration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01656v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01656v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaole Tang, Xiang Gu, Xiaoyi He, Xin Hu, Jian Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  All-in-one image restoration has emerged as a practical and promising
low-level vision task for real-world applications. In this context, the key
issue lies in how to deal with different types of degraded images
simultaneously. In this work, we present a Degradation-Aware
Residual-Conditioned Optimal Transport (DA-RCOT) approach that models
(all-in-one) image restoration as an optimal transport (OT) problem for
unpaired and paired settings, introducing the transport residual as a
degradation-specific cue for both the transport cost and the transport map.
Specifically, we formalize image restoration with a residual-guided OT
objective by exploiting the degradation-specific patterns of the Fourier
residual in the transport cost. More crucially, we design the transport map for
restoration as a two-pass DA-RCOT map, in which the transport residual is
computed in the first pass and then encoded as multi-scale residual embeddings
to condition the second-pass restoration. This conditioning process injects
intrinsic degradation knowledge (e.g., degradation type and level) and
structural information from the multi-scale residual embeddings into the OT
map, which thereby can dynamically adjust its behaviors for all-in-one
restoration. Extensive experiments across five degradations demonstrate the
favorable performance of DA-RCOT as compared to state-of-the-art methods, in
terms of distortion measures, perceptual quality, and image structure
preservation. Notably, DA-RCOT delivers superior adaptability to real-world
scenarios even with multiple degradations and shows distinctive robustness to
both degradation levels and the number of degradations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Gastrointestinal Diagnostics: A CNN-Based Model for VCE Image
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01652v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01652v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vaneeta Ahlawat, Rohit Sharma,  Urush
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, the diagnosis of gastrointestinal (GI) diseases has advanced
greatly with the advent of high-tech video capsule endoscopy (VCE) technology,
which allows for non-invasive observation of the digestive system. The MisaHub
Capsule Vision Challenge encourages the development of vendor-independent
artificial intelligence models that can autonomously classify GI anomalies from
VCE images. This paper presents CNN architecture designed specifically for
multiclass classification of ten gut pathologies, including angioectasia,
bleeding, erosion, erythema, foreign bodies, lymphangiectasia, polyps, ulcers,
and worms as well as their normal state.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figuers</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MapEx: Indoor Structure Exploration with Probabilistic Information Gain
  from Global Map Predictions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15590v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15590v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cherie Ho, Seungchan Kim, Brady Moon, Aditya Parandekar, Narek Harutyunyan, Chen Wang, Katia Sycara, Graeme Best, Sebastian Scherer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exploration is a critical challenge in robotics, centered on understanding
unknown environments. In this work, we focus on robots exploring structured
indoor environments which are often predictable and composed of repeating
patterns. Most existing approaches, such as conventional frontier approaches,
have difficulty leveraging the predictability and explore with simple
heuristics such as `closest first'. Recent works use deep learning techniques
to predict unknown regions of the map, using these predictions for information
gain calculation. However, these approaches are often sensitive to the
predicted map quality or do not reason over sensor coverage. To overcome these
issues, our key insight is to jointly reason over what the robot can observe
and its uncertainty to calculate probabilistic information gain. We introduce
MapEx, a new exploration framework that uses predicted maps to form
probabilistic sensor model for information gain estimation. MapEx generates
multiple predicted maps based on observed information, and takes into
consideration both the computed variances of predicted maps and estimated
visible area to estimate the information gain of a given viewpoint. Experiments
on the real-world KTH dataset showed on average 12.4% improvement than
representative map-prediction based exploration and 25.4% improvement than
nearest frontier approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SynCo: Synthetic Hard Negatives in Contrastive Learning for Better
  Unsupervised Visual Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02401v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02401v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolaos Giakoumoglou, Tania Stathaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive learning has become a dominant approach in self-supervised visual
representation learning. Hard negatives - samples closely resembling the anchor
- are key to enhancing learned representations' discriminative power. However,
efficiently leveraging hard negatives remains challenging. We introduce SynCo
(Synthetic Negatives in Contrastive learning), a novel approach that improves
model performance by generating synthetic hard negatives on the representation
space. Building on the MoCo framework, SynCo introduces six strategies for
creating diverse synthetic hard negatives on-the-fly with minimal computational
overhead. SynCo achieves faster training and better representation learning,
reaching 67.9% top-1 accuracy on ImageNet ILSVRC-2012 linear evaluation after
200 pretraining epochs, surpassing MoCo's 67.5% using the same ResNet-50
encoder. It also transfers more effectively to detection tasks: on PASCAL VOC,
it outperforms both the supervised baseline and MoCo with 82.5% AP; on COCO, it
sets new benchmarks with 40.9% AP for bounding box detection and 35.5% AP for
instance segmentation. Our synthetic hard negative generation approach
significantly enhances visual representations learned through self-supervised
contrastive learning. Code is available at
https://github.com/giakoumoglou/synco.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distilling Invariant Representations with Dual Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09474v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09474v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolaos Giakoumoglou, Tania Stathaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge distillation (KD) has been widely used to transfer knowledge from
large, accurate models (teachers) to smaller, efficient ones (students). Recent
methods have explored enforcing consistency by incorporating causal
interpretations to distill invariant representations. In this work, we extend
this line of research by introducing a dual augmentation strategy to promote
invariant feature learning in both teacher and student models. Our approach
leverages different augmentations applied to both models during distillation,
pushing the student to capture robust, transferable features. This dual
augmentation strategy complements invariant causal distillation by ensuring
that the learned representations remain stable across a wider range of data
variations and transformations. Extensive experiments on CIFAR-100 demonstrate
the effectiveness of this approach, achieving competitive results in
same-architecture KD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper presents preliminary results from a project that we have
  since discontinued, as our research focus has shifted to new directions</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AnyV2V: A <span class="highlight-title">Tuning</span>-Free Framework For Any Video-to-Video Editing Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14468v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14468v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max Ku, Cong Wei, Weiming Ren, Harry Yang, Wenhu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the dynamic field of digital content creation using generative models,
state-of-the-art video editing models still do not offer the level of quality
and control that users desire. Previous works on video editing either extended
from image-based generative models in a zero-shot manner or necessitated
extensive fine-tuning, which can hinder the production of fluid video edits.
Furthermore, these methods frequently rely on textual input as the editing
guidance, leading to ambiguities and limiting the types of edits they can
perform. Recognizing these challenges, we introduce AnyV2V, a novel tuning-free
paradigm designed to simplify video editing into two primary steps: (1)
employing an off-the-shelf image editing model to modify the first frame, (2)
utilizing an existing image-to-video generation model to generate the edited
video through temporal feature injection. AnyV2V can leverage any existing
image editing tools to support an extensive array of video editing tasks,
including prompt-based editing, reference-based style transfer, subject-driven
editing, and identity manipulation, which were unattainable by previous
methods. AnyV2V can also support any video length. Our evaluation shows that
AnyV2V achieved CLIP-scores comparable to other baseline methods. Furthermore,
AnyV2V significantly outperformed these baselines in human evaluations,
demonstrating notable improvements in visual consistency with the source video
while producing high-quality edits across all editing tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Transactions on Machine Learning Research (TMLR 2024)
  (11/2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ParallelEdits: Efficient Multi-Aspect Text-Driven Image Editing with
  Attention Grouping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.00985v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.00985v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingzhen Huang, Jialing Cai, Shan Jia, Vishnu Suresh Lokhande, Siwei Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-driven image synthesis has made significant advancements with the
development of diffusion models, transforming how visual content is generated
from text prompts. Despite these advances, text-driven image editing, a key
area in computer graphics, faces unique challenges. A major challenge is making
simultaneous edits across multiple objects or attributes. Applying these
methods sequentially for multi-attribute edits increases computational demands
and efficiency losses. In this paper, we address these challenges with
significant contributions. Our main contribution is the development of
ParallelEdits, a method that seamlessly manages simultaneous edits across
multiple attributes. In contrast to previous approaches, ParallelEdits not only
preserves the quality of single attribute edits but also significantly improves
the performance of multitasking edits. This is achieved through innovative
attention distribution mechanism and multi-branch design that operates across
several processing heads. Additionally, we introduce the PIE-Bench++ dataset,
an expansion of the original PIE-Bench dataset, to better support evaluating
image-editing tasks involving multiple objects and attributes simultaneously.
This dataset is a benchmark for evaluating text-driven image editing methods in
multifaceted scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GenAI-Bench: Evaluating and Improving Compositional Text-to-Visual
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13743v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13743v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baiqi Li, Zhiqiu Lin, Deepak Pathak, Jiayao Li, Yixin Fei, Kewen Wu, Tiffany Ling, Xide Xia, Pengchuan Zhang, Graham Neubig, Deva Ramanan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While text-to-visual models now produce photo-realistic images and videos,
they struggle with compositional text prompts involving attributes,
relationships, and higher-order reasoning such as logic and comparison. In this
work, we conduct an extensive human study on GenAI-Bench to evaluate the
performance of leading image and video generation models in various aspects of
compositional text-to-visual generation. We also compare automated evaluation
metrics against our collected human ratings and find that VQAScore -- a metric
measuring the likelihood that a VQA model views an image as accurately
depicting the prompt -- significantly outperforms previous metrics such as
CLIPScore. In addition, VQAScore can improve generation in a black-box manner
(without finetuning) via simply ranking a few (3 to 9) candidate images.
Ranking by VQAScore is 2x to 3x more effective than other scoring methods like
PickScore, HPSv2, and ImageReward at improving human alignment ratings for
DALL-E 3 and Stable Diffusion, especially on compositional prompts that require
advanced visio-linguistic reasoning. We release a new GenAI-Rank benchmark with
over 40,000 human ratings to evaluate scoring metrics on ranking images
generated from the same prompt. Lastly, we discuss promising areas for
improvement in VQAScore, such as addressing fine-grained visual details. We
will release all human ratings (over 80,000) to facilitate scientific
benchmarking of both generative models and automated metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We open-source our dataset, model, and code at:
  https://linzhiqiu.github.io/papers/genai_bench ; Project page:
  https://linzhiqiu.github.io/papers/genai_bench ; GenAI-Bench was first
  introduced in arxiv:2404.01291. This article extends it with an additional
  GenAI-Rank benchmark</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LaB-GATr: geometric algebra transformers for large biomedical surface
  and volume meshes <span class="chip">MICCAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07536v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07536v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian Suk, Baris Imre, Jelmer M. Wolterink
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many anatomical structures can be described by surface or volume meshes.
Machine learning is a promising tool to extract information from these 3D
models. However, high-fidelity meshes often contain hundreds of thousands of
vertices, which creates unique challenges in building deep neural network
architectures. Furthermore, patient-specific meshes may not be canonically
aligned which limits the generalisation of machine learning algorithms. We
propose LaB-GATr, a transfomer neural network with geometric tokenisation that
can effectively learn with large-scale (bio-)medical surface and volume meshes
through sequence compression and interpolation. Our method extends the recently
proposed geometric algebra transformer (GATr) and thus respects all Euclidean
symmetries, i.e. rotation, translation and reflection, effectively mitigating
the problem of canonical alignment between patients. LaB-GATr achieves
state-of-the-art results on three tasks in cardiovascular hemodynamics
modelling and neurodevelopmental phenotype prediction, featuring meshes of up
to 200,000 vertices. Our results demonstrate that LaB-GATr is a powerful
architecture for learning with high-fidelity meshes which has the potential to
enable interesting downstream applications. Our implementation is publicly
available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>First published in "Medical Image Computing and Computer Assisted
  Intervention" (MICCAI), pp 185-195, 2024 by Springer Nature</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Multimodal</span>ity Helps Few-Shot 3D Point Cloud Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22489v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22489v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaochong An, Guolei Sun, Yun Liu, Runjia Li, Min Wu, Ming-Ming Cheng, Ender Konukoglu, Serge Belongie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot 3D point cloud segmentation (FS-PCS) aims at generalizing models to
segment novel categories with minimal annotated support samples. While existing
FS-PCS methods have shown promise, they primarily focus on unimodal point cloud
inputs, overlooking the potential benefits of leveraging multimodal
information. In this paper, we address this gap by introducing a cost-free
multimodal FS-PCS setup, utilizing textual labels and the potentially available
2D image modality. Under this easy-to-achieve setup, we present the MultiModal
Few-Shot SegNet (MM-FSS), a model effectively harnessing complementary
information from multiple modalities. MM-FSS employs a shared backbone with two
heads to extract intermodal and unimodal visual features, and a pretrained text
encoder to generate text embeddings. To fully exploit the multimodal
information, we propose a Multimodal Correlation Fusion (MCF) module to
generate multimodal correlations, and a Multimodal Semantic Fusion (MSF) module
to refine the correlations using text-aware semantic guidance. Additionally, we
propose a simple yet effective Test-time Adaptive Cross-modal Calibration
(TACC) technique to mitigate training bias, further improving generalization.
Experimental results on S3DIS and ScanNet datasets demonstrate significant
performance improvements achieved by our method. The efficacy of our approach
indicates the benefits of leveraging commonly-ignored free modalities for
FS-PCS, providing valuable insights for future research. The code is available
at https://github.com/ZhaochongAn/Multimodality-3D-Few-Shot
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Why are Visually-Grounded Language Models Bad at Image Classification? <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.18415v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.18415v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhui Zhang, Alyssa Unell, Xiaohan Wang, Dhruba Ghosh, Yuchang Su, Ludwig Schmidt, Serena Yeung-Levy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image classification is one of the most fundamental capabilities of machine
vision intelligence. In this work, we revisit the image classification task
using visually-grounded language models (VLMs) such as GPT-4V and LLaVA. We
find that existing proprietary and public VLMs, despite often using CLIP as a
vision encoder and having many more parameters, significantly underperform CLIP
on standard image classification benchmarks like ImageNet. To understand the
reason, we explore several hypotheses concerning the inference algorithms,
training objectives, and data processing in VLMs. Our analysis reveals that the
primary cause is data-related: critical information for image classification is
encoded in the VLM's latent space but can only be effectively decoded with
enough training data. Specifically, there is a strong correlation between the
frequency of class exposure during VLM training and instruction-tuning and the
VLM's performance in those classes; when trained with sufficient data, VLMs can
match the accuracy of state-of-the-art classification models. Based on these
findings, we enhance a VLM by integrating classification-focused datasets into
its training, and demonstrate that the enhanced classification performance of
the VLM transfers to its general capabilities, resulting in an improvement of
11.8% on the newly collected ImageWikiQA dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BrepGen: A B-rep Generative Diffusion Model with Structured Latent
  Geometry <span class="chip">SIGGRAPH 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.15563v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.15563v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Xu, Joseph G. Lambourne, Pradeep Kumar Jayaraman, Zhengqing Wang, Karl D. D. Willis, Yasutaka Furukawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents BrepGen, a diffusion-based generative approach that
directly outputs a Boundary representation (B-rep) Computer-Aided Design (CAD)
model. BrepGen represents a B-rep model as a novel structured latent geometry
in a hierarchical tree. With the root node representing a whole CAD solid, each
element of a B-rep model (i.e., a face, an edge, or a vertex) progressively
turns into a child-node from top to bottom. B-rep geometry information goes
into the nodes as the global bounding box of each primitive along with a latent
code describing the local geometric shape. The B-rep topology information is
implicitly represented by node duplication. When two faces share an edge, the
edge curve will appear twice in the tree, and a T-junction vertex with three
incident edges appears six times in the tree with identical node features.
Starting from the root and progressing to the leaf, BrepGen employs
Transformer-based diffusion models to sequentially denoise node features while
duplicated nodes are detected and merged, recovering the B-Rep topology
information. Extensive experiments show that BrepGen advances the task of CAD
B-rep generation, surpassing existing methods on various benchmarks. Results on
our newly collected furniture dataset further showcase its exceptional
capability in generating complicated geometry. While previous methods were
limited to generating simple prismatic shapes, BrepGen incorporates free-form
and doubly-curved surfaces for the first time. Additional applications of
BrepGen include CAD autocomplete and design interpolation. The code, pretrained
models, and dataset are available at https://github.com/samxuxiang/BrepGen.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACM SIGGRAPH 2024. Code at
  https://github.com/samxuxiang/BrepGen</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-11-02T00:00:00Z">2024-11-02</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">12</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Artificial Intelligence Driven Course Generation: A Case Study Using
  ChatGPT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01369v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01369v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Djaber Rouabhia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study explores Artificial Intelligence use, specifically ChatGPT, in
creating educational content. The study aims to elaborate on using ChatGPT to
create course materials. The main objective is to assess the efficiency,
quality, and impact of AI-driven course generation, and to create a Multimedia
Databases course as a case study. The study highlights the potential of AI to
revolutionize educational content creation, making it more accessible,
personalized, and efficient. The course content was generated in less than one
day through iterative methods, using prompts for translation, content
expansion, practical examples, assignments, supplementary materials, and LaTeX
formatting. Each part was verified immediately after generation to ensure
accuracy. Post-generation analysis with Detectia and Turnitin showed similarity
rates of 8.7% and 13%, indicating high originality. Experts and university
committees reviewed and approved the course, with English university teachers
praising its language quality. ChatGPT also created a well-structured and
diversified exam for the module. Key findings reveal significant time
efficiency, comprehensive content coverage, and high flexibility. The study
underscores AI's transformative potential in education, addressing challenges
related to data privacy, technology dependence, content accuracy, and
algorithmic biases. The conclusions emphasize the need for collaboration
between educators, policymakers, and technology developers to harness AI's
benefits in education fully.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Online and Offline Evaluations of Collaborative Filtering and Content
  Based Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01354v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01354v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Elahi, Armin Zirak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems are widely used AI applications designed to help users
efficiently discover relevant items. The effectiveness of such systems is tied
to the satisfaction of both users and providers. However, user satisfaction is
complex and cannot be easily framed mathematically using information retrieval
and accuracy metrics. While many studies evaluate accuracy through offline
tests, a growing number of researchers argue that online evaluation methods
such as A/B testing are better suited for this purpose. We have employed a
variety of algorithms on different types of datasets divergent in size and
subject, producing recommendations in various platforms, including media
streaming services, digital publishing websites, e-commerce systems, and news
broadcasting networks. Notably, our target websites and datasets are in Persian
(Farsi) language.
  This study provides a comparative analysis of a large-scale recommender
system that has been operating for the past year across about 70 websites in
Iran, processing roughly 300 requests per second collectively. The system
employs user-based and item-based recommendations using content-based,
collaborative filtering, trend-based methods, and hybrid approaches. Through
both offline and online evaluations, we aim to identify where these algorithms
perform most efficiently and determine the best method for our specific needs,
considering the dataset and system scale. Our methods of evaluation include
manual evaluation, offline tests including accuracy and ranking metrics like
hit-rate@k and nDCG, and online tests consisting of click-through rate (CTR).
Additionally we analyzed and proposed methods to address cold-start and
popularity bias.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AMREx: AMR for Explainable Fact Verification <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01343v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01343v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chathuri Jayaweera, Sangpil Youm, Bonnie Dorr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advent of social media networks and the vast amount of information
circulating through them, automatic fact verification is an essential component
to prevent the spread of misinformation. It is even more useful to have fact
verification systems that provide explanations along with their classifications
to ensure accurate predictions. To address both of these requirements, we
implement AMREx, an Abstract Meaning Representation (AMR)-based veracity
prediction and explanation system for fact verification using a combination of
Smatch, an AMR evaluation metric to measure meaning containment and textual
similarity, and demonstrate its effectiveness in producing partially
explainable justifications using two community standard fact verification
datasets, FEVER and AVeriTeC. AMREx surpasses the AVeriTec baseline accuracy
showing the effectiveness of our approach for real-world claim verification. It
follows an interpretable pipeline and returns an explainable AMR node mapping
to clarify the system's veracity predictions when applicable. We further
demonstrate that AMREx output can be used to prompt LLMs to generate
natural-language explanations using the AMR mappings as a guide to lessen the
probability of hallucinations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This study implements, evaluates, and analyzes an Abstract Meaning
  Representation (AMR) based partially explainable system for fact
  verification/ veracity classification. Accepted by EMNLP Workshop on Fact
  Extraction and VERification (FEVER) 2024, 11 pages, 7 figures,</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Be like a Goldfish, Don't Memorize! Mitigating Memorization in
  Generative LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10209v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10209v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhimanyu Hans, Yuxin Wen, Neel Jain, John Kirchenbauer, Hamid Kazemi, Prajwal Singhania, Siddharth Singh, Gowthami Somepalli, Jonas Geiping, Abhinav Bhatele, Tom Goldstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models can memorize and repeat their training data, causing
privacy and copyright risks. To mitigate memorization, we introduce a subtle
modification to the next-token training objective that we call the goldfish
loss. During training, randomly sampled subsets of tokens are excluded from the
loss computation. These dropped tokens are not memorized by the model, which
prevents verbatim reproduction of a complete chain of tokens from the training
set. We run extensive experiments training billion-scale Llama-2 models, both
pre-trained and trained from scratch, and demonstrate significant reductions in
extractable memorization with little to no impact on downstream benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 figures, and 1 table in the main body. Code available at
  https://github.com/ahans30/goldfish-loss and checkpoints at
  https://huggingface.co/collections/tomg-group-umd/goldfish-loss-mitigating-memorization-in-llms-66c175becb6aab07744f7272</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Narrative Feature or Structured Feature? A Study of Large Language
  Models to Identify Cancer Patients at Risk of Heart Failure 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11425v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11425v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyi Chen, Mengyuan Zhang, Mustafa Mohammed Ahmed, Yi Guo, Thomas J. George, Jiang Bian, Yonghui Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cancer treatments are known to introduce cardiotoxicity, negatively impacting
outcomes and survivorship. Identifying cancer patients at risk of heart failure
(HF) is critical to improving cancer treatment outcomes and safety. This study
examined machine learning (ML) models to identify cancer patients at risk of HF
using electronic health records (EHRs), including traditional ML, Time-Aware
long short-term memory (T-LSTM), and large language models (LLMs) using novel
narrative features derived from the structured medical codes. We identified a
cancer cohort of 12,806 patients from the University of Florida Health,
diagnosed with lung, breast, and colorectal cancers, among which 1,602
individuals developed HF after cancer. The LLM, GatorTron-3.9B, achieved the
best F1 scores, outperforming the traditional support vector machines by 39%,
the T-LSTM deep learning model by 7%, and a widely used transformer model,
BERT, by 5.6%. The analysis shows that the proposed narrative features
remarkably increased feature density and improved performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CogDevelop2K: Reversed Cognitive Development in <span class="highlight-title">Multimodal</span> Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10855v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10855v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijiang Li, Qingying Gao, Haoran Sun, Haiyun Lyu, Dezhi Luo, Hokin Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Are Multi-modal Large Language Models (MLLMs) stochastic parrots? Do they
genuinely understand? This paper aims to explore the core cognitive abilities
that human intelligence builds upon to perceive, comprehend, and reason in
MLLMs. To this end, we propose CogDevelop2K, a comprehensive benchmark that
spans 12 sub-concepts from primitive knowledge like object permanence and
boundary to more complex abilities like intentionality understanding,
structured via the developmental trajectory of a human mind. We evaluate 46
MLLMs on our benchmarks. Surprisingly, we observe a reversed cognitive
developmental trajectory compared to humans. Comprehensively, we further
evaluate the influence of evaluation strategies and prompting techniques.
Website with this $\href{https://growing-ai-like-a-child.github.io/}{link}$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Website with this
  $\href{https://growing-ai-like-a-child.github.io/}{link}$</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InversionView: A General-Purpose Method for Reading Information from
  Neural Activations <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17653v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17653v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinting Huang, Madhur Panwar, Navin Goyal, Michael Hahn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The inner workings of neural networks can be better understood if we can
fully decipher the information encoded in neural activations. In this paper, we
argue that this information is embodied by the subset of inputs that give rise
to similar activations. We propose InversionView, which allows us to
practically inspect this subset by sampling from a trained decoder model
conditioned on activations. This helps uncover the information content of
activation vectors, and facilitates understanding of the algorithms implemented
by transformer models. We present four case studies where we investigate models
ranging from small transformers to GPT-2. In these studies, we show that
InversionView can reveal clear information contained in activations, including
basic information about tokens appearing in the context, as well as more
complex information, such as the count of certain tokens, their relative
positions, and abstract knowledge about the subject. We also provide causally
verified circuits to confirm the decoded information.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024; ICML 2024 Mechanistic Interpretability Workshop oral</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bayesian scaling laws for in-context learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16531v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16531v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aryaman Arora, Dan Jurafsky, Christopher Potts, Noah D. Goodman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context learning (ICL) is a powerful technique for getting language models
to perform complex tasks with no training updates. Prior work has established
strong correlations between the number of in-context examples provided and the
accuracy of the model's predictions. In this paper, we seek to explain this
correlation by showing that ICL approximates a Bayesian learner. This
perspective gives rise to a family of novel Bayesian scaling laws for ICL. In
experiments with \mbox{GPT-2} models of different sizes, our scaling laws
exceed or match existing scaling laws in accuracy while also offering
interpretable terms for task priors, learning efficiency, and per-example
probabilities. To illustrate the analytic power that such interpretable scaling
laws provide, we report on controlled synthetic dataset experiments designed to
inform real-world studies of safety alignment. In our experimental protocol, we
use SFT to suppress an unwanted existing model capability and then use ICL to
try to bring that capability back (many-shot jailbreaking). We then experiment
on real-world instruction-tuned LLMs using capabilities benchmarks as well as a
new many-shot jailbreaking dataset. In all cases, Bayesian scaling laws
accurately predict the conditions under which ICL will cause the suppressed
behavior to reemerge, which sheds light on the ineffectiveness of post-training
at increasing LLM safety.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages main text, 26 pages total</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Desert Camels and Oil Sheikhs: Arab-Centric Red Teaming of Frontier LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24049v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24049v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammed Saeed, Elgizouli Mohamed, Mukhtar Mohamed, Shaina Raza, Shady Shehata, Muhammad Abdul-Mageed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are widely used but raise ethical concerns due
to embedded social biases. This study examines LLM biases against Arabs versus
Westerners across eight domains, including women's rights, terrorism, and
anti-Semitism and assesses model resistance to perpetuating these biases. To
this end, we create two datasets: one to evaluate LLM bias toward Arabs versus
Westerners and another to test model safety against prompts that exaggerate
negative traits ("jailbreaks"). We evaluate six LLMs -- GPT-4, GPT-4o, LlaMA
3.1 (8B & 405B), Mistral 7B, and Claude 3.5 Sonnet. We find 79% of cases
displaying negative biases toward Arabs, with LlaMA 3.1-405B being the most
biased. Our jailbreak tests reveal GPT-4o as the most vulnerable, despite being
an optimized version, followed by LlaMA 3.1-8B and Mistral 7B. All LLMs except
Claude exhibit attack success rates above 87% in three categories. We also find
Claude 3.5 Sonnet the safest, but it still displays biases in seven of eight
categories. Despite being an optimized version of GPT4, We find GPT-4o to be
more prone to biases and jailbreaks, suggesting optimization flaws. Our
findings underscore the pressing need for more robust bias mitigation
strategies and strengthened security measures in LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative linguistics contribution to artificial intelligence: Where
  this contribution lies? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20221v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20221v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammed Q. Shormani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article aims to characterize Generative linguistics (GL) contribution to
artificial intelligence (AI), alluding to the debate among linguists and AI
scientists on whether linguistics belongs to humanities or science. In this
article, I will try not to be biased as a linguist, studying the phenomenon
from an independent scientific perspective. The article walks the
researcher/reader through the scientific theorems and rationales involved in AI
which belong from GL, specifically the Chomsky School. It, thus, provides good
evidence from syntax, semantics, language faculty, Universal Grammar,
computational system of human language, language acquisition, human brain,
programming languages (e.g. Python), Large Language Models, and unbiased AI
scientists that this contribution is huge, and that this contribution cannot be
denied. It concludes that however the huge GL contribution to AI, there are
still points of divergence including the nature and type of language input.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A hybrid transformer and attention based recurrent neural network for
  robust and interpretable sentiment analysis of tweets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.00297v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.00297v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Abrar Jahin, Md Sakib Hossain Shovon, M. F. Mridha, Md Rashedul Islam, Yutaka Watanobe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sentiment analysis is crucial for understanding public opinion and consumer
behavior. Existing models face challenges with linguistic diversity,
generalizability, and explainability. We propose TRABSA, a hybrid framework
integrating transformer-based architectures, attention mechanisms, and BiLSTM
networks to address this. Leveraging RoBERTa-trained on 124M tweets, we bridge
gaps in sentiment analysis benchmarks, ensuring state-of-the-art accuracy.
Augmenting datasets with tweets from 32 countries and US states, we compare six
word-embedding techniques and three lexicon-based labeling techniques,
selecting the best for optimal sentiment analysis. TRABSA outperforms
traditional ML and deep learning models with 94% accuracy and significant
precision, recall, and F1-score gains. Evaluation across diverse datasets
demonstrates consistent superiority and generalizability. SHAP and LIME
analyses enhance interpretability, improving confidence in predictions. Our
study facilitates pandemic resource management, aiding resource planning,
policy formation, and vaccination tactics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Judging the Judges: Evaluating Alignment and Vulnerabilities in
  LLMs-as-Judges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12624v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12624v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aman Singh Thakur, Kartik Choudhary, Venkat Srinik Ramayapally, Sankaran Vaidyanathan, Dieuwke Hupkes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offering a promising solution to the scalability challenges associated with
human evaluation, the LLM-as-a-judge paradigm is rapidly gaining traction as an
approach to evaluating large language models (LLMs). However, there are still
many open questions about the strengths and weaknesses of this paradigm, and
what potential biases it may hold. In this paper, we present a comprehensive
study of the performance of various LLMs acting as judges, focusing on a clean
scenario in which inter-human agreement is high. Investigating thirteen judge
models of different model sizes and families, judging answers of nine different
'examtaker models' - both base and instruction-tuned - we find that only the
best (and largest) models achieve reasonable alignment with humans. However,
they are still quite far behind inter-human agreement and their assigned scores
may still differ with up to 5 points from human-assigned scores. In terms of
their ranking of the nine exam-taker models, instead, also smaller models and
even the lexical metric contains may provide a reasonable signal. Through error
analysis and other studies, we identify vulnerabilities in judge models, such
as their sensitivity to prompt complexity and length, and a tendency toward
leniency. The fact that even the best judges differ from humans in this
comparatively simple setup suggest that caution may be wise when using judges
in more complex setups. Lastly, our research rediscovers the importance of
using alignment metrics beyond simple percent alignment, showing that judges
with high percent agreement can still assign vastly different scores.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-11-01T00:00:00Z">2024-11-01</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">51</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Regression-aware Inference with LLMs <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04182v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04182v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michal Lukasik, Harikrishna Narasimhan, Aditya Krishna Menon, Felix Yu, Sanjiv Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown strong results on a range of
applications, including regression and scoring tasks. Typically, one obtains
outputs from an LLM via autoregressive sampling from the model's output
distribution. We show that this inference strategy can be sub-optimal for
common regression and scoring evaluation metrics. As a remedy, we build on
prior work on Minimum Bayes Risk decoding, and propose alternate inference
strategies that estimate the Bayes-optimal solution for regression and scoring
metrics in closed-form from sampled responses. We show that our proposal
significantly improves over baselines across datasets and models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP Findings 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HDLCopilot: Natural Language Exploration of Hardware Designs and
  Libraries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12749v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12749v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manar Abdelatty, Jacob Rosenstein, Sherief Reda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hardware design workflows often involve working with Process Design Kits
(PDKs) from various fabrication labs, each containing its own set of standard
cell libraries optimized for metrics such as speed, power, or density. These
libraries include multiple views for information on timing and electrical
properties of cells, cell layout details, and process design rules. Engineers
typically navigate between the design and the target technology to make
informed decisions on different design scenarios, such as selecting specific
gates for area optimization or enhancing critical path speed. Navigating this
complex landscape to retrieve specific information about gates or design rules
is often time-consuming and error-prone. To address this, we present
HDLCopilot, a multi-agent collaborative framework powered by large language
models that enables engineers to streamline interactions with hardware design
and PDKs through natural language queries. HDLCopilot enables engineers to
quickly access relevant information on gates and design rules, evaluate
tradeoffs related to area, speed, and power in order to make informed decisions
efficiently and accurately. The framework achieves an execution accuracy of
96.33\% on a diverse set of complex natural language queries. HDLCopilot
positions itself as a powerful assistant in hardware design workflows,
enhancing productivity and reducing potential human errors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DuQuant: Distributing Outliers via Dual Transformation Makes Stronger
  Quantized LLMs <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.01721v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.01721v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haokun Lin, Haobo Xu, Yichen Wu, Jingzhi Cui, Yingtao Zhang, Linzhan Mou, Linqi Song, Zhenan Sun, Ying Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantization of large language models (LLMs) faces significant challenges,
particularly due to the presence of outlier activations that impede efficient
low-bit representation. Traditional approaches predominantly address Normal
Outliers, which are activations across all tokens with relatively large
magnitudes. However, these methods struggle with smoothing Massive Outliers
that display significantly larger values, which leads to significant
performance degradation in low-bit quantization. In this paper, we introduce
DuQuant, a novel approach that utilizes rotation and permutation
transformations to more effectively mitigate both massive and normal outliers.
First, DuQuant starts by constructing the rotation matrix, using specific
outlier dimensions as prior knowledge, to redistribute outliers to adjacent
channels by block-wise rotation. Second, We further employ a zigzag permutation
to balance the distribution of outliers across blocks, thereby reducing
block-wise variance. A subsequent rotation further smooths the activation
landscape, enhancing model performance. DuQuant simplifies the quantization
process and excels in managing outliers, outperforming the state-of-the-art
baselines across various sizes and types of LLMs on multiple tasks, even with
4-bit weight-activation quantization. Our code is available at
https://github.com/Hsu1023/DuQuant.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 Oral, Website at https://duquant.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Introducing MAPO: Momentum-Aided Gradient Descent <span class="highlight-title">Prompt</span> Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.19499v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.19499v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anthony Cui, Pranav Nandyalam, Ethan Cheung, Kevin Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Momentum-Aided Prompt Optimization (MAPO) enhances the efficiency and
efficacy of prompt optimization for Large Language Models (LLMs). Building on
ProTeGi, MAPO uses positive natural language "gradients" and a momentum-based
extension to refine prompts effectively. By tracking gradient history, MAPO
avoids local minima and oscillations. It also utilizes beam search and an Upper
Confidence Bound (UCB) algorithm for balanced candidate expansion and
selection. Benchmark testing shows that MAPO achieves faster convergence time
with fewer API calls and higher F1 scores than ProTeGi, proving it as a robust
and scalable solution for automated prompt engineering in LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Large Language Model Agents Simulate Human Trust Behavior? <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.04559v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.04559v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengxing Xie, Canyu Chen, Feiran Jia, Ziyu Ye, Shiyang Lai, Kai Shu, Jindong Gu, Adel Bibi, Ziniu Hu, David Jurgens, James Evans, Philip Torr, Bernard Ghanem, Guohao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Model (LLM) agents have been increasingly adopted as
simulation tools to model humans in social science and role-playing
applications. However, one fundamental question remains: can LLM agents really
simulate human behavior? In this paper, we focus on one critical and elemental
behavior in human interactions, trust, and investigate whether LLM agents can
simulate human trust behavior. We first find that LLM agents generally exhibit
trust behavior, referred to as agent trust, under the framework of Trust Games,
which are widely recognized in behavioral economics. Then, we discover that
GPT-4 agents manifest high behavioral alignment with humans in terms of trust
behavior, indicating the feasibility of simulating human trust behavior with
LLM agents. In addition, we probe the biases of agent trust and differences in
agent trust towards other LLM agents and humans. We also explore the intrinsic
properties of agent trust under conditions including external manipulations and
advanced reasoning strategies. Our study provides new insights into the
behaviors of LLM agents and the fundamental analogy between LLMs and humans
beyond value alignment. We further illustrate broader implications of our
discoveries for applications where trust is paramount.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Proceedings of NeurIPS 2024. The first two authors
  contributed equally. 10 pages for main paper, 56 pages including appendix.
  Project website: https://agent-trust.camel-ai.org</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SelfCodeAlign: Self-Alignment for Code Generation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24198v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24198v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxiang Wei, Federico Cassano, Jiawei Liu, Yifeng Ding, Naman Jain, Zachary Mueller, Harm de Vries, Leandro von Werra, Arjun Guha, Lingming Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction tuning is a supervised fine-tuning approach that significantly
improves the ability of large language models (LLMs) to follow human
instructions. We propose SelfCodeAlign, the first fully transparent and
permissive pipeline for self-aligning code LLMs without extensive human
annotations or distillation. SelfCodeAlign employs the same base model for
inference throughout the data generation process. It first extracts diverse
coding concepts from high-quality seed snippets to generate new tasks. It then
samples multiple responses per task, pairs each with test cases, and validates
them in a sandbox environment. Finally, passing examples are selected for
instruction tuning. In our primary experiments, we use SelfCodeAlign with
CodeQwen1.5-7B to generate a dataset of 74k instruction-response pairs.
Finetuning on this dataset leads to a model that achieves a 67.1 pass@1 on
HumanEval+, surpassing CodeLlama-70B-Instruct despite being ten times smaller.
Across all benchmarks, this finetuned model consistently outperforms the
original version trained with OctoPack, the previous state-of-the-art method
for instruction tuning without human annotations or distillation. Additionally,
we show that SelfCodeAlign is effective across LLMs of various sizes, from 3B
to 33B, and that the base models can benefit more from alignment with their own
data distribution. We further validate each component's effectiveness in our
pipeline, showing that SelfCodeAlign outperforms both direct distillation from
GPT-4o and leading GPT-3.5-based distillation methods, such as OSS-Instruct and
Evol-Instruct. SelfCodeAlign has also led to the creation of
StarCoder2-Instruct, the first fully transparent, permissively licensed, and
self-aligned code LLM that achieves state-of-the-art coding performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language Imbalance Driven Rewarding for Multilingual Self-improving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08964v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08964v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wen Yang, Junhong Wu, Chen Wang, Chengqing Zong, Jiajun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have achieved state-of-the-art performance
across numerous tasks. However, these advancements have predominantly benefited
"first-class" languages such as English and Chinese, leaving many other
languages underrepresented. This imbalance, while limiting broader
applications, generates a natural preference ranking between languages,
offering an opportunity to bootstrap the multilingual capabilities of LLM in a
self-improving manner. Thus, we propose $\textit{Language Imbalance Driven
Rewarding}$, where the inherent imbalance between dominant and non-dominant
languages within LLMs is leveraged as a reward signal. Iterative DPO training
demonstrates that this approach not only enhances LLM performance in
non-dominant languages but also improves the dominant language's capacity,
thereby yielding an iterative reward signal. Fine-tuning
Meta-Llama-3-8B-Instruct over two iterations of this approach results in
continuous improvements in multilingual performance across
instruction-following and arithmetic reasoning tasks, evidenced by an average
improvement of 7.46% win rate on the X-AlpacaEval leaderboard and 13.9%
accuracy on the MGSM benchmark. This work serves as an initial exploration,
paving the way for multilingual self-improvement of LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LongRAG: A Dual-Perspective Retrieval-Augmented Generation Paradigm for
  Long-Context Question Answering <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18050v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18050v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingfei Zhao, Ruobing Wang, Yukuo Cen, Daren Zha, Shicheng Tan, Yuxiao Dong, Jie Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-Context Question Answering (LCQA), a challenging task, aims to reason
over long-context documents to yield accurate answers to questions. Existing
long-context Large Language Models (LLMs) for LCQA often struggle with the
"lost in the middle" issue. Retrieval-Augmented Generation (RAG) mitigates this
issue by providing external factual evidence. However, its chunking strategy
disrupts the global long-context information, and its low-quality retrieval in
long contexts hinders LLMs from identifying effective factual details due to
substantial noise. To this end, we propose LongRAG, a general,
dual-perspective, and robust LLM-based RAG system paradigm for LCQA to enhance
RAG's understanding of complex long-context knowledge (i.e., global information
and factual details). We design LongRAG as a plug-and-play paradigm,
facilitating adaptation to various domains and LLMs. Extensive experiments on
three multi-hop datasets demonstrate that LongRAG significantly outperforms
long-context LLMs (up by 6.94%), advanced RAG (up by 6.16%), and Vanilla RAG
(up by 17.25%). Furthermore, we conduct quantitative ablation studies and
multi-dimensional analyses, highlighting the effectiveness of the system's
components and fine-tuning strategies. Data and code are available at
https://github.com/QingFei1/LongRAG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Main, Final</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.15420v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.15420v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        João Monteiro, Étienne Marcotte, Pierre-André Noël, Valentina Zantedeschi, David Vázquez, Nicolas Chapados, Christopher Pal, Perouz Taslakian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context learning (ICL) approaches typically leverage prompting to
condition decoder-only language model generation on reference information.
Just-in-time processing of a context is inefficient due to the quadratic cost
of self-attention operations, and caching is desirable. However, caching
transformer states can easily require almost as much space as the model
parameters. When the right context isn't known in advance, caching ICL can be
challenging. This work addresses these limitations by introducing models that,
inspired by the encoder-decoder architecture, use cross-attention to condition
generation on reference text without the prompt. More precisely, we leverage
pre-trained decoder-only models and only train a small number of added layers.
We use Question-Answering (QA) as a testbed to evaluate the ability of our
models to perform conditional generation and observe that they outperform ICL,
are comparable to fine-tuned prompted LLMs, and drastically reduce the space
footprint relative to standard KV caching by two orders of magnitude.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ INC-Math: Integrating Natural Language and Code for Enhanced
  Mathematical Reasoning in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19381v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19381v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuyuan Xiong, Simeng Han, Ziyue Zhou, Arman Cohan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are commonly used to generate solutions for
mathematical reasoning problems in the following formats: natural language,
code, or a combination of both. In this paper, we explore fundamental questions
related to solving mathematical reasoning problems using natural language and
code with state-of-the-art LLMs, including GPT-4o-mini and LLama-3.1-8b-Turbo.
Our findings show that LLMs are better at reasoning in natural language
compared to code. Additionally, although natural language and code serve as
complementary forms of reasoning, they can affect each other in a negative way
in certain scenarios. These insights motivate our development of a new
prompting method, INC-Math, which leverages an LLM to dynamically select the
most appropriate reasoning form, resulting in improved performance over
comparable baselines with GPT-4o-mini.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nova: A Practical and Advanced Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14940v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14940v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingan Lin, Fan Yang, Yanjun Shen, Haoze Sun, Tianpeng Li, Tao Zhang, Chenzheng Zhu, Tao Zhang, Miao Zheng, Xu Li, Yijie Zhou, Mingyang Chen, Yanzhao Qin, Youquan Li, Hao Liang, Fei Li, Yadong Li, Mang Wang, Guosheng Dong, Kun Fang, Jianhua Xu, Bin Cui, Wentao Zhang, Zenan Zhou, Weipeng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Nova, a suite of practical alignment techniques employed in a
series of empirically validated high-performing models. This represents the
first comprehensive account of alignment methodologies, offering valuable
insights for advancing AI research. We investigate the critical components that
enhance model performance during the alignment process, including optimization
methods, data strategies, capability enhancements, and evaluation processes.
The process spans three key stages: Prompt Augmentation System(PAS), Supervised
Fine-Tuning(SFT), and Preference Alignment. The problems encountered, the
solutions applied, and the improvements made are thoroughly recorded.
  Through comparisons across well-established benchmarks, we highlight the
technological advancements enabled by Nova Alignment. Importantly,
Qwen2-Nova-72B and Llama3-PBM-Nova-70B are instruct versions of the Qwen2-72B
and Llama-3-70B base models, optimized through Nova. The Nova models show
significant core improvements, with user experience gains of 17% to 28%, and
excels on specialized benchmarks. In open-source benchmark evaluations, both
Qwen2-Nova-72B and Llama3-PBM-Nova-70B consistently outperform their respective
official instruct versions across nearly all datasets. This report aims to
clarify the key technologies behind the alignment process, fostering a deeper
understanding within the community. Llama3-PBM-Nova-70B model is available at
https://huggingface.co/PKU-Baichuan-MLSystemLab/Llama3-PBM-Nova-70B.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TaskBench: Benchmarking Large Language Models for Task Automation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.18760v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.18760v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongliang Shen, Kaitao Song, Xu Tan, Wenqi Zhang, Kan Ren, Siyu Yuan, Weiming Lu, Dongsheng Li, Yueting Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, the remarkable progress of large language models (LLMs) has
sparked interest in task automation, which involves decomposing complex tasks
described by user instructions into sub-tasks and invoking external tools to
execute them, playing a central role in autonomous agents. However, there is a
lack of systematic and standardized benchmarks to promote the development of
LLMs in task automation. To address this, we introduce TaskBench, a
comprehensive framework to evaluate the capability of LLMs in task automation.
Specifically, task automation can be divided into three critical stages: task
decomposition, tool selection, and parameter prediction. To tackle the
complexities inherent in these stages, we introduce the concept of Tool Graph
to represent decomposed tasks and adopt a back-instruct method to generate
high-quality user instructions. We propose TaskEval, a multi-faceted evaluation
methodology that assesses LLM performance across these three stages. Our
approach combines automated construction with rigorous human verification,
ensuring high consistency with human evaluation. Experimental results
demonstrate that TaskBench effectively reflects the capabilities of various
LLMs in task automation. It provides insights into model performance across
different task complexities and domains, pushing the boundaries of what current
models can achieve. TaskBench offers a scalable, adaptable, and reliable
benchmark for advancing LLM-based autonomous agents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models for Patient Comments Multi-Label Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23528v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23528v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hajar Sakai, Sarah S. Lam, Mohammadsadegh Mikaeili, Joshua Bosire, Franziska Jovin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Patient experience and care quality are crucial for a hospital's
sustainability and reputation. The analysis of patient feedback offers valuable
insight into patient satisfaction and outcomes. However, the unstructured
nature of these comments poses challenges for traditional machine learning
methods following a supervised learning paradigm. This is due to the
unavailability of labeled data and the nuances these texts encompass. This
research explores leveraging Large Language Models (LLMs) in conducting
Multi-label Text Classification (MLTC) of inpatient comments shared after a
stay in the hospital. GPT-4 Turbo was leveraged to conduct the classification.
However, given the sensitive nature of patients' comments, a security layer is
introduced before feeding the data to the LLM through a Protected Health
Information (PHI) detection framework, which ensures patients'
de-identification. Additionally, using the prompt engineering framework,
zero-shot learning, in-context learning, and chain-of-thought prompting were
experimented with. Results demonstrate that GPT-4 Turbo, whether following a
zero-shot or few-shot setting, outperforms traditional methods and Pre-trained
Language Models (PLMs) and achieves the highest overall performance with an
F1-score of 76.12% and a weighted F1-score of 73.61% followed closely by the
few-shot learning results. Subsequently, the results' association with other
patient experience structured variables (e.g., rating) was conducted. The study
enhances MLTC through the application of LLMs, offering healthcare
practitioners an efficient method to gain deeper insights into patient feedback
and deliver prompt, appropriate responses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ $\texttt{MixGR}$: Enhancing Retriever Generalization for Scientific
  Domain through Complementary Granularity <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10691v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10691v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fengyu Cai, Xinran Zhao, Tong Chen, Sihao Chen, Hongming Zhang, Iryna Gurevych, Heinz Koeppl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies show the growing significance of document retrieval in the
generation of LLMs, i.e., RAG, within the scientific domain by bridging their
knowledge gap. However, dense retrievers often struggle with domain-specific
retrieval and complex query-document relationships, particularly when query
segments correspond to various parts of a document. To alleviate such prevalent
challenges, this paper introduces $\texttt{MixGR}$, which improves dense
retrievers' awareness of query-document matching across various levels of
granularity in queries and documents using a zero-shot approach.
$\texttt{MixGR}$ fuses various metrics based on these granularities to a united
score that reflects a comprehensive query-document similarity. Our experiments
demonstrate that $\texttt{MixGR}$ outperforms previous document retrieval by
24.7%, 9.8%, and 6.9% on nDCG@5 with unsupervised, supervised, and LLM-based
retrievers, respectively, averaged on queries containing multiple subqueries
from five scientific retrieval datasets. Moreover, the efficacy of two
downstream scientific question-answering tasks highlights the advantage of
$\texttt{MixGR}$ to boost the application of LLMs in the scientific domain. The
code and experimental datasets are available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Pre-trained Speech Language Model with Efficient Hierarchical
  Transformer <span class="chip">ACL2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.00976v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.00976v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongxin Zhu, Dan Su, Liqiang He, Linli Xu, Dong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While recent advancements in speech language models have achieved significant
progress, they face remarkable challenges in modeling the long acoustic
sequences of neural audio codecs. In this paper, we introduce
\textbf{G}enerative \textbf{P}re-trained \textbf{S}peech \textbf{T}ransformer
(GPST), a hierarchical transformer designed for efficient speech language
modeling. GPST quantizes audio waveforms into two distinct types of discrete
speech representations and integrates them within a hierarchical transformer
architecture, allowing for a unified one-stage generation process and enhancing
Hi-Res audio generation capabilities. By training on large corpora of speeches
in an end-to-end unsupervised manner, GPST can generate syntactically
consistent speech with diverse speaker identities. Given a brief 3-second
prompt, GPST can produce natural and coherent personalized speech,
demonstrating in-context learning abilities. Moreover, our approach can be
easily extended to spoken cross-lingual speech generation by incorporating
multi-lingual semantic tokens and universal acoustic tokens. Experimental
results indicate that GPST significantly outperforms the existing speech
language models in terms of word error rate, speech quality, and speaker
similarity. The code is available at \url{https://github.com/youngsheen/GPST}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accept in ACL2024-main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Semantic Density: Uncertainty Quantification for Large Language Models
  through Confidence Measurement in Semantic Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.13845v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.13845v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Qiu, Risto Miikkulainen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the widespread application of Large Language Models (LLMs) to various
domains, concerns regarding the trustworthiness of LLMs in safety-critical
scenarios have been raised, due to their unpredictable tendency to hallucinate
and generate misinformation. Existing LLMs do not have an inherent
functionality to provide the users with an uncertainty/confidence metric for
each response it generates, making it difficult to evaluate trustworthiness.
Although several studies aim to develop uncertainty quantification methods for
LLMs, they have fundamental limitations, such as being restricted to
classification tasks, requiring additional training and data, considering only
lexical instead of semantic information, and being prompt-wise but not
response-wise. A new framework is proposed in this paper to address these
issues. Semantic density extracts uncertainty/confidence information for each
response from a probability distribution perspective in semantic space. It has
no restriction on task types and is "off-the-shelf" for new models and tasks.
Experiments on seven state-of-the-art LLMs, including the latest Llama 3 and
Mixtral-8x22B models, on four free-form question-answering benchmarks
demonstrate the superior performance and robustness of semantic density
compared to prior approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Neurips 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LaCour!: Enabling Research on Argumentation in Hearings of the European
  Court of Human Rights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.05061v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.05061v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lena Held, Ivan Habernal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Why does an argument end up in the final court decision? Was it deliberated
or questioned during the oral hearings? Was there something in the hearings
that triggered a particular judge to write a dissenting opinion? Despite the
availability of the final judgments of the European Court of Human Rights
(ECHR), none of these legal research questions can currently be answered as the
ECHR's multilingual oral hearings are not transcribed, structured, or
speaker-attributed. We address this fundamental gap by presenting LaCour!, the
first corpus of textual oral arguments of the ECHR, consisting of 154 full
hearings (2.1 million tokens from over 267 hours of video footage) in English,
French, and other court languages, each linked to the corresponding final
judgment documents. In addition to the transcribed and partially manually
corrected text from the video, we provide sentence-level timestamps and
manually annotated role and language labels. We also showcase LaCour! in a set
of preliminary experiments that explore the interplay between questions and
dissenting opinions. Apart from the use cases in legal NLP, we hope that law
students or other interested parties will also use LaCour! as a learning
resource, as it is freely available in various formats at
https://huggingface.co/datasets/TrustHLT/LaCour.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Agent Large Language Models for Conversational Task-Solving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22932v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22932v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonas Becker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In an era where single large language models have dominated the landscape of
artificial intelligence for years, multi-agent systems arise as new
protagonists in conversational task-solving. While previous studies have
showcased their potential in reasoning tasks and creative endeavors, an
analysis of their limitations concerning the conversational paradigms and the
impact of individual agents is missing. It remains unascertained how
multi-agent discussions perform across tasks of varying complexity and how the
structure of these conversations influences the process. To fill that gap, this
work systematically evaluates multi-agent systems across various discussion
paradigms, assessing their strengths and weaknesses in both generative tasks
and question-answering tasks. Alongside the experiments, I propose a taxonomy
of 20 multi-agent research studies from 2022 to 2024, followed by the
introduction of a framework for deploying multi-agent LLMs in conversational
task-solving. I demonstrate that while multi-agent systems excel in complex
reasoning tasks, outperforming a single model by leveraging expert personas,
they fail on basic tasks. Concretely, I identify three challenges that arise:
1) While longer discussions enhance reasoning, agents fail to maintain
conformity to strict task requirements, which leads to problem drift, making
shorter conversations more effective for basic tasks. 2) Prolonged discussions
risk alignment collapse, raising new safety concerns for these systems. 3) I
showcase discussion monopolization through long generations, posing the problem
of fairness in decision-making for tasks like summarization. This work uncovers
both the potential and challenges that arise with multi-agent interaction and
varying conversational paradigms, providing insights into how future research
could improve the efficiency, performance, and safety of multi-agent LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LongVILA: Scaling Long-Context Visual Language Models for Long Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.10188v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.10188v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fuzhao Xue, Yukang Chen, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, Ethan He, Hongxu Yin, Pavlo Molchanov, Jan Kautz, Linxi Fan, Yuke Zhu, Yao Lu, Song Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-context capability is critical for multi-modal foundation models,
especially for long video understanding. We introduce LongVILA, a full-stack
solution for long-context visual-language models by co-designing the algorithm
and system. For model training, we upgrade existing VLMs to support long video
understanding by incorporating two additional stages, i.e., long context
extension and long video supervised fine-tuning. However, training on long
video is computationally and memory intensive. We introduce the long-context
Multi-Modal Sequence Parallelism (MM-SP) system that efficiently parallelizes
long video training and inference, enabling 2M context length training on 256
GPUs without any gradient checkpointing. LongVILA efficiently extends the
number of video frames of VILA from 8 to 2048, improving the long video
captioning score from 2.00 to 3.26 (out of 5), achieving 99.8% accuracy in
6,000-frame (more than 1 million tokens) video needle-in-a-haystack.
LongVILA-7B demonstrates strong accuracy on the VideoMME benchmark, i.e., 61.8%
with subtitle. Besides, MM-SP is 2.1x - 5.7x faster than ring style sequence
parallelism and 1.1x - 1.4x faster than Megatron with a hybrid context and
tensor parallelism. Moreover, it seamlessly integrates with Hugging Face
Transformers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and models are available at
  https://github.com/NVlabs/VILA/blob/main/LongVILA.md</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Aligning Large Language Models with Human Opinions through Persona
  Selection and Value--Belief--Norm Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08385v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08385v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Do Xuan Long, Kenji Kawaguchi, Min-Yen Kan, Nancy F. Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reasoning and predicting human opinions with large language models (LLMs) is
essential yet challenging. Current methods employ role-playing with personae
but face two major issues: LLMs are sensitive to even a single irrelevant
persona, skewing predictions by up to 30%, and LLMs fail to reason
strategically over personae. We propose Chain-of-Opinion (COO), a simple
four-step solution modeling which and how to reason with personae, inspired by
the Value--Belief--Norm (VBN) theory. COO differentiates between explicit
personae (demographics and ideology) and implicit personae (historical
opinions), involves: (1) filtering irrelevant attributes from explicit
personae, (2) ranking implicit personae into a preferential list for selecting
top-k, (3) applying novel VBN reasoning to extract user environmental and
personal value, belief, and norm variables for accurate and reliable
predictions, and (4) iterating VBN reasoning with progressively larger lists of
implicit personae to handle potential persona insufficiency. COO efficiently
achieves new state-of-the-art opinion prediction via prompting with only 5
inference calls, improving prior techniques by up to 4%. Notably, fine-tuning
LMs with COO data results in significantly better opinion-aligned models, by up
to 23%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Du-IN: Discrete units-guided mask modeling for decoding speech from
  Intracranial Neural signals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.11459v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.11459v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Zheng, Hai-Teng Wang, Wei-Bang Jiang, Zhong-Tao Chen, Li He, Pei-Yang Lin, Peng-Hu Wei, Guo-Guang Zhao, Yun-Zhe Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Invasive brain-computer interfaces with Electrocorticography (ECoG) have
shown promise for high-performance speech decoding in medical applications, but
less damaging methods like intracranial stereo-electroencephalography (sEEG)
remain underexplored. With rapid advances in representation learning,
leveraging abundant recordings to enhance speech decoding is increasingly
attractive. However, popular methods often pre-train temporal models based on
brain-level tokens, overlooking that brain activities in different regions are
highly desynchronized during tasks. Alternatively, they pre-train
spatial-temporal models based on channel-level tokens but fail to evaluate them
on challenging tasks like speech decoding, which requires intricate processing
in specific language-related areas. To address this issue, we collected a
well-annotated Chinese word-reading sEEG dataset targeting language-related
brain networks from 12 subjects. Using this benchmark, we developed the Du-IN
model, which extracts contextual embeddings based on region-level tokens
through discrete codex-guided mask modeling. Our model achieves
state-of-the-art performance on the 61-word classification task, surpassing all
baselines. Model comparisons and ablation studies reveal that our design
choices, including (i) temporal modeling based on region-level tokens by
utilizing 1D depthwise convolution to fuse channels in the ventral sensorimotor
cortex (vSMC) and superior temporal gyrus (STG) and (ii) self-supervision
through discrete codex-guided mask modeling, significantly contribute to this
performance. Overall, our approach -- inspired by neuroscience findings and
capitalizing on region-level representations from specific brain regions -- is
suitable for invasive brain modeling and represents a promising neuro-inspired
AI approach in brain-computer interfaces.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Systematic Survey on Large Language Models for Algorithm Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14716v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14716v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fei Liu, Yiming Yao, Ping Guo, Zhiyuan Yang, Zhe Zhao, Xi Lin, Xialiang Tong, Mingxuan Yuan, Zhichao Lu, Zhenkun Wang, Qingfu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Algorithm Design (AD) is crucial for effective problem-solving across various
domains. The advent of Large Language Models (LLMs) has notably enhanced the
automation and innovation within this field, offering new perspectives and
promising solutions. Over the past three years, the integration of LLMs into AD
(LLM4AD) has seen substantial progress, with applications spanning
optimization, machine learning, mathematical reasoning, and scientific
discovery. Given the rapid advancements and expanding scope of this field, a
systematic review is both timely and necessary. This paper provides a
systematic review of LLM4AD. First, we offer an overview and summary of
existing studies. Then, we introduce a taxonomy and review the literature
across four dimensions: the roles of LLMs, search methods, prompt methods, and
application domains with a discussion of potential and achievements of LLMs in
AD. Finally, we identify current challenges and highlight several promising
directions for future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Shortcut-connected Expert Parallelism for Accelerating
  Mixture-of-Experts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.05019v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.05019v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weilin Cai, Juyong Jiang, Le Qin, Junwei Cui, Sunghun Kim, Jiayi Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Expert parallelism has been introduced as a strategy to distribute the
computational workload of sparsely-gated mixture-of-experts (MoE) models across
multiple computing devices, facilitating the execution of these increasingly
large-scale models. However, the All-to-All communication intrinsic to expert
parallelism constitutes a significant overhead, diminishing the MoE models'
efficiency. Current optimization approaches offer some relief, yet they are
constrained by the sequential interdependence of communication and computation
operations. To address this limitation, we present a novel shortcut-connected
MoE (ScMoE) architecture with an overlapping parallel strategy, which
effectively decouples communication from its conventional sequence, allowing
for a substantial overlap of 70% to 100% with computation. When compared with
the prevalent top-2 MoE architecture, ScMoE demonstrates training speed
improvements of 30% and 11%, and inference improvements of 40% and 15%, in our
distributed environments with PCIe and NVLink hardware, respectively, where
communication constitutes 60% and 15% of the total MoE time consumption.
Building on the ScMoE architecture, we further implement an expert offloading
strategy to facilitate memory-limited inference, optimizing latency through the
overlap of expert migration. Additionally, extensive experiments and
theoretical analyses indicate that ScMoE not only achieves comparable but in
some instances surpasses the model quality of existing approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Block Transformer: Global-to-Local Language Modeling for Fast Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.02657v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.02657v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Namgyu Ho, Sangmin Bae, Taehyeon Kim, Hyunjik Jo, Yireun Kim, Tal Schuster, Adam Fisch, James Thorne, Se-Young Yun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the Block Transformer which adopts hierarchical global-to-local
modeling to autoregressive transformers to mitigate the inference bottlenecks
associated with self-attention. Self-attention requires the key-value (KV)
cache of all previous sequences to be retrieved from memory at every decoding
step to retrieve context information, leading to two primary bottlenecks during
batch inference. First, there is a significant delay in obtaining the first
token, as the information of the entire prompt must first be processed to
prefill the KV cache. Second, computation of subsequent tokens is bottlenecked
by the high memory I/O demand of fetching the entire KV cache, which grows
linearly with sequence length, incurring quadratic memory reads overall. We
design the Block Transformer to strategically mitigate these costs, by
incorporating coarsity and locality into an integrated global-to-local
architecture. At the lower layers, we aggregate tokens into fixed size blocks
to apply attention across the entire sequence at coarse-grained detail, to
capture the global context while minimizing KV cache overhead. At upper layers,
we apply attention within each block to decode individual tokens, to model
fine-grained details with a lightweight local KV cache. We pretrain vanilla and
Block Transformers from scratch and demonstrate that Block Transformers reach
10--20x inference throughput compared to vanilla transformers with equivalent
perplexity and zero-shot task performance. Code is available at
https://github.com/itsnamgyu/block-transformer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages, 24 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Robust <span class="highlight-title">Multimodal</span> Sentiment Analysis with Incomplete Data <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.20012v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.20012v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Zhang, Wenbin Wang, Tianshu Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of Multimodal Sentiment Analysis (MSA) has recently witnessed an
emerging direction seeking to tackle the issue of data incompleteness.
Recognizing that the language modality typically contains dense sentiment
information, we consider it as the dominant modality and present an innovative
Language-dominated Noise-resistant Learning Network (LNLN) to achieve robust
MSA. The proposed LNLN features a dominant modality correction (DMC) module and
dominant modality based multimodal learning (DMML) module, which enhances the
model's robustness across various noise scenarios by ensuring the quality of
dominant modality representations. Aside from the methodical design, we perform
comprehensive experiments under random data missing scenarios, utilizing
diverse and meaningful settings on several popular datasets (\textit{e.g.,}
MOSI, MOSEI, and SIMS), providing additional uniformity, transparency, and
fairness compared to existing evaluations in the literature. Empirically, LNLN
consistently outperforms existing baselines, demonstrating superior performance
across these challenging and extensive evaluation metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MAPLE: Mobile App Prediction Leveraging Large Language Model Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.08648v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.08648v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yonchanok Khaokaew, Hao Xue, Flora D. Salim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, predicting mobile app usage has become increasingly
important for areas like app recommendation, user behaviour analysis, and
mobile resource management. Existing models, however, struggle with the
heterogeneous nature of contextual data and the user cold start problem. This
study introduces a novel prediction model, Mobile App Prediction Leveraging
Large Language Model Embeddings (MAPLE), which employs Large Language Models
(LLMs) and installed app similarity to overcome these challenges. MAPLE
utilises the power of LLMs to process contextual data and discern intricate
relationships within it effectively. Additionally, we explore the use of
installed app similarity to address the cold start problem, facilitating the
modelling of user preferences and habits, even for new users with limited
historical data. In essence, our research presents MAPLE as a novel, potent,
and practical approach to app usage prediction, making significant strides in
resolving issues faced by existing models. MAPLE stands out as a comprehensive
and effective solution, setting a new benchmark for more precise and
personalised app usage predictions. In tests on two real-world datasets, MAPLE
surpasses contemporary models in both standard and cold start scenarios. These
outcomes validate MAPLE's capacity for precise app usage predictions and its
resilience against the cold start problem. This enhanced performance stems from
the model's proficiency in capturing complex temporal patterns and leveraging
contextual information. As a result, MAPLE can potentially improve personalised
mobile app usage predictions and user experiences markedly.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ $FastDoc$: Domain-Specific Fast Continual Pre-training Technique using
  Document-Level Metadata and Taxonomy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06190v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06190v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhilash Nandy, Manav Nitin Kapadnis, Sohan Patnaik, Yash Parag Butala, Pawan Goyal, Niloy Ganguly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose $FastDoc$ (Fast Continual Pre-training Technique
using Document Level Metadata and Taxonomy), a novel, compute-efficient
framework that utilizes Document metadata and Domain-Specific Taxonomy as
supervision signals to continually pre-train transformer encoder on a
domain-specific corpus. The main innovation is that during domain-specific
pretraining, an open-domain encoder is continually pre-trained using
sentence-level embeddings as inputs (to accommodate long documents), however,
fine-tuning is done with token-level embeddings as inputs to this encoder. We
perform such domain-specific pre-training on three different domains namely
customer support, scientific, and legal domains, and compare performance on 6
different downstream tasks and 9 different datasets. The novel use of
document-level supervision along with sentence-level embedding input for
pre-training reduces pre-training compute by around $1,000$, $4,500$, and $500$
times compared to MLM and/or NSP in Customer Support, Scientific, and Legal
Domains, respectively. The reduced training time does not lead to a
deterioration in performance. In fact we show that $FastDoc$ either outperforms
or performs on par with several competitive transformer-based baselines in
terms of character-level F1 scores and other automated metrics in the Customer
Support, Scientific, and Legal Domains. Moreover, reduced training aids in
mitigating the risk of catastrophic forgetting. Thus, unlike baselines,
$FastDoc$ shows a negligible drop in performance on open domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Transactions on Machine Learning Research (TMLR), 36
  pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adversarial Representation Engineering: A General Model Editing
  Framework for Large Language Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.13752v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.13752v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihao Zhang, Zeming Wei, Jun Sun, Meng Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since the rapid development of Large Language Models (LLMs) has achieved
remarkable success, understanding and rectifying their internal complex
mechanisms has become an urgent issue. Recent research has attempted to
interpret their behaviors through the lens of inner representation. However,
developing practical and efficient methods for applying these representations
for general and flexible model editing remains challenging. In this work, we
explore how to leverage insights from representation engineering to guide the
editing of LLMs by deploying a representation sensor as an editing oracle. We
first identify the importance of a robust and reliable sensor during editing,
then propose an Adversarial Representation Engineering (ARE) framework to
provide a unified and interpretable approach for conceptual model editing
without compromising baseline performance. Experiments on multiple tasks
demonstrate the effectiveness of ARE in various model editing scenarios. Our
code and data are available at
https://github.com/Zhang-Yihao/Adversarial-Representation-Engineering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LADDER: Language Driven Slice Discovery and Error Rectification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.07832v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.07832v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shantanu Ghosh, Rayan Syed, Chenyu Wang, Clare B. Poynton, Shyam Visweswaran, Kayhan Batmanghelich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Error slice discovery associates structured patterns with model errors.
Existing methods discover error slices by clustering the error-prone samples
with similar patterns or assigning discrete attributes to each sample for
post-hoc analysis. While these methods aim for interpretability and easier
mitigation through reweighting or rebalancing, they may not capture the full
complexity of error patterns due to incomplete or missing attributes. Contrary
to the existing approach, this paper utilizes the reasoning capabilities of the
Large Language Model (LLM) to analyze complex error patterns and generate
testable hypotheses. This paper proposes LADDER: Language Driven slice
Discovery and Error Rectification. It first projects the model's representation
into a language-aligned feature space (eg CLIP) to preserve semantics in the
original model feature space. This ensures the accurate retrieval of sentences
that highlight the model's errors. Next, the LLM utilizes the sentences and
generates hypotheses to discover error slices. Finally, we mitigate the error
by fine-tuning the classification head by creating a group-balanced dataset
using the hypotheses. Our entire method does not require any attribute
annotation, either explicitly or through external tagging models. We validate
our method with \textbf{five} image classification datasets. The code is
available (https://github.com/batmanlab/Ladder).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting the Impact of Pursuing Modularity for Code Generation <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11406v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11406v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deokyeong Kang, Ki Jung Seo, Taeuk Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modular programming, which aims to construct the final program by integrating
smaller, independent building blocks, has been regarded as a desirable practice
in software development. However, with the rise of recent code generation
agents built upon large language models (LLMs), a question emerges: is this
traditional practice equally effective for these new tools? In this work, we
assess the impact of modularity in code generation by introducing a novel
metric for its quantitative measurement. Surprisingly, unlike conventional
wisdom on the topic, we find that modularity is not a core factor for improving
the performance of code generation models. We also explore potential
explanations for why LLMs do not exhibit a preference for modular code compared
to non-modular code.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Faithful Natural Language Explanations: A Study Using Activation
  Patching in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14155v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14155v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Jie Yeo, Ranjan Satapathy, Erik Cambria
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are capable of generating persuasive Natural
Language Explanations (NLEs) to justify their answers. However, the
faithfulness of these explanations should not be readily trusted at face value.
Recent studies have proposed various methods to measure the faithfulness of
NLEs, typically by inserting perturbations at the explanation or feature level.
We argue that these approaches are neither comprehensive nor correctly designed
according to the established definition of faithfulness. Moreover, we highlight
the risks of grounding faithfulness findings on out-of-distribution samples. In
this work, we leverage a causal mediation technique called activation patching,
to measure the faithfulness of an explanation towards supporting the explained
answer. Our proposed metric, Causal Faithfulness quantifies the consistency of
causal attributions between explanations and the corresponding model outputs as
the indicator of faithfulness. We experimented across models varying from 2B to
27B parameters and found that models that underwent alignment tuning tend to
produce more faithful and plausible explanations. We find that Causal
Faithfulness is a promising improvement over existing faithfulness tests by
taking into account the model's internal computations and avoiding out of
distribution concerns that could otherwise undermine the validity of
faithfulness assessments. We release the code in
\url{https://github.com/wj210/Causal-Faithfulness}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AutoManual: Generating Instruction Manuals by LLM Agents via Interactive
  Environmental Learning <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.16247v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.16247v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minghao Chen, Yihang Li, Yanting Yang, Shiyu Yu, Binbin Lin, Xiaofei He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLM) based agents have shown promise in autonomously
completing tasks across various domains, e.g., robotics, games, and web
navigation. However, these agents typically require elaborate design and expert
prompts to solve tasks in specific domains, which limits their adaptability. We
introduce AutoManual, a framework enabling LLM agents to autonomously build
their understanding through interaction and adapt to new environments.
AutoManual categorizes environmental knowledge into diverse rules and optimizes
them in an online fashion by two agents: 1) The Planner codes actionable plans
based on current rules for interacting with the environment. 2) The Builder
updates the rules through a well-structured rule system that facilitates online
rule management and essential detail retention. To mitigate hallucinations in
managing rules, we introduce a *case-conditioned prompting* strategy for the
Builder. Finally, the Formulator agent compiles these rules into a
comprehensive manual. The self-generated manual can not only improve the
adaptability but also guide the planning of smaller LLMs while being
human-readable. Given only one simple demonstration, AutoManual significantly
improves task success rates, achieving 97.4\% with GPT-4-turbo and 86.2\% with
GPT-3.5-turbo on ALFWorld benchmark tasks. The code is available at
https://github.com/minghchen/automanual.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In-Context Transfer Learning: Demonstration Synthesis by Transferring
  Similar Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01548v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01548v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dingzirui Wang, Xuanliang Zhang, Qiguang Chen, Longxu Dou, Xiao Xu, Rongyu Cao, Yingwei Ma, Qingfu Zhu, Wanxiang Che, Binhua Li, Fei Huang, Yongbin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context learning (ICL) is an effective approach to help large language
models (LLMs) adapt to various tasks by providing demonstrations of the target
task. Considering the high cost of labeling demonstrations, many methods
propose synthesizing demonstrations from scratch using LLMs. However, the
quality of the demonstrations synthesized from scratch is limited by the
capabilities and knowledge of LLMs. To address this, inspired by transfer
learning, we propose In-Context Transfer Learning (ICTL), which synthesizes
target task demonstrations by transferring labeled demonstrations from similar
source tasks. ICTL consists of two steps: source sampling and target transfer.
First, we define an optimization objective, which minimizes transfer error to
sample source demonstrations similar to the target task. Then, we employ LLMs
to transfer the sampled source demonstrations to the target task, matching the
definition and format of the target task. Experiments on Super-NI show that
ICTL outperforms synthesis from scratch by 2.0% on average, demonstrating the
effectiveness of our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lightweight Transducer Based on Frame-Level Criterion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13698v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13698v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Genshun Wan, Mengzhi Wang, Tingzhi Mao, Hang Chen, Zhongfu Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The transducer model trained based on sequence-level criterion requires a lot
of memory due to the generation of the large probability matrix. We proposed a
lightweight transducer model based on frame-level criterion, which uses the
results of the CTC forced alignment algorithm to determine the label for each
frame. Then the encoder output can be combined with the decoder output at the
corresponding time, rather than adding each element output by the encoder to
each element output by the decoder as in the transducer. This significantly
reduces memory and computation requirements. To address the problem of
imbalanced classification caused by excessive blanks in the label, we decouple
the blank and non-blank probabilities and truncate the gradient of the blank
classifier to the main network. Experiments on the AISHELL-1 demonstrate that
this enables the lightweight transducer to achieve similar results to
transducer. Additionally, we use richer information to predict the probability
of blank, achieving superior results to transducer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Interspeech 2024, code repository:
  https://github.com/wangmengzhi/Lightweight-Transducer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CRAG -- Comprehensive RAG Benchmark <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04744v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04744v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Yang, Kai Sun, Hao Xin, Yushi Sun, Nikita Bhalla, Xiangsen Chen, Sajal Choudhary, Rongze Daniel Gui, Ziran Will Jiang, Ziyu Jiang, Lingkun Kong, Brian Moran, Jiaqi Wang, Yifan Ethan Xu, An Yan, Chenyu Yang, Eting Yuan, Hanwen Zha, Nan Tang, Lei Chen, Nicolas Scheffer, Yue Liu, Nirav Shah, Rakesh Wanga, Anuj Kumar, Wen-tau Yih, Xin Luna Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) has recently emerged as a promising
solution to alleviate Large Language Model (LLM)'s deficiency in lack of
knowledge. Existing RAG datasets, however, do not adequately represent the
diverse and dynamic nature of real-world Question Answering (QA) tasks. To
bridge this gap, we introduce the Comprehensive RAG Benchmark (CRAG), a factual
question answering benchmark of 4,409 question-answer pairs and mock APIs to
simulate web and Knowledge Graph (KG) search. CRAG is designed to encapsulate a
diverse array of questions across five domains and eight question categories,
reflecting varied entity popularity from popular to long-tail, and temporal
dynamisms ranging from years to seconds. Our evaluation of this benchmark
highlights the gap to fully trustworthy QA. Whereas most advanced LLMs achieve
<=34% accuracy on CRAG, adding RAG in a straightforward manner improves the
accuracy only to 44%. State-of-the-art industry RAG solutions only answer 63%
of questions without any hallucination. CRAG also reveals much lower accuracy
in answering questions regarding facts with higher dynamism, lower popularity,
or higher complexity, suggesting future research directions. The CRAG benchmark
laid the groundwork for a KDD Cup 2024 challenge and attracted thousands of
participants and submissions. We commit to maintaining CRAG to serve research
communities in advancing RAG solutions and general QA solutions. CRAG is
available at https://github.com/facebookresearch/CRAG/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 Datasets and Benchmarks Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TART: An Open-Source Tool-Augmented Framework for Explainable
  Table-based Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11724v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11724v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyuan Lu, Liangming Pan, Yubo Ma, Preslav Nakov, Min-Yen Kan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current Large Language Models (LLMs) exhibit limited ability to understand
table structures and to apply precise numerical reasoning, which is crucial for
tasks such as table question answering (TQA) and table-based fact verification
(TFV). To address these challenges, we introduce our Tool-Augmented Reasoning
framework for Tables (TART), which integrates LLMs with specialized tools. TART
contains three key components: a table formatter to ensure accurate data
representation, a tool maker to develop specific computational tools, and an
explanation generator to maintain explainability. We also present the TOOLTAB
dataset, a new benchmark designed specifically for training LLMs in table-tool
integration. Our experiments indicate that TART achieves substantial
improvements over existing methods (e.g., Chain-of-Thought) by improving both
the precision of data processing and the clarity of the reasoning process.
Notably, TART paired with CodeLlama achieves 90.0% of the accuracy of the
closed-sourced LLM GPT-3.5-turbo, highlighting its robustness in diverse
real-world scenarios. All the code and data are available at
https://github.com/XinyuanLu00/TART.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>technical report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When Large Language Models Meet Vector Databases: A Survey 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.01763v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.01763v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhi Jing, Yongye Su, Yikun Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This survey explores the synergistic potential of Large Language Models
(LLMs) and Vector Databases (VecDBs), a burgeoning but rapidly evolving
research area. With the proliferation of LLMs comes a host of challenges,
including hallucinations, outdated knowledge, prohibitive commercial
application costs, and memory issues. VecDBs emerge as a compelling solution to
these issues by offering an efficient means to store, retrieve, and manage the
high-dimensional vector representations intrinsic to LLM operations. Through
this nuanced review, we delineate the foundational principles of LLMs and
VecDBs and critically analyze their integration's impact on enhancing LLM
functionalities. This discourse extends into a discussion on the speculative
future developments in this domain, aiming to catalyze further research into
optimizing the confluence of LLMs and VecDBs for advanced data handling and
knowledge extraction capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models as Efficient Reward Function Searchers for
  Custom-Environment Multi-Objective Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02428v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02428v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanwen Xie, Jingzehua Xu, Yiyuan Yang, Yimian Ding, Shuai Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Achieving the effective design and improvement of reward functions in
reinforcement learning (RL) tasks with complex custom environments and multiple
requirements presents considerable challenges. In this paper, we propose ERFSL,
an efficient reward function searcher using LLMs, which enables LLMs to be
effective white-box searchers and highlights their advanced semantic
understanding capabilities. Specifically, we generate reward components for
each numerically explicit user requirement and employ a reward critic to
identify the correct code form. Then, LLMs assign weights to the reward
components to balance their values and iteratively adjust the weights without
ambiguity and redundant adjustments by flexibly adopting directional mutation
and crossover strategies, similar to genetic algorithms, based on the context
provided by the training log analyzer. We applied the framework to an
underwater data collection RL task without direct human feedback or reward
examples (zero-shot learning). The reward critic successfully corrects the
reward code with only one feedback instance for each requirement, effectively
preventing unrectifiable errors. The initialization of weights enables the
acquisition of different reward functions within the Pareto solution set
without the need for weight search. Even in cases where a weight is 500 times
off, on average, only 5.2 iterations are needed to meet user requirements. The
ERFSL also works well with most prompts utilizing GPT-4o mini, as we decompose
the weight searching process to reduce the requirement for numerical and
long-context understanding capabilities
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Large Language Models for Suicide Detection on Social Media
  with Limited Labels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04501v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04501v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vy Nguyen, Chau Pham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing frequency of suicidal thoughts highlights the importance of
early detection and intervention. Social media platforms, where users often
share personal experiences and seek help, could be utilized to identify
individuals at risk. However, the large volume of daily posts makes manual
review impractical. This paper explores the use of Large Language Models (LLMs)
to automatically detect suicidal content in text-based social media posts. We
propose a novel method for generating pseudo-labels for unlabeled data by
prompting LLMs, along with traditional classification fine-tuning techniques to
enhance label accuracy. To create a strong suicide detection model, we develop
an ensemble approach involving prompting with Qwen2-72B-Instruct, and using
fine-tuned models such as Llama3-8B, Llama3.1-8B, and Gemma2-9B. We evaluate
our approach on the dataset of the Suicide Ideation Detection on Social Media
Challenge, a track of the IEEE Big Data 2024 Big Data Cup. Additionally, we
conduct a comprehensive analysis to assess the impact of different models and
fine-tuning strategies on detection performance. Experimental results show that
the ensemble model significantly improves the detection accuracy, by 5% points
compared with the individual models. It achieves a weight F1 score of 0.770 on
the public test set, and 0.731 on the private test set, providing a promising
solution for identifying suicidal content in social media. Our analysis shows
that the choice of LLMs affects the prompting performance, with larger models
providing better accuracy. Our code and checkpoints are publicly available at
https://github.com/khanhvynguyen/Suicide_Detection_LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IEEE International Conference on Big Data 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Parameter-Efficient <span class="highlight-title">Fine-Tuning</span> in Large Models: A Survey of
  Methodologies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.19878v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.19878v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luping Wang, Sheng Chen, Linnan Jiang, Shu Pan, Runze Cai, Sen Yang, Fei Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The large models, as predicted by scaling raw forecasts, have made
groundbreaking progress in many fields, particularly in natural language
generation tasks, where they have approached or even surpassed human levels.
However, the unprecedented scale of their parameters brings significant
computational and storage costs. These large models require substantial
computational resources and GPU memory to operate. When adapting large models
to specific downstream tasks, their massive parameter scale poses a significant
challenge in fine-tuning on hardware platforms with limited computational power
and GPU memory. To address this issue, Parameter-Efficient Fine-Tuning (PEFT)
offers a practical solution by efficiently adjusting the parameters of large
pre-trained models to suit various downstream tasks. Specifically, PEFT adjusts
the parameters of pre-trained large models to adapt to specific tasks or
domains, minimizing the introduction of additional parameters and the
computational resources required. This review mainly introduces the preliminary
knowledge of PEFT, the core ideas and principles of various PEFT algorithms,
the applications of PEFT, and potential future research directions. By reading
this review, we believe that interested parties can quickly grasp the PEFT
methodology, thereby accelerating its development and innovation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Channel-Wise Mixed-Precision Quantization for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13056v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13056v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihan Chen, Bike Xie, Jundong Li, Cong Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated remarkable success across a
wide range of language tasks, but their deployment on edge devices remains
challenging due to the substantial memory requirements imposed by their large
parameter sizes. Weight-only quantization presents a promising solution to
reduce the memory footprint of LLMs. However, existing approaches primarily
focus on integer-bit quantization, limiting their adaptability to
fractional-bit quantization tasks and preventing the full utilization of
available storage space on devices. In this paper, we introduce Channel-Wise
Mixed-Precision Quantization (CMPQ), a novel mixed-precision quantization
method that allocates quantization precision in a channel-wise pattern based on
activation distributions. By assigning different precision levels to different
weight channels, CMPQ can adapt to any bit-width constraint. CMPQ employs a
non-uniform quantization strategy and incorporates two outlier extraction
techniques that collaboratively preserve the critical information, thereby
minimizing the quantization loss. Experiments on different sizes of LLMs
demonstrate that CMPQ not only enhances performance in integer-bit quantization
tasks but also achieves significant performance gains with a modest increase in
memory usage. CMPQ thus represents an adaptive and effective approach to LLM
quantization, offering substantial benefits across diverse device capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM-ESR: Large Language Models Enhancement for Long-tailed Sequential
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.20646v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.20646v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qidong Liu, Xian Wu, Yejing Wang, Zijian Zhang, Feng Tian, Yefeng Zheng, Xiangyu Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommender systems (SRS) aim to predict users' subsequent choices
based on their historical interactions and have found applications in diverse
fields such as e-commerce and social media. However, in real-world systems,
most users interact with only a handful of items, while the majority of items
are seldom consumed. These two issues, known as the long-tail user and
long-tail item challenges, often pose difficulties for existing SRS. These
challenges can adversely affect user experience and seller benefits, making
them crucial to address. Though a few works have addressed the challenges, they
still struggle with the seesaw or noisy issues due to the intrinsic scarcity of
interactions. The advancements in large language models (LLMs) present a
promising solution to these problems from a semantic perspective. As one of the
pioneers in this field, we propose the Large Language Models Enhancement
framework for Sequential Recommendation (LLM-ESR). This framework utilizes
semantic embeddings derived from LLMs to enhance SRS without adding extra
inference load from LLMs. To address the long-tail item challenge, we design a
dual-view modeling framework that combines semantics from LLMs and
collaborative signals from conventional SRS. For the long-tail user challenge,
we propose a retrieval augmented self-distillation method to enhance user
preference representation using more informative interactions from similar
users. To verify the effectiveness and versatility of our proposed enhancement
framework, we conduct extensive experiments on three real-world datasets using
three popular SRS models. The results show that our method surpasses existing
baselines consistently, and benefits long-tail users and items especially. The
implementation code is available at
https://github.com/Applied-Machine-Learning-Lab/LLM-ESR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by NeruIPS'24 (Spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Re-Label Method For Data-Centric Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.04391v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.04391v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tong Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In industry deep learning application, our manually labeled data has a
certain number of noisy data. To solve this problem and achieve more than 90
score in dev dataset, we present a simple method to find the noisy data and
re-label the noisy data by human, given the model predictions as references in
human labeling. In this paper, we illustrate our idea for a broad set of deep
learning tasks, includes classification, sequence tagging, object detection,
sequence generation, click-through rate prediction. The dev dataset evaluation
results and human evaluation results verify our idea.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ProSwitch: Knowledge-Guided Instruction <span class="highlight-title">Tuning</span> to Switch Between
  Professional and Non-Professional Answers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09131v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09131v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang Zong, Yuyan Chen, Weiming Lu, Jian Shao, Yueting Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated efficacy in various linguistic
applications, including text summarization and controlled text generation.
However, studies into their capacity of switching between styles via
instruction tuning remain underexplored. This study concentrates on the
style-switching abilities of LLMs and introduces a novel approach, named
ProSwitch, which enables a language model to switch between professional and
non-professional answers, by tuning and evaluating through the guidance of
domain and style knowledge. ProSwitch unfolds across three phases:
LLM-augmented preparation to collect domain knowledge and QA pairs, instruction
tuning to optimize LLMs with multiple levels of knowledge, and comprehensive
evaluation to assess both style discrimination and reference-based quality of
generated text. Comparative analysis of ProSwitch against general and
specialized LLMs reveals that our approach outperforms baselines in switching
between professional and non-professional answers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages main body, 16 pages total</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.13623v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.13623v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaofan Tao, Qian Liu, Longxu Dou, Niklas Muennighoff, Zhongwei Wan, Ping Luo, Min Lin, Ngai Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research on scaling large language models (LLMs) has primarily focused on
model parameters and training data size, overlooking the role of vocabulary
size. We investigate how vocabulary size impacts LLM scaling laws by training
models ranging from 33M to 3B parameters on up to 500B characters with various
vocabulary configurations. We propose three complementary approaches for
predicting the compute-optimal vocabulary size: IsoFLOPs analysis, derivative
estimation, and parametric fit of the loss function. Our approaches converge on
the conclusion that the optimal vocabulary size depends on the compute budget,
with larger models requiring larger vocabularies. Most LLMs, however, use
insufficient vocabulary sizes. For example, we predict that the optimal
vocabulary size of Llama2-70B should have been at least 216K, 7 times larger
than its vocabulary of 32K. We validate our predictions empirically by training
models with 3B parameters across different FLOPs budgets. Adopting our
predicted optimal vocabulary size consistently improves downstream performance
over commonly used vocabulary sizes. By increasing the vocabulary size from the
conventional 32K to 43K, we improve performance on ARC-Challenge from 29.1 to
32.0 with the same 2.3e21 FLOPs. Our work highlights the importance of jointly
considering tokenization and model scaling for efficient pre-training. The code
and demo are available at https://github.com/sail-sg/scaling-with-vocab and
https://hf.co/spaces/sail/scaling-with-vocab-demo.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MoA: Mixture of Sparse Attention for Automatic Large Language Model
  Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14909v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14909v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyu Fu, Haofeng Huang, Xuefei Ning, Genghan Zhang, Boju Chen, Tianqi Wu, Hongyi Wang, Zixiao Huang, Shiyao Li, Shengen Yan, Guohao Dai, Huazhong Yang, Yu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse attention can effectively mitigate the significant memory and
throughput demands of Large Language Models (LLMs) in long contexts. Existing
methods typically employ a uniform sparse attention mask, applying the same
sparse pattern across different attention heads and input lengths. However,
this uniform approach fails to capture the diverse attention patterns inherent
in LLMs, ignoring their distinct accuracy-latency trade-offs. To address this
challenge, we propose the Mixture of Attention (MoA), which automatically
tailors distinct sparse attention configurations to different heads and layers.
MoA constructs and navigates a search space of various attention patterns and
their scaling rules relative to input sequence lengths. It profiles the model,
evaluates potential configurations, and pinpoints the optimal sparse attention
compression plan. MoA adapts to varying input sizes, revealing that some
attention heads expand their focus to accommodate longer sequences, while other
heads consistently concentrate on fixed-length local contexts. Experiments show
that MoA increases the effective context length by $3.9\times$ with the same
average attention span, boosting retrieval accuracy by $1.5-7.1\times$ over the
uniform-attention baseline across Vicuna-{7B,13B}, and Llama3-{8B,70B} models.
Moreover, MoA narrows the capability gaps between sparse and dense models,
reducing the maximum relative performance drop from $9\%-36\%$ to within $5\%$
across two long-context understanding benchmarks. MoA achieves a
$1.2-1.4\times$ GPU memory reduction, boosting decode throughput by
$6.6-8.2\times$ and $1.7-1.9\times$ compared to FlashAttention2 and vLLM, with
minimal impact on performance. Our code is available at
\url{https://github.com/thu-nics/MoA}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CMM-Math: A Chinese <span class="highlight-title">Multimodal</span> Math Dataset To Evaluate and Enhance the
  Mathematics Reasoning of Large <span class="highlight-title">Multimodal</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02834v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02834v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wentao Liu, Qianjun Pan, Yi Zhang, Zhuo Liu, Ji Wu, Jie Zhou, Aimin Zhou, Qin Chen, Bo Jiang, Liang He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have obtained promising results in mathematical
reasoning, which is a foundational skill for human intelligence. Most previous
studies focus on improving and measuring the performance of LLMs based on
textual math reasoning datasets (e.g., MATH, GSM8K). Recently, a few
researchers have released English multimodal math datasets (e.g., MATHVISTA and
MATH-V) to evaluate the effectiveness of large multimodal models (LMMs). In
this paper, we release a Chinese multimodal math (CMM-Math) dataset, including
benchmark and training parts, to evaluate and enhance the mathematical
reasoning of LMMs. CMM-Math contains over 28,000 high-quality samples,
featuring a variety of problem types (e.g., multiple-choice, fill-in-the-blank,
and so on) with detailed solutions across 12 grade levels from elementary to
high school in China. Specifically, the visual context may be present in the
questions or opinions, which makes this dataset more challenging. Through
comprehensive analysis, we discover that state-of-the-art LMMs on the CMM-Math
dataset face challenges, emphasizing the necessity for further improvements in
LMM development. We also propose a Multimodal Mathematical LMM (Math-LMM) to
handle the problems with mixed input of multiple images and text segments. We
train our model using three stages, including foundational pre-training,
foundational fine-tuning, and mathematical fine-tuning. The extensive
experiments indicate that our model effectively improves math reasoning
performance by comparing it with the SOTA LMMs over three multimodal
mathematical datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Plurals: A System for Guiding LLMs Via Simulated Social Ensembles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17213v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17213v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joshua Ashkinaze, Emily Fry, Narendra Edara, Eric Gilbert, Ceren Budak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent debates raised concerns that language models may favor certain
viewpoints. But what if the solution is not to aim for a 'view from nowhere'
but rather to leverage different viewpoints? We introduce Plurals, a system and
Python library for pluralistic AI deliberation. Plurals consists of Agents
(LLMs, optionally with personas) which deliberate within customizable
Structures, with Moderators overseeing deliberation. Plurals is a generator of
simulated social ensembles. Plurals integrates with government datasets to
create nationally representative personas, includes deliberation templates
inspired by democratic deliberation theory, and allows users to customize both
information-sharing structures and deliberation behavior within Structures. Six
case studies demonstrate fidelity to theoretical constructs and efficacy. Three
randomized experiments show simulated focus groups produced output resonant
with an online sample of the relevant audiences (chosen over zero-shot
generation in 75% of trials). Plurals is both a paradigm and a concrete system
for pluralistic AI. The Plurals library is available at
https://github.com/josh-ashkinaze/plurals and will be continually updated.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-environment Topic Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24126v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24126v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominic Sobhani, Amir Feder, David Blei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Probabilistic topic models are a powerful tool for extracting latent themes
from large text datasets. In many text datasets, we also observe per-document
covariates (e.g., source, style, political affiliation) that act as
environments that modulate a "global" (environment-agnostic) topic
representation. Accurately learning these representations is important for
prediction on new documents in unseen environments and for estimating the
causal effect of topics on real-world outcomes. To this end, we introduce the
Multi-environment Topic Model (MTM), an unsupervised probabilistic model that
separates global and environment-specific terms. Through experimentation on
various political content, from ads to tweets and speeches, we show that the
MTM produces interpretable global topics with distinct environment-specific
words. On multi-environment data, the MTM outperforms strong baselines in and
out-of-distribution. It also enables the discovery of accurate causal effects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Is your benchmark truly adversarial? AdvScore: Evaluating Human-Grounded
  Adversarialness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16342v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16342v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yoo Yeon Sung, Maharshi Gor, Eve Fleisig, Ishani Mondal, Jordan Lee Boyd-Graber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial datasets should ensure AI robustness that matches human
performance. However, as models evolve, datasets can become obsolete. Thus,
adversarial datasets should be periodically updated based on their degradation
in adversarialness. Given the lack of a standardized metric for measuring
adversarialness, we propose AdvScore, a human-grounded evaluation metric.
AdvScore assesses a dataset's true adversarialness by capturing models' and
humans' varying abilities, while also identifying poor examples. AdvScore then
motivates a new dataset creation pipeline for realistic and high-quality
adversarial samples, enabling us to collect an adversarial question answering
(QA) dataset, AdvQA. We apply AdvScore using 9,347 human responses and ten
language model predictions to track the models' improvement over five years
(from 2020 to 2024). AdvScore assesses whether adversarial datasets remain
suitable for model evaluation, measures model improvements, and provides
guidance for better alignment with human capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2401.11185</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Table Transformers for Imputing Textual Attributes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02128v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02128v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ting-Ruen Wei, Yuan Wang, Yoshitaka Inoue, Hsin-Tai Wu, Yi Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Missing data in tabular dataset is a common issue as the performance of
downstream tasks usually depends on the completeness of the training dataset.
Previous missing data imputation methods focus on numeric and categorical
columns, but we propose a novel end-to-end approach called Table Transformers
for Imputing Textual Attributes (TTITA) based on the transformer to impute
unstructured textual columns using other columns in the table. We conduct
extensive experiments on three datasets, and our approach shows competitive
performance outperforming baseline models such as recurrent neural networks and
Llama2. The performance improvement is more significant when the target
sequence has a longer length. Additionally, we incorporate multi-task learning
to simultaneously impute for heterogeneous columns, boosting the performance
for text imputation. We also qualitatively compare with ChatGPT for realistic
applications.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">46</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VascX Models: Model Ensembles for Retinal Vascular Analysis from Color
  Fundus Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16016v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16016v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jose Vargas Quiros, Bart Liefers, Karin van Garderen, Jeroen Vermeulen, Eyened Reading Center, Sinergia Consortium, Caroline Klaver
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce VascX models, a comprehensive set of model ensembles for
analyzing retinal vasculature from color fundus images (CFIs). Annotated CFIs
were aggregated from public datasets . Additional CFIs, mainly from the
population-based Rotterdam Study were annotated by graders for arteries and
veins at pixel level, resulting in a dataset diverse in patient demographics
and imaging conditions. VascX models demonstrated superior segmentation
performance across datasets, image quality levels, and anatomic regions when
compared to existing, publicly available models, likely due to the increased
size and variety of our training set. Important improvements were observed in
artery-vein and disc segmentation performance, particularly in segmentations of
these structures on CFIs of intermediate quality, common in large cohorts and
clinical datasets. Importantly, these improvements translated into
significantly more accurate vascular features when we compared features
extracted from VascX segmentation masks with features extracted from
segmentation masks generated by previous models. With VascX models we provide a
robust, ready-to-use set of model ensembles and inference code aimed at
simplifying the implementation and enhancing the quality of automated retinal
vasculature analyses. The precise vessel parameters generated by the model can
serve as starting points for the identification of disease patterns in and
outside of the eye.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DELTA: Dense Efficient Long-range 3D Tracking for any video 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24211v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24211v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tuan Duc Ngo, Peiye Zhuang, Chuang Gan, Evangelos Kalogerakis, Sergey Tulyakov, Hsin-Ying Lee, Chaoyang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tracking dense 3D motion from monocular videos remains challenging,
particularly when aiming for pixel-level precision over long sequences. We
introduce DELTA, a novel method that efficiently tracks every pixel in 3D
space, enabling accurate motion estimation across entire videos. Our approach
leverages a joint global-local attention mechanism for reduced-resolution
tracking, followed by a transformer-based upsampler to achieve high-resolution
predictions. Unlike existing methods, which are limited by computational
inefficiency or sparse tracking, DELTA delivers dense 3D tracking at scale,
running over 8x faster than previous methods while achieving state-of-the-art
accuracy. Furthermore, we explore the impact of depth representation on
tracking performance and identify log-depth as the optimal choice. Extensive
experiments demonstrate the superiority of DELTA on multiple benchmarks,
achieving new state-of-the-art results in both 2D and 3D dense tracking tasks.
Our method provides a robust solution for applications requiring fine-grained,
long-term motion tracking in 3D space.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://snap-research.github.io/DELTA/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BehAVE: Behaviour Alignment of Video Game Encodings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.01335v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.01335v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nemanja Rašajski, Chintan Trivedi, Konstantinos Makantasis, Antonios Liapis, Georgios N. Yannakakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain randomisation enhances the transferability of vision models across
visually distinct domains with similar content. However, current methods
heavily depend on intricate simulation engines, hampering feasibility and
scalability. This paper introduces BehAVE, a video understanding framework that
utilises existing commercial video games for domain randomisation without
accessing their simulation engines. BehAVE taps into the visual diversity of
video games for randomisation and uses textual descriptions of player actions
to align videos with similar content. We evaluate BehAVE across 25 first-person
shooter (FPS) games using various video and text foundation models,
demonstrating its robustness in domain randomisation. BehAVE effectively aligns
player behavioural patterns and achieves zero-shot transfer to multiple unseen
FPS games when trained on just one game. In a more challenging scenario, BehAVE
enhances the zero-shot transferability of foundation models to unseen FPS
games, even when trained on a game of a different genre, with improvements of
up to 22%. BehAVE is available online at https://github.com/nrasajski/BehAVE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Aligning Motion-Blurred Images Using Contrastive Learning on
  Overcomplete Pixels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07410v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07410v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonid Pogorelyuk, Stefan T. Radev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new contrastive objective for learning overcomplete pixel-level
features that are invariant to motion blur. Other invariances (e.g., pose,
illumination, or weather) can be learned by applying the corresponding
transformations on unlabeled images during self-supervised training. We
showcase that a simple U-Net trained with our objective can produce local
features useful for aligning the frames of an unseen video captured with a
moving camera under realistic and challenging conditions. Using a carefully
designed toy example, we also show that the overcomplete pixels can encode the
identity of objects in an image and the pixel coordinates relative to these
objects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GeoSplatting: Towards Geometry Guided Gaussian Splatting for
  Physically-based Inverse Rendering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24204v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24204v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Ye, Chong Gao, Guanbin Li, Wenzheng Chen, Baoquan Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of physically-based inverse rendering using 3D
Gaussian Splatting (3DGS) representations. While recent 3DGS methods have
achieved remarkable results in novel view synthesis (NVS), accurately capturing
high-fidelity geometry, physically interpretable materials and lighting remains
challenging, as it requires precise geometry modeling to provide accurate
surface normals, along with physically-based rendering (PBR) techniques to
ensure correct material and lighting disentanglement. Previous 3DGS methods
resort to approximating surface normals, but often struggle with noisy local
geometry, leading to inaccurate normal estimation and suboptimal
material-lighting decomposition. In this paper, we introduce GeoSplatting, a
novel hybrid representation that augments 3DGS with explicit geometric guidance
and differentiable PBR equations. Specifically, we bridge isosurface and 3DGS
together, where we first extract isosurface mesh from a scalar field, then
convert it into 3DGS points and formulate PBR equations for them in a fully
differentiable manner. In GeoSplatting, 3DGS is grounded on the mesh geometry,
enabling precise surface normal modeling, which facilitates the use of PBR
frameworks for material decomposition. This approach further maintains the
efficiency and quality of NVS from 3DGS while ensuring accurate geometry from
the isosurface. Comprehensive evaluations across diverse datasets demonstrate
the superiority of GeoSplatting, consistently outperforming existing methods
both quantitatively and qualitatively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://pku-vcl-geometry.github.io/GeoSplatting/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HENASY: Learning to Assemble Scene-Entities for Egocentric
  Video-Language Model <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.00307v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.00307v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khoa Vo, Thinh Phan, Kashu Yamazaki, Minh Tran, Ngan Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current video-language models (VLMs) rely extensively on instance-level
alignment between video and language modalities, which presents two major
limitations: (1) visual reasoning disobeys the natural perception that humans
do in first-person perspective, leading to a lack of reasoning interpretation;
and (2) learning is limited in capturing inherent fine-grained relationships
between two modalities.
  In this paper, we take an inspiration from human perception and explore a
compositional approach for egocentric video representation. We introduce HENASY
(Hierarchical ENtities ASsemblY), which includes a spatiotemporal token
grouping mechanism to explicitly assemble dynamically evolving scene entities
through time and model their relationship for video representation. By
leveraging compositional structure understanding, HENASY possesses strong
interpretability via visual grounding with free-form text queries. We further
explore a suite of multi-grained contrastive losses to facilitate
entity-centric understandings. This comprises three alignment types:
video-narration, noun-entity, verb-entities alignments.
  Our method demonstrates strong interpretability in both quantitative and
qualitative experiments; while maintaining competitive performances on five
downstream tasks via zero-shot transfer or as video/text representation,
including video/text retrieval, action recognition, multi-choice query, natural
language query, and moments query.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CaptainCook4D: A Dataset for Understanding Errors in Procedural
  Activities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.14556v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.14556v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohith Peddi, Shivvrat Arya, Bharath Challa, Likhitha Pallapothula, Akshay Vyas, Bhavya Gouripeddi, Jikai Wang, Qifan Zhang, Vasundhara Komaragiri, Eric Ragan, Nicholas Ruozzi, Yu Xiang, Vibhav Gogate
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Following step-by-step procedures is an essential component of various
activities carried out by individuals in their daily lives. These procedures
serve as a guiding framework that helps to achieve goals efficiently, whether
it is assembling furniture or preparing a recipe. However, the complexity and
duration of procedural activities inherently increase the likelihood of making
errors. Understanding such procedural activities from a sequence of frames is a
challenging task that demands an accurate interpretation of visual information
and the ability to reason about the structure of the activity. To this end, we
collect a new egocentric 4D dataset, CaptainCook4D, comprising 384 recordings
(94.5 hours) of people performing recipes in real kitchen environments. This
dataset consists of two distinct types of activity: one in which participants
adhere to the provided recipe instructions and another in which they deviate
and induce errors. We provide 5.3K step annotations and 10K fine-grained action
annotations and benchmark the dataset for the following tasks: supervised error
recognition, multistep localization, and procedure learning
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 2024 Neural Information Processing Systems Datasets
  and Benchmarks Track, Project Page:
  https://captaincook4d.github.io/captain-cook/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Comparing YOLO11 and YOLOv8 for instance segmentation of occluded and
  non-occluded immature green fruits in complex orchard environment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.19869v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.19869v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ranjan Sapkota, Manoj Karkee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study conducted a comprehensive performance evaluation on YOLO11 and
YOLOv8, the latest in the "You Only Look Once" (YOLO) series, focusing on their
instance segmentation capabilities for immature green apples in orchard
environments. YOLO11n-seg achieved the highest mask precision across all
categories with a notable score of 0.831, highlighting its effectiveness in
fruit detection. YOLO11m-seg and YOLO11l-seg excelled in non-occluded and
occluded fruitlet segmentation with scores of 0.851 and 0.829, respectively.
Additionally, YOLO11x-seg led in mask recall for all categories, achieving a
score of 0.815, with YOLO11m-seg performing best for non-occluded immature
green fruitlets at 0.858 and YOLOv8x-seg leading the occluded category with
0.800. In terms of mean average precision at a 50\% intersection over union
(mAP@50), YOLO11m-seg consistently outperformed, registering the highest scores
for both box and mask segmentation, at 0.876 and 0.860 for the "All" class and
0.908 and 0.909 for non-occluded immature fruitlets, respectively. YOLO11l-seg
and YOLOv8l-seg shared the top box mAP@50 for occluded immature fruitlets at
0.847, while YOLO11m-seg achieved the highest mask mAP@50 of 0.810. Despite the
advancements in YOLO11, YOLOv8n surpassed its counterparts in image processing
speed, with an impressive inference speed of 3.3 milliseconds, compared to the
fastest YOLO11 series model at 4.8 milliseconds, underscoring its suitability
for real-time agricultural applications related to complex green fruit
environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 Pages, 10 Figures, 3 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Digital Twins in Additive Manufacturing: A Systematic Review 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.00877v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.00877v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Manjurul Ahsan, Yingtao Liu, Shivakumar Raman, Zahed Siddique
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Digital Twins (DTs) are becoming popular in Additive Manufacturing (AM) due
to their ability to create virtual replicas of physical components of AM
machines, which helps in real-time production monitoring. Advanced techniques
such as Machine Learning (ML), Augmented Reality (AR), and simulation-based
models play key roles in developing intelligent and adaptable DTs in
manufacturing processes. However, questions remain regarding scalability, the
integration of high-quality data, and the computational power required for
real-time applications in developing DTs. Understanding the current state of
DTs in AM is essential to address these challenges and fully utilize their
potential in advancing AM processes. Considering this opportunity, this work
aims to provide a comprehensive overview of DTs in AM by addressing the
following four research questions: (1) What are the key types of DTs used in AM
and their specific applications? (2) What are the recent developments and
implementations of DTs? (3) How are DTs employed in process improvement and
hybrid manufacturing? (4) How are DTs integrated with Industry 4.0
technologies? By discussing current applications and techniques, we aim to
offer a better understanding and potential future research directions for
researchers and practitioners in AM and DTs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A survey on deep learning in medical image registration: new
  technologies, uncertainty, evaluation metrics, and beyond 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.15615v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.15615v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyu Chen, Yihao Liu, Shuwen Wei, Zhangxing Bian, Shalini Subramanian, Aaron Carass, Jerry L. Prince, Yong Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning technologies have dramatically reshaped the field of medical
image registration over the past decade. The initial developments, such as
regression-based and U-Net-based networks, established the foundation for deep
learning in image registration. Subsequent progress has been made in various
aspects of deep learning-based registration, including similarity measures,
deformation regularizations, network architectures, and uncertainty estimation.
These advancements have not only enriched the field of image registration but
have also facilitated its application in a wide range of tasks, including atlas
construction, multi-atlas segmentation, motion estimation, and 2D-3D
registration. In this paper, we present a comprehensive overview of the most
recent advancements in deep learning-based image registration. We begin with a
concise introduction to the core concepts of deep learning-based image
registration. Then, we delve into innovative network architectures, loss
functions specific to registration, and methods for estimating registration
uncertainty. Additionally, this paper explores appropriate evaluation metrics
for assessing the performance of deep learning models in registration tasks.
Finally, we highlight the practical applications of these novel techniques in
medical imaging and discuss the future prospects of deep learning-based image
registration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Medical Image Analysis ((c) MedIA). A list of
  open-sourced code from the papers reviewed has been organized and is
  available at https://bit.ly/3QgFJ9z</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DenoiseRep: Denoising Model for Representation Learning <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.08773v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.08773v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengrui Xu, Guan'an Wang, Xiaowen Huang, Jitao Sang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The denoising model has been proven a powerful generative model but has
little exploration of discriminative tasks. Representation learning is
important in discriminative tasks, which is defined as "learning
representations (or features) of the data that make it easier to extract useful
information when building classifiers or other predictors". In this paper, we
propose a novel Denoising Model for Representation Learning (DenoiseRep) to
improve feature discrimination with joint feature extraction and denoising.
DenoiseRep views each embedding layer in a backbone as a denoising layer,
processing the cascaded embedding layers as if we are recursively denoise
features step-by-step. This unifies the frameworks of feature extraction and
denoising, where the former progressively embeds features from low-level to
high-level, and the latter recursively denoises features step-by-step. After
that, DenoiseRep fuses the parameters of feature extraction and denoising
layers, and theoretically demonstrates its equivalence before and after the
fusion, thus making feature denoising computation-free. DenoiseRep is a
label-free algorithm that incrementally improves features but also
complementary to the label if available. Experimental results on various
discriminative vision tasks, including re-identification (Market-1501,
DukeMTMC-reID, MSMT17, CUHK-03, vehicleID), image classification (ImageNet,
UB200, Oxford-Pet, Flowers), object detection (COCO), image segmentation
(ADE20K) show stability and impressive improvements. We also validate its
effectiveness on the CNN (ResNet) and Transformer (ViT, Swin, Vmamda)
architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024,oral</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Return of Unconditional Generation: A Self-supervised Representation
  Generation Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.03701v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.03701v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianhong Li, Dina Katabi, <span class="highlight-author">Kaiming</span> He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unconditional generation -- the problem of modeling data distribution without
relying on human-annotated labels -- is a long-standing and fundamental
challenge in generative models, creating a potential of learning from
large-scale unlabeled data. In the literature, the generation quality of an
unconditional method has been much worse than that of its conditional
counterpart. This gap can be attributed to the lack of semantic information
provided by labels. In this work, we show that one can close this gap by
generating semantic representations in the representation space produced by a
self-supervised encoder. These representations can be used to condition the
image generator. This framework, called Representation-Conditioned Generation
(RCG), provides an effective solution to the unconditional generation problem
without using labels. Through comprehensive experiments, we observe that RCG
significantly improves unconditional generation quality: e.g., it achieves a
new state-of-the-art FID of 2.15 on ImageNet 256x256, largely reducing the
previous best of 5.91 by a relative 64%. Our unconditional results are situated
in the same tier as the leading class-conditional ones. We hope these
encouraging observations will attract the community's attention to the
fundamental problem of unconditional generation. Code is available at
https://github.com/LTH14/rcg.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Neurips 2024 (Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Erasing Self-Supervised Learning Backdoor by Cluster Activation Masking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.07955v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.07955v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengsheng Qian, Dizhan Xue, Yifei Wang, Shengjie Zhang, Huaiwen Zhang, Changsheng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-Supervised Learning (SSL) is an effective paradigm for learning
representations from unlabeled data, such as text, images, and videos. However,
researchers have recently found that SSL is vulnerable to backdoor attacks. The
attacker can embed hidden SSL backdoors via a few poisoned examples in the
training dataset and maliciously manipulate the behavior of downstream models.
To defend against SSL backdoor attacks, a feasible route is to detect and
remove the poisonous samples in the training set. However, the existing SSL
backdoor defense method fails to detect the poisonous samples precisely. In
this paper, we propose to erase the SSL backdoor by cluster activation masking
and propose a novel PoisonCAM method. After obtaining the threat model trained
on the poisoned dataset, our method can precisely detect poisonous samples
based on the assumption that masking the backdoor trigger can effectively
change the activation of a downstream clustering model. In experiments, our
PoisonCAM achieves 96\% accuracy for backdoor trigger detection compared to 3\%
of the state-of-the-art method on poisoned ImageNet-100. Moreover, our proposed
PoisonCAM significantly improves the performance of the trained SSL model under
backdoor attacks compared to the state-of-the-art method. Our code, data, and
trained models will be open once this paper is accepted.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Autoregressive Image Generation without Vector Quantization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11838v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11838v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, <span class="highlight-author">Kaiming</span> He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional wisdom holds that autoregressive models for image generation are
typically accompanied by vector-quantized tokens. We observe that while a
discrete-valued space can facilitate representing a categorical distribution,
it is not a necessity for autoregressive modeling. In this work, we propose to
model the per-token probability distribution using a diffusion procedure, which
allows us to apply autoregressive models in a continuous-valued space. Rather
than using categorical cross-entropy loss, we define a Diffusion Loss function
to model the per-token probability. This approach eliminates the need for
discrete-valued tokenizers. We evaluate its effectiveness across a wide range
of cases, including standard autoregressive models and generalized masked
autoregressive (MAR) variants. By removing vector quantization, our image
generator achieves strong results while enjoying the speed advantage of
sequence modeling. We hope this work will motivate the use of autoregressive
generation in other continuous-valued domains and applications. Code is
available at: https://github.com/LTH14/mar.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Neurips 2024 (Spotlight). Code: https://github.com/LTH14/mar</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Disentangling spatio-temporal knowledge for weakly supervised object
  detection and segmentation in surgical video <span class="chip">WACV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15794v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15794v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guiqiu Liao, Matjaz Jogan, Sai Koushik, Eric Eaton, Daniel A. Hashimoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Weakly supervised video object segmentation (WSVOS) enables the
identification of segmentation maps without requiring an extensive training
dataset of object masks, relying instead on coarse video labels indicating
object presence. Current state-of-the-art methods either require multiple
independent stages of processing that employ motion cues or, in the case of
end-to-end trainable networks, lack in segmentation accuracy, in part due to
the difficulty of learning segmentation maps from videos with transient object
presence. This limits the application of WSVOS for semantic annotation of
surgical videos where multiple surgical tools frequently move in and out of the
field of view, a problem that is more difficult than typically encountered in
WSVOS. This paper introduces Video Spatio-Temporal Disentanglement Networks
(VDST-Net), a framework to disentangle spatiotemporal information using
semi-decoupled knowledge distillation to predict high-quality class activation
maps (CAMs). A teacher network designed to resolve temporal conflicts when
specifics about object location and timing in the video are not provided works
with a student network that integrates information over time by leveraging
temporal dependencies. We demonstrate the efficacy of our framework on a public
reference dataset and on a more challenging surgical video dataset where
objects are, on average, present in less than 60\% of annotated frames. Our
method outperforms state-of-the-art techniques and generates superior
segmentation masks under video-level weak supervision.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE/CVF Winter Conference on Applications of Computer
  Vision (WACV)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ConvBKI: Real-Time Probabilistic Semantic Mapping Network with
  Quantifiable Uncertainty 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.16020v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.16020v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joey Wilson, Yuewei Fu, Joshua Friesen, Parker Ewen, Andrew Capodieci, Paramsothy Jayakumar, Kira Barton, Maani Ghaffari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we develop a modular neural network for real-time
{\color{black}(> 10 Hz)} semantic mapping in uncertain environments, which
explicitly updates per-voxel probabilistic distributions within a neural
network layer. Our approach combines the reliability of classical probabilistic
algorithms with the performance and efficiency of modern neural networks.
Although robotic perception is often divided between modern differentiable
methods and classical explicit methods, a union of both is necessary for
real-time and trustworthy performance. We introduce a novel Convolutional
Bayesian Kernel Inference (ConvBKI) layer which incorporates semantic
segmentation predictions online into a 3D map through a depthwise convolution
layer by leveraging conjugate priors. We compare ConvBKI against
state-of-the-art deep learning approaches and probabilistic algorithms for
mapping to evaluate reliability and performance. We also create a Robot
Operating System (ROS) package of ConvBKI and test it on real-world
perceptually challenging off-road driving data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2209.10663</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Kuro Siwo: 33 billion $m^2$ under the water. A global multi-temporal
  satellite dataset for rapid flood mapping <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12056v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12056v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolaos Ioannis Bountos, Maria Sdraka, Angelos Zavras, Ilektra Karasante, Andreas Karavias, Themistocles Herekakis, Angeliki Thanasou, Dimitrios Michail, Ioannis Papoutsis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Global floods, exacerbated by climate change, pose severe threats to human
life, infrastructure, and the environment. Recent catastrophic events in
Pakistan and New Zealand underscore the urgent need for precise flood mapping
to guide restoration efforts, understand vulnerabilities, and prepare for
future occurrences. While Synthetic Aperture Radar (SAR) remote sensing offers
day-and-night, all-weather imaging capabilities, its application in deep
learning for flood segmentation is limited by the lack of large annotated
datasets. To address this, we introduce Kuro Siwo, a manually annotated
multi-temporal dataset, spanning 43 flood events globally. Our dataset maps
more than 338 billion $m^2$ of land, with 33 billion designated as either
flooded areas or permanent water bodies. Kuro Siwo includes a highly processed
product optimized for flood mapping based on SAR Ground Range Detected, and a
primal SAR Single Look Complex product with minimal preprocessing, designed to
promote research on the exploitation of both the phase and amplitude
information and to offer maximum flexibility for downstream task preprocessing.
To leverage advances in large scale self-supervised pretraining methods for
remote sensing data, we augment Kuro Siwo with a large unlabeled set of SAR
samples. Finally, we provide an extensive benchmark, namely BlackBench,
offering strong baselines for a diverse set of flood events from Europe,
America, Africa, Asia and Australia.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 38th Conference on Neural Information Processing
  Systems (NeurIPS 2024) Track on Datasets and Benchmarks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On-Air Deep Learning Integrated Semantic Inference Models for Enhanced
  Earth Observation Satellite Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15246v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15246v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hong-fu Chou, Vu Nguyen Ha, Prabhu Thiruvasagam, Thanh-Dung Le, Geoffrey Eappen, Ti Ti Nguyen, Luis M. Garces-Socarras, Jorge L. Gonzalez-Rios, Juan Carlos Merlano-Duncan, Symeon Chatzinotas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Earth Observation (EO) systems are crucial for cartography, disaster
surveillance, and resource administration. Nonetheless, they encounter
considerable obstacles in the processing and transmission of extensive data,
especially in specialized domains such as precision agriculture and real-time
disaster response. Earth observation satellites, outfitted with remote sensing
technology, gather data from onboard sensors and IoT-enabled terrestrial
objects, delivering important information remotely. Domain-adapted Large
Language Models (LLMs) provide a solution by enabling the integration of raw
and processed EO data. Through domain adaptation, LLMs improve the assimilation
and analysis of many data sources, tackling the intricacies of specialized
datasets in agriculture and disaster response. This data synthesis, directed by
LLMs, enhances the precision and pertinence of conveyed information. This study
provides a thorough examination of using semantic inference and deep learning
for sophisticated EO systems. It presents an innovative architecture for
semantic communication in EO satellite networks, designed to improve data
transmission efficiency using semantic processing methodologies. Recent
advancements in onboard processing technologies enable dependable, adaptable,
and energy-efficient data management in orbit. These improvements guarantee
reliable performance in adverse space circumstances using radiation-hardened
and reconfigurable technology. Collectively, these advancements enable
next-generation satellite missions with improved processing capabilities,
crucial for operational flexibility and real-time decision-making in 6G
satellite communication.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 7 figures, Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Video Diffusion Models are Training-free Motion Interpreter and
  Controller 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14864v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14864v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeqi Xiao, Yifan Zhou, Shuai Yang, Xingang Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video generation primarily aims to model authentic and customized motion
across frames, making understanding and controlling the motion a crucial topic.
Most diffusion-based studies on video motion focus on motion customization with
training-based paradigms, which, however, demands substantial training
resources and necessitates retraining for diverse models. Crucially, these
approaches do not explore how video diffusion models encode cross-frame motion
information in their features, lacking interpretability and transparency in
their effectiveness. To answer this question, this paper introduces a novel
perspective to understand, localize, and manipulate motion-aware features in
video diffusion models. Through analysis using Principal Component Analysis
(PCA), our work discloses that robust motion-aware feature already exists in
video diffusion models. We present a new MOtion FeaTure (MOFT) by eliminating
content correlation information and filtering motion channels. MOFT provides a
distinct set of benefits, including the ability to encode comprehensive motion
information with clear interpretability, extraction without the need for
training, and generalizability across diverse architectures. Leveraging MOFT,
we propose a novel training-free video motion control framework. Our method
demonstrates competitive performance in generating natural and faithful motion,
providing architecture-agnostic insights and applicability in a variety of
downstream tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://xizaoqu.github.io/moft/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Generalization in Visual Reasoning via Self-Ensemble 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20883v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20883v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tien-Huy Nguyen, Quang-Khai Tran, Anh-Tuan Quang-Hoang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The cognitive faculty of visual reasoning necessitates the integration of
multimodal perceptual processing and commonsense and external knowledge of the
world. In recent years, a plethora of large vision-language models (LVLMs) have
been proposed, demonstrating outstanding power and exceptional proficiency in
commonsense reasoning across diverse domains and tasks. Nevertheless, training
such LVLMs requires a lot of costly resources. Recent approaches, instead of
training LVLMs from scratch on various large datasets, focus on exploring ways
to take advantage of the capabilities of many different LVLMs, such as ensemble
methods. In this work, we propose self-ensemble, a novel method that improves
the generalization and visual reasoning of the model without updating any
parameters, a training-free method. Our key insight is that we realized that
LVLM itself can ensemble without the need for any other LVLMs, which helps to
unlock their internal capabilities. Extensive experiments on various benchmarks
demonstrate the effectiveness of our method in achieving state-of-the-art
(SOTA) performance on SketchyVQA, Outside Knowledge VQA, and
out-of-distribution VQA tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FRoundation: Are Foundation Models Ready for Face Recognition? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23831v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23831v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tahar Chettaoui, Naser Damer, Fadi Boutros
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models are predominantly trained in an unsupervised or
self-supervised manner on highly diverse and large-scale datasets, making them
broadly applicable to various downstream tasks. In this work, we investigate
for the first time whether such models are suitable for the specific domain of
face recognition. We further propose and demonstrate the adaptation of these
models for face recognition across different levels of data availability.
Extensive experiments are conducted on multiple foundation models and datasets
of varying scales for training and fine-tuning, with evaluation on a wide range
of benchmarks. Our results indicate that, despite their versatility,
pre-trained foundation models underperform in face recognition compared to
similar architectures trained specifically for this task. However, fine-tuning
foundation models yields promising results, often surpassing models trained
from scratch when training data is limited. Even with access to large-scale
face recognition training datasets, fine-tuned foundation models perform
comparably to models trained from scratch, but with lower training
computational costs and without relying on the assumption of extensive data
availability. Our analysis also explores bias in face recognition, with
slightly higher bias observed in some settings when using foundation models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LongVILA: Scaling Long-Context Visual Language Models for Long Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.10188v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.10188v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fuzhao Xue, Yukang Chen, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, Ethan He, Hongxu Yin, Pavlo Molchanov, Jan Kautz, Linxi Fan, Yuke Zhu, Yao Lu, Song Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-context capability is critical for multi-modal foundation models,
especially for long video understanding. We introduce LongVILA, a full-stack
solution for long-context visual-language models by co-designing the algorithm
and system. For model training, we upgrade existing VLMs to support long video
understanding by incorporating two additional stages, i.e., long context
extension and long video supervised fine-tuning. However, training on long
video is computationally and memory intensive. We introduce the long-context
Multi-Modal Sequence Parallelism (MM-SP) system that efficiently parallelizes
long video training and inference, enabling 2M context length training on 256
GPUs without any gradient checkpointing. LongVILA efficiently extends the
number of video frames of VILA from 8 to 2048, improving the long video
captioning score from 2.00 to 3.26 (out of 5), achieving 99.8% accuracy in
6,000-frame (more than 1 million tokens) video needle-in-a-haystack.
LongVILA-7B demonstrates strong accuracy on the VideoMME benchmark, i.e., 61.8%
with subtitle. Besides, MM-SP is 2.1x - 5.7x faster than ring style sequence
parallelism and 1.1x - 1.4x faster than Megatron with a hybrid context and
tensor parallelism. Moreover, it seamlessly integrates with Hugging Face
Transformers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and models are available at
  https://github.com/NVlabs/VILA/blob/main/LongVILA.md</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Conditional GAN for Enhancing Diffusion Models in Efficient and
  Authentic Global Gesture Generation from Audios <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20359v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20359v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongkang Cheng, Mingjiang Liang, Shaoli Huang, Gaoge Han, Jifeng Ning, Wei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-driven simultaneous gesture generation is vital for human-computer
communication, AI games, and film production. While previous research has shown
promise, there are still limitations. Methods based on VAEs are accompanied by
issues of local jitter and global instability, whereas methods based on
diffusion models are hampered by low generation efficiency. This is because the
denoising process of DDPM in the latter relies on the assumption that the noise
added at each step is sampled from a unimodal distribution, and the noise
values are small. DDIM borrows the idea from the Euler method for solving
differential equations, disrupts the Markov chain process, and increases the
noise step size to reduce the number of denoising steps, thereby accelerating
generation. However, simply increasing the step size during the step-by-step
denoising process causes the results to gradually deviate from the original
data distribution, leading to a significant drop in the quality of the
generated actions and the emergence of unnatural artifacts. In this paper, we
break the assumptions of DDPM and achieves breakthrough progress in denoising
speed and fidelity. Specifically, we introduce a conditional GAN to capture
audio control signals and implicitly match the multimodal denoising
distribution between the diffusion and denoising steps within the same sampling
step, aiming to sample larger noise values and apply fewer denoising steps for
high-speed generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WACV 2025 (Round 1)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RopeTP: Global Human Motion Recovery via Integrating Robust Pose
  Estimation with Diffusion Trajectory Prior <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20358v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20358v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingjiang Liang, Yongkang Cheng, Hualin Liang, Shaoli Huang, Wei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present RopeTP, a novel framework that combines Robust pose estimation
with a diffusion Trajectory Prior to reconstruct global human motion from
videos. At the heart of RopeTP is a hierarchical attention mechanism that
significantly improves context awareness, which is essential for accurately
inferring the posture of occluded body parts. This is achieved by exploiting
the relationships with visible anatomical structures, enhancing the accuracy of
local pose estimations. The improved robustness of these local estimations
allows for the reconstruction of precise and stable global trajectories.
Additionally, RopeTP incorporates a diffusion trajectory model that predicts
realistic human motion from local pose sequences. This model ensures that the
generated trajectories are not only consistent with observed local actions but
also unfold naturally over time, thereby improving the realism and stability of
3D human motion reconstruction. Extensive experimental validation shows that
RopeTP surpasses current methods on two benchmark datasets, particularly
excelling in scenarios with occlusions. It also outperforms methods that rely
on SLAM for initial camera estimates and extensive optimization, delivering
more accurate and realistic trajectories.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WACV 2025 (Round 1)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adversarial Purification and <span class="highlight-title">Fine-tuning</span> for Robust UDC Image
  Restoration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13629v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13629v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenbo Song, Zhenyuan Zhang, Kaihao Zhang, Zhaoxin Fan, Jianfeng Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study delves into the enhancement of Under-Display Camera (UDC) image
restoration models, focusing on their robustness against adversarial attacks.
Despite its innovative approach to seamless display integration, UDC technology
faces unique image degradation challenges exacerbated by the susceptibility to
adversarial perturbations. Our research initially conducts an in-depth
robustness evaluation of deep-learning-based UDC image restoration models by
employing several white-box and black-box attacking methods. This evaluation is
pivotal in understanding the vulnerabilities of current UDC image restoration
techniques. Following the assessment, we introduce a defense framework
integrating adversarial purification with subsequent fine-tuning processes.
First, our approach employs diffusion-based adversarial purification,
effectively neutralizing adversarial perturbations. Then, we apply the
fine-tuning methodologies to refine the image restoration models further,
ensuring that the quality and fidelity of the restored images are maintained.
The effectiveness of our proposed approach is validated through extensive
experiments, showing marked improvements in resilience against typical
adversarial attacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Failure to meet expectations</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Posture-Informed Muscular Force Learning for Robust Hand Pressure
  Estimation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23629v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23629v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyungjin Seo, Junghoon Seo, Hanseok Jeong, Sangpil Kim, Sang Ho Yoon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present PiMForce, a novel framework that enhances hand pressure estimation
by leveraging 3D hand posture information to augment forearm surface
electromyography (sEMG) signals. Our approach utilizes detailed spatial
information from 3D hand poses in conjunction with dynamic muscle activity from
sEMG to enable accurate and robust whole-hand pressure measurements under
diverse hand-object interactions. We also developed a multimodal data
collection system that combines a pressure glove, an sEMG armband, and a
markerless finger-tracking module. We created a comprehensive dataset from 21
participants, capturing synchronized data of hand posture, sEMG signals, and
exerted hand pressure across various hand postures and hand-object interaction
scenarios using our collection system. Our framework enables precise hand
pressure estimation in complex and natural interaction scenarios. Our approach
substantially mitigates the limitations of traditional sEMG-based or
vision-based methods by integrating 3D hand posture information with sEMG
signals. Video demos, data, and code are available online.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024. Project Page Link:
  https://pimforce.hcitech.org/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LADDER: Language Driven Slice Discovery and Error Rectification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.07832v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.07832v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shantanu Ghosh, Rayan Syed, Chenyu Wang, Clare B. Poynton, Shyam Visweswaran, Kayhan Batmanghelich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Error slice discovery associates structured patterns with model errors.
Existing methods discover error slices by clustering the error-prone samples
with similar patterns or assigning discrete attributes to each sample for
post-hoc analysis. While these methods aim for interpretability and easier
mitigation through reweighting or rebalancing, they may not capture the full
complexity of error patterns due to incomplete or missing attributes. Contrary
to the existing approach, this paper utilizes the reasoning capabilities of the
Large Language Model (LLM) to analyze complex error patterns and generate
testable hypotheses. This paper proposes LADDER: Language Driven slice
Discovery and Error Rectification. It first projects the model's representation
into a language-aligned feature space (eg CLIP) to preserve semantics in the
original model feature space. This ensures the accurate retrieval of sentences
that highlight the model's errors. Next, the LLM utilizes the sentences and
generates hypotheses to discover error slices. Finally, we mitigate the error
by fine-tuning the classification head by creating a group-balanced dataset
using the hypotheses. Our entire method does not require any attribute
annotation, either explicitly or through external tagging models. We validate
our method with \textbf{five} image classification datasets. The code is
available (https://github.com/batmanlab/Ladder).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Human Action Recognition (HAR) Using Skeleton-based Spatial Temporal
  Relative Transformer Network: ST-RTR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23806v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23806v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Faisal Mehmood, Enqing Chen, Touqeer Abbas, Samah M. Alzanin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human Action Recognition (HAR) is an interesting research area in
human-computer interaction used to monitor the activities of elderly and
disabled individuals affected by physical and mental health. In the recent era,
skeleton-based HAR has received much attention because skeleton data has shown
that it can handle changes in striking, body size, camera views, and complex
backgrounds. One key characteristic of ST-GCN is automatically learning spatial
and temporal patterns from skeleton sequences. It has some limitations, as this
method only works for short-range correlation due to its limited receptive
field. Consequently, understanding human action requires long-range
interconnection. To address this issue, we developed a spatial-temporal
relative transformer ST-RTR model. The ST-RTR includes joint and relay nodes,
which allow efficient communication and data transmission within the network.
These nodes help to break the inherent spatial and temporal skeleton
topologies, which enables the model to understand long-range human action
better. Furthermore, we combine ST-RTR with a fusion model for further
performance improvements. To assess the performance of the ST-RTR method, we
conducted experiments on three skeleton-based HAR benchmarks: NTU RGB+D 60, NTU
RGB+D 120, and UAV-Human. It boosted CS and CV by 2.11 % and 1.45% on NTU RGB+D
60, 1.25% and 1.05% on NTU RGB+D 120. On UAV-Human datasets, accuracy improved
by 2.54%. The experimental outcomes explain that the proposed ST-RTR model
significantly improves action recognition associated with the standard ST-GCN
method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DQ-DETR: DETR with Dynamic Query for Tiny Object Detection <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.03507v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.03507v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi-Xin Huang, Hou-I Liu, Hong-Han Shuai, Wen-Huang Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite previous DETR-like methods having performed successfully in generic
object detection, tiny object detection is still a challenging task for them
since the positional information of object queries is not customized for
detecting tiny objects, whose scale is extraordinarily smaller than general
objects. Also, DETR-like methods using a fixed number of queries make them
unsuitable for aerial datasets, which only contain tiny objects, and the
numbers of instances are imbalanced between different images. Thus, we present
a simple yet effective model, named DQ-DETR, which consists of three different
components: categorical counting module, counting-guided feature enhancement,
and dynamic query selection to solve the above-mentioned problems. DQ-DETR uses
the prediction and density maps from the categorical counting module to
dynamically adjust the number of object queries and improve the positional
information of queries. Our model DQ-DETR outperforms previous CNN-based and
DETR-like methods, achieving state-of-the-art mAP 30.2% on the AI-TOD-V2
dataset, which mostly consists of tiny objects. Our code will be available at
https://github.com/hoiliu-0801/DQ-DETR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV 2024. Our code will be available at
  https://github.com/hoiliu-0801/DQ-DETR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast Samplers for Inverse Problems in Iterative Refinement Models <span class="chip">NeurIPS'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17673v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17673v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kushagra Pandey, Ruihan Yang, Stephan Mandt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Constructing fast samplers for unconditional diffusion and flow-matching
models has received much attention recently; however, existing methods for
solving inverse problems, such as super-resolution, inpainting, or deblurring,
still require hundreds to thousands of iterative steps to obtain high-quality
results. We propose a plug-and-play framework for constructing efficient
samplers for inverse problems, requiring only pre-trained diffusion or
flow-matching models. We present Conditional Conjugate Integrators, which
leverage the specific form of the inverse problem to project the respective
conditional diffusion/flow dynamics into a more amenable space for sampling.
Our method complements popular posterior approximation methods for solving
inverse problems using diffusion/flow models. We evaluate the proposed method's
performance on various linear image restoration tasks across multiple datasets,
employing diffusion and flow-matching models. Notably, on challenging inverse
problems like 4x super-resolution on the ImageNet dataset, our method can
generate high-quality samples in as few as 5 conditional sampling steps and
outperforms competing baselines requiring 20-1000 steps. Our code will be
publicly available at https://github.com/mandt-lab/c-pigdm
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>43 pages, NeurIPS'24 Camera Ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SMART: Scalable Multi-agent Real-time Motion Generation via Next-token
  Prediction <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15677v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15677v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Wu, Xiaoxin Feng, Ziyan Gao, Yuheng Kan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data-driven autonomous driving motion generation tasks are frequently
impacted by the limitations of dataset size and the domain gap between
datasets, which precludes their extensive application in real-world scenarios.
To address this issue, we introduce SMART, a novel autonomous driving motion
generation paradigm that models vectorized map and agent trajectory data into
discrete sequence tokens. These tokens are then processed through a
decoder-only transformer architecture to train for the next token prediction
task across spatial-temporal series. This GPT-style method allows the model to
learn the motion distribution in real driving scenarios. SMART achieves
state-of-the-art performance across most of the metrics on the generative Sim
Agents challenge, ranking 1st on the leaderboards of Waymo Open Motion Dataset
(WOMD), demonstrating remarkable inference speed. Moreover, SMART represents
the generative model in the autonomous driving motion domain, exhibiting
zero-shot generalization capabilities: Using only the NuPlan dataset for
training and WOMD for validation, SMART achieved a competitive score of 0.72 on
the Sim Agents challenge. Lastly, we have collected over 1 billion motion
tokens from multiple datasets, validating the model's scalability. These
results suggest that SMART has initially emulated two important properties:
scalability and zero-shot generalization, and preliminarily meets the needs of
large-scale real-time simulation applications. We have released all the code to
promote the exploration of models for motion generation in the autonomous
driving field. The source code is available at
https://github.com/rainmaker22/SMART.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ProvNeRF: Modeling per Point Provenance in NeRFs as a Stochastic Field <span class="chip">NeurIPS
  2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.08140v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.08140v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kiyohiro Nakayama, Mikaela Angelina Uy, Yang You, Ke Li, Leonidas J. Guibas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural radiance fields (NeRFs) have gained popularity with multiple works
showing promising results across various applications. However, to the best of
our knowledge, existing works do not explicitly model the distribution of
training camera poses, or consequently the triangulation quality, a key factor
affecting reconstruction quality dating back to classical vision literature. We
close this gap with ProvNeRF, an approach that models the \textbf{provenance}
for each point -- i.e., the locations where it is likely visible -- of NeRFs as
a stochastic field. We achieve this by extending implicit maximum likelihood
estimation (IMLE) to functional space with an optimizable objective. We show
that modeling per-point provenance during the NeRF optimization enriches the
model with information on triangulation leading to improvements in novel view
synthesis and uncertainty estimation under the challenging sparse,
unconstrained view setting against competitive baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38th Conference on Neural Information Processing Systems (NeurIPS
  2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Visual Scene Understanding: Incremental Scene Graph Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.01636v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.01636v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naitik Khandelwal, Xiao Liu, Mengmi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scene graph generation (SGG) analyzes images to extract meaningful
information about objects and their relationships. In the dynamic visual world,
it is crucial for AI systems to continuously detect new objects and establish
their relationships with existing ones. Recently, numerous studies have focused
on continual learning within the domains of object detection and image
recognition. However, a limited amount of research focuses on a more
challenging continual learning problem in SGG. This increased difficulty arises
from the intricate interactions and dynamic relationships among objects, and
their associated contexts. Thus, in continual learning, SGG models are often
required to expand, modify, retain, and reason scene graphs within the process
of adaptive visual scene understanding. To systematically explore Continual
Scene Graph Generation (CSEGG), we present a comprehensive benchmark comprising
three learning regimes: relationship incremental, scene incremental, and
relationship generalization. Moreover, we introduce a ``Replays via Analysis by
Synthesis" method named RAS. This approach leverages the scene graphs,
decomposes and re-composes them to represent different scenes, and replays the
synthesized scenes based on these compositional scene graphs. The replayed
synthesized scenes act as a means to practice and refine proficiency in SGG in
known and unknown environments. Our experimental results not only highlight the
challenges of directly combining existing continual learning methods with SGG
backbones but also demonstrate the effectiveness of our proposed approach,
enhancing CSEGG efficiency while simultaneously preserving privacy and memory
usage. All data and source code are publicly available online.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ STONE: A Submodular Optimization Framework for Active 3D Object
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03918v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03918v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruiyu Mao, Sarthak Kumar Maharana, Rishabh K Iyer, Yunhui Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D object detection is fundamentally important for various emerging
applications, including autonomous driving and robotics. A key requirement for
training an accurate 3D object detector is the availability of a large amount
of LiDAR-based point cloud data. Unfortunately, labeling point cloud data is
extremely challenging, as accurate 3D bounding boxes and semantic labels are
required for each potential object. This paper proposes a unified active 3D
object detection framework, for greatly reducing the labeling cost of training
3D object detectors. Our framework is based on a novel formulation of
submodular optimization, specifically tailored to the problem of active 3D
object detection. In particular, we address two fundamental challenges
associated with active 3D object detection: data imbalance and the need to
cover the distribution of the data, including LiDAR-based point cloud data of
varying difficulty levels. Extensive experiments demonstrate that our method
achieves state-of-the-art performance with high computational efficiency
compared to existing active learning methods. The code is available at
https://github.com/RuiyuM/STONE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Make Continual Learning Stronger via C-Flat 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.00986v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.00986v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ang Bian, Wei Li, Hangjie Yuan, Chengrong Yu, Mang Wang, Zixiang Zhao, Aojun Lu, Pengliang Ji, Tao Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model generalization ability upon incrementally acquiring dynamically
updating knowledge from sequentially arriving tasks is crucial to tackle the
sensitivity-stability dilemma in Continual Learning (CL). Weight loss landscape
sharpness minimization seeking for flat minima lying in neighborhoods with
uniform low loss or smooth gradient is proven to be a strong training regime
improving model generalization compared with loss minimization based optimizer
like SGD. Yet only a few works have discussed this training regime for CL,
proving that dedicated designed zeroth-order sharpness optimizer can improve CL
performance. In this work, we propose a Continual Flatness (C-Flat) method
featuring a flatter loss landscape tailored for CL. C-Flat could be easily
called with only one line of code and is plug-and-play to any CL methods. A
general framework of C-Flat applied to all CL categories and a thorough
comparison with loss minima optimizer and flat minima based CL approaches is
presented in this paper, showing that our method can boost CL performance in
almost all cases. Code is available at https://github.com/WanNaa/C-Flat.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HSIGene: A Foundation Model For Hyperspectral Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.12470v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.12470v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Pang, Xiangyong Cao, Datao Tang, Shuang Xu, Xueru Bai, Feng Zhou, Deyu Meng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyperspectral image (HSI) plays a vital role in various fields such as
agriculture and environmental monitoring. However, due to the expensive
acquisition cost, the number of hyperspectral images is limited, degenerating
the performance of downstream tasks. Although some recent studies have
attempted to employ diffusion models to synthesize HSIs, they still struggle
with the scarcity of HSIs, affecting the reliability and diversity of the
generated images. Some studies propose to incorporate multi-modal data to
enhance spatial diversity, but the spectral fidelity cannot be ensured. In
addition, existing HSI synthesis models are typically uncontrollable or only
support single-condition control, limiting their ability to generate accurate
and reliable HSIs. To alleviate these issues, we propose HSIGene, a novel HSI
generation foundation model which is based on latent diffusion and supports
multi-condition control, allowing for more precise and reliable HSI generation.
To enhance the spatial diversity of the training data while preserving spectral
fidelity, we propose a new data augmentation method based on spatial
super-resolution, in which HSIs are upscaled first, and thus abundant training
patches could be obtained by cropping the high-resolution HSIs. In addition, to
improve the perceptual quality of the augmented data, we introduce a novel
two-stage HSI super-resolution framework, which first applies RGB bands
super-resolution and then utilizes our proposed Rectangular Guided Attention
Network (RGAN) for guided HSI super-resolution. Experiments demonstrate that
the proposed model is capable of generating a vast quantity of realistic HSIs
for downstream tasks such as denoising and super-resolution. The code and
models are available at https://github.com/LiPang/HSIGene.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GrounDiT: Grounding Diffusion Transformers via Noisy Patch
  Transplantation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20474v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20474v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Phillip Y. Lee, Taehoon Yoon, Minhyuk Sung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce GrounDiT, a novel training-free spatial grounding technique for
text-to-image generation using Diffusion Transformers (DiT). Spatial grounding
with bounding boxes has gained attention for its simplicity and versatility,
allowing for enhanced user control in image generation. However, prior
training-free approaches often rely on updating the noisy image during the
reverse diffusion process via backpropagation from custom loss functions, which
frequently struggle to provide precise control over individual bounding boxes.
In this work, we leverage the flexibility of the Transformer architecture,
demonstrating that DiT can generate noisy patches corresponding to each
bounding box, fully encoding the target object and allowing for fine-grained
control over each region. Our approach builds on an intriguing property of DiT,
which we refer to as semantic sharing. Due to semantic sharing, when a smaller
patch is jointly denoised alongside a generatable-size image, the two become
semantic clones. Each patch is denoised in its own branch of the generation
process and then transplanted into the corresponding region of the original
noisy image at each timestep, resulting in robust spatial grounding for each
bounding box. In our experiments on the HRS and DrawBench benchmarks, we
achieve state-of-the-art performance compared to previous training-free
approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024. Project Page:
  https://groundit-diffusion.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In-Context LoRA for Diffusion Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23775v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23775v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lianghua Huang, Wei Wang, Zhi-Fan Wu, Yupeng Shi, Huanzhang Dou, Chen Liang, Yutong Feng, Yu Liu, Jingren Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research arXiv:2410.15027 has explored the use of diffusion
transformers (DiTs) for task-agnostic image generation by simply concatenating
attention tokens across images. However, despite substantial computational
resources, the fidelity of the generated images remains suboptimal. In this
study, we reevaluate and streamline this framework by hypothesizing that
text-to-image DiTs inherently possess in-context generation capabilities,
requiring only minimal tuning to activate them. Through diverse task
experiments, we qualitatively demonstrate that existing text-to-image DiTs can
effectively perform in-context generation without any tuning. Building on this
insight, we propose a remarkably simple pipeline to leverage the in-context
abilities of DiTs: (1) concatenate images instead of tokens, (2) perform joint
captioning of multiple images, and (3) apply task-specific LoRA tuning using
small datasets (e.g., $20\sim 100$ samples) instead of full-parameter tuning
with large datasets. We name our models In-Context LoRA (IC-LoRA). This
approach requires no modifications to the original DiT models, only changes to
the training data. Remarkably, our pipeline generates high-fidelity image sets
that better adhere to prompts. While task-specific in terms of tuning data, our
framework remains task-agnostic in architecture and pipeline, offering a
powerful tool for the community and providing valuable insights for further
research on product-level task-agnostic generation systems. We release our
code, data, and models at https://github.com/ali-vilab/In-Context-LoRA
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tech report. Project page:
  https://ali-vilab.github.io/In-Context-LoRA-Page/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Uni-Med: A Unified Medical Generalist Foundation Model For Multi-Task
  Learning Via Connector-MoE 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17508v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17508v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xun Zhu, Ying Hu, Fanbin Mo, Miao Li, Ji Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal large language models (MLLMs) have shown impressive capabilities
as a general-purpose interface for various visual and linguistic tasks.
However, building a unified MLLM for multi-task learning in the medical field
remains a thorny challenge. To mitigate the tug-of-war problem of multi-modal
multi-task optimization in MLLMs, recent advances primarily focus on improving
the LLM components, while neglecting the connector that bridges the gap between
modalities. In this paper, we introduce Uni-Med, a novel medical generalist
foundation model which consists of a universal visual feature extraction
module, a connector mixture-of-experts (CMoE) module, and an LLM. Benefiting
from the proposed CMoE that leverages a well-designed router with a mixture of
projection experts at the connector, Uni-Med achieves efficient solution to the
tug-of-war problem and can perform six different medical tasks including
question answering, visual question answering, report generation, referring
expression comprehension, referring expression generation and image
classification. To the best of our knowledge, Uni-Med is the first effort to
tackle multi-task interference at the connector in MLLMs. Extensive ablation
experiments validate the effectiveness of introducing CMoE under any
configuration, with up to an average 8% performance gains. We further provide
interpretation analysis of the tug-of-war problem from the perspective of
gradient optimization and parameter statistics. Compared to previous
state-of-the-art medical MLLMs, Uni-Med achieves competitive or superior
evaluation metrics on diverse tasks. Code and resources are available at
https://github.com/tsinghua-msiip/Uni-Med.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DPEC: Dual-Path Error Compensation Method for Enhanced Low-Light Image
  Clarity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.09553v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.09553v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuang Wang, Qianwen Lu, Boxing Peng, Yihe Nie, Qingchuan Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For the task of low-light image enhancement, deep learning-based algorithms
have demonstrated superiority and effectiveness compared to traditional
methods. However, these methods, primarily based on Retinex theory, tend to
overlook the noise and color distortions in input images, leading to
significant noise amplification and local color distortions in enhanced
results. To address these issues, we propose the Dual-Path Error Compensation
(DPEC) method, designed to improve image quality under low-light conditions by
preserving local texture details while restoring global image brightness
without amplifying noise. DPEC incorporates precise pixel-level error
estimation to capture subtle differences and an independent denoising mechanism
to prevent noise amplification. We introduce the HIS-Retinex loss to guide
DPEC's training, ensuring the brightness distribution of enhanced images
closely aligns with real-world conditions. To balance computational speed and
resource efficiency while training DPEC for a comprehensive understanding of
the global context, we integrated the VMamba architecture into its backbone.
Comprehensive quantitative and qualitative experimental results demonstrate
that our algorithm significantly outperforms state-of-the-art methods in
low-light image enhancement. The code is publicly available online at
https://github.com/wangshuang233/DPEC.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Foodfusion: A Novel Approach for Food Image Composition via Diffusion
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.14135v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.14135v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaohua Shi, Xuan Wang, Si Shi, Xule Wang, Mingrui Zhu, Nannan Wang, Xinbo Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Food image composition requires the use of existing dish images and
background images to synthesize a natural new image, while diffusion models
have made significant advancements in image generation, enabling the
construction of end-to-end architectures that yield promising results. However,
existing diffusion models face challenges in processing and fusing information
from multiple images and lack access to high-quality publicly available
datasets, which prevents the application of diffusion models in food image
composition. In this paper, we introduce a large-scale, high-quality food image
composite dataset, FC22k, which comprises 22,000 foreground, background, and
ground truth ternary image pairs. Additionally, we propose a novel food image
composition method, Foodfusion, which leverages the capabilities of the
pre-trained diffusion models and incorporates a Fusion Module for processing
and integrating foreground and background information. This fused information
aligns the foreground features with the background structure by merging the
global structural information at the cross-attention layer of the denoising
UNet. To further enhance the content and structure of the background, we also
integrate a Content-Structure Control Module. Extensive experiments demonstrate
the effectiveness and scalability of our proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Detecting Brittle Decisions for Free: Leveraging Margin Consistency in
  Deep Robust Classifiers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18451v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18451v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonas Ngnawé, Sabyasachi Sahoo, Yann Pequignot, Frédéric Precioso, Christian Gagné
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite extensive research on adversarial training strategies to improve
robustness, the decisions of even the most robust deep learning models can
still be quite sensitive to imperceptible perturbations, creating serious risks
when deploying them for high-stakes real-world applications. While detecting
such cases may be critical, evaluating a model's vulnerability at a
per-instance level using adversarial attacks is computationally too intensive
and unsuitable for real-time deployment scenarios. The input space margin is
the exact score to detect non-robust samples and is intractable for deep neural
networks. This paper introduces the concept of margin consistency -- a property
that links the input space margins and the logit margins in robust models --
for efficient detection of vulnerable samples. First, we establish that margin
consistency is a necessary and sufficient condition to use a model's logit
margin as a score for identifying non-robust samples. Next, through
comprehensive empirical analysis of various robustly trained models on CIFAR10
and CIFAR100 datasets, we show that they indicate high margin consistency with
a strong correlation between their input space margins and the logit margins.
Then, we show that we can effectively and confidently use the logit margin to
detect brittle decisions with such models. Finally, we address cases where the
model is not sufficiently margin-consistent by learning a pseudo-margin from
the feature representation. Our findings highlight the potential of leveraging
deep representations to assess adversarial vulnerability in deployment
scenarios efficiently.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures, 2 tables. Version Update: Neurips Camera Ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Framework for Real-Time Volcano-Seismic Event Recognition Based on
  Multi-Station Seismograms and Semantic Segmentation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20595v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20595v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Camilo Espinosa-Curilem, Millaray Curilem, Daniel Basualto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In volcano monitoring, effective recognition of seismic events is essential
for understanding volcanic activity and raising timely warning alerts.
Traditional methods rely on manual analysis, which can be subjective and
labor-intensive. Furthermore, current automatic approaches often tackle
detection and classification separately, mostly rely on single station
information and generally require tailored preprocessing and representations to
perform predictions. These limitations often hinder their application to
real-time monitoring and utilization across different volcano conditions. This
study introduces a novel approach that utilizes Semantic Segmentation models to
automate seismic event recognition by applying a straight forward
transformation of multi-channel 1D signals into 2D representations, enabling
their use as images. Our framework employs a data-driven, end-to-end design
that integrates multi-station seismic data with minimal preprocessing,
performing both detection and classification simultaneously for five seismic
event classes. We evaluated four state-of-the-art segmentation models (UNet,
UNet++, DeepLabV3+ and SwinUNet) on approximately 25.000 seismic events
recorded at four different Chilean volcanoes: Nevados del Chill\'an Volcanic
Complex, Laguna del Maule, Villarrica and Puyehue-Cord\'on Caulle. Among these
models, the UNet architecture was identified as the most effective model,
achieving mean F1 and Intersection over Union (IoU) scores of up to 0.91 and
0.88, respectively, and demonstrating superior noise robustness and model
flexibility to unseen volcano datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 9 figures. This is a pre-print, it is currently under
  review for publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Question to Exploration: Test-Time Adaptation in Semantic
  Segmentation? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05341v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05341v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang'an Yi, Haotian Chen, Yifan Zhang, Yonghui Xu, Yan Zhou, Lizhen Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Test-time adaptation (TTA) aims to adapt a model, initially trained on
training data, to test data with potential distribution shifts. Most existing
TTA methods focus on classification problems. The pronounced success of
classification might lead numerous newcomers and engineers to assume that
classic TTA techniques can be directly applied to the more challenging task of
semantic segmentation. However, this belief is still an open question. In this
paper, we investigate the applicability of existing classic TTA strategies in
semantic segmentation. Our comprehensive results have led to three key
observations. First, the classic normalization updating strategy only brings
slight performance improvement, and in some cases, it might even adversely
affect the results. Even with the application of advanced distribution
estimation techniques like batch renormalization, the problem remains
unresolved. Second, although the teacher-student scheme does enhance the
training stability for segmentation TTA in the presence of noisy pseudo-labels
and temporal correlation, it cannot directly result in performance improvement
compared to the original model without TTA under complex data distribution.
Third, segmentation TTA suffers a severe long-tailed class-imbalance problem,
which is substantially more complex than that in TTA for classification. This
long-tailed challenge negatively affects segmentation TTA performance, even
when the accuracy of pseudo-labels is high. Besides those observations, we find
that visual prompt tuning (VisPT) is promising in segmentation TTA and propose
a novel method named TTAP. The outstanding performance of TTAP has also been
verified. We hope the community can give more attention to this challenging,
yet important, segmentation TTA task in the future. The source code is
available at: \textit{https://github.com/ycarobot/TTAP
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LRM-Zero: Training Large Reconstruction Models with Synthesized Data <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09371v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09371v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Desai Xie, Sai Bi, Zhixin Shu, Kai Zhang, Zexiang Xu, Yi Zhou, Sören Pirk, Arie Kaufman, Xin Sun, Hao Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present LRM-Zero, a Large Reconstruction Model (LRM) trained entirely on
synthesized 3D data, achieving high-quality sparse-view 3D reconstruction. The
core of LRM-Zero is our procedural 3D dataset, Zeroverse, which is
automatically synthesized from simple primitive shapes with random texturing
and augmentations (e.g., height fields, boolean differences, and wireframes).
Unlike previous 3D datasets (e.g., Objaverse) which are often captured or
crafted by humans to approximate real 3D data, Zeroverse completely ignores
realistic global semantics but is rich in complex geometric and texture details
that are locally similar to or even more intricate than real objects. We
demonstrate that our LRM-Zero, trained with our fully synthesized Zeroverse,
can achieve high visual quality in the reconstruction of real-world objects,
competitive with models trained on Objaverse. We also analyze several critical
design choices of Zeroverse that contribute to LRM-Zero's capability and
training stability. Our work demonstrates that 3D reconstruction, one of the
core tasks in 3D vision, can potentially be addressed without the semantics of
real-world objects. The Zeroverse's procedural synthesis code and interactive
visualization are available at: https://desaixie.github.io/lrm-zero/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 8 figures. Our code and interactive visualization are
  available at: https://desaixie.github.io/lrm-zero/. v2: NeurIPS 2024 Camera
  Ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiffusionPDE: Generative PDE-Solving Under Partial Observation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17763v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17763v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahe Huang, Guandao Yang, Zichen Wang, Jeong Joon Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a general framework for solving partial differential equations
(PDEs) using generative diffusion models. In particular, we focus on the
scenarios where we do not have the full knowledge of the scene necessary to
apply classical solvers. Most existing forward or inverse PDE approaches
perform poorly when the observations on the data or the underlying coefficients
are incomplete, which is a common assumption for real-world measurements. In
this work, we propose DiffusionPDE that can simultaneously fill in the missing
information and solve a PDE by modeling the joint distribution of the solution
and coefficient spaces. We show that the learned generative priors lead to a
versatile framework for accurately solving a wide range of PDEs under partial
observation, significantly outperforming the state-of-the-art methods for both
forward and inverse directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024. Project page:
  https://jhhuangchloe.github.io/Diffusion-PDE/</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ $\texttt{MixGR}$: Enhancing Retriever Generalization for Scientific
  Domain through Complementary Granularity <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10691v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10691v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fengyu Cai, Xinran Zhao, Tong Chen, Sihao Chen, Hongming Zhang, Iryna Gurevych, Heinz Koeppl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies show the growing significance of document retrieval in the
generation of LLMs, i.e., RAG, within the scientific domain by bridging their
knowledge gap. However, dense retrievers often struggle with domain-specific
retrieval and complex query-document relationships, particularly when query
segments correspond to various parts of a document. To alleviate such prevalent
challenges, this paper introduces $\texttt{MixGR}$, which improves dense
retrievers' awareness of query-document matching across various levels of
granularity in queries and documents using a zero-shot approach.
$\texttt{MixGR}$ fuses various metrics based on these granularities to a united
score that reflects a comprehensive query-document similarity. Our experiments
demonstrate that $\texttt{MixGR}$ outperforms previous document retrieval by
24.7%, 9.8%, and 6.9% on nDCG@5 with unsupervised, supervised, and LLM-based
retrievers, respectively, averaged on queries containing multiple subqueries
from five scientific retrieval datasets. Moreover, the efficacy of two
downstream scientific question-answering tasks highlights the advantage of
$\texttt{MixGR}$ to boost the application of LLMs in the scientific domain. The
code and experimental datasets are available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Customizing Language Models with Instance-wise LoRA for Sequential
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.10159v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.10159v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyu Kong, Jiancan Wu, An Zhang, Leheng Sheng, Hui Lin, Xiang Wang, Xiangnan He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommendation systems predict the next interaction item based on
users' past interactions, aligning recommendations with individual preferences.
Leveraging the strengths of Large Language Models (LLMs) in knowledge
comprehension and reasoning, recent approaches are eager to apply LLMs to
sequential recommendation. A common paradigm is converting user behavior
sequences into instruction data, and fine-tuning the LLM with
parameter-efficient fine-tuning (PEFT) methods like Low-Rank Adaption (LoRA).
However, the uniform application of LoRA across diverse user behaviors is
insufficient to capture individual variability, resulting in negative transfer
between disparate sequences. To address these challenges, we propose
Instance-wise LoRA (iLoRA). We innovatively treat the sequential recommendation
task as a form of multi-task learning, integrating LoRA with the Mixture of
Experts (MoE) framework. This approach encourages different experts to capture
various aspects of user behavior. Additionally, we introduce a sequence
representation guided gate function that generates customized expert
participation weights for each user sequence, which allows dynamic parameter
adjustment for instance-wise recommendations. In sequential recommendation,
iLoRA achieves an average relative improvement of 11.4\% over basic LoRA in the
hit ratio metric, with less than a 1\% relative increase in trainable
parameters. Extensive experiments on three benchmark datasets demonstrate the
effectiveness of iLoRA, highlighting its superior performance compared to
existing methods in mitigating negative transfer and improving recommendation
accuracy. Our data and code are available at
https://github.com/AkaliKong/iLoRA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM-ESR: Large Language Models Enhancement for Long-tailed Sequential
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.20646v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.20646v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qidong Liu, Xian Wu, Yejing Wang, Zijian Zhang, Feng Tian, Yefeng Zheng, Xiangyu Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommender systems (SRS) aim to predict users' subsequent choices
based on their historical interactions and have found applications in diverse
fields such as e-commerce and social media. However, in real-world systems,
most users interact with only a handful of items, while the majority of items
are seldom consumed. These two issues, known as the long-tail user and
long-tail item challenges, often pose difficulties for existing SRS. These
challenges can adversely affect user experience and seller benefits, making
them crucial to address. Though a few works have addressed the challenges, they
still struggle with the seesaw or noisy issues due to the intrinsic scarcity of
interactions. The advancements in large language models (LLMs) present a
promising solution to these problems from a semantic perspective. As one of the
pioneers in this field, we propose the Large Language Models Enhancement
framework for Sequential Recommendation (LLM-ESR). This framework utilizes
semantic embeddings derived from LLMs to enhance SRS without adding extra
inference load from LLMs. To address the long-tail item challenge, we design a
dual-view modeling framework that combines semantics from LLMs and
collaborative signals from conventional SRS. For the long-tail user challenge,
we propose a retrieval augmented self-distillation method to enhance user
preference representation using more informative interactions from similar
users. To verify the effectiveness and versatility of our proposed enhancement
framework, we conduct extensive experiments on three real-world datasets using
three popular SRS models. The results show that our method surpasses existing
baselines consistently, and benefits long-tail users and items especially. The
implementation code is available at
https://github.com/Applied-Machine-Learning-Lab/LLM-ESR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by NeruIPS'24 (Spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MACRec: a Multi-Agent Collaboration Framework for Recommendation <span class="chip">SIGIR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15235v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15235v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhefan Wang, Yuanqing Yu, Wendi Zheng, Weizhi Ma, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLM-based agents have gained considerable attention for their decision-making
skills and ability to handle complex tasks. Recognizing the current gap in
leveraging agent capabilities for multi-agent collaboration in recommendation
systems, we introduce MACRec, a novel framework designed to enhance
recommendation systems through multi-agent collaboration. Unlike existing work
on using agents for user/item simulation, we aim to deploy multi-agents to
tackle recommendation tasks directly. In our framework, recommendation tasks
are addressed through the collaborative efforts of various specialized agents,
including Manager, User/Item Analyst, Reflector, Searcher, and Task
Interpreter, with different working flows. Furthermore, we provide application
examples of how developers can easily use MACRec on various recommendation
tasks, including rating prediction, sequential recommendation, conversational
recommendation, and explanation generation of recommendation results. The
framework and demonstration video are publicly available at
https://github.com/wzf2000/MACRec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SIGIR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unveiling User Satisfaction and Creator Productivity Trade-Offs in
  Recommendation Platforms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23683v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23683v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Yao, Yiming Liao, Jingzhou Liu, Shaoliang Nie, Qifan Wang, Haifeng Xu, Hongning Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  On User-Generated Content (UGC) platforms, recommendation algorithms
significantly impact creators' motivation to produce content as they compete
for algorithmically allocated user traffic. This phenomenon subtly shapes the
volume and diversity of the content pool, which is crucial for the platform's
sustainability. In this work, we demonstrate, both theoretically and
empirically, that a purely relevance-driven policy with low exploration
strength boosts short-term user satisfaction but undermines the long-term
richness of the content pool. In contrast, a more aggressive exploration policy
may slightly compromise user satisfaction but promote higher content creation
volume. Our findings reveal a fundamental trade-off between immediate user
satisfaction and overall content production on UGC platforms. Building on this
finding, we propose an efficient optimization method to identify the optimal
exploration strength, balancing user and creator engagement. Our model can
serve as a pre-deployment audit tool for recommendation algorithms on UGC
platforms, helping to align their immediate objectives with sustainable,
long-term goals.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">85</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adversarially Robust Decision Transformer <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.18414v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.18414v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaohang Tang, Afonso Marques, Parameswaran Kamalaruban, Ilija Bogunovic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decision Transformer (DT), as one of the representative Reinforcement
Learning via Supervised Learning (RvS) methods, has achieved strong performance
in offline learning tasks by leveraging the powerful Transformer architecture
for sequential decision-making. However, in adversarial environments, these
methods can be non-robust, since the return is dependent on the strategies of
both the decision-maker and adversary. Training a probabilistic model
conditioned on observed return to predict action can fail to generalize, as the
trajectories that achieve a return in the dataset might have done so due to a
suboptimal behavior adversary. To address this, we propose a worst-case-aware
RvS algorithm, the Adversarially Robust Decision Transformer (ARDT), which
learns and conditions the policy on in-sample minimax returns-to-go. ARDT
aligns the target return with the worst-case return learned through minimax
expectile regression, thereby enhancing robustness against powerful test-time
adversaries. In experiments conducted on sequential games with full data
coverage, ARDT can generate a maximin (Nash Equilibrium) strategy, the solution
with the largest adversarial robustness. In large-scale sequential games and
continuous adversarial RL environments with partial data coverage, ARDT
demonstrates significantly superior robustness to powerful test-time
adversaries and attains higher worst-case returns compared to contemporary DT
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Highly Accurate Real-space Electron Densities with Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.01306v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.01306v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lixue Cheng, P. Bernát Szabó, Zeno Schätzle, Derk P. Kooi, Jonas Köhler, Klaas J. H. Giesbertz, Frank Noé, Jan Hermann, Paola Gori-Giorgi, Adam Foster
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Variational ab-initio methods in quantum chemistry stand out among other
methods in providing direct access to the wave function. This allows in
principle straightforward extraction of any other observable of interest,
besides the energy, but in practice this extraction is often technically
difficult and computationally impractical. Here, we consider the electron
density as a central observable in quantum chemistry and introduce a novel
method to obtain accurate densities from real-space many-electron wave
functions by representing the density with a neural network that captures known
asymptotic properties and is trained from the wave function by score matching
and noise-contrastive estimation. We use variational quantum Monte Carlo with
deep-learning ans\"atze (deep QMC) to obtain highly accurate wave functions
free of basis set errors, and from them, using our novel method,
correspondingly accurate electron densities, which we demonstrate by
calculating dipole moments, nuclear forces, contact densities, and other
density-based properties.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 9 figures in the main text</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scalable Training of Trustworthy and Energy-Efficient Predictive Graph
  Foundation Models for Atomistic Materials Modeling: A Case Study with
  HydraGNN 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12909v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12909v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Massimiliano Lupo Pasini, Jong Youl Choi, Kshitij Mehta, Pei Zhang, David Rogers, Jonghyun Bae, Khaled Z. Ibrahim, Ashwin M. Aji, Karl W. Schulz, Jorda Polo, Prasanna Balaprakash
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present our work on developing and training scalable, trustworthy, and
energy-efficient predictive graph foundation models (GFMs) using HydraGNN, a
multi-headed graph convolutional neural network architecture. HydraGNN expands
the boundaries of graph neural network (GNN) computations in both training
scale and data diversity. It abstracts over message passing algorithms,
allowing both reproduction of and comparison across algorithmic innovations
that define nearest-neighbor convolution in GNNs. This work discusses a series
of optimizations that have allowed scaling up the GFMs training to tens of
thousands of GPUs on datasets consisting of hundreds of millions of graphs. Our
GFMs use multi-task learning (MTL) to simultaneously learn graph-level and
node-level properties of atomistic structures, such as energy and atomic
forces. Using over 154 million atomistic structures for training, we illustrate
the performance of our approach along with the lessons learned on two
state-of-the-art United States Department of Energy (US-DOE) supercomputers,
namely the Perlmutter petascale system at the National Energy Research
Scientific Computing Center and the Frontier exascale system at Oak Ridge
Leadership Computing Facility. The HydraGNN architecture enables the GFM to
achieve near-linear strong scaling performance using more than 2,000 GPUs on
Perlmutter and 16,000 GPUs on Frontier.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>51 pages, 32 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nyström Kernel Stein Discrepancy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.08401v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.08401v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Kalinke, Zoltan Szabo, Bharath K. Sriperumbudur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Kernel methods underpin many of the most successful approaches in data
science and statistics, and they allow representing probability measures as
elements of a reproducing kernel Hilbert space without loss of information.
Recently, the kernel Stein discrepancy (KSD), which combines Stein's method
with the flexibility of kernel techniques, gained considerable attention.
Through the Stein operator, KSD allows the construction of powerful
goodness-of-fit tests where it is sufficient to know the target distribution up
to a multiplicative constant. However, the typical U- and V-statistic-based KSD
estimators suffer from a quadratic runtime complexity, which hinders their
application in large-scale settings. In this work, we propose a Nystr\"om-based
KSD acceleration -- with runtime $\mathcal O\left(mn+m^3\right)$ for $n$
samples and $m\ll n$ Nystr\"om points -- , show its $\sqrt{n}$-consistency with
a classical sub-Gaussian assumption, and demonstrate its applicability for
goodness-of-fit testing on a suite of benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Broader applicability of main result, consistency of quadratic time
  estimator</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Provable optimal transport with transformers: The essence of depth and
  <span class="highlight-title">prompt</span> engineering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.19931v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.19931v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hadi Daneshmand
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Can we establish provable performance guarantees for transformers?
Establishing such theoretical guarantees is a milestone in developing
trustworthy generative AI. In this paper, we take a step toward addressing this
question by focusing on optimal transport, a fundamental problem at the
intersection of combinatorial and continuous optimization. Leveraging the
computational power of attention layers, we prove that a transformer with fixed
parameters can effectively solve the optimal transport problem in Wasserstein-2
with entropic regularization for an arbitrary number of points. Consequently,
the transformer can sort lists of arbitrary sizes up to an approximation
factor. Our results rely on an engineered prompt that enables the transformer
to implement gradient descent with adaptive stepsizes on the dual optimal
transport. Combining the convergence analysis of gradient descent with Sinkhorn
dynamics, we establish an explicit approximation bound for optimal transport
with transformers, which improves as depth increases. Our findings provide
novel insights into the essence of prompt engineering and depth for solving
optimal transport. In particular, prompt engineering boosts the algorithmic
expressivity of transformers, allowing them implement an optimization method.
With increasing depth, transformers can simulate several iterations of gradient
descent.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MAMMAL -- Molecular Aligned <span class="highlight-title">Multi-Modal</span> Architecture and Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22367v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22367v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yoel Shoshan, Moshiko Raboh, Michal Ozery-Flato, Vadim Ratner, Alex Golts, Jeffrey K. Weber, Ella Barkan, Simona Rabinovici-Cohen, Sagi Polaczek, Ido Amos, Ben Shapira, Liam Hazan, Matan Ninio, Sivan Ravid, Michael M. Danziger, Joseph A. Morrone, Parthasarathy Suryanarayanan, Michal Rosen-Zvi, Efrat Hexter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Drug discovery typically consists of multiple steps, including identifying a
target protein key to a disease's etiology, validating that interacting with
this target could prevent symptoms or cure the disease, discovering a small
molecule or biologic therapeutic to interact with it, and optimizing the
candidate molecule through a complex landscape of required properties. Drug
discovery related tasks often involve prediction and generation while
considering multiple entities that potentially interact, which poses a
challenge for typical AI models. For this purpose we present MAMMAL - Molecular
Aligned Multi-Modal Architecture and Language - a method that we applied to
create a versatile multi-task multi-align foundation model that learns from
large-scale biological datasets (2 billion samples) across diverse modalities,
including proteins, small molecules, and genes. We introduce a prompt syntax
that supports a wide range of classification, regression, and generation tasks.
It allows combining different modalities and entity types as inputs and/or
outputs. Our model handles combinations of tokens and scalars and enables the
generation of small molecules and proteins, property prediction, and
transcriptomic lab test predictions. We evaluated the model on 11 diverse
downstream tasks spanning different steps within a typical drug discovery
pipeline, where it reaches new SOTA in 9 tasks and is comparable to SOTA in 2
tasks. This performance is achieved while using a unified architecture serving
all tasks, in contrast to the original SOTA performance achieved using tailored
architectures.
  The model code and pretrained weights are publicly available at
https://github.com/BiomedSciAI/biomed-multi-alignment and
https://huggingface.co/ibm/biomed.omics.bl.sm.ma-ted-458m.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Study of Plasticity Loss in On-Policy Deep Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19153v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19153v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arthur Juliani, Jordan T. Ash
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual learning with deep neural networks presents challenges distinct
from both the fixed-dataset and convex continual learning regimes. One such
challenge is plasticity loss, wherein a neural network trained in an online
fashion displays a degraded ability to fit new tasks. This problem has been
extensively studied in both supervised learning and off-policy reinforcement
learning (RL), where a number of remedies have been proposed. Still, plasticity
loss has received less attention in the on-policy deep RL setting. Here we
perform an extensive set of experiments examining plasticity loss and a variety
of mitigation methods in on-policy deep RL. We demonstrate that plasticity loss
is pervasive under domain shift in this regime, and that a number of methods
developed to resolve it in other settings fail, sometimes even performing worse
than applying no intervention at all. In contrast, we find that a class of
``regenerative'' methods are able to consistently mitigate plasticity loss in a
variety of contexts, including in gridworld tasks and more challenging
environments like Montezuma's Revenge and ProcGen.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Human-in-the-Loop Causal Discovery under Latent Confounding using
  Ancestral GFlowNets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.12032v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.12032v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tiago da Silva, Eliezer Silva, António Góis, Dominik Heider, Samuel Kaski, Diego Mesquita, Adèle Ribeiro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Structure learning is the crux of causal inference. Notably, causal discovery
(CD) algorithms are brittle when data is scarce, possibly inferring imprecise
causal relations that contradict expert knowledge -- especially when
considering latent confounders. To aggravate the issue, most CD methods do not
provide uncertainty estimates, making it hard for users to interpret results
and improve the inference process. Surprisingly, while CD is a human-centered
affair, no works have focused on building methods that both 1) output
uncertainty estimates that can be verified by experts and 2) interact with
those experts to iteratively refine CD. To solve these issues, we start by
proposing to sample (causal) ancestral graphs proportionally to a belief
distribution based on a score function, such as the Bayesian information
criterion (BIC), using generative flow networks. Then, we leverage the
diversity in candidate graphs and introduce an optimal experimental design to
iteratively probe the expert about the relations among variables, effectively
reducing the uncertainty of our belief over ancestral graphs. Finally, we
update our samples to incorporate human feedback via importance sampling.
Importantly, our method does not require causal sufficiency (i.e., unobserved
confounders may exist). Experiments with synthetic observational data show that
our method can accurately sample from distributions over ancestral graphs and
that we can greatly improve inference quality with human aid.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Adversarial Training in LLMs with Continuous Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15589v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15589v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sophie Xhonneux, Alessandro Sordoni, Stephan Günnemann, Gauthier Gidel, Leo Schwinn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are vulnerable to adversarial attacks that can
bypass their safety guardrails. In many domains, adversarial training has
proven to be one of the most promising methods to reliably improve robustness
against such attacks. Yet, in the context of LLMs, current methods for
adversarial training are hindered by the high computational costs required to
perform discrete adversarial attacks at each training iteration. We address
this problem by instead calculating adversarial attacks in the continuous
embedding space of the LLM, which is orders of magnitudes more efficient. We
propose a fast adversarial training algorithm (C-AdvUL) composed of two losses:
the first makes the model robust on continuous embedding attacks computed on an
adversarial behaviour dataset; the second ensures the usefulness of the final
model by fine-tuning on utility data. Moreover, we introduce C-AdvIPO, an
adversarial variant of IPO that does not require utility data for adversarially
robust alignment. Our empirical evaluation on five models from different
families (Gemma, Phi3, Mistral, Zephyr, Llama2) and at different scales (2B,
3.8B, 7B) shows that both algorithms substantially enhance LLM robustness
against discrete attacks (GCG, AutoDAN, PAIR), while maintaining utility. Our
results demonstrate that robustness to continuous perturbations can extrapolate
to discrete threat models. Thereby, we present a path toward scalable
adversarial training algorithms for robustly aligning LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diffusion Spectral Representation for Reinforcement Learning <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16121v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16121v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dmitry Shribak, Chen-Xiao Gao, Yitong Li, Chenjun Xiao, Bo Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion-based models have achieved notable empirical successes in
reinforcement learning (RL) due to their expressiveness in modeling complex
distributions. Despite existing methods being promising, the key challenge of
extending existing methods for broader real-world applications lies in the
computational cost at inference time, i.e., sampling from a diffusion model is
considerably slow as it often requires tens to hundreds of iterations to
generate even one sample. To circumvent this issue, we propose to leverage the
flexibility of diffusion models for RL from a representation learning
perspective. In particular, by exploiting the connection between diffusion
models and energy-based models, we develop Diffusion Spectral Representation
(Diff-SR), a coherent algorithm framework that enables extracting sufficient
representations for value functions in Markov decision processes (MDP) and
partially observable Markov decision processes (POMDP). We further demonstrate
how Diff-SR facilitates efficient policy optimization and practical algorithms
while explicitly bypassing the difficulty and inference cost of sampling from
the diffusion model. Finally, we provide comprehensive empirical studies to
verify the benefits of Diff-SR in delivering robust and advantageous
performance across various benchmarks with both fully and partially observable
settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Limitations of Fractal Dimension as a Measure of Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.02234v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.02234v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charlie B. Tan, Inés García-Redondo, Qiquan Wang, Michael M. Bronstein, Anthea Monod
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bounding and predicting the generalization gap of overparameterized neural
networks remains a central open problem in theoretical machine learning. There
is a recent and growing body of literature that proposes the framework of
fractals to model optimization trajectories of neural networks, motivating
generalization bounds and measures based on the fractal dimension of the
trajectory. Notably, the persistent homology dimension has been proposed to
correlate with the generalization gap. This paper performs an empirical
evaluation of these persistent homology-based generalization measures, with an
in-depth statistical analysis. Our study reveals confounding effects in the
observed correlation between generalization and topological measures due to the
variation of hyperparameters. We also observe that fractal dimension fails to
predict generalization of models trained from poor initializations. We lastly
reveal the intriguing manifestation of model-wise double descent in these
topological generalization measures. Our work forms a basis for a deeper
investigation of the causal relationships between fractal geometry, topological
data analysis, and neural network optimization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SelfCodeAlign: Self-Alignment for Code Generation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24198v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24198v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxiang Wei, Federico Cassano, Jiawei Liu, Yifeng Ding, Naman Jain, Zachary Mueller, Harm de Vries, Leandro von Werra, Arjun Guha, Lingming Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction tuning is a supervised fine-tuning approach that significantly
improves the ability of large language models (LLMs) to follow human
instructions. We propose SelfCodeAlign, the first fully transparent and
permissive pipeline for self-aligning code LLMs without extensive human
annotations or distillation. SelfCodeAlign employs the same base model for
inference throughout the data generation process. It first extracts diverse
coding concepts from high-quality seed snippets to generate new tasks. It then
samples multiple responses per task, pairs each with test cases, and validates
them in a sandbox environment. Finally, passing examples are selected for
instruction tuning. In our primary experiments, we use SelfCodeAlign with
CodeQwen1.5-7B to generate a dataset of 74k instruction-response pairs.
Finetuning on this dataset leads to a model that achieves a 67.1 pass@1 on
HumanEval+, surpassing CodeLlama-70B-Instruct despite being ten times smaller.
Across all benchmarks, this finetuned model consistently outperforms the
original version trained with OctoPack, the previous state-of-the-art method
for instruction tuning without human annotations or distillation. Additionally,
we show that SelfCodeAlign is effective across LLMs of various sizes, from 3B
to 33B, and that the base models can benefit more from alignment with their own
data distribution. We further validate each component's effectiveness in our
pipeline, showing that SelfCodeAlign outperforms both direct distillation from
GPT-4o and leading GPT-3.5-based distillation methods, such as OSS-Instruct and
Evol-Instruct. SelfCodeAlign has also led to the creation of
StarCoder2-Instruct, the first fully transparent, permissively licensed, and
self-aligned code LLM that achieves state-of-the-art coding performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Flexible Fairness-Aware Learning via Inverse Conditional Permutation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.05678v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.05678v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuheng Lai, Leying Guan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Equalized odds, as a popular notion of algorithmic fairness, aims to ensure
that sensitive variables, such as race and gender, do not unfairly influence
the algorithm's prediction when conditioning on the true outcome. Despite rapid
advancements, current research primarily focuses on equalized odds violations
caused by a single sensitive attribute, leaving the challenge of simultaneously
accounting for multiple attributes largely unaddressed. We bridge this gap by
introducing an in-processing fairness-aware learning approach, FairICP, which
integrates adversarial learning with a novel inverse conditional permutation
scheme. FairICP offers a theoretically justified, flexible, and efficient
scheme to promote equalized odds under fairness conditions described by complex
and multidimensional sensitive attributes. The efficacy and adaptability of our
method are demonstrated through both simulation studies and empirical analyses
of real-world datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Simplifying Latent Dynamics with Softly State-Invariant World Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.17835v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.17835v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tankred Saanum, Peter Dayan, Eric Schulz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To solve control problems via model-based reasoning or planning, an agent
needs to know how its actions affect the state of the world. The actions an
agent has at its disposal often change the state of the environment in
systematic ways. However, existing techniques for world modelling do not
guarantee that the effect of actions are represented in such systematic ways.
We introduce the Parsimonious Latent Space Model (PLSM), a world model that
regularizes the latent dynamics to make the effect of the agent's actions more
predictable. Our approach minimizes the mutual information between latent
states and the change that an action produces in the agent's latent state, in
turn minimizing the dependence the state has on the dynamics. This makes the
world model softly state-invariant. We combine PLSM with different model
classes used for i) future latent state prediction, ii) planning, and iii)
model-free reinforcement learning. We find that our regularization improves
accuracy, generalization, and performance in downstream tasks, highlighting the
importance of systematic treatment of actions in world models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Designing User-Centric Behavioral Interventions to Prevent Dysglycemia
  with Novel Counterfactual Explanations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.01684v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.01684v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Asiful Arefeen, Hassan Ghasemzadeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monitoring unexpected health events and taking actionable measures to avert
them beforehand is central to maintaining health and preventing disease.
Therefore, a tool capable of predicting adverse health events and offering
users actionable feedback about how to make changes in their diet, exercise,
and medication to prevent abnormal health events could have significant
societal impacts. Counterfactual explanations can provide insights into why a
model made a particular prediction by generating hypothetical instances that
are similar to the original input but lead to a different prediction outcome.
Therefore, counterfactuals can be viewed as a means to design AI-driven health
interventions to not only predict but also prevent adverse health outcomes such
as blood glucose spikes, diabetes, and heart disease. In this paper, we design
\textit{\textbf{ExAct}}, a novel model-agnostic framework for generating
counterfactual explanations for chronic disease prevention and management.
Leveraging insights from adversarial learning, ExAct characterizes the decision
boundary for high-dimensional data and performs a grid search to generate
actionable interventions. ExAct is unique in integrating prior knowledge about
user preferences of feasible explanations into the process of counterfactual
generation. ExAct is evaluated extensively using four real-world datasets and
external simulators. With $82.8\%$ average validity in the simulation-aided
validation, ExAct surpasses the state-of-the-art techniques for generating
counterfactual explanations by at least $10\%$. Besides, counterfactuals from
ExAct exhibit at least $6.6\%$ improved proximity compared to previous
research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Node Representation by Boosting Target-Aware Contrastive Loss 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03901v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03901v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ying-Chun Lin, Jennifer Neville
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graphs model complex relationships between entities, with nodes and edges
capturing intricate connections. Node representation learning involves
transforming nodes into low-dimensional embeddings. These embeddings are
typically used as features for downstream tasks. Therefore, their quality has a
significant impact on task performance. Existing approaches for node
representation learning span (semi-)supervised, unsupervised, and
self-supervised paradigms. In graph domains, (semi-)supervised learning often
only optimizes models based on class labels, neglecting other abundant graph
signals, which limits generalization. While self-supervised or unsupervised
learning produces representations that better capture underlying graph signals,
the usefulness of these captured signals for downstream target tasks can vary.
To bridge this gap, we introduce Target-Aware Contrastive Learning
(Target-aware CL) which aims to enhance target task performance by maximizing
the mutual information between the target task and node representations with a
self-supervised learning process. This is achieved through a sampling function,
XGBoost Sampler (XGSampler), to sample proper positive examples for the
proposed Target-Aware Contrastive Loss (XTCL). By minimizing XTCL, Target-aware
CL increases the mutual information between the target task and node
representations, such that model generalization is improved. Additionally,
XGSampler enhances the interpretability of each signal by showing the weights
for sampling the proper positive examples. We show experimentally that XTCL
significantly improves the performance on two target tasks: node classification
and link prediction tasks, compared to state-of-the-art models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dimension-free deterministic equivalents for random feature regression <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15699v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15699v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonardo Defilippis, Bruno Loureiro, Theodor Misiakiewicz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work we investigate the generalization performance of random feature
ridge regression (RFRR). Our main contribution is a general deterministic
equivalent for the test error of RFRR. Specifically, under a certain
concentration property, we show that the test error is well approximated by a
closed-form expression that only depends on the feature map eigenvalues.
Notably, our approximation guarantee is non-asymptotic, multiplicative, and
independent of the feature map dimension -- allowing for infinite-dimensional
features. We expect this deterministic equivalent to hold broadly beyond our
theoretical analysis, and we empirically validate its predictions on various
real and synthetic datasets. As an application, we derive sharp excess error
rates under standard power-law assumptions of the spectrum and target decay. In
particular, we provide a tight result for the smallest number of features
achieving optimal minimax error rate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 camera-ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Recurrent Neural Networks for Predicting Motor Movements from
  Primate Motor Cortex Neural Recordings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22283v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22283v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanxi Wang, Zuowen Wang, Shih-Chii Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an efficient deep learning solution for decoding motor
movements from neural recordings in non-human primates. An Autoencoder Gated
Recurrent Unit (AEGRU) model was adopted as the model architecture for this
task. The autoencoder is only used during the training stage to achieve better
generalization. Together with the preprocessing techniques, our model achieved
0.71 $R^2$ score, surpassing the baseline models in Neurobench and is ranked
first for $R^2$ in the IEEE BioCAS 2024 Grand Challenge on Neural Decoding.
Model pruning is also applied leading to a reduction of 41.4% of the
multiply-accumulate (MAC) operations with little change in the $R^2$ score
compared to the unpruned model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nova: A Practical and Advanced Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14940v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14940v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingan Lin, Fan Yang, Yanjun Shen, Haoze Sun, Tianpeng Li, Tao Zhang, Chenzheng Zhu, Tao Zhang, Miao Zheng, Xu Li, Yijie Zhou, Mingyang Chen, Yanzhao Qin, Youquan Li, Hao Liang, Fei Li, Yadong Li, Mang Wang, Guosheng Dong, Kun Fang, Jianhua Xu, Bin Cui, Wentao Zhang, Zenan Zhou, Weipeng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Nova, a suite of practical alignment techniques employed in a
series of empirically validated high-performing models. This represents the
first comprehensive account of alignment methodologies, offering valuable
insights for advancing AI research. We investigate the critical components that
enhance model performance during the alignment process, including optimization
methods, data strategies, capability enhancements, and evaluation processes.
The process spans three key stages: Prompt Augmentation System(PAS), Supervised
Fine-Tuning(SFT), and Preference Alignment. The problems encountered, the
solutions applied, and the improvements made are thoroughly recorded.
  Through comparisons across well-established benchmarks, we highlight the
technological advancements enabled by Nova Alignment. Importantly,
Qwen2-Nova-72B and Llama3-PBM-Nova-70B are instruct versions of the Qwen2-72B
and Llama-3-70B base models, optimized through Nova. The Nova models show
significant core improvements, with user experience gains of 17% to 28%, and
excels on specialized benchmarks. In open-source benchmark evaluations, both
Qwen2-Nova-72B and Llama3-PBM-Nova-70B consistently outperform their respective
official instruct versions across nearly all datasets. This report aims to
clarify the key technologies behind the alignment process, fostering a deeper
understanding within the community. Llama3-PBM-Nova-70B model is available at
https://huggingface.co/PKU-Baichuan-MLSystemLab/Llama3-PBM-Nova-70B.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Erasing Self-Supervised Learning Backdoor by Cluster Activation Masking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.07955v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.07955v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengsheng Qian, Dizhan Xue, Yifei Wang, Shengjie Zhang, Huaiwen Zhang, Changsheng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-Supervised Learning (SSL) is an effective paradigm for learning
representations from unlabeled data, such as text, images, and videos. However,
researchers have recently found that SSL is vulnerable to backdoor attacks. The
attacker can embed hidden SSL backdoors via a few poisoned examples in the
training dataset and maliciously manipulate the behavior of downstream models.
To defend against SSL backdoor attacks, a feasible route is to detect and
remove the poisonous samples in the training set. However, the existing SSL
backdoor defense method fails to detect the poisonous samples precisely. In
this paper, we propose to erase the SSL backdoor by cluster activation masking
and propose a novel PoisonCAM method. After obtaining the threat model trained
on the poisoned dataset, our method can precisely detect poisonous samples
based on the assumption that masking the backdoor trigger can effectively
change the activation of a downstream clustering model. In experiments, our
PoisonCAM achieves 96\% accuracy for backdoor trigger detection compared to 3\%
of the state-of-the-art method on poisoned ImageNet-100. Moreover, our proposed
PoisonCAM significantly improves the performance of the trained SSL model under
backdoor attacks compared to the state-of-the-art method. Our code, data, and
trained models will be open once this paper is accepted.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neur2BiLO: Neural Bilevel Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.02552v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.02552v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Justin Dumouchelle, Esther Julien, Jannis Kurtz, Elias B. Khalil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bilevel optimization deals with nested problems in which a leader takes the
first decision to minimize their objective function while accounting for a
follower's best-response reaction. Constrained bilevel problems with integer
variables are particularly notorious for their hardness. While exact solvers
have been proposed for mixed-integer linear bilevel optimization, they tend to
scale poorly with problem size and are hard to generalize to the non-linear
case. On the other hand, problem-specific algorithms (exact and heuristic) are
limited in scope. Under a data-driven setting in which similar instances of a
bilevel problem are solved routinely, our proposed framework, Neur2BiLO, embeds
a neural network approximation of the leader's or follower's value function,
trained via supervised regression, into an easy-to-solve mixed-integer program.
Neur2BiLO serves as a heuristic that produces high-quality solutions extremely
fast for four applications with linear and non-linear objectives and pure and
mixed-integer variables.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ QuanTA: Efficient High-Rank <span class="highlight-title">Fine-Tuning</span> of LLMs with Quantum-Informed
  Tensor Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.00132v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.00132v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuo Chen, Rumen Dangovski, Charlotte Loh, Owen Dugan, Di Luo, Marin Soljačić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Quantum-informed Tensor Adaptation (QuanTA), a novel,
easy-to-implement, fine-tuning method with no inference overhead for
large-scale pre-trained language models. By leveraging quantum-inspired methods
derived from quantum circuit structures, QuanTA enables efficient high-rank
fine-tuning, surpassing the limitations of Low-Rank Adaptation (LoRA)--low-rank
approximation may fail for complicated downstream tasks. Our approach is
theoretically supported by the universality theorem and the rank representation
theorem to achieve efficient high-rank adaptations. Experiments demonstrate
that QuanTA significantly enhances commonsense reasoning, arithmetic reasoning,
and scalability compared to traditional methods. Furthermore, QuanTA shows
superior performance with fewer trainable parameters compared to other
approaches and can be designed to integrate with existing fine-tuning
algorithms for further improvement, providing a scalable and efficient solution
for fine-tuning large language models and advancing state-of-the-art in natural
language processing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reinforced In-Context Black-Box Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17423v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17423v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Song, Chenxiao Gao, Ke Xue, Chenyang Wu, Dong Li, Jianye Hao, Zongzhang Zhang, Chao Qian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Black-Box Optimization (BBO) has found successful applications in many fields
of science and engineering. Recently, there has been a growing interest in
meta-learning particular components of BBO algorithms to speed up optimization
and get rid of tedious hand-crafted heuristics. As an extension, learning the
entire algorithm from data requires the least labor from experts and can
provide the most flexibility. In this paper, we propose RIBBO, a method to
reinforce-learn a BBO algorithm from offline data in an end-to-end fashion.
RIBBO employs expressive sequence models to learn the optimization histories
produced by multiple behavior algorithms and tasks, leveraging the in-context
learning ability of large models to extract task information and make decisions
accordingly. Central to our method is to augment the optimization histories
with \textit{regret-to-go} tokens, which are designed to represent the
performance of an algorithm based on cumulative regret over the future part of
the histories. The integration of regret-to-go tokens enables RIBBO to
automatically generate sequences of query points that satisfy the user-desired
regret, which is verified by its universally good empirical performance on
diverse problems, including BBO benchmark functions, hyper-parameter
optimization and robot control problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Multimodal</span> Fusion on Low-quality Data: A Comprehensive Survey 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.18947v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.18947v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyang Zhang, Yake Wei, Zongbo Han, Huazhu Fu, Xi Peng, Cheng Deng, Qinghua Hu, Cai Xu, Jie Wen, Di Hu, Changqing Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal fusion focuses on integrating information from multiple modalities
with the goal of more accurate prediction, which has achieved remarkable
progress in a wide range of scenarios, including autonomous driving and medical
diagnosis. However, the reliability of multimodal fusion remains largely
unexplored especially under low-quality data settings. This paper surveys the
common challenges and recent advances of multimodal fusion in the wild and
presents them in a comprehensive taxonomy. From a data-centric view, we
identify four main challenges that are faced by multimodal fusion on
low-quality data, namely (1) noisy multimodal data that are contaminated with
heterogeneous noises, (2) incomplete multimodal data that some modalities are
missing, (3) imbalanced multimodal data that the qualities or properties of
different modalities are significantly different and (4) quality-varying
multimodal data that the quality of each modality dynamically changes with
respect to different samples. This new taxonomy will enable researchers to
understand the state of the field and identify several potential directions. We
also provide discussion for the open problems in this field together with
interesting future research directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Feel free to comment on our manuscript: qingyangzhang@tju$.$edu$.$cn</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Analysis of Bootstrap and Subsampling in High-dimensional Regularized
  Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13622v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13622v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucas Clarté, Adrien Vandenbroucque, Guillaume Dalle, Bruno Loureiro, Florent Krzakala, Lenka Zdeborová
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate popular resampling methods for estimating the uncertainty of
statistical models, such as subsampling, bootstrap and the jackknife, and their
performance in high-dimensional supervised regression tasks. We provide a tight
asymptotic description of the biases and variances estimated by these methods
in the context of generalized linear models, such as ridge and logistic
regression, taking the limit where the number of samples $n$ and dimension $d$
of the covariates grow at a comparable fixed rate $\alpha\!=\! n/d$. Our
findings are three-fold: i) resampling methods are fraught with problems in
high dimensions and exhibit the double-descent-like behavior typical of these
situations; ii) only when $\alpha$ is large enough do they provide consistent
and reliable error estimations (we give convergence rates); iii) in the
over-parametrized regime $\alpha\!<\!1$ relevant to modern machine learning
practice, their predictions are not consistent, even with optimal
regularization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Theoretical Foundations of Deep Selective State-Space Models <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.19047v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.19047v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicola Muca Cirone, Antonio Orvieto, Benjamin Walker, Cristopher Salvi, Terry Lyons
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Structured state-space models (SSMs) such as S4, stemming from the seminal
work of Gu et al., are gaining popularity as effective approaches for modeling
sequential data. Deep SSMs demonstrate outstanding performance across a diverse
set of domains, at a reduced training and inference cost compared to
attention-based transformers. Recent developments show that if the linear
recurrence powering SSMs allows for multiplicative interactions between inputs
and hidden states (e.g. GateLoop, Mamba, GLA), then the resulting architecture
can surpass in both in accuracy and efficiency attention-powered foundation
models trained on text, at scales of billion parameters. In this paper, we give
theoretical grounding to this recent finding using tools from Rough Path
Theory: we show that when random linear recurrences are equipped with simple
input-controlled transitions (selectivity mechanism), then the hidden state is
provably a low-dimensional projection of a powerful mathematical object called
the signature of the input -- capturing non-linear interactions between tokens
at distinct timescales. Our theory not only motivates the success of modern
selective state-space models such as Mamba but also provides a solid framework
to understand the expressive power of future SSM variants.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS Version w/ minor edits</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accelerating Transfer Learning with Near-Data Computation on Cloud
  Object Stores <span class="chip">SoCC '24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.08650v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.08650v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Diana Petrescu, Arsany Guirguis, Do Le Quoc, Javier Picorel, Rachid Guerraoui, Florin Dinu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Storage disaggregation underlies today's cloud and is naturally complemented
by pushing down some computation to storage, thus mitigating the potential
network bottleneck between the storage and compute tiers. We show how ML
training benefits from storage pushdowns by focusing on transfer learning (TL),
the widespread technique that democratizes ML by reusing existing knowledge on
related tasks. We propose HAPI, a new TL processing system centered around two
complementary techniques that address challenges introduced by disaggregation.
First, applications must carefully balance execution across tiers for
performance. HAPI judiciously splits the TL computation during the feature
extraction phase yielding pushdowns that not only improve network time but also
improve total TL training time by overlapping the execution of consecutive
training iterations across tiers. Second, operators want resource efficiency
from the storage-side computational resources. HAPI employs storage-side batch
size adaptation allowing increased storage-side pushdown concurrency without
affecting training accuracy. HAPI yields up to 2.5x training speed-up while
choosing in 86.8% of cases the best performing split point or one that is at
most 5% off from the best.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in the proceedings of SoCC '24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Kuro Siwo: 33 billion $m^2$ under the water. A global multi-temporal
  satellite dataset for rapid flood mapping <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12056v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12056v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolaos Ioannis Bountos, Maria Sdraka, Angelos Zavras, Ilektra Karasante, Andreas Karavias, Themistocles Herekakis, Angeliki Thanasou, Dimitrios Michail, Ioannis Papoutsis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Global floods, exacerbated by climate change, pose severe threats to human
life, infrastructure, and the environment. Recent catastrophic events in
Pakistan and New Zealand underscore the urgent need for precise flood mapping
to guide restoration efforts, understand vulnerabilities, and prepare for
future occurrences. While Synthetic Aperture Radar (SAR) remote sensing offers
day-and-night, all-weather imaging capabilities, its application in deep
learning for flood segmentation is limited by the lack of large annotated
datasets. To address this, we introduce Kuro Siwo, a manually annotated
multi-temporal dataset, spanning 43 flood events globally. Our dataset maps
more than 338 billion $m^2$ of land, with 33 billion designated as either
flooded areas or permanent water bodies. Kuro Siwo includes a highly processed
product optimized for flood mapping based on SAR Ground Range Detected, and a
primal SAR Single Look Complex product with minimal preprocessing, designed to
promote research on the exploitation of both the phase and amplitude
information and to offer maximum flexibility for downstream task preprocessing.
To leverage advances in large scale self-supervised pretraining methods for
remote sensing data, we augment Kuro Siwo with a large unlabeled set of SAR
samples. Finally, we provide an extensive benchmark, namely BlackBench,
offering strong baselines for a diverse set of flood events from Europe,
America, Africa, Asia and Australia.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 38th Conference on Neural Information Processing
  Systems (NeurIPS 2024) Track on Datasets and Benchmarks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On-Air Deep Learning Integrated Semantic Inference Models for Enhanced
  Earth Observation Satellite Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15246v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15246v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hong-fu Chou, Vu Nguyen Ha, Prabhu Thiruvasagam, Thanh-Dung Le, Geoffrey Eappen, Ti Ti Nguyen, Luis M. Garces-Socarras, Jorge L. Gonzalez-Rios, Juan Carlos Merlano-Duncan, Symeon Chatzinotas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Earth Observation (EO) systems are crucial for cartography, disaster
surveillance, and resource administration. Nonetheless, they encounter
considerable obstacles in the processing and transmission of extensive data,
especially in specialized domains such as precision agriculture and real-time
disaster response. Earth observation satellites, outfitted with remote sensing
technology, gather data from onboard sensors and IoT-enabled terrestrial
objects, delivering important information remotely. Domain-adapted Large
Language Models (LLMs) provide a solution by enabling the integration of raw
and processed EO data. Through domain adaptation, LLMs improve the assimilation
and analysis of many data sources, tackling the intricacies of specialized
datasets in agriculture and disaster response. This data synthesis, directed by
LLMs, enhances the precision and pertinence of conveyed information. This study
provides a thorough examination of using semantic inference and deep learning
for sophisticated EO systems. It presents an innovative architecture for
semantic communication in EO satellite networks, designed to improve data
transmission efficiency using semantic processing methodologies. Recent
advancements in onboard processing technologies enable dependable, adaptable,
and energy-efficient data management in orbit. These improvements guarantee
reliable performance in adverse space circumstances using radiation-hardened
and reconfigurable technology. Collectively, these advancements enable
next-generation satellite missions with improved processing capabilities,
crucial for operational flexibility and real-time decision-making in 6G
satellite communication.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 7 figures, Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Intruding with Words: Towards Understanding Graph Injection Attacks at
  the Text Level <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.16405v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.16405v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runlin Lei, Yuwei Hu, Yuchen Ren, Zhewei Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) excel across various applications but remain
vulnerable to adversarial attacks, particularly Graph Injection Attacks (GIAs),
which inject malicious nodes into the original graph and pose realistic
threats. Text-attributed graphs (TAGs), where nodes are associated with textual
features, are crucial due to their prevalence in real-world applications and
are commonly used to evaluate these vulnerabilities. However, existing research
only focuses on embedding-level GIAs, which inject node embeddings rather than
actual textual content, limiting their applicability and simplifying detection.
In this paper, we pioneer the exploration of GIAs at the text level, presenting
three novel attack designs that inject textual content into the graph. Through
theoretical and empirical analysis, we demonstrate that text interpretability,
a factor previously overlooked at the embedding level, plays a crucial role in
attack strength. Among the designs we investigate, the Word-frequency-based
Text-level GIA (WTGIA) is particularly notable for its balance between
performance and interpretability. Despite the success of WTGIA, we discover
that defenders can easily enhance their defenses with customized text embedding
methods or large language model (LLM)--based predictors. These insights
underscore the necessity for further research into the potential and practical
significance of text-level GIAs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tree Ensembles for Contextual Bandits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.06963v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.06963v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hannes Nilsson, Rikard Johansson, Niklas Åkerblom, Morteza Haghir Chehreghani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new framework for contextual multi-armed bandits based on tree
ensembles. Our framework adapts two widely used bandit methods, Upper
Confidence Bound and Thompson Sampling, for both standard and combinatorial
settings. As part of this framework, we propose a novel method of estimating
the uncertainty in tree ensemble predictions. We further demonstrate the
effectiveness of our framework via several experimental studies, employing
XGBoost and random forests, two popular tree ensemble methods. Compared to
state-of-the-art methods based on decision trees and neural networks, our
methods exhibit superior performance in terms of both regret minimization and
computational runtime, when applied to benchmark datasets and the real-world
application of navigation over road networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contributed equally to this work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bounds and Sensitivity Analysis of the Causal Effect Under
  Outcome-Independent MNAR Confounding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06726v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06726v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jose M. Peña
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We report assumption-free bounds for any contrast between the probabilities
of the potential outcome under exposure and non-exposure when the confounders
are missing not at random. We assume that the missingness mechanism is
outcome-independent. We also report a sensitivity analysis method to complement
our bounds.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unity by Diversity: Improved Representation Learning in <span class="highlight-title">Multimodal</span> VAEs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05300v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05300v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas M. Sutter, Yang Meng, Andrea Agostini, Daphné Chopard, Norbert Fortin, Julia E. Vogt, Bahbak Shahbaba, Stephan Mandt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Variational Autoencoders for multimodal data hold promise for many tasks in
data analysis, such as representation learning, conditional generation, and
imputation. Current architectures either share the encoder output, decoder
input, or both across modalities to learn a shared representation. Such
architectures impose hard constraints on the model. In this work, we show that
a better latent representation can be obtained by replacing these hard
constraints with a soft constraint. We propose a new mixture-of-experts prior,
softly guiding each modality's latent representation towards a shared aggregate
posterior. This approach results in a superior latent representation and allows
each encoding to preserve information better from its uncompressed original
features. In extensive experiments on multiple benchmark datasets and two
challenging real-world datasets, we show improved learned latent
representations and imputation of missing data modalities compared to existing
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Neurips 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficiency for Free: Ideal Data Are Transportable Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14669v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14669v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Sun, Yi Jiang, Tao Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data, the seminal opportunity and challenge in modern machine learning,
currently constrains the scalability of representation learning and impedes the
pace of model evolution. In this work, we investigate the efficiency properties
of data from both optimization and generalization perspectives. Our theoretical
and empirical analysis reveals an unexpected finding: for a given task,
utilizing a publicly available, task- and architecture-agnostic model (referred
to as the `prior model' in this paper) can effectively produce efficient data.
Building on this insight, we propose the Representation Learning Accelerator
(\algopt), which promotes the formation and utilization of efficient data,
thereby accelerating representation learning. Utilizing a ResNet-18 pre-trained
on CIFAR-10 as a prior model to inform ResNet-50 training on ImageNet-1K
reduces computational costs by 50% while maintaining the same accuracy as the
model trained with the original BYOL, which requires 100% cost. Our code is
available at: \url{https://github.com/LINs-lab/ReLA}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/LINs-lab/ReLA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improved Generation of Adversarial Examples Against Safety-aligned LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.20778v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.20778v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qizhang Li, Yiwen Guo, Wangmeng Zuo, Hao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial prompts generated using gradient-based methods exhibit
outstanding performance in performing automatic jailbreak attacks against
safety-aligned LLMs. Nevertheless, due to the discrete nature of texts, the
input gradient of LLMs struggles to precisely reflect the magnitude of loss
change that results from token replacements in the prompt, leading to limited
attack success rates against safety-aligned LLMs, even in the white-box
setting. In this paper, we explore a new perspective on this problem,
suggesting that it can be alleviated by leveraging innovations inspired in
transfer-based attacks that were originally proposed for attacking black-box
image classification models. For the first time, we appropriate the ideologies
of effective methods among these transfer-based attacks, i.e., Skip Gradient
Method and Intermediate Level Attack, into gradient-based adversarial prompt
generation and achieve significant performance gains without introducing
obvious computational cost. Meanwhile, by discussing mechanisms behind the
gains, new insights are drawn, and proper combinations of these methods are
also developed. Our empirical results show that 87% of the query-specific
adversarial suffixes generated by the developed combination can induce
Llama-2-7B-Chat to produce the output that exactly matches the target string on
AdvBench. This match rate is 33% higher than that of a very strong baseline
known as GCG, demonstrating advanced discrete optimization for adversarial
prompt generation against LLMs. In addition, without introducing obvious cost,
the combination achieves >30% absolute increase in attack success rates
compared with GCG when generating both query-specific (38% -> 68%) and
universal adversarial prompts (26.68% -> 60.32%) for attacking the
Llama-2-7B-Chat model on AdvBench. Code at:
https://github.com/qizhangli/Gradient-based-Jailbreak-Attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DASH: Warm-Starting Neural Network Training in Stationary Settings
  without Loss of Plasticity <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23495v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23495v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baekrok Shin, Junsoo Oh, Hanseul Cho, Chulhee Yun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Warm-starting neural network training by initializing networks with
previously learned weights is appealing, as practical neural networks are often
deployed under a continuous influx of new data. However, it often leads to loss
of plasticity, where the network loses its ability to learn new information,
resulting in worse generalization than training from scratch. This occurs even
under stationary data distributions, and its underlying mechanism is poorly
understood. We develop a framework emulating real-world neural network training
and identify noise memorization as the primary cause of plasticity loss when
warm-starting on stationary data. Motivated by this, we propose Direction-Aware
SHrinking (DASH), a method aiming to mitigate plasticity loss by selectively
forgetting memorized noise while preserving learned features. We validate our
approach on vision tasks, demonstrating improvements in test accuracy and
training efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TEAM: Topological Evolution-aware Framework for Traffic
  Forecasting--Extended Version <span class="chip">VLDB 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.19192v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.19192v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Duc Kieu, Tung Kieu, Peng Han, Bin Yang, Christian S. Jensen, Bac Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the global trend towards urbanization, people increasingly move to and
live in cities that then continue to grow. Traffic forecasting plays an
important role in the intelligent transportation systems of cities as well as
in spatio-temporal data mining. State-of-the-art forecasting is achieved by
deep-learning approaches due to their ability to contend with complex
spatio-temporal dynamics. However, existing methods assume the input is
fixed-topology road networks and static traffic time series. These assumptions
fail to align with urbanization, where time series are collected continuously
and road networks evolve over time. In such settings, deep-learning models
require frequent re-initialization and re-training, imposing high computational
costs. To enable much more efficient training without jeopardizing model
accuracy, we propose the Topological Evolution-aware Framework (TEAM) for
traffic forecasting that incorporates convolution and attention. This
combination of mechanisms enables better adaptation to newly collected time
series, while being able to maintain learned knowledge from old time series.
TEAM features a continual learning module based on the Wasserstein metric that
acts as a buffer that can identify the most stable and the most changing
network nodes. Then, only data related to stable nodes is employed for
re-training when consolidating a model. Further, only data of new nodes and
their adjacent nodes as well as data pertaining to changing nodes are used to
re-train the model. Empirical studies with two real-world traffic datasets
offer evidence that TEAM is capable of much lower re-training costs than
existing methods are, without jeopardizing forecasting accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages. An extended version of "TEAM: Topological Evolution-aware
  Framework for Traffic Forecasting" accepted at PVLDB 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamical similarity analysis uniquely captures how computations develop
  in RNNs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24070v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24070v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quentin Guilhot, Michał Wójcik, Jascha Achterberg, Rui Ponte Costa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Methods for analyzing representations in neural systems are increasingly
popular tools in neuroscience and mechanistic interpretability. Measures
comparing neural activations across conditions, architectures, and species give
scalable ways to understand information transformation within different neural
networks. However, recent findings show that some metrics respond to spurious
signals, leading to misleading results. Establishing benchmark test cases is
thus essential for identifying the most reliable metric and potential
improvements. We propose that compositional learning in recurrent neural
networks (RNNs) can provide a test case for dynamical representation alignment
metrics. Implementing this case allows us to evaluate if metrics can identify
representations that develop throughout learning and determine if
representations identified by metrics reflect the network's actual
computations. Building both attractor and RNN based test cases, we show that
the recently proposed Dynamical Similarity Analysis (DSA) is more noise robust
and reliably identifies behaviorally relevant representations compared to prior
metrics (Procrustes, CKA). We also demonstrate how such test cases can extend
beyond metric evaluation to study new architectures. Specifically, testing DSA
in modern (Mamba) state space models suggests that these models, unlike RNNs,
may not require changes in recurrent dynamics due to their expressive hidden
states. Overall, we develop test cases that showcase how DSA's enhanced ability
to detect dynamical motifs makes it highly effective for identifying ongoing
computations in RNNs and revealing how networks learn tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Systematic Survey on Large Language Models for Algorithm Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14716v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14716v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fei Liu, Yiming Yao, Ping Guo, Zhiyuan Yang, Zhe Zhao, Xi Lin, Xialiang Tong, Mingxuan Yuan, Zhichao Lu, Zhenkun Wang, Qingfu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Algorithm Design (AD) is crucial for effective problem-solving across various
domains. The advent of Large Language Models (LLMs) has notably enhanced the
automation and innovation within this field, offering new perspectives and
promising solutions. Over the past three years, the integration of LLMs into AD
(LLM4AD) has seen substantial progress, with applications spanning
optimization, machine learning, mathematical reasoning, and scientific
discovery. Given the rapid advancements and expanding scope of this field, a
systematic review is both timely and necessary. This paper provides a
systematic review of LLM4AD. First, we offer an overview and summary of
existing studies. Then, we introduce a taxonomy and review the literature
across four dimensions: the roles of LLMs, search methods, prompt methods, and
application domains with a discussion of potential and achievements of LLMs in
AD. Finally, we identify current challenges and highlight several promising
directions for future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Continuous-time q-learning for mean-field control problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.16208v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.16208v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoli Wei, Xiang Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies the q-learning, recently coined as the continuous time
counterpart of Q-learning by Jia and Zhou (2023), for continuous time
Mckean-Vlasov control problems in the setting of entropy-regularized
reinforcement learning. In contrast to the single agent's control problem in
Jia and Zhou (2023), the mean-field interaction of agents renders the
definition of the q-function more subtle, for which we reveal that two distinct
q-functions naturally arise: (i) the integrated q-function (denoted by $q$) as
the first-order approximation of the integrated Q-function introduced in Gu,
Guo, Wei and Xu (2023), which can be learnt by a weak martingale condition
involving test policies; and (ii) the essential q-function (denoted by $q_e$)
that is employed in the policy improvement iterations. We show that two
q-functions are related via an integral representation under all test policies.
Based on the weak martingale condition and our proposed searching method of
test policies, some model-free learning algorithms are devised. In two
examples, one in LQ control framework and one beyond LQ control framework, we
can obtain the exact parameterization of the optimal value function and
q-functions and illustrate our algorithms with simulation experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Keywords: Continuous-time reinforcement learning, integrated
  q-function, mean-field control, weak martingale characterization, test
  policies</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Shortcut-connected Expert Parallelism for Accelerating
  Mixture-of-Experts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.05019v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.05019v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weilin Cai, Juyong Jiang, Le Qin, Junwei Cui, Sunghun Kim, Jiayi Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Expert parallelism has been introduced as a strategy to distribute the
computational workload of sparsely-gated mixture-of-experts (MoE) models across
multiple computing devices, facilitating the execution of these increasingly
large-scale models. However, the All-to-All communication intrinsic to expert
parallelism constitutes a significant overhead, diminishing the MoE models'
efficiency. Current optimization approaches offer some relief, yet they are
constrained by the sequential interdependence of communication and computation
operations. To address this limitation, we present a novel shortcut-connected
MoE (ScMoE) architecture with an overlapping parallel strategy, which
effectively decouples communication from its conventional sequence, allowing
for a substantial overlap of 70% to 100% with computation. When compared with
the prevalent top-2 MoE architecture, ScMoE demonstrates training speed
improvements of 30% and 11%, and inference improvements of 40% and 15%, in our
distributed environments with PCIe and NVLink hardware, respectively, where
communication constitutes 60% and 15% of the total MoE time consumption.
Building on the ScMoE architecture, we further implement an expert offloading
strategy to facilitate memory-limited inference, optimizing latency through the
overlap of expert migration. Additionally, extensive experiments and
theoretical analyses indicate that ScMoE not only achieves comparable but in
some instances surpasses the model quality of existing approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Block Transformer: Global-to-Local Language Modeling for Fast Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.02657v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.02657v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Namgyu Ho, Sangmin Bae, Taehyeon Kim, Hyunjik Jo, Yireun Kim, Tal Schuster, Adam Fisch, James Thorne, Se-Young Yun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the Block Transformer which adopts hierarchical global-to-local
modeling to autoregressive transformers to mitigate the inference bottlenecks
associated with self-attention. Self-attention requires the key-value (KV)
cache of all previous sequences to be retrieved from memory at every decoding
step to retrieve context information, leading to two primary bottlenecks during
batch inference. First, there is a significant delay in obtaining the first
token, as the information of the entire prompt must first be processed to
prefill the KV cache. Second, computation of subsequent tokens is bottlenecked
by the high memory I/O demand of fetching the entire KV cache, which grows
linearly with sequence length, incurring quadratic memory reads overall. We
design the Block Transformer to strategically mitigate these costs, by
incorporating coarsity and locality into an integrated global-to-local
architecture. At the lower layers, we aggregate tokens into fixed size blocks
to apply attention across the entire sequence at coarse-grained detail, to
capture the global context while minimizing KV cache overhead. At upper layers,
we apply attention within each block to decode individual tokens, to model
fine-grained details with a lightweight local KV cache. We pretrain vanilla and
Block Transformers from scratch and demonstrate that Block Transformers reach
10--20x inference throughput compared to vanilla transformers with equivalent
perplexity and zero-shot task performance. Code is available at
https://github.com/itsnamgyu/block-transformer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages, 24 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CrysToGraph: A Comprehensive Predictive Model for Crystal Materials
  Properties and the Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.16131v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.16131v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyi Wang, Ji Sun, Jinzhe Liang, Li Zhai, Zitian Tang, Zijian Li, Wei Zhai, Xusheng Wang, Weihao Gao, Sheng Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ionic bonding across the lattice and ordered microscopic structures endow
crystals with unique symmetry and determine their macroscopic properties.
Unconventional crystals, in particular, exhibit non-traditional lattice
structures or possess exotic physical properties, making them intriguing
subjects for investigation. Therefore, to accurately predict the physical and
chemical properties of crystals, it is crucial to consider long-range orders.
While GNN excels at capturing the local environment of atoms in crystals, they
often face challenges in effectively capturing longer-ranged interactions due
to their limited depth. In this paper, we propose CrysToGraph
($\textbf{Crys}$tals with $\textbf{T}$ransformers $\textbf{o}$n
$\textbf{Graph}$s), a novel transformer-based geometric graph network designed
specifically for unconventional crystalline systems, and UnconvBench, a
comprehensive benchmark to evaluate models' predictive performance on
unconventional crystal materials such as defected crystals, low-dimension
crystals and MOF. CrysToGraph effectively captures short-range interactions
with transformer-based graph convolution blocks as well as long-range
interactions with graph-wise transformer blocks. CrysToGraph proofs its
effectiveness in modelling unconventional crystal materials in multiple tasks,
and moreover, it outperforms most existing methods, achieving new
state-of-the-art results on the benchmarks of both unconventional crystals and
traditional crystals.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph Convolutions Enrich the Self-Attention in Transformers! <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.04234v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.04234v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeongwhan Choi, Hyowon Wi, Jayoung Kim, Yehjin Shin, Kookjin Lee, Nathaniel Trask, Noseong Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers, renowned for their self-attention mechanism, have achieved
state-of-the-art performance across various tasks in natural language
processing, computer vision, time-series modeling, etc. However, one of the
challenges with deep Transformer models is the oversmoothing problem, where
representations across layers converge to indistinguishable values, leading to
significant performance degradation. We interpret the original self-attention
as a simple graph filter and redesign it from a graph signal processing (GSP)
perspective. We propose a graph-filter-based self-attention (GFSA) to learn a
general yet effective one, whose complexity, however, is slightly larger than
that of the original self-attention mechanism. We demonstrate that GFSA
improves the performance of Transformers in various fields, including computer
vision, natural language processing, graph-level tasks, speech recognition, and
code classification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024. Jeongwhan Choi and Hyowon Wi are co-first
  authors with equal contributions</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Breaking Determinism: Fuzzy Modeling of Sequential Recommendation Using
  Discrete State Space Diffusion Model <span class="chip">NeurIPS'2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23994v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23994v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenjia Xie, Hao Wang, Luankang Zhang, Rui Zhou, Defu Lian, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommendation (SR) aims to predict items that users may be
interested in based on their historical behavior sequences. We revisit SR from
a novel information-theoretic perspective and find that conventional sequential
modeling methods fail to adequately capture the randomness and unpredictability
of user behavior. Inspired by fuzzy information processing theory, this paper
introduces the DDSR model, which uses fuzzy sets of interaction sequences to
overcome the limitations and better capture the evolution of users' real
interests. Formally based on diffusion transition processes in discrete state
spaces, which is unlike common diffusion models such as DDPM that operate in
continuous domains. It is better suited for discrete data, using structured
transitions instead of arbitrary noise introduction to avoid information loss.
Additionally, to address the inefficiency of matrix transformations due to the
vast discrete space, we use semantic labels derived from quantization or RQ-VAE
to replace item IDs, enhancing efficiency and improving cold start issues.
Testing on three public benchmark datasets shows that DDSR outperforms existing
state-of-the-art methods in various settings, demonstrating its potential and
effectiveness in handling SR tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS'2024, 10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ $FastDoc$: Domain-Specific Fast Continual Pre-training Technique using
  Document-Level Metadata and Taxonomy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06190v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06190v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhilash Nandy, Manav Nitin Kapadnis, Sohan Patnaik, Yash Parag Butala, Pawan Goyal, Niloy Ganguly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose $FastDoc$ (Fast Continual Pre-training Technique
using Document Level Metadata and Taxonomy), a novel, compute-efficient
framework that utilizes Document metadata and Domain-Specific Taxonomy as
supervision signals to continually pre-train transformer encoder on a
domain-specific corpus. The main innovation is that during domain-specific
pretraining, an open-domain encoder is continually pre-trained using
sentence-level embeddings as inputs (to accommodate long documents), however,
fine-tuning is done with token-level embeddings as inputs to this encoder. We
perform such domain-specific pre-training on three different domains namely
customer support, scientific, and legal domains, and compare performance on 6
different downstream tasks and 9 different datasets. The novel use of
document-level supervision along with sentence-level embedding input for
pre-training reduces pre-training compute by around $1,000$, $4,500$, and $500$
times compared to MLM and/or NSP in Customer Support, Scientific, and Legal
Domains, respectively. The reduced training time does not lead to a
deterioration in performance. In fact we show that $FastDoc$ either outperforms
or performs on par with several competitive transformer-based baselines in
terms of character-level F1 scores and other automated metrics in the Customer
Support, Scientific, and Legal Domains. Moreover, reduced training aids in
mitigating the risk of catastrophic forgetting. Thus, unlike baselines,
$FastDoc$ shows a negligible drop in performance on open domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Transactions on Machine Learning Research (TMLR), 36
  pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adversarial Representation Engineering: A General Model Editing
  Framework for Large Language Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.13752v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.13752v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihao Zhang, Zeming Wei, Jun Sun, Meng Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since the rapid development of Large Language Models (LLMs) has achieved
remarkable success, understanding and rectifying their internal complex
mechanisms has become an urgent issue. Recent research has attempted to
interpret their behaviors through the lens of inner representation. However,
developing practical and efficient methods for applying these representations
for general and flexible model editing remains challenging. In this work, we
explore how to leverage insights from representation engineering to guide the
editing of LLMs by deploying a representation sensor as an editing oracle. We
first identify the importance of a robust and reliable sensor during editing,
then propose an Adversarial Representation Engineering (ARE) framework to
provide a unified and interpretable approach for conceptual model editing
without compromising baseline performance. Experiments on multiple tasks
demonstrate the effectiveness of ARE in various model editing scenarios. Our
code and data are available at
https://github.com/Zhang-Yihao/Adversarial-Representation-Engineering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Offline Reinforcement Learning with OOD State Correction and OOD Action
  Suppression <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.19400v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.19400v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixiu Mao, Qi Wang, Chen Chen, Yun Qu, Xiangyang Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In offline reinforcement learning (RL), addressing the out-of-distribution
(OOD) action issue has been a focus, but we argue that there exists an OOD
state issue that also impairs performance yet has been underexplored. Such an
issue describes the scenario when the agent encounters states out of the
offline dataset during the test phase, leading to uncontrolled behavior and
performance degradation. To this end, we propose SCAS, a simple yet effective
approach that unifies OOD state correction and OOD action suppression in
offline RL. Technically, SCAS achieves value-aware OOD state correction,
capable of correcting the agent from OOD states to high-value in-distribution
states. Theoretical and empirical results show that SCAS also exhibits the
effect of suppressing OOD actions. On standard offline RL benchmarks, SCAS
achieves excellent performance without additional hyperparameter tuning.
Moreover, benefiting from its OOD state correction feature, SCAS demonstrates
enhanced robustness against environmental perturbations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Are large language models superhuman chemists? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.01475v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.01475v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adrian Mirza, Nawaf Alampara, Sreekanth Kunchapu, Martiño Ríos-García, Benedict Emoekabu, Aswanth Krishnan, Tanya Gupta, Mara Schilling-Wilhelmi, Macjonathan Okereke, Anagha Aneesh, Amir Mohammad Elahi, Mehrdad Asgari, Juliane Eberhardt, Hani M. Elbeheiry, María Victoria Gil, Maximilian Greiner, Caroline T. Holick, Christina Glaubitz, Tim Hoffmann, Abdelrahman Ibrahim, Lea C. Klepsch, Yannik Köster, Fabian Alexander Kreth, Jakob Meyer, Santiago Miret, Jan Matthias Peschel, Michael Ringleb, Nicole Roesner, Johanna Schreiber, Ulrich S. Schubert, Leanne M. Stafast, Dinga Wonanke, Michael Pieler, Philippe Schwaller, Kevin Maik Jablonka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have gained widespread interest due to their
ability to process human language and perform tasks on which they have not been
explicitly trained.
  However, we possess only a limited systematic understanding of the chemical
capabilities of LLMs, which would be required to improve models and mitigate
potential harm. Here, we introduce "ChemBench," an automated framework for
evaluating the chemical knowledge and reasoning abilities of state-of-the-art
LLMs against the expertise of chemists.
  We curated more than 2,700 question-answer pairs, evaluated leading open- and
closed-source LLMs, and found that the best models outperformed the best human
chemists in our study on average. However, the models struggle with some basic
tasks and provide overconfident predictions.
  These findings reveal LLMs' impressive chemical capabilities while
emphasizing the need for further research to improve their safety and
usefulness. They also suggest adapting chemistry education and show the value
of benchmarking frameworks for evaluating LLMs in specific domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Long Term Memory: The Foundation of AI Self-Evolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15665v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15665v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xun Jiang, Feng Li, Han Zhao, Jiaying Wang, Jun Shao, Shihao Xu, Shu Zhang, Weiling Chen, Xavier Tang, Yize Chen, Mengyue Wu, Weizhi Ma, Mengdi Wang, Tianqiao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) like GPTs, trained on vast datasets, have
demonstrated impressive capabilities in language understanding, reasoning, and
planning, achieving human-level performance in various tasks. Most studies
focus on enhancing these models by training on ever-larger datasets to build
more powerful foundation models. While training stronger models is important,
enabling models to evolve during inference is equally crucial, a process we
refer to as AI self-evolution. Unlike large-scale training, self-evolution may
rely on limited data or interactions. Inspired by the columnar organization of
the human cerebral cortex, we hypothesize that AI models could develop
cognitive abilities and build internal representations through iterative
interactions with their environment. To achieve this, models need long-term
memory (LTM) to store and manage processed interaction data. LTM supports
self-evolution by representing diverse experiences across environments and
agents. In this report, we explore AI self-evolution and its potential to
enhance models during inference. We examine LTM's role in lifelong learning,
allowing models to evolve based on accumulated interactions. We outline the
structure of LTM and the systems needed for effective data retention and
representation. We also classify approaches for building personalized models
with LTM data and show how these models achieve self-evolution through
interaction. Using LTM, our multi-agent framework OMNE achieved first place on
the GAIA benchmark, demonstrating LTM's potential for AI self-evolution.
Finally, we present a roadmap for future research, emphasizing the importance
of LTM for advancing AI technology and its practical applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>56 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pretraining Codomain Attention Neural Operators for Solving Multiphysics
  PDEs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12553v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12553v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Ashiqur Rahman, Robert Joseph George, Mogab Elleithy, Daniel Leibovici, Zongyi Li, Boris Bonev, Colin White, Julius Berner, Raymond A. Yeh, Jean Kossaifi, Kamyar Azizzadenesheli, Anima Anandkumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing neural operator architectures face challenges when solving
multiphysics problems with coupled partial differential equations (PDEs) due to
complex geometries, interactions between physical variables, and the limited
amounts of high-resolution training data. To address these issues, we propose
Codomain Attention Neural Operator (CoDA-NO), which tokenizes functions along
the codomain or channel space, enabling self-supervised learning or pretraining
of multiple PDE systems. Specifically, we extend positional encoding,
self-attention, and normalization layers to function spaces. CoDA-NO can learn
representations of different PDE systems with a single model. We evaluate
CoDA-NO's potential as a backbone for learning multiphysics PDEs over multiple
systems by considering few-shot learning settings. On complex downstream tasks
with limited data, such as fluid flow simulations, fluid-structure
interactions, and Rayleigh-B\'enard convection, we found CoDA-NO to outperform
existing methods by over 36%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InfoRM: Mitigating Reward Hacking in RLHF via Information-Theoretic
  Reward Modeling <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09345v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09345v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchun Miao, Sen Zhang, Liang Ding, Rong Bao, Lefei Zhang, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the success of reinforcement learning from human feedback (RLHF) in
aligning language models with human values, reward hacking, also termed reward
overoptimization, remains a critical challenge. This issue primarily arises
from reward misgeneralization, where reward models (RMs) compute reward using
spurious features that are irrelevant to human preferences. In this work, we
tackle this problem from an information-theoretic perspective and propose a
framework for reward modeling, namely InfoRM, by introducing a variational
information bottleneck objective to filter out irrelevant information. Notably,
we further identify a correlation between overoptimization and outliers in the
IB latent space of InfoRM, establishing it as a promising tool for detecting
reward overoptimization. Inspired by this finding, we propose the Cluster
Separation Index (CSI), which quantifies deviations in the IB latent space, as
an indicator of reward overoptimization to facilitate the development of online
mitigation strategies. Extensive experiments on a wide range of settings and RM
scales (70M, 440M, 1.4B, and 7B) demonstrate the effectiveness of InfoRM.
Further analyses reveal that InfoRM's overoptimization detection mechanism is
not only effective but also robust across a broad range of datasets, signifying
a notable advancement in the field of RLHF. The code will be released upon
acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper has been accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast Samplers for Inverse Problems in Iterative Refinement Models <span class="chip">NeurIPS'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17673v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17673v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kushagra Pandey, Ruihan Yang, Stephan Mandt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Constructing fast samplers for unconditional diffusion and flow-matching
models has received much attention recently; however, existing methods for
solving inverse problems, such as super-resolution, inpainting, or deblurring,
still require hundreds to thousands of iterative steps to obtain high-quality
results. We propose a plug-and-play framework for constructing efficient
samplers for inverse problems, requiring only pre-trained diffusion or
flow-matching models. We present Conditional Conjugate Integrators, which
leverage the specific form of the inverse problem to project the respective
conditional diffusion/flow dynamics into a more amenable space for sampling.
Our method complements popular posterior approximation methods for solving
inverse problems using diffusion/flow models. We evaluate the proposed method's
performance on various linear image restoration tasks across multiple datasets,
employing diffusion and flow-matching models. Notably, on challenging inverse
problems like 4x super-resolution on the ImageNet dataset, our method can
generate high-quality samples in as few as 5 conditional sampling steps and
outperforms competing baselines requiring 20-1000 steps. Our code will be
publicly available at https://github.com/mandt-lab/c-pigdm
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>43 pages, NeurIPS'24 Camera Ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In-Context Transfer Learning: Demonstration Synthesis by Transferring
  Similar Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01548v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01548v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dingzirui Wang, Xuanliang Zhang, Qiguang Chen, Longxu Dou, Xiao Xu, Rongyu Cao, Yingwei Ma, Qingfu Zhu, Wanxiang Che, Binhua Li, Fei Huang, Yongbin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context learning (ICL) is an effective approach to help large language
models (LLMs) adapt to various tasks by providing demonstrations of the target
task. Considering the high cost of labeling demonstrations, many methods
propose synthesizing demonstrations from scratch using LLMs. However, the
quality of the demonstrations synthesized from scratch is limited by the
capabilities and knowledge of LLMs. To address this, inspired by transfer
learning, we propose In-Context Transfer Learning (ICTL), which synthesizes
target task demonstrations by transferring labeled demonstrations from similar
source tasks. ICTL consists of two steps: source sampling and target transfer.
First, we define an optimization objective, which minimizes transfer error to
sample source demonstrations similar to the target task. Then, we employ LLMs
to transfer the sampled source demonstrations to the target task, matching the
definition and format of the target task. Experiments on Super-NI show that
ICTL outperforms synthesis from scratch by 2.0% on average, demonstrating the
effectiveness of our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FoundTS: Comprehensive and Unified Benchmarking of Foundation Models for
  Time Series Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11802v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11802v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Li, Xiangfei Qiu, Peng Chen, Yihang Wang, Hanyin Cheng, Yang Shu, Jilin Hu, Chenjuan Guo, Aoying Zhou, Qingsong Wen, Christian S. Jensen, Bin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time Series Forecasting (TSF) is key functionality in numerous fields,
including in finance, weather services, and energy management. While TSF
methods are emerging these days, many of them require domain-specific data
collection and model training and struggle with poor generalization performance
on new domains. Foundation models aim to overcome this limitation. Pre-trained
on large-scale language or time series data, they exhibit promising inferencing
capabilities in new or unseen data. This has spurred a surge in new TSF
foundation models. We propose a new benchmark, FoundTS, to enable thorough and
fair evaluation and comparison of such models. FoundTS covers a variety of TSF
foundation models, including those based on large language models and those
pretrained on time series. Next, FoundTS supports different forecasting
strategies, including zero-shot, few-shot, and full-shot, thereby facilitating
more thorough evaluations. Finally, FoundTS offers a pipeline that standardizes
evaluation processes such as dataset splitting, loading, normalization, and
few-shot sampling, thereby facilitating fair evaluations. Building on this, we
report on an extensive evaluation of TSF foundation models on a broad range of
datasets from diverse domains and with different statistical characteristics.
Specifically, we identify pros and cons and inherent limitations of existing
foundation models, and we identify directions for future model design. We make
our code and datasets available at
https://anonymous.4open.science/r/FoundTS-C2B0.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Make Continual Learning Stronger via C-Flat 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.00986v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.00986v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ang Bian, Wei Li, Hangjie Yuan, Chengrong Yu, Mang Wang, Zixiang Zhao, Aojun Lu, Pengliang Ji, Tao Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model generalization ability upon incrementally acquiring dynamically
updating knowledge from sequentially arriving tasks is crucial to tackle the
sensitivity-stability dilemma in Continual Learning (CL). Weight loss landscape
sharpness minimization seeking for flat minima lying in neighborhoods with
uniform low loss or smooth gradient is proven to be a strong training regime
improving model generalization compared with loss minimization based optimizer
like SGD. Yet only a few works have discussed this training regime for CL,
proving that dedicated designed zeroth-order sharpness optimizer can improve CL
performance. In this work, we propose a Continual Flatness (C-Flat) method
featuring a flatter loss landscape tailored for CL. C-Flat could be easily
called with only one line of code and is plug-and-play to any CL methods. A
general framework of C-Flat applied to all CL categories and a thorough
comparison with loss minima optimizer and flat minima based CL approaches is
presented in this paper, showing that our method can boost CL performance in
almost all cases. Code is available at https://github.com/WanNaa/C-Flat.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ QGFN: Controllable Greediness with Action Values <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.05234v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.05234v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elaine Lau, Stephen Zhewen Lu, Ling Pan, Doina Precup, Emmanuel Bengio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Flow Networks (GFlowNets; GFNs) are a family of energy-based
generative methods for combinatorial objects, capable of generating diverse and
high-utility samples. However, consistently biasing GFNs towards producing
high-utility samples is non-trivial. In this work, we leverage connections
between GFNs and reinforcement learning (RL) and propose to combine the GFN
policy with an action-value estimate, $Q$, to create greedier sampling policies
which can be controlled by a mixing parameter. We show that several variants of
the proposed method, QGFN, are able to improve on the number of high-reward
samples generated in a variety of tasks without sacrificing diversity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 38th Conference on Neural Information Processing Systems
  (NeurIPS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Macroscopic Dynamics from Partial Microscopic Observations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23938v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23938v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengyi Chen, Qianxiao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Macroscopic observables of a system are of keen interest in real applications
such as the design of novel materials. Current methods rely on microscopic
trajectory simulations, where the forces on all microscopic coordinates need to
be computed or measured. However, this can be computationally prohibitive for
realistic systems. In this paper, we propose a method to learn macroscopic
dynamics requiring only force computations on a subset of the microscopic
coordinates. Our method relies on a sparsity assumption: the force on each
microscopic coordinate relies only on a small number of other coordinates. The
main idea of our approach is to map the training procedure on the macroscopic
coordinates back to the microscopic coordinates, on which partial force
computations can be used as stochastic estimation to update model parameters.
We provide a theoretical justification of this under suitable conditions. We
demonstrate the accuracy, force computation efficiency, and robustness of our
method on learning macroscopic closure models from a variety of microscopic
systems, including those modeled by partial differential equations or molecular
dynamics simulations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Equitable Federated Learning with Activation Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.19207v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.19207v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antesh Upadhyay, Abolfazl Hashemi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning is a prominent distributed learning paradigm that
incorporates collaboration among diverse clients, promotes data locality, and
thus ensures privacy. These clients have their own technological, cultural, and
other biases in the process of data generation. However, the present standard
often ignores this bias/heterogeneity, perpetuating bias against certain groups
rather than mitigating it. In response to this concern, we propose an equitable
clustering-based framework where the clients are categorized/clustered based on
how similar they are to each other. We propose a unique way to construct the
similarity matrix that uses activation vectors. Furthermore, we propose a
client weighing mechanism to ensure that each cluster receives equal importance
and establish $O(1/\sqrt{K})$ rate of convergence to reach an
$\epsilon-$stationary solution. We assess the effectiveness of our proposed
strategy against common baselines, demonstrating its efficacy in terms of
reducing the bias existing amongst various client clusters and consequently
ameliorating algorithmic bias against specific groups.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Understanding How Transformers Learn In-context Through a
  Representation Learning Lens 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.13220v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.13220v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruifeng Ren, Yong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained large language models based on Transformers have demonstrated
remarkable in-context learning (ICL) abilities. With just a few demonstration
examples, the models can implement new tasks without any parameter updates.
However, it is still an open question to understand the mechanism of ICL. In
this paper, we attempt to explore the ICL process in Transformers through a
lens of representation learning. Initially, leveraging kernel methods, we
figure out a dual model for one softmax attention layer. The ICL inference
process of the attention layer aligns with the training procedure of its dual
model, generating token representation predictions that are equivalent to the
dual model's test outputs. We delve into the training process of this dual
model from a representation learning standpoint and further derive a
generalization error bound related to the quantity of demonstration tokens.
Subsequently, we extend our theoretical conclusions to more complicated
scenarios, including one Transformer layer and multiple attention layers.
Furthermore, drawing inspiration from existing representation learning methods
especially contrastive learning, we propose potential modifications for the
attention layer. Finally, experiments are designed to support our findings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IntraMix: Intra-Class Mixup Generation for Accurate Labels and Neighbors <span class="chip">NeurIPS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.00957v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.00957v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shenghe Zheng, Hongzhi Wang, Xianglong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have shown great performance in various tasks,
with the core idea of learning from data labels and aggregating messages within
the neighborhood of nodes. However, the common challenges in graphs are
twofold: insufficient accurate (high-quality) labels and limited neighbors for
nodes, resulting in weak GNNs. Existing graph augmentation methods typically
address only one of these challenges, often adding training costs or relying on
oversimplified or knowledge-intensive strategies, limiting their
generalization. To simultaneously address both challenges faced by graphs in a
generalized way, we propose an elegant method called IntraMix. Considering the
incompatibility of vanilla Mixup with the complex topology of graphs, IntraMix
innovatively employs Mixup among inaccurate labeled data of the same class,
generating high-quality labeled data at minimal cost. Additionally, it finds
data with high confidence of being clustered into the same group as the
generated data to serve as their neighbors, thereby enriching the neighborhoods
of graphs. IntraMix efficiently tackles both issues faced by graphs and
challenges the prior notion of the limited effectiveness of Mixup in node
classification. IntraMix is a theoretically grounded plug-in-play method that
can be readily applied to all GNNs. Extensive experiments demonstrate the
effectiveness of IntraMix across various GNNs and datasets. Our code is
available at: https://github.com/Zhengsh123/IntraMix.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When Large Language Models Meet Vector Databases: A Survey 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.01763v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.01763v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhi Jing, Yongye Su, Yikun Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This survey explores the synergistic potential of Large Language Models
(LLMs) and Vector Databases (VecDBs), a burgeoning but rapidly evolving
research area. With the proliferation of LLMs comes a host of challenges,
including hallucinations, outdated knowledge, prohibitive commercial
application costs, and memory issues. VecDBs emerge as a compelling solution to
these issues by offering an efficient means to store, retrieve, and manage the
high-dimensional vector representations intrinsic to LLM operations. Through
this nuanced review, we delineate the foundational principles of LLMs and
VecDBs and critically analyze their integration's impact on enhancing LLM
functionalities. This discourse extends into a discussion on the speculative
future developments in this domain, aiming to catalyze further research into
optimizing the confluence of LLMs and VecDBs for advanced data handling and
knowledge extraction capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Network Matrix Product Operator: A Multi-Dimensionally Integrable
  Machine Learning Potential 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23858v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23858v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kentaro Hino, Yuki Kurashige
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A neural network-based machine learning potential energy surface (PES)
expressed in a matrix product operator (NN-MPO) is proposed. The MPO form
enables efficient evaluation of high-dimensional integrals that arise in
solving the time-dependent and time-independent Schr\"odinger equation and
effectively overcomes the so-called curse of dimensionality. This starkly
contrasts with other neural network-based machine learning PES methods, such as
multi-layer perceptrons (MLPs), where evaluating high-dimensional integrals is
not straightforward due to the fully connected topology in their backbone
architecture. Nevertheless, the NN-MPO retains the high representational
capacity of neural networks. NN-MPO can achieve spectroscopic accuracy with a
test mean absolute error (MAE) of 3.03 cm$^{-1}$ for a fully coupled
six-dimensional ab initio PES, using only 625 training points distributed
across a 0 to 17,000 cm$^{-1}$ energy range. Our Python implementation is
available at https://github.com/KenHino/Pompon.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models as Efficient Reward Function Searchers for
  Custom-Environment Multi-Objective Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02428v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02428v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanwen Xie, Jingzehua Xu, Yiyuan Yang, Yimian Ding, Shuai Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Achieving the effective design and improvement of reward functions in
reinforcement learning (RL) tasks with complex custom environments and multiple
requirements presents considerable challenges. In this paper, we propose ERFSL,
an efficient reward function searcher using LLMs, which enables LLMs to be
effective white-box searchers and highlights their advanced semantic
understanding capabilities. Specifically, we generate reward components for
each numerically explicit user requirement and employ a reward critic to
identify the correct code form. Then, LLMs assign weights to the reward
components to balance their values and iteratively adjust the weights without
ambiguity and redundant adjustments by flexibly adopting directional mutation
and crossover strategies, similar to genetic algorithms, based on the context
provided by the training log analyzer. We applied the framework to an
underwater data collection RL task without direct human feedback or reward
examples (zero-shot learning). The reward critic successfully corrects the
reward code with only one feedback instance for each requirement, effectively
preventing unrectifiable errors. The initialization of weights enables the
acquisition of different reward functions within the Pareto solution set
without the need for weight search. Even in cases where a weight is 500 times
off, on average, only 5.2 iterations are needed to meet user requirements. The
ERFSL also works well with most prompts utilizing GPT-4o mini, as we decompose
the weight searching process to reduce the requirement for numerical and
long-context understanding capabilities
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Model-agnostic clean-label backdoor mitigation in cybersecurity
  environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.08159v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.08159v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giorgio Severi, Simona Boboila, John Holodnak, Kendra Kratkiewicz, Rauf Izmailov, Michael J. De Lucia, Alina Oprea
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The training phase of machine learning models is a delicate step, especially
in cybersecurity contexts. Recent research has surfaced a series of insidious
training-time attacks that inject backdoors in models designed for security
classification tasks without altering the training labels. With this work, we
propose new techniques that leverage insights in cybersecurity threat models to
effectively mitigate these clean-label poisoning attacks, while preserving the
model utility. By performing density-based clustering on a carefully chosen
feature subspace, and progressively isolating the suspicious clusters through a
novel iterative scoring procedure, our defensive mechanism can mitigate the
attacks without requiring many of the common assumptions in the existing
backdoor defense literature. To show the generality of our proposed mitigation,
we evaluate it on two clean-label model-agnostic attacks on two different
classic cybersecurity data modalities: network flows classification and malware
classification, using gradient boosting and neural network models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Large Language Models for Suicide Detection on Social Media
  with Limited Labels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04501v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04501v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vy Nguyen, Chau Pham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing frequency of suicidal thoughts highlights the importance of
early detection and intervention. Social media platforms, where users often
share personal experiences and seek help, could be utilized to identify
individuals at risk. However, the large volume of daily posts makes manual
review impractical. This paper explores the use of Large Language Models (LLMs)
to automatically detect suicidal content in text-based social media posts. We
propose a novel method for generating pseudo-labels for unlabeled data by
prompting LLMs, along with traditional classification fine-tuning techniques to
enhance label accuracy. To create a strong suicide detection model, we develop
an ensemble approach involving prompting with Qwen2-72B-Instruct, and using
fine-tuned models such as Llama3-8B, Llama3.1-8B, and Gemma2-9B. We evaluate
our approach on the dataset of the Suicide Ideation Detection on Social Media
Challenge, a track of the IEEE Big Data 2024 Big Data Cup. Additionally, we
conduct a comprehensive analysis to assess the impact of different models and
fine-tuning strategies on detection performance. Experimental results show that
the ensemble model significantly improves the detection accuracy, by 5% points
compared with the individual models. It achieves a weight F1 score of 0.770 on
the public test set, and 0.731 on the private test set, providing a promising
solution for identifying suicidal content in social media. Our analysis shows
that the choice of LLMs affects the prompting performance, with larger models
providing better accuracy. Our code and checkpoints are publicly available at
https://github.com/khanhvynguyen/Suicide_Detection_LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IEEE International Conference on Big Data 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OSLO: One-Shot Label-Only Membership Inference Attacks <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.16978v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.16978v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuefeng Peng, Jaechul Roh, Subhransu Maji, Amir Houmansadr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce One-Shot Label-Only (OSLO) membership inference attacks (MIAs),
which accurately infer a given sample's membership in a target model's training
set with high precision using just \emph{a single query}, where the target
model only returns the predicted hard label. This is in contrast to
state-of-the-art label-only attacks which require $\sim6000$ queries, yet get
attack precisions lower than OSLO's. OSLO leverages transfer-based black-box
adversarial attacks. The core idea is that a member sample exhibits more
resistance to adversarial perturbations than a non-member. We compare OSLO
against state-of-the-art label-only attacks and demonstrate that, despite
requiring only one query, our method significantly outperforms previous attacks
in terms of precision and true positive rate (TPR) under the same false
positive rates (FPR). For example, compared to previous label-only MIAs, OSLO
achieves a TPR that is at least 7$\times$ higher under a 1\% FPR and at least
22$\times$ higher under a 0.1\% FPR on CIFAR100 for a ResNet18 model. We
evaluated multiple defense mechanisms against OSLO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Group Crosscoders for Mechanistic Analysis of Symmetry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24184v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24184v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liv Gorton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce group crosscoders, an extension of crosscoders that
systematically discover and analyse symmetrical features in neural networks.
While neural networks often develop equivariant representations without
explicit architectural constraints, understanding these emergent symmetries has
traditionally relied on manual analysis. Group crosscoders automate this
process by performing dictionary learning across transformed versions of inputs
under a symmetry group. Applied to InceptionV1's mixed3b layer using the
dihedral group $\mathrm{D}_{32}$, our method reveals several key insights:
First, it naturally clusters features into interpretable families that
correspond to previously hypothesised feature types, providing more precise
separation than standard sparse autoencoders. Second, our transform block
analysis enables the automatic characterisation of feature symmetries,
revealing how different geometric features (such as curves versus lines)
exhibit distinct patterns of invariance and equivariance. These results
demonstrate that group crosscoders can provide systematic insights into how
neural networks represent symmetry, offering a promising new tool for
mechanistic interpretability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Parameter-Efficient <span class="highlight-title">Fine-Tuning</span> in Large Models: A Survey of
  Methodologies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.19878v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.19878v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luping Wang, Sheng Chen, Linnan Jiang, Shu Pan, Runze Cai, Sen Yang, Fei Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The large models, as predicted by scaling raw forecasts, have made
groundbreaking progress in many fields, particularly in natural language
generation tasks, where they have approached or even surpassed human levels.
However, the unprecedented scale of their parameters brings significant
computational and storage costs. These large models require substantial
computational resources and GPU memory to operate. When adapting large models
to specific downstream tasks, their massive parameter scale poses a significant
challenge in fine-tuning on hardware platforms with limited computational power
and GPU memory. To address this issue, Parameter-Efficient Fine-Tuning (PEFT)
offers a practical solution by efficiently adjusting the parameters of large
pre-trained models to suit various downstream tasks. Specifically, PEFT adjusts
the parameters of pre-trained large models to adapt to specific tasks or
domains, minimizing the introduction of additional parameters and the
computational resources required. This review mainly introduces the preliminary
knowledge of PEFT, the core ideas and principles of various PEFT algorithms,
the applications of PEFT, and potential future research directions. By reading
this review, we believe that interested parties can quickly grasp the PEFT
methodology, thereby accelerating its development and innovation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adversarial Federated Consensus Learning for Surface Defect
  Classification Under Data Heterogeneity in IIoT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15711v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15711v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jixuan Cui, Jun Li, Zhen Mei, Yiyang Ni, Wen Chen, Zengxiang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The challenge of data scarcity hinders the application of deep learning in
industrial surface defect classification (SDC), as it's difficult to collect
and centralize sufficient training data from various entities in Industrial
Internet of Things (IIoT) due to privacy concerns. Federated learning (FL)
provides a solution by enabling collaborative global model training across
clients while maintaining privacy. However, performance may suffer due to data
heterogeneity-discrepancies in data distributions among clients. In this paper,
we propose a novel personalized FL (PFL) approach, named Adversarial Federated
Consensus Learning (AFedCL), for the challenge of data heterogeneity across
different clients in SDC. First, we develop a dynamic consensus construction
strategy to mitigate the performance degradation caused by data heterogeneity.
Through adversarial training, local models from different clients utilize the
global model as a bridge to achieve distribution alignment, alleviating the
problem of global knowledge forgetting. Complementing this strategy, we propose
a consensus-aware aggregation mechanism. It assigns aggregation weights to
different clients based on their efficacy in global knowledge learning, thereby
enhancing the global model's generalization capabilities. Finally, we design an
adaptive feature fusion module to further enhance global knowledge utilization
efficiency. Personalized fusion weights are gradually adjusted for each client
to optimally balance global and local features. Compared with state-of-the-art
FL methods like FedALA, the proposed AFedCL method achieves an accuracy
increase of up to 5.67% on three SDC datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Re-Label Method For Data-Centric Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.04391v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.04391v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tong Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In industry deep learning application, our manually labeled data has a
certain number of noisy data. To solve this problem and achieve more than 90
score in dev dataset, we present a simple method to find the noisy data and
re-label the noisy data by human, given the model predictions as references in
human labeling. In this paper, we illustrate our idea for a broad set of deep
learning tasks, includes classification, sequence tagging, object detection,
sequence generation, click-through rate prediction. The dev dataset evaluation
results and human evaluation results verify our idea.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LSEAttention is All You Need for Time Series Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23749v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23749v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dizhen Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based architectures have achieved remarkable success in natural
language processing and computer vision. However, their performance in
multivariate long-term forecasting often lags behind simpler linear baselines.
Previous studies have identified the traditional attention mechanism as a
significant factor contributing to this limitation. To unlock the full
potential of transformers for multivariate time series forecasting, I introduce
\textbf{LSEAttention}, an approach designed to address entropy collapse and
training instability commonly observed in transformer models. I validate the
effectiveness of LSEAttention across various real-world multivariate time
series datasets, demonstrating that it not only outperforms existing time
series transformer models but also exceeds the performance of some
state-of-the-art models on specific datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages with referencing, 1 figure, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Uni-Med: A Unified Medical Generalist Foundation Model For Multi-Task
  Learning Via Connector-MoE 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17508v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17508v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xun Zhu, Ying Hu, Fanbin Mo, Miao Li, Ji Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal large language models (MLLMs) have shown impressive capabilities
as a general-purpose interface for various visual and linguistic tasks.
However, building a unified MLLM for multi-task learning in the medical field
remains a thorny challenge. To mitigate the tug-of-war problem of multi-modal
multi-task optimization in MLLMs, recent advances primarily focus on improving
the LLM components, while neglecting the connector that bridges the gap between
modalities. In this paper, we introduce Uni-Med, a novel medical generalist
foundation model which consists of a universal visual feature extraction
module, a connector mixture-of-experts (CMoE) module, and an LLM. Benefiting
from the proposed CMoE that leverages a well-designed router with a mixture of
projection experts at the connector, Uni-Med achieves efficient solution to the
tug-of-war problem and can perform six different medical tasks including
question answering, visual question answering, report generation, referring
expression comprehension, referring expression generation and image
classification. To the best of our knowledge, Uni-Med is the first effort to
tackle multi-task interference at the connector in MLLMs. Extensive ablation
experiments validate the effectiveness of introducing CMoE under any
configuration, with up to an average 8% performance gains. We further provide
interpretation analysis of the tug-of-war problem from the perspective of
gradient optimization and parameter statistics. Compared to previous
state-of-the-art medical MLLMs, Uni-Med achieves competitive or superior
evaluation metrics on diverse tasks. Code and resources are available at
https://github.com/tsinghua-msiip/Uni-Med.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MoA: Mixture of Sparse Attention for Automatic Large Language Model
  Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14909v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14909v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyu Fu, Haofeng Huang, Xuefei Ning, Genghan Zhang, Boju Chen, Tianqi Wu, Hongyi Wang, Zixiao Huang, Shiyao Li, Shengen Yan, Guohao Dai, Huazhong Yang, Yu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse attention can effectively mitigate the significant memory and
throughput demands of Large Language Models (LLMs) in long contexts. Existing
methods typically employ a uniform sparse attention mask, applying the same
sparse pattern across different attention heads and input lengths. However,
this uniform approach fails to capture the diverse attention patterns inherent
in LLMs, ignoring their distinct accuracy-latency trade-offs. To address this
challenge, we propose the Mixture of Attention (MoA), which automatically
tailors distinct sparse attention configurations to different heads and layers.
MoA constructs and navigates a search space of various attention patterns and
their scaling rules relative to input sequence lengths. It profiles the model,
evaluates potential configurations, and pinpoints the optimal sparse attention
compression plan. MoA adapts to varying input sizes, revealing that some
attention heads expand their focus to accommodate longer sequences, while other
heads consistently concentrate on fixed-length local contexts. Experiments show
that MoA increases the effective context length by $3.9\times$ with the same
average attention span, boosting retrieval accuracy by $1.5-7.1\times$ over the
uniform-attention baseline across Vicuna-{7B,13B}, and Llama3-{8B,70B} models.
Moreover, MoA narrows the capability gaps between sparse and dense models,
reducing the maximum relative performance drop from $9\%-36\%$ to within $5\%$
across two long-context understanding benchmarks. MoA achieves a
$1.2-1.4\times$ GPU memory reduction, boosting decode throughput by
$6.6-8.2\times$ and $1.7-1.9\times$ compared to FlashAttention2 and vLLM, with
minimal impact on performance. Our code is available at
\url{https://github.com/thu-nics/MoA}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Detecting Brittle Decisions for Free: Leveraging Margin Consistency in
  Deep Robust Classifiers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18451v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18451v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonas Ngnawé, Sabyasachi Sahoo, Yann Pequignot, Frédéric Precioso, Christian Gagné
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite extensive research on adversarial training strategies to improve
robustness, the decisions of even the most robust deep learning models can
still be quite sensitive to imperceptible perturbations, creating serious risks
when deploying them for high-stakes real-world applications. While detecting
such cases may be critical, evaluating a model's vulnerability at a
per-instance level using adversarial attacks is computationally too intensive
and unsuitable for real-time deployment scenarios. The input space margin is
the exact score to detect non-robust samples and is intractable for deep neural
networks. This paper introduces the concept of margin consistency -- a property
that links the input space margins and the logit margins in robust models --
for efficient detection of vulnerable samples. First, we establish that margin
consistency is a necessary and sufficient condition to use a model's logit
margin as a score for identifying non-robust samples. Next, through
comprehensive empirical analysis of various robustly trained models on CIFAR10
and CIFAR100 datasets, we show that they indicate high margin consistency with
a strong correlation between their input space margins and the logit margins.
Then, we show that we can effectively and confidently use the logit margin to
detect brittle decisions with such models. Finally, we address cases where the
model is not sufficiently margin-consistent by learning a pseudo-margin from
the feature representation. Our findings highlight the potential of leveraging
deep representations to assess adversarial vulnerability in deployment
scenarios efficiently.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures, 2 tables. Version Update: Neurips Camera Ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimizing Monotone Chance-Constrained Submodular Functions Using
  Evolutionary Multi-Objective Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2006.11444v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2006.11444v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aneta Neumann, Frank Neumann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many real-world optimization problems can be stated in terms of submodular
functions. Furthermore, these real-world problems often involve uncertainties
which may lead to the violation of given constraints. A lot of evolutionary
multi-objective algorithms following the Pareto optimization approach have
recently been analyzed and applied to submodular problems with different types
of constraints. We present a first runtime analysis of evolutionary
multi-objective algorithms based on Pareto optimization for chance-constrained
submodular functions. Here the constraint involves stochastic components and
the constraint can only be violated with a small probability of alpha. We
investigate the classical GSEMO algorithm for two different bi-objective
formulations using tail bounds to determine the feasibility of solutions. We
show that the algorithm GSEMO obtains the same worst case performance
guarantees for monotone submodular functions as recently analyzed greedy
algorithms for the case of uniform IID weights and uniformly distributed
weights with the same dispersion when using the appropriate bi-objective
formulation. As part of our investigations, we also point out situations where
the use of tail bounds in the first bi-objective formulation can prevent GSEMO
from obtaining good solutions in the case of uniformly distributed weights with
the same dispersion if the objective function is submodular but non-monotone
due to a single element impacting monotonicity. Furthermore, we investigate the
behavior of the evolutionary multi-objective algorithms GSEMO, NSGA-II and
SPEA2 on different submodular chance-constrained network problems. Our
experimental results show that the use of evolutionary multi-objective
algorithms leads to significant performance improvements compared to
state-of-the-art greedy algorithms for submodular optimization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in the Evolutionary Computation Journal 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the SDEs and Scaling Rules for Adaptive Gradient Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.10287v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.10287v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sadhika Malladi, Kaifeng Lyu, Abhishek Panigrahi, Sanjeev Arora
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Approximating Stochastic Gradient Descent (SGD) as a Stochastic Differential
Equation (SDE) has allowed researchers to enjoy the benefits of studying a
continuous optimization trajectory while carefully preserving the stochasticity
of SGD. Analogous study of adaptive gradient methods, such as RMSprop and Adam,
has been challenging because there were no rigorously proven SDE approximations
for these methods. This paper derives the SDE approximations for RMSprop and
Adam, giving theoretical guarantees of their correctness as well as
experimental validation of their applicability to common large-scaling vision
and language settings. A key practical result is the derivation of a
$\textit{square root scaling rule}$ to adjust the optimization hyperparameters
of RMSprop and Adam when changing batch size, and its empirical validation in
deep learning settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>revised for correcting errors in some figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robustness of graph embedding methods for community detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.00636v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.00636v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhi-Feng Wei, Pablo Moriano, Ramakrishnan Kannan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study investigates the robustness of graph embedding methods for
community detection in the face of network perturbations, specifically edge
deletions. Graph embedding techniques, which represent nodes as low-dimensional
vectors, are widely used for various graph machine learning tasks due to their
ability to capture structural properties of networks effectively. However, the
impact of perturbations on the performance of these methods remains relatively
understudied. The research considers state-of-the-art graph embedding methods
from two families: matrix factorization (e.g., LE, LLE, HOPE, M-NMF) and random
walk-based (e.g., DeepWalk, LINE, node2vec). Through experiments conducted on
both synthetic and real-world networks, the study reveals varying degrees of
robustness within each family of graph embedding methods. The robustness is
found to be influenced by factors such as network size, initial community
partition strength, and the type of perturbation. Notably, node2vec and LLE
consistently demonstrate higher robustness for community detection across
different scenarios, including networks with degree and community size
heterogeneity. These findings highlight the importance of selecting an
appropriate graph embedding method based on the specific characteristics of the
network and the task at hand, particularly in scenarios where robustness to
perturbations is crucial.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 26 figures, 3 tables. Comments are welcome</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Discrete Dictionary-based Decomposition Layer for Structured
  Representation Learning <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.06976v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.06976v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taewon Park, Hyun-Chul Kim, Minho Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neuro-symbolic neural networks have been extensively studied to integrate
symbolic operations with neural networks, thereby improving systematic
generalization. Specifically, Tensor Product Representation (TPR) framework
enables neural networks to perform differentiable symbolic operations by
encoding the symbolic structure of data within vector spaces. However,
TPR-based neural networks often struggle to decompose unseen data into
structured TPR representations, undermining their symbolic operations. To
address this decomposition problem, we propose a Discrete Dictionary-based
Decomposition (D3) layer designed to enhance the decomposition capabilities of
TPR-based models. D3 employs discrete, learnable key-value dictionaries trained
to capture symbolic features essential for decomposition operations. It
leverages the prior knowledge acquired during training to generate structured
TPR representations by mapping input data to pre-learned symbolic features
within these dictionaries. D3 is a straightforward drop-in layer that can be
seamlessly integrated into any TPR-based model without modifications. Our
experimental results demonstrate that D3 significantly improves the systematic
generalization of various TPR-based models while requiring fewer additional
parameters. Notably, D3 outperforms baseline models on the synthetic task that
demands the systematic decomposition of unseen combinatorial data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Role of Attention Masks and LayerNorm in Transformers <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.18781v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.18781v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyi Wu, Amir Ajorlou, Yifei Wang, Stefanie Jegelka, Ali Jadbabaie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-attention is the key mechanism of transformers, which are the essential
building blocks of modern foundation models. Recent studies have shown that
pure self-attention suffers from an increasing degree of rank collapse as depth
increases, limiting model expressivity and further utilization of model depth.
The existing literature on rank collapse, however, has mostly overlooked other
critical components in transformers that may alleviate the rank collapse issue.
In this paper, we provide a general analysis of rank collapse under
self-attention, taking into account the effects of attention masks and layer
normalization (LayerNorm). In particular, we find that although pure masked
attention still suffers from exponential collapse to a rank one subspace,
sparse or local masked attention can provably slow down the collapse rate. In
the case of self-attention with LayerNorm, we first show that for certain
classes of value matrices, collapse to a rank one subspace still happens
exponentially. However, through construction of nontrivial counterexamples, we
then establish that with proper choice of value matrices, a general class of
sequences may not converge to a rank one subspace, and the self-attention
dynamics with LayerNorm can simultaneously possess a rich set of equilibria
with any possible rank between one and full. Our result refutes the previous
hypothesis that LayerNorm plays no role in the rank collapse of self-attention
and suggests that self-attention with LayerNorm constitutes a much more
expressive, versatile nonlinear dynamical system than what was originally
thought.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024. Fixed errors in v1 and added new remarks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Framework for Real-Time Volcano-Seismic Event Recognition Based on
  Multi-Station Seismograms and Semantic Segmentation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20595v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20595v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Camilo Espinosa-Curilem, Millaray Curilem, Daniel Basualto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In volcano monitoring, effective recognition of seismic events is essential
for understanding volcanic activity and raising timely warning alerts.
Traditional methods rely on manual analysis, which can be subjective and
labor-intensive. Furthermore, current automatic approaches often tackle
detection and classification separately, mostly rely on single station
information and generally require tailored preprocessing and representations to
perform predictions. These limitations often hinder their application to
real-time monitoring and utilization across different volcano conditions. This
study introduces a novel approach that utilizes Semantic Segmentation models to
automate seismic event recognition by applying a straight forward
transformation of multi-channel 1D signals into 2D representations, enabling
their use as images. Our framework employs a data-driven, end-to-end design
that integrates multi-station seismic data with minimal preprocessing,
performing both detection and classification simultaneously for five seismic
event classes. We evaluated four state-of-the-art segmentation models (UNet,
UNet++, DeepLabV3+ and SwinUNet) on approximately 25.000 seismic events
recorded at four different Chilean volcanoes: Nevados del Chill\'an Volcanic
Complex, Laguna del Maule, Villarrica and Puyehue-Cord\'on Caulle. Among these
models, the UNet architecture was identified as the most effective model,
achieving mean F1 and Intersection over Union (IoU) scores of up to 0.91 and
0.88, respectively, and demonstrating superior noise robustness and model
flexibility to unseen volcano datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 9 figures. This is a pre-print, it is currently under
  review for publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decision-Making Behavior Evaluation Framework for LLMs under Uncertain
  Context 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05972v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05972v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingru Jia, Zehua Yuan, Junhao Pan, Paul E. McNamara, Deming Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When making decisions under uncertainty, individuals often deviate from
rational behavior, which can be evaluated across three dimensions: risk
preference, probability weighting, and loss aversion. Given the widespread use
of large language models (LLMs) in decision-making processes, it is crucial to
assess whether their behavior aligns with human norms and ethical expectations
or exhibits potential biases. Several empirical studies have investigated the
rationality and social behavior performance of LLMs, yet their internal
decision-making tendencies and capabilities remain inadequately understood.
This paper proposes a framework, grounded in behavioral economics, to evaluate
the decision-making behaviors of LLMs. Through a multiple-choice-list
experiment, we estimate the degree of risk preference, probability weighting,
and loss aversion in a context-free setting for three commercial LLMs:
ChatGPT-4.0-Turbo, Claude-3-Opus, and Gemini-1.0-pro. Our results reveal that
LLMs generally exhibit patterns similar to humans, such as risk aversion and
loss aversion, with a tendency to overweight small probabilities. However,
there are significant variations in the degree to which these behaviors are
expressed across different LLMs. We also explore their behavior when embedded
with socio-demographic features, uncovering significant disparities. For
instance, when modeled with attributes of sexual minority groups or physical
disabilities, Claude-3-Opus displays increased risk aversion, leading to more
conservative choices. These findings underscore the need for careful
consideration of the ethical implications and potential biases in deploying
LLMs in decision-making scenarios. Therefore, this study advocates for
developing standards and guidelines to ensure that LLMs operate within ethical
boundaries while enhancing their utility in complex decision-making
environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Jingru Jia and Zehua Yuan have equal contribution</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LRM-Zero: Training Large Reconstruction Models with Synthesized Data <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09371v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09371v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Desai Xie, Sai Bi, Zhixin Shu, Kai Zhang, Zexiang Xu, Yi Zhou, Sören Pirk, Arie Kaufman, Xin Sun, Hao Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present LRM-Zero, a Large Reconstruction Model (LRM) trained entirely on
synthesized 3D data, achieving high-quality sparse-view 3D reconstruction. The
core of LRM-Zero is our procedural 3D dataset, Zeroverse, which is
automatically synthesized from simple primitive shapes with random texturing
and augmentations (e.g., height fields, boolean differences, and wireframes).
Unlike previous 3D datasets (e.g., Objaverse) which are often captured or
crafted by humans to approximate real 3D data, Zeroverse completely ignores
realistic global semantics but is rich in complex geometric and texture details
that are locally similar to or even more intricate than real objects. We
demonstrate that our LRM-Zero, trained with our fully synthesized Zeroverse,
can achieve high visual quality in the reconstruction of real-world objects,
competitive with models trained on Objaverse. We also analyze several critical
design choices of Zeroverse that contribute to LRM-Zero's capability and
training stability. Our work demonstrates that 3D reconstruction, one of the
core tasks in 3D vision, can potentially be addressed without the semantics of
real-world objects. The Zeroverse's procedural synthesis code and interactive
visualization are available at: https://desaixie.github.io/lrm-zero/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 8 figures. Our code and interactive visualization are
  available at: https://desaixie.github.io/lrm-zero/. v2: NeurIPS 2024 Camera
  Ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiffusionPDE: Generative PDE-Solving Under Partial Observation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17763v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17763v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahe Huang, Guandao Yang, Zichen Wang, Jeong Joon Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a general framework for solving partial differential equations
(PDEs) using generative diffusion models. In particular, we focus on the
scenarios where we do not have the full knowledge of the scene necessary to
apply classical solvers. Most existing forward or inverse PDE approaches
perform poorly when the observations on the data or the underlying coefficients
are incomplete, which is a common assumption for real-world measurements. In
this work, we propose DiffusionPDE that can simultaneously fill in the missing
information and solve a PDE by modeling the joint distribution of the solution
and coefficient spaces. We show that the learned generative priors lead to a
versatile framework for accurately solving a wide range of PDEs under partial
observation, significantly outperforming the state-of-the-art methods for both
forward and inverse directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024. Project page:
  https://jhhuangchloe.github.io/Diffusion-PDE/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Trace is the Next AutoDiff: Generative Optimization with Rich Feedback,
  Execution Traces, and LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16218v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16218v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ching-An Cheng, Allen Nie, Adith Swaminathan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study a class of optimization problems motivated by automating the design
and update of AI systems like coding assistants, robots, and copilots. AutoDiff
frameworks, like PyTorch, enable efficient end-to-end optimization of
differentiable systems. However, general computational workflows can be
non-differentiable and involve rich feedback (e.g. console output or user's
responses), heterogeneous parameters (e.g. prompts, codes), and intricate
objectives (beyond maximizing a score). We investigate end-to-end generative
optimization -- using generative models such as LLMs within the optimizer for
automatic updating of general computational workflows. We discover that
workflow execution traces are akin to back-propagated gradients in AutoDiff and
can provide key information to interpret feedback for efficient optimization.
Formally, we frame a new mathematical setup, Optimization with Trace Oracle
(OPTO). In OPTO, an optimizer receives an execution trace along with feedback
on the computed output and updates parameters iteratively. We provide a Python
library, Trace, that efficiently converts a workflow optimization problem into
an OPTO instance using PyTorch-like syntax. Using Trace, we develop a general
LLM-based generative optimizer called OptoPrime. In empirical studies, we find
that OptoPrime is capable of first-order numerical optimization, prompt
optimization, hyper-parameter tuning, robot controller design, code debugging,
etc., and is often competitive with specialized optimizers for each domain. We
envision Trace as an open research platform for devising novel generative
optimizers and developing the next generation of interactive learning agents.
Website: https://microsoft.github.io/Trace/.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Robust <span class="highlight-title">Multimodal</span> Sentiment Analysis with Incomplete Data <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.20012v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.20012v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Zhang, Wenbin Wang, Tianshu Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of Multimodal Sentiment Analysis (MSA) has recently witnessed an
emerging direction seeking to tackle the issue of data incompleteness.
Recognizing that the language modality typically contains dense sentiment
information, we consider it as the dominant modality and present an innovative
Language-dominated Noise-resistant Learning Network (LNLN) to achieve robust
MSA. The proposed LNLN features a dominant modality correction (DMC) module and
dominant modality based multimodal learning (DMML) module, which enhances the
model's robustness across various noise scenarios by ensuring the quality of
dominant modality representations. Aside from the methodical design, we perform
comprehensive experiments under random data missing scenarios, utilizing
diverse and meaningful settings on several popular datasets (\textit{e.g.,}
MOSI, MOSEI, and SIMS), providing additional uniformity, transparency, and
fairness compared to existing evaluations in the literature. Empirically, LNLN
consistently outperforms existing baselines, demonstrating superior performance
across these challenging and extensive evaluation metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2024-11-09T05:33:11.863359676Z">
            2024-11-09 05:33:11 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
